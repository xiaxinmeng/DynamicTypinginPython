commit 80444539849453d45fa2ecfcce7fbeb47d863897
Author: Zhuoran Liu <49047362+lzr-google@users.noreply.github.com>
Date:   Wed May 22 10:29:21 2019 -0700

    Add TPU SavedModel exporter  and refactor OD code (#6737)
    
    247226201  by ronnyvotel:
    
        Updating the visualization tools to accept unique_ids for color coding.
    
    --
    247067830  by Zhichao Lu:
    
        Add box_encodings_clip_range options for the convolutional box predictor (for TPU compatibility).
    
    --
    246888475  by Zhichao Lu:
    
        Remove unused _update_eval_steps function.
    
    --
    246163259  by lzc:
    
        Add a gather op that can handle ignore indices (which are "-1"s in this case).
    
    --
    246084944  by Zhichao Lu:
    
        Keras based implementation for SSD + MobilenetV2 + FPN.
    
    --
    245544227  by rathodv:
    
        Add batch_get_targets method to target assigner module to gather any groundtruth tensors based on the results of target assigner.
    
    --
    245540854  by rathodv:
    
        Update target assigner to return match tensor instead of a match object.
    
    --
    245434441  by Zhichao Lu:
    
        Add README for tpu_exporters package.
    
    --
    245381834  by lzc:
    
        Internal change.
    
    --
    245298983  by Zhichao Lu:
    
        Add conditional_shape_resizer to config_util
    
    --
    245134666  by Zhichao Lu:
    
        Adds ConditionalShapeResizer to the ImageResizer proto which enables resizing only if input image height or width is is greater or smaller than a certain size. Also enables specification of resize method in resize_to_{max, min}_dimension methods.
    
    --
    245093975  by Zhichao Lu:
    
        Exporting SavedModel for Object Detection TPU inference. (faster-rcnn)
    
    --
    245072421  by Zhichao Lu:
    
        Adds a new image resizing method "resize_to_max_dimension" which resizes images only if a dimension is greater than the maximum desired value while maintaining aspect ratio.
    
    --
    244946998  by lzc:
    
        Internal Changes.
    
    --
    244943693  by Zhichao Lu:
    
        Add a custom config to mobilenet v2 that makes it more detection friendly.
    
    --
    244754158  by derekjchow:
    
        Internal change.
    
    --
    244699875  by Zhichao Lu:
    
        Add check_range=False to box_list_ops.to_normalized_coordinates when training
        for instance segmentation.  This is consistent with other calls when training
        for object detection.  There could be wrongly annotated boxes in the dataset.
    
    --
    244507425  by rathodv:
    
        Support bfloat16 for ssd models.
    
    --
    244399982  by Zhichao Lu:
    
        Exporting SavedModel for Object Detection TPU inference. (ssd)
    
    --
    244209387  by Zhichao Lu:
    
        Internal change.
    
    --
    243922296  by rathodv:
    
        Change `raw_detection_scores` to contain softmax/sigmoid scores (not logits) for `raw_ detection_boxes`.
    
    --
    243883978  by Zhichao Lu:
    
        Add a sample fully conv config.
    
    --
    243369455  by Zhichao Lu:
    
        Fix regularization loss gap in Keras and Slim.
    
    --
    243292002  by lzc:
    
        Internal changes.
    
    --
    243097958  by Zhichao Lu:
    
        Exporting SavedModel for Object Detection TPU inference. (ssd model)
    
    --
    243007177  by Zhichao Lu:
    
        Exporting SavedModel for Object Detection TPU inference. (ssd model)
    
    --
    242776550  by Zhichao Lu:
    
        Make object detection pre-processing run on GPU.  tf.map_fn() uses
        TensorArrayV3 ops, which have no int32 GPU implementation.  Cast to int64,
        then cast back to int32.
    
    --
    242723128  by Zhichao Lu:
    
        Using sorted dictionaries for additional heads in non_max_suppression to ensure tensor order
    
    --
    242495311  by Zhichao Lu:
    
        Update documentation to reflect new TFLite examples repo location
    
    --
    242230527  by Zhichao Lu:
    
        Fix Dropout bugs for WeightSharedConvolutionalBoxPred.
    
    --
    242226573  by Zhichao Lu:
    
        Create Keras-based WeightSharedConvolutionalBoxPredictor.
    
    --
    241806074  by Zhichao Lu:
    
        Add inference in unit tests of TFX OD template.
    
    --
    241641498  by lzc:
    
        Internal change.
    
    --
    241637481  by Zhichao Lu:
    
        matmul_crop_and_resize(): Switch to dynamic shaping, so that not all dimensions are required to be known.
    
    --
    241429980  by Zhichao Lu:
    
        Internal change
    
    --
    241167237  by Zhichao Lu:
    
        Adds a faster_rcnn_inception_resnet_v2 Keras feature extractor, and updates the model builder to construct it.
    
    --
    241088616  by Zhichao Lu:
    
        Make it compatible with different dtype, e.g. float32, bfloat16, etc.
    
    --
    240897364  by lzc:
    
        Use image_np_expanded in object_detection_tutorial notebook.
    
    --
    240890393  by Zhichao Lu:
    
        Disable multicore inference for OD template as its not yet compatible.
    
    --
    240352168  by Zhichao Lu:
    
        Make SSDResnetV1FpnFeatureExtractor not protected to allow inheritance.
    
    --
    240351470  by lzc:
    
        Internal change.
    
    --
    239878928  by Zhichao Lu:
    
        Defines Keras box predictors for Faster RCNN and RFCN
    
    --
    239872103  by Zhichao Lu:
    
        Delete duplicated inputs in test.
    
    --
    239714273  by Zhichao Lu:
    
        Adding scope variable to all class heads
    
    --
    239698643  by Zhichao Lu:
    
        Create FPN feature extractor for object detection.
    
    --
    239696657  by Zhichao Lu:
    
        Internal Change.
    
    --
    239299404  by Zhichao Lu:
    
        Allows the faster rcnn meta-architecture to support Keras subcomponents
    
    --
    238502595  by Zhichao Lu:
    
        Lay the groundwork for symmetric quantization.
    
    --
    238496885  by Zhichao Lu:
    
        Add flexible_grid_anchor_generator
    
    --
    238138727  by lzc:
    
        Remove dead code.
    
        _USE_C_SHAPES has been forced True in TensorFlow releases since
        TensorFlow 1.9
        (https://github.com/tensorflow/tensorflow/commit/1d74a69443f741e69f9f52cb6bc2940b4d4ae3b7)
    
    --
    238123936  by rathodv:
    
        Add num_matched_groundtruth summary to target assigner in SSD.
    
    --
    238103345  by ronnyvotel:
    
        Raising error if input file pattern does not match any files.
        Also printing the number of evaluation images for coco metrics.
    
    --
    238044081  by Zhichao Lu:
    
        Fix docstring to state the correct dimensionality of `class_predictions_with_background`.
    
    --
    237920279  by Zhichao Lu:
    
        [XLA] Rework debug flags for dumping HLO.
    
        The following flags (usually passed via the XLA_FLAGS envvar) are removed:
    
          xla_dump_computations_to
          xla_dump_executions_to
          xla_dump_ir_to
          xla_dump_optimized_hlo_proto_to
          xla_dump_per_pass_hlo_proto_to
          xla_dump_unoptimized_hlo_proto_to
          xla_generate_hlo_graph
          xla_generate_hlo_text_to
          xla_hlo_dump_as_html
          xla_hlo_graph_path
          xla_log_hlo_text
    
        The following new flags are added:
    
          xla_dump_to
          xla_dump_hlo_module_re
          xla_dump_hlo_pass_re
          xla_dump_hlo_as_text
          xla_dump_hlo_as_proto
          xla_dump_hlo_as_dot
          xla_dump_hlo_as_url
          xla_dump_hlo_as_html
          xla_dump_ir
          xla_dump_hlo_snapshots
    
        The default is not to dump anything at all, but as soon as some dumping flag is
        specified, we enable the following defaults (most of which can be overridden).
    
         * dump to stdout (overridden by --xla_dump_to)
         * dump HLO modules at the very beginning and end of the optimization pipeline
         * don't dump between any HLO passes (overridden by --xla_dump_hlo_pass_re)
         * dump all HLO modules (overridden by --xla_dump_hlo_module_re)
         * dump in textual format (overridden by
           --xla_dump_hlo_as_{text,proto,dot,url,html}).
    
        For example, to dump optimized and unoptimized HLO text and protos to /tmp/foo,
        pass
    
          --xla_dump_to=/tmp/foo --xla_dump_hlo_as_text --xla_dump_hlo_as_proto
    
        For details on these flags' meanings, see xla.proto.
    
        The intent of this change is to make dumping both simpler to use and more
        powerful.
    
        For example:
    
         * Previously there was no way to dump the HLO module during the pass pipeline
           in HLO text format; the only option was --dump_per_pass_hlo_proto_to, which
           dumped in proto format.
    
           Now this is --xla_dump_pass_re=.* --xla_dump_hlo_as_text.  (In fact, the
           second flag is not necessary in this case, as dumping as text is the
           default.)
    
         * Previously there was no way to dump HLO as a graph before and after
           compilation; the only option was --xla_generate_hlo_graph, which would dump
           before/after every pass.
    
           Now this is --xla_dump_hlo_as_{dot,url,html} (depending on what format you
           want the graph in).
    
         * Previously, there was no coordination between the filenames written by the
           various flags, so info about one module might be dumped with various
           filename prefixes.  Now the filenames are consistent and all dumps from a
           particular module are next to each other.
    
        If you only specify some of these flags, we try to figure out what you wanted.
        For example:
    
         * --xla_dump_to implies --xla_dump_hlo_as_text unless you specify some
           other --xla_dump_as_* flag.
    
         * --xla_dump_hlo_as_text or --xla_dump_ir implies dumping to stdout unless you
           specify a different --xla_dump_to directory.  You can explicitly dump to
           stdout with --xla_dump_to=-.
    
        As part of this change, I simplified the debugging code in the HLO passes for
        dumping HLO modules.  Previously, many tests explicitly VLOG'ed the HLO module
        before, after, and sometimes during the pass.  I removed these VLOGs.  If you
        want dumps before/during/after an HLO pass, use --xla_dump_pass_re=<pass_name>.
    
    --
    237510043  by lzc:
    
        Internal Change.
    
    --
    237469515  by Zhichao Lu:
    
        Parameterize model_builder.build in inputs.py.
    
    --
    237293511  by rathodv:
    
        Remove multiclass_scores from tensor_dict in transform_data_fn always.
    
    --
    237260333  by ronnyvotel:
    
        Updating faster_rcnn_meta_arch to define prediction dictionary fields that are batched.
    
    --
    
    PiperOrigin-RevId: 247226201

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index b2bcf5db..9313d32d 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -65,6 +65,8 @@ Extras:
   * <a href='g3doc/detection_model_zoo.md'>Tensorflow detection model zoo</a><br>
   * <a href='g3doc/exporting_models.md'>
       Exporting a trained model for inference</a><br>
+  * <a href='g3doc/tpu_exporters.md'>
+      Exporting a trained model for TPU inference</a><br>
   * <a href='g3doc/defining_your_own_model.md'>
       Defining your own model architecture</a><br>
   * <a href='g3doc/using_your_own_dataset.md'>
diff --git a/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py b/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
new file mode 100644
index 00000000..01153b1b
--- /dev/null
+++ b/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
@@ -0,0 +1,134 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Generates grid anchors on the fly corresponding to multiple CNN layers."""
+
+import tensorflow as tf
+
+from object_detection.anchor_generators import grid_anchor_generator
+from object_detection.core import anchor_generator
+from object_detection.core import box_list_ops
+
+
+class FlexibleGridAnchorGenerator(anchor_generator.AnchorGenerator):
+  """Generate a grid of anchors for multiple CNN layers of different scale."""
+
+  def __init__(self, base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+               normalize_coordinates=True):
+    """Constructs a FlexibleGridAnchorGenerator.
+
+    This generator is more flexible than the multiple_grid_anchor_generator
+    and multiscale_grid_anchor_generator, and can generate any of the anchors
+    that they can generate, plus additional anchor configurations. In
+    particular, it allows the explicit specification of scale and aspect ratios
+    at each layer without making any assumptions between the relationship
+    between scales and aspect ratios between layers.
+
+    Args:
+      base_sizes: list of tuples of anchor base sizes. For example, setting
+        base_sizes=[(1, 2, 3), (4, 5)] means that we want 3 anchors at each
+        grid point on the first layer with the base sizes of 1, 2, and 3, and 2
+        anchors at each grid point on the second layer with the base sizes of
+        4 and 5.
+      aspect_ratios: list or tuple of aspect ratios. For example, setting
+        aspect_ratios=[(1.0, 2.0, 0.5), (1.0, 2.0)] means that we want 3 anchors
+        at each grid point on the first layer with aspect ratios of 1.0, 2.0,
+        and 0.5, and 2 anchors at each grid point on the sercond layer with the
+        base sizes of 1.0 and 2.0.
+      anchor_strides: list of pairs of strides in pixels (in y and x directions
+        respectively). For example, setting anchor_strides=[(25, 25), (50, 50)]
+        means that we want the anchors corresponding to the first layer to be
+        strided by 25 pixels and those in the second layer to be strided by 50
+        pixels in both y and x directions.
+      anchor_offsets: list of pairs of offsets in pixels (in y and x directions
+        respectively). The offset specifies where we want the center of the
+        (0, 0)-th anchor to lie for each layer. For example, setting
+        anchor_offsets=[(10, 10), (20, 20)]) means that we want the
+        (0, 0)-th anchor of the first layer to lie at (10, 10) in pixel space
+        and likewise that we want the (0, 0)-th anchor of the second layer to
+        lie at (25, 25) in pixel space.
+      normalize_coordinates: whether to produce anchors in normalized
+        coordinates. (defaults to True).
+    """
+    self._base_sizes = base_sizes
+    self._aspect_ratios = aspect_ratios
+    self._anchor_strides = anchor_strides
+    self._anchor_offsets = anchor_offsets
+    self._normalize_coordinates = normalize_coordinates
+
+  def name_scope(self):
+    return 'FlexibleGridAnchorGenerator'
+
+  def num_anchors_per_location(self):
+    """Returns the number of anchors per spatial location.
+
+    Returns:
+      a list of integers, one for each expected feature map to be passed to
+      the Generate function.
+    """
+    return [len(size) for size in self._base_sizes]
+
+  def _generate(self, feature_map_shape_list, im_height=1, im_width=1):
+    """Generates a collection of bounding boxes to be used as anchors.
+
+    Currently we require the input image shape to be statically defined.  That
+    is, im_height and im_width should be integers rather than tensors.
+
+    Args:
+      feature_map_shape_list: list of pairs of convnet layer resolutions in the
+        format [(height_0, width_0), (height_1, width_1), ...]. For example,
+        setting feature_map_shape_list=[(8, 8), (7, 7)] asks for anchors that
+        correspond to an 8x8 layer followed by a 7x7 layer.
+      im_height: the height of the image to generate the grid for. If both
+        im_height and im_width are 1, anchors can only be generated in
+        absolute coordinates.
+      im_width: the width of the image to generate the grid for. If both
+        im_height and im_width are 1, anchors can only be generated in
+        absolute coordinates.
+
+    Returns:
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
+        the input feature map shapes.
+    Raises:
+      ValueError: if im_height and im_width are 1, but normalized coordinates
+        were requested.
+    """
+    anchor_grid_list = []
+    for (feat_shape, base_sizes, aspect_ratios, anchor_stride, anchor_offset
+        ) in zip(feature_map_shape_list, self._base_sizes, self._aspect_ratios,
+                 self._anchor_strides, self._anchor_offsets):
+      anchor_grid = grid_anchor_generator.tile_anchors(
+          feat_shape[0],
+          feat_shape[1],
+          tf.to_float(tf.convert_to_tensor(base_sizes)),
+          tf.to_float(tf.convert_to_tensor(aspect_ratios)),
+          tf.constant([1.0, 1.0]),
+          tf.to_float(tf.convert_to_tensor(anchor_stride)),
+          tf.to_float(tf.convert_to_tensor(anchor_offset)))
+      num_anchors = anchor_grid.num_boxes_static()
+      if num_anchors is None:
+        num_anchors = anchor_grid.num_boxes()
+      anchor_indices = tf.zeros([num_anchors])
+      anchor_grid.add_field('feature_map_index', anchor_indices)
+      if self._normalize_coordinates:
+        if im_height == 1 or im_width == 1:
+          raise ValueError(
+              'Normalized coordinates were requested upon construction of the '
+              'FlexibleGridAnchorGenerator, but a subsequent call to '
+              'generate did not supply dimension information.')
+        anchor_grid = box_list_ops.to_normalized_coordinates(
+            anchor_grid, im_height, im_width, check_range=False)
+      anchor_grid_list.append(anchor_grid)
+
+    return anchor_grid_list
diff --git a/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
new file mode 100644
index 00000000..2f09771b
--- /dev/null
+++ b/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
@@ -0,0 +1,294 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for anchor_generators.flexible_grid_anchor_generator_test.py."""
+import numpy as np
+import tensorflow as tf
+
+from object_detection.anchor_generators import flexible_grid_anchor_generator as fg
+from object_detection.utils import test_case
+
+
+class FlexibleGridAnchorGeneratorTest(test_case.TestCase):
+
+  def test_construct_single_anchor(self):
+    anchor_strides = [(32, 32),]
+    anchor_offsets = [(16, 16),]
+    base_sizes = [(128.0,)]
+    aspect_ratios = [(1.0,)]
+    im_height = 64
+    im_width = 64
+    feature_map_shape_list = [(2, 2)]
+    exp_anchor_corners = [[-48, -48, 80, 80],
+                          [-48, -16, 80, 112],
+                          [-16, -48, 112, 80],
+                          [-16, -16, 112, 112]]
+    anchor_generator = fg.FlexibleGridAnchorGenerator(
+        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+        normalize_coordinates=False)
+    anchors_list = anchor_generator.generate(
+        feature_map_shape_list, im_height=im_height, im_width=im_width)
+    anchor_corners = anchors_list[0].get()
+
+    with self.test_session():
+      anchor_corners_out = anchor_corners.eval()
+      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_unit_dimensions(self):
+    anchor_strides = [(32, 32),]
+    anchor_offsets = [(16, 16),]
+    base_sizes = [(32.0,)]
+    aspect_ratios = [(1.0,)]
+    im_height = 1
+    im_width = 1
+    feature_map_shape_list = [(2, 2)]
+    # Positive offsets are produced.
+    exp_anchor_corners = [[0, 0, 32, 32],
+                          [0, 32, 32, 64],
+                          [32, 0, 64, 32],
+                          [32, 32, 64, 64]]
+
+    anchor_generator = fg.FlexibleGridAnchorGenerator(
+        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+        normalize_coordinates=False)
+    anchors_list = anchor_generator.generate(
+        feature_map_shape_list, im_height=im_height, im_width=im_width)
+    anchor_corners = anchors_list[0].get()
+
+    with self.test_session():
+      anchor_corners_out = anchor_corners.eval()
+      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_normalized_anchors_fails_with_unit_dimensions(self):
+    anchor_generator = fg.FlexibleGridAnchorGenerator(
+        [(32.0,)], [(1.0,)], [(32, 32),], [(16, 16),],
+        normalize_coordinates=True)
+    with self.assertRaisesRegexp(ValueError, 'Normalized coordinates'):
+      anchor_generator.generate(
+          feature_map_shape_list=[(2, 2)], im_height=1, im_width=1)
+
+  def test_construct_single_anchor_in_normalized_coordinates(self):
+    anchor_strides = [(32, 32),]
+    anchor_offsets = [(16, 16),]
+    base_sizes = [(128.0,)]
+    aspect_ratios = [(1.0,)]
+    im_height = 64
+    im_width = 128
+    feature_map_shape_list = [(2, 2)]
+    exp_anchor_corners = [[-48./64, -48./128, 80./64, 80./128],
+                          [-48./64, -16./128, 80./64, 112./128],
+                          [-16./64, -48./128, 112./64, 80./128],
+                          [-16./64, -16./128, 112./64, 112./128]]
+    anchor_generator = fg.FlexibleGridAnchorGenerator(
+        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+        normalize_coordinates=True)
+    anchors_list = anchor_generator.generate(
+        feature_map_shape_list, im_height=im_height, im_width=im_width)
+    anchor_corners = anchors_list[0].get()
+
+    with self.test_session():
+      anchor_corners_out = anchor_corners.eval()
+      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_num_anchors_per_location(self):
+    anchor_strides = [(32, 32), (64, 64)]
+    anchor_offsets = [(16, 16), (32, 32)]
+    base_sizes = [(32.0, 64.0, 96.0, 32.0, 64.0, 96.0),
+                  (64.0, 128.0, 172.0, 64.0, 128.0, 172.0)]
+    aspect_ratios = [(1.0, 1.0, 1.0, 2.0, 2.0, 2.0),
+                     (1.0, 1.0, 1.0, 2.0, 2.0, 2.0)]
+    anchor_generator = fg.FlexibleGridAnchorGenerator(
+        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+        normalize_coordinates=False)
+    self.assertEqual(anchor_generator.num_anchors_per_location(), [6, 6])
+
+  def test_construct_single_anchor_dynamic_size(self):
+    anchor_strides = [(32, 32),]
+    anchor_offsets = [(0, 0),]
+    base_sizes = [(128.0,)]
+    aspect_ratios = [(1.0,)]
+    im_height = tf.constant(64)
+    im_width = tf.constant(64)
+    feature_map_shape_list = [(2, 2)]
+    # Zero offsets are used.
+    exp_anchor_corners = [[-64, -64, 64, 64],
+                          [-64, -32, 64, 96],
+                          [-32, -64, 96, 64],
+                          [-32, -32, 96, 96]]
+
+    anchor_generator = fg.FlexibleGridAnchorGenerator(
+        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+        normalize_coordinates=False)
+    anchors_list = anchor_generator.generate(
+        feature_map_shape_list, im_height=im_height, im_width=im_width)
+    anchor_corners = anchors_list[0].get()
+
+    with self.test_session():
+      anchor_corners_out = anchor_corners.eval()
+      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_with_odd_input_dimension(self):
+
+    def graph_fn():
+      anchor_strides = [(32, 32),]
+      anchor_offsets = [(0, 0),]
+      base_sizes = [(128.0,)]
+      aspect_ratios = [(1.0,)]
+      im_height = 65
+      im_width = 65
+      feature_map_shape_list = [(3, 3)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return (anchor_corners,)
+    anchor_corners_out = self.execute(graph_fn, [])
+    exp_anchor_corners = [[-64, -64, 64, 64],
+                          [-64, -32, 64, 96],
+                          [-64, 0, 64, 128],
+                          [-32, -64, 96, 64],
+                          [-32, -32, 96, 96],
+                          [-32, 0, 96, 128],
+                          [0, -64, 128, 64],
+                          [0, -32, 128, 96],
+                          [0, 0, 128, 128]]
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_on_two_feature_maps(self):
+
+    def graph_fn():
+      anchor_strides = [(32, 32), (64, 64)]
+      anchor_offsets = [(16, 16), (32, 32)]
+      base_sizes = [(128.0,), (256.0,)]
+      aspect_ratios = [(1.0,), (1.0,)]
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(2, 2), (1, 1)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
+      return anchor_corners
+
+    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)
+    exp_anchor_corners = [[-48, -48, 80, 80],
+                          [-48, -16, 80, 112],
+                          [-16, -48, 112, 80],
+                          [-16, -16, 112, 112],
+                          [-96, -96, 160, 160]]
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_with_two_scales_per_octave(self):
+
+    def graph_fn():
+      anchor_strides = [(64, 64),]
+      anchor_offsets = [(32, 32),]
+      base_sizes = [(256.0, 362.03867)]
+      aspect_ratios = [(1.0, 1.0)]
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(1, 1)]
+
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
+      return anchor_corners
+    # There are 4 set of anchors in this configuration. The order is:
+    # [[2**0.0 intermediate scale + 1.0 aspect],
+    #  [2**0.5 intermediate scale + 1.0 aspect]]
+    exp_anchor_corners = [[-96., -96., 160., 160.],
+                          [-149.0193, -149.0193, 213.0193, 213.0193]]
+
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_with_two_scales_per_octave_and_aspect(self):
+    def graph_fn():
+      anchor_strides = [(64, 64),]
+      anchor_offsets = [(32, 32),]
+      base_sizes = [(256.0, 362.03867, 256.0, 362.03867)]
+      aspect_ratios = [(1.0, 1.0, 2.0, 2.0)]
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(1, 1)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
+      return anchor_corners
+    # There are 4 set of anchors in this configuration. The order is:
+    # [[2**0.0 intermediate scale + 1.0 aspect],
+    #  [2**0.5 intermediate scale + 1.0 aspect],
+    #  [2**0.0 intermediate scale + 2.0 aspect],
+    #  [2**0.5 intermediate scale + 2.0 aspect]]
+
+    exp_anchor_corners = [[-96., -96., 160., 160.],
+                          [-149.0193, -149.0193, 213.0193, 213.0193],
+                          [-58.50967, -149.0193, 122.50967, 213.0193],
+                          [-96., -224., 160., 288.]]
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchors_on_feature_maps_with_dynamic_shape(self):
+
+    def graph_fn(feature_map1_height, feature_map1_width, feature_map2_height,
+                 feature_map2_width):
+      anchor_strides = [(32, 32), (64, 64)]
+      anchor_offsets = [(16, 16), (32, 32)]
+      base_sizes = [(128.0,), (256.0,)]
+      aspect_ratios = [(1.0,), (1.0,)]
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(feature_map1_height, feature_map1_width),
+                                (feature_map2_height, feature_map2_width)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
+      return anchor_corners
+
+    anchor_corners_out = np.concatenate(
+        self.execute_cpu(graph_fn, [
+            np.array(2, dtype=np.int32),
+            np.array(2, dtype=np.int32),
+            np.array(1, dtype=np.int32),
+            np.array(1, dtype=np.int32)
+        ]),
+        axis=0)
+    exp_anchor_corners = [[-48, -48, 80, 80],
+                          [-48, -16, 80, 112],
+                          [-16, -48, 112, 80],
+                          [-16, -16, 112, 112],
+                          [-96, -96, 160, 160]]
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/builders/anchor_generator_builder.py b/research/object_detection/builders/anchor_generator_builder.py
index 54cec3a1..81219a1b 100644
--- a/research/object_detection/builders/anchor_generator_builder.py
+++ b/research/object_detection/builders/anchor_generator_builder.py
@@ -15,6 +15,7 @@
 
 """A function to build an object detection anchor generator from config."""
 
+from object_detection.anchor_generators import flexible_grid_anchor_generator
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.anchor_generators import multiple_grid_anchor_generator
 from object_detection.anchor_generators import multiscale_grid_anchor_generator
@@ -90,5 +91,19 @@ def build(anchor_generator_config):
         cfg.scales_per_octave,
         cfg.normalize_coordinates
     )
+  elif anchor_generator_config.WhichOneof(
+      'anchor_generator_oneof') == 'flexible_grid_anchor_generator':
+    cfg = anchor_generator_config.flexible_grid_anchor_generator
+    base_sizes = []
+    aspect_ratios = []
+    strides = []
+    offsets = []
+    for anchor_grid in cfg.anchor_grid:
+      base_sizes.append(tuple(anchor_grid.base_sizes))
+      aspect_ratios.append(tuple(anchor_grid.aspect_ratios))
+      strides.append((anchor_grid.height_stride, anchor_grid.width_stride))
+      offsets.append((anchor_grid.height_offset, anchor_grid.width_offset))
+    return flexible_grid_anchor_generator.FlexibleGridAnchorGenerator(
+        base_sizes, aspect_ratios, strides, offsets, cfg.normalize_coordinates)
   else:
     raise ValueError('Empty anchor generator.')
diff --git a/research/object_detection/builders/anchor_generator_builder_test.py b/research/object_detection/builders/anchor_generator_builder_test.py
index abecb0f5..4b2cf5df 100644
--- a/research/object_detection/builders/anchor_generator_builder_test.py
+++ b/research/object_detection/builders/anchor_generator_builder_test.py
@@ -20,6 +20,7 @@ import math
 import tensorflow as tf
 
 from google.protobuf import text_format
+from object_detection.anchor_generators import flexible_grid_anchor_generator
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.anchor_generators import multiple_grid_anchor_generator
 from object_detection.anchor_generators import multiscale_grid_anchor_generator
@@ -43,8 +44,8 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               grid_anchor_generator.GridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          grid_anchor_generator.GridAnchorGenerator)
     self.assertListEqual(anchor_generator_object._scales, [])
     self.assertListEqual(anchor_generator_object._aspect_ratios, [])
     self.assertAllEqual(anchor_generator_object._anchor_offset, [0, 0])
@@ -68,8 +69,8 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               grid_anchor_generator.GridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          grid_anchor_generator.GridAnchorGenerator)
     self.assert_almost_list_equal(anchor_generator_object._scales,
                                   [0.4, 2.2])
     self.assert_almost_list_equal(anchor_generator_object._aspect_ratios,
@@ -88,9 +89,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               multiple_grid_anchor_generator.
-                               MultipleGridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          multiple_grid_anchor_generator.
+                          MultipleGridAnchorGenerator)
     for actual_scales, expected_scales in zip(
         list(anchor_generator_object._scales),
         [(0.1, 0.2, 0.2),
@@ -118,9 +119,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               multiple_grid_anchor_generator.
-                               MultipleGridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          multiple_grid_anchor_generator.
+                          MultipleGridAnchorGenerator)
     for actual_scales, expected_scales in zip(
         list(anchor_generator_object._scales),
         [(0.1, math.sqrt(0.1 * 0.15)),
@@ -143,9 +144,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               multiple_grid_anchor_generator.
-                               MultipleGridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          multiple_grid_anchor_generator.
+                          MultipleGridAnchorGenerator)
     for actual_aspect_ratio, expected_aspect_ratio in zip(
         list(anchor_generator_object._aspect_ratios),
         6 * [(0.5, 0.5)]):
@@ -162,9 +163,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               multiple_grid_anchor_generator.
-                               MultipleGridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          multiple_grid_anchor_generator.
+                          MultipleGridAnchorGenerator)
 
     for actual_scales, expected_scales in zip(
         list(anchor_generator_object._scales),
@@ -204,9 +205,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               multiple_grid_anchor_generator.
-                               MultipleGridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          multiple_grid_anchor_generator.
+                          MultipleGridAnchorGenerator)
 
     for actual_scales, expected_scales in zip(
         list(anchor_generator_object._scales),
@@ -246,9 +247,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               multiscale_grid_anchor_generator.
-                               MultiscaleGridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          multiscale_grid_anchor_generator.
+                          MultiscaleGridAnchorGenerator)
     for level, anchor_grid_info in zip(
         range(3, 8), anchor_generator_object._anchor_grid_info):
       self.assertEqual(set(anchor_grid_info.keys()), set(['level', 'info']))
@@ -273,11 +274,59 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
     anchor_generator_object = anchor_generator_builder.build(
         anchor_generator_proto)
-    self.assertTrue(isinstance(anchor_generator_object,
-                               multiscale_grid_anchor_generator.
-                               MultiscaleGridAnchorGenerator))
+    self.assertIsInstance(anchor_generator_object,
+                          multiscale_grid_anchor_generator.
+                          MultiscaleGridAnchorGenerator)
     self.assertFalse(anchor_generator_object._normalize_coordinates)
 
+  def test_build_flexible_anchor_generator(self):
+    anchor_generator_text_proto = """
+      flexible_grid_anchor_generator {
+        anchor_grid {
+          base_sizes: [1.5]
+          aspect_ratios: [1.0]
+          height_stride: 16
+          width_stride: 20
+          height_offset: 8
+          width_offset: 9
+        }
+        anchor_grid {
+          base_sizes: [1.0, 2.0]
+          aspect_ratios: [1.0, 0.5]
+          height_stride: 32
+          width_stride: 30
+          height_offset: 10
+          width_offset: 11
+        }
+      }
+    """
+    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()
+    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
+    anchor_generator_object = anchor_generator_builder.build(
+        anchor_generator_proto)
+    self.assertIsInstance(anchor_generator_object,
+                          flexible_grid_anchor_generator.
+                          FlexibleGridAnchorGenerator)
+
+    for actual_base_sizes, expected_base_sizes in zip(
+        list(anchor_generator_object._base_sizes), [(1.5,), (1.0, 2.0)]):
+      self.assert_almost_list_equal(expected_base_sizes, actual_base_sizes)
+
+    for actual_aspect_ratios, expected_aspect_ratios in zip(
+        list(anchor_generator_object._aspect_ratios), [(1.0,), (1.0, 0.5)]):
+      self.assert_almost_list_equal(expected_aspect_ratios,
+                                    actual_aspect_ratios)
+
+    for actual_strides, expected_strides in zip(
+        list(anchor_generator_object._anchor_strides), [(16, 20), (32, 30)]):
+      self.assert_almost_list_equal(expected_strides, actual_strides)
+
+    for actual_offsets, expected_offsets in zip(
+        list(anchor_generator_object._anchor_offsets), [(8, 9), (10, 11)]):
+      self.assert_almost_list_equal(expected_offsets, actual_offsets)
+
+    self.assertTrue(anchor_generator_object._normalize_coordinates)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/box_predictor_builder.py b/research/object_detection/builders/box_predictor_builder.py
index 53a88daa..abb3f2f8 100644
--- a/research/object_detection/builders/box_predictor_builder.py
+++ b/research/object_detection/builders/box_predictor_builder.py
@@ -20,11 +20,14 @@ import tensorflow as tf
 from object_detection.predictors import convolutional_box_predictor
 from object_detection.predictors import convolutional_keras_box_predictor
 from object_detection.predictors import mask_rcnn_box_predictor
+from object_detection.predictors import mask_rcnn_keras_box_predictor
 from object_detection.predictors import rfcn_box_predictor
+from object_detection.predictors import rfcn_keras_box_predictor
 from object_detection.predictors.heads import box_head
 from object_detection.predictors.heads import class_head
 from object_detection.predictors.heads import keras_box_head
 from object_detection.predictors.heads import keras_class_head
+from object_detection.predictors.heads import keras_mask_head
 from object_detection.predictors.heads import mask_head
 from object_detection.protos import box_predictor_pb2
 
@@ -42,7 +45,8 @@ def build_convolutional_box_predictor(is_training,
                                       apply_sigmoid_to_scores=False,
                                       add_background_class=True,
                                       class_prediction_bias_init=0.0,
-                                      use_depthwise=False,):
+                                      use_depthwise=False,
+                                      box_encodings_clip_range=None):
   """Builds the ConvolutionalBoxPredictor from the arguments.
 
   Args:
@@ -77,6 +81,7 @@ def build_convolutional_box_predictor(is_training,
       conv2d layer before class prediction.
     use_depthwise: Whether to use depthwise convolutions for prediction
       steps. Default is False.
+    box_encodings_clip_range: Min and max values for clipping the box_encodings.
 
   Returns:
     A ConvolutionalBoxPredictor class.
@@ -85,7 +90,8 @@ def build_convolutional_box_predictor(is_training,
       is_training=is_training,
       box_code_size=box_code_size,
       kernel_size=kernel_size,
-      use_depthwise=use_depthwise)
+      use_depthwise=use_depthwise,
+      box_encodings_clip_range=box_encodings_clip_range)
   class_prediction_head = class_head.ConvolutionalClassHead(
       is_training=is_training,
       num_class_slots=num_classes + 1 if add_background_class else num_classes,
@@ -124,6 +130,7 @@ def build_convolutional_keras_box_predictor(is_training,
                                             add_background_class=True,
                                             class_prediction_bias_init=0.0,
                                             use_depthwise=False,
+                                            box_encodings_clip_range=None,
                                             name='BoxPredictor'):
   """Builds the Keras ConvolutionalBoxPredictor from the arguments.
 
@@ -168,6 +175,7 @@ def build_convolutional_keras_box_predictor(is_training,
       conv2d layer before class prediction.
     use_depthwise: Whether to use depthwise convolutions for prediction
       steps. Default is False.
+    box_encodings_clip_range: Min and max values for clipping the box_encodings.
     name: A string name scope to assign to the box predictor. If `None`, Keras
       will auto-generate one from the class name.
 
@@ -189,6 +197,7 @@ def build_convolutional_keras_box_predictor(is_training,
             freeze_batchnorm=freeze_batchnorm,
             num_predictions_per_location=num_predictions_per_location,
             use_depthwise=use_depthwise,
+            box_encodings_clip_range=box_encodings_clip_range,
             name='ConvolutionalBoxHead_%d' % stack_index))
     class_prediction_heads.append(
         keras_class_head.ConvolutionalClassHead(
@@ -300,6 +309,224 @@ def build_weight_shared_convolutional_box_predictor(
       use_depthwise=use_depthwise)
 
 
+def build_weight_shared_convolutional_keras_box_predictor(
+    is_training,
+    num_classes,
+    conv_hyperparams,
+    freeze_batchnorm,
+    inplace_batchnorm_update,
+    num_predictions_per_location_list,
+    depth,
+    num_layers_before_predictor,
+    box_code_size,
+    kernel_size=3,
+    add_background_class=True,
+    class_prediction_bias_init=0.0,
+    use_dropout=False,
+    dropout_keep_prob=0.8,
+    share_prediction_tower=False,
+    apply_batch_norm=True,
+    use_depthwise=False,
+    score_converter_fn=tf.identity,
+    box_encodings_clip_range=None,
+    name='WeightSharedConvolutionalBoxPredictor'):
+  """Builds the Keras WeightSharedConvolutionalBoxPredictor from the arguments.
+
+  Args:
+    is_training: Indicates whether the BoxPredictor is in training mode.
+    num_classes: number of classes.  Note that num_classes *does not*
+      include the background category, so if groundtruth labels take values
+      in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+      assigned classification targets can range from {0,... K}).
+    conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+      containing hyperparameters for convolution ops.
+    freeze_batchnorm: Whether to freeze batch norm parameters during
+      training or not. When training with a small batch size (e.g. 1), it is
+      desirable to freeze batch norm update and use pretrained batch norm
+      params.
+    inplace_batchnorm_update: Whether to update batch norm moving average
+      values inplace. When this is false train op must add a control
+      dependency on tf.graphkeys.UPDATE_OPS collection in order to update
+      batch norm statistics.
+    num_predictions_per_location_list: A list of integers representing the
+      number of box predictions to be made per spatial location for each
+      feature map.
+    depth: depth of conv layers.
+    num_layers_before_predictor: Number of the additional conv layers before
+      the predictor.
+    box_code_size: Size of encoding for each box.
+    kernel_size: Size of final convolution kernel.
+    add_background_class: Whether to add an implicit background class.
+    class_prediction_bias_init: constant value to initialize bias of the last
+      conv2d layer before class prediction.
+    use_dropout: Whether to apply dropout to class prediction head.
+        dropout_keep_prob: Probability of keeping activiations.
+    share_prediction_tower: Whether to share the multi-layer tower between box
+      prediction and class prediction heads.
+    apply_batch_norm: Whether to apply batch normalization to conv layers in
+      this predictor.
+    use_depthwise: Whether to use depthwise separable conv2d instead of conv2d.
+    score_converter_fn: Callable score converter to perform elementwise op on
+      class scores.
+    box_encodings_clip_range: Min and max values for clipping the box_encodings.
+    name: A string name scope to assign to the box predictor. If `None`, Keras
+      will auto-generate one from the class name.
+
+  Returns:
+    A Keras WeightSharedConvolutionalBoxPredictor class.
+  """
+  if len(set(num_predictions_per_location_list)) > 1:
+    raise ValueError('num predictions per location must be same for all'
+                     'feature maps, found: {}'.format(
+                         num_predictions_per_location_list))
+  num_predictions_per_location = num_predictions_per_location_list[0]
+
+  box_prediction_head = keras_box_head.WeightSharedConvolutionalBoxHead(
+      box_code_size=box_code_size,
+      kernel_size=kernel_size,
+      conv_hyperparams=conv_hyperparams,
+      num_predictions_per_location=num_predictions_per_location,
+      use_depthwise=use_depthwise,
+      box_encodings_clip_range=box_encodings_clip_range,
+      name='WeightSharedConvolutionalBoxHead')
+  class_prediction_head = keras_class_head.WeightSharedConvolutionalClassHead(
+      num_class_slots=(
+          num_classes + 1 if add_background_class else num_classes),
+      use_dropout=use_dropout,
+      dropout_keep_prob=dropout_keep_prob,
+      kernel_size=kernel_size,
+      conv_hyperparams=conv_hyperparams,
+      num_predictions_per_location=num_predictions_per_location,
+      class_prediction_bias_init=class_prediction_bias_init,
+      use_depthwise=use_depthwise,
+      score_converter_fn=score_converter_fn,
+      name='WeightSharedConvolutionalClassHead')
+  other_heads = {}
+
+  return (
+      convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=is_training,
+          num_classes=num_classes,
+          box_prediction_head=box_prediction_head,
+          class_prediction_head=class_prediction_head,
+          other_heads=other_heads,
+          conv_hyperparams=conv_hyperparams,
+          depth=depth,
+          num_layers_before_predictor=num_layers_before_predictor,
+          freeze_batchnorm=freeze_batchnorm,
+          inplace_batchnorm_update=inplace_batchnorm_update,
+          kernel_size=kernel_size,
+          apply_batch_norm=apply_batch_norm,
+          share_prediction_tower=share_prediction_tower,
+          use_depthwise=use_depthwise,
+          name=name))
+
+
+
+
+def build_mask_rcnn_keras_box_predictor(is_training,
+                                        num_classes,
+                                        fc_hyperparams,
+                                        freeze_batchnorm,
+                                        use_dropout,
+                                        dropout_keep_prob,
+                                        box_code_size,
+                                        add_background_class=True,
+                                        share_box_across_classes=False,
+                                        predict_instance_masks=False,
+                                        conv_hyperparams=None,
+                                        mask_height=14,
+                                        mask_width=14,
+                                        mask_prediction_num_conv_layers=2,
+                                        mask_prediction_conv_depth=256,
+                                        masks_are_class_agnostic=False,
+                                        convolve_then_upsample_masks=False):
+  """Builds and returns a MaskRCNNKerasBoxPredictor class.
+
+  Args:
+    is_training: Indicates whether the BoxPredictor is in training mode.
+    num_classes: number of classes.  Note that num_classes *does not*
+      include the background category, so if groundtruth labels take values
+      in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+      assigned classification targets can range from {0,... K}).
+    fc_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+      containing hyperparameters for fully connected dense ops.
+    freeze_batchnorm: Whether to freeze batch norm parameters during
+      training or not. When training with a small batch size (e.g. 1), it is
+      desirable to freeze batch norm update and use pretrained batch norm
+      params.
+    use_dropout: Option to use dropout or not.  Note that a single dropout
+      op is applied here prior to both box and class predictions, which stands
+      in contrast to the ConvolutionalBoxPredictor below.
+    dropout_keep_prob: Keep probability for dropout.
+      This is only used if use_dropout is True.
+    box_code_size: Size of encoding for each box.
+    add_background_class: Whether to add an implicit background class.
+    share_box_across_classes: Whether to share boxes across classes rather
+      than use a different box for each class.
+    predict_instance_masks: If True, will add a third stage mask prediction
+      to the returned class.
+    conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+      containing hyperparameters for convolution ops.
+    mask_height: Desired output mask height. The default value is 14.
+    mask_width: Desired output mask width. The default value is 14.
+    mask_prediction_num_conv_layers: Number of convolution layers applied to
+      the image_features in mask prediction branch.
+    mask_prediction_conv_depth: The depth for the first conv2d_transpose op
+      applied to the image_features in the mask prediction branch. If set
+      to 0, the depth of the convolution layers will be automatically chosen
+      based on the number of object classes and the number of channels in the
+      image features.
+    masks_are_class_agnostic: Boolean determining if the mask-head is
+      class-agnostic or not.
+    convolve_then_upsample_masks: Whether to apply convolutions on mask
+      features before upsampling using nearest neighbor resizing. Otherwise,
+      mask features are resized to [`mask_height`, `mask_width`] using
+      bilinear resizing before applying convolutions.
+
+  Returns:
+    A MaskRCNNKerasBoxPredictor class.
+  """
+  box_prediction_head = keras_box_head.MaskRCNNBoxHead(
+      is_training=is_training,
+      num_classes=num_classes,
+      fc_hyperparams=fc_hyperparams,
+      freeze_batchnorm=freeze_batchnorm,
+      use_dropout=use_dropout,
+      dropout_keep_prob=dropout_keep_prob,
+      box_code_size=box_code_size,
+      share_box_across_classes=share_box_across_classes)
+  class_prediction_head = keras_class_head.MaskRCNNClassHead(
+      is_training=is_training,
+      num_class_slots=num_classes + 1 if add_background_class else num_classes,
+      fc_hyperparams=fc_hyperparams,
+      freeze_batchnorm=freeze_batchnorm,
+      use_dropout=use_dropout,
+      dropout_keep_prob=dropout_keep_prob)
+  third_stage_heads = {}
+  if predict_instance_masks:
+    third_stage_heads[
+        mask_rcnn_box_predictor.
+        MASK_PREDICTIONS] = keras_mask_head.MaskRCNNMaskHead(
+            is_training=is_training,
+            num_classes=num_classes,
+            conv_hyperparams=conv_hyperparams,
+            freeze_batchnorm=freeze_batchnorm,
+            mask_height=mask_height,
+            mask_width=mask_width,
+            mask_prediction_num_conv_layers=mask_prediction_num_conv_layers,
+            mask_prediction_conv_depth=mask_prediction_conv_depth,
+            masks_are_class_agnostic=masks_are_class_agnostic,
+            convolve_then_upsample=convolve_then_upsample_masks)
+  return mask_rcnn_keras_box_predictor.MaskRCNNKerasBoxPredictor(
+      is_training=is_training,
+      num_classes=num_classes,
+      freeze_batchnorm=freeze_batchnorm,
+      box_prediction_head=box_prediction_head,
+      class_prediction_head=class_prediction_head,
+      third_stage_heads=third_stage_heads)
+
+
 def build_mask_rcnn_box_predictor(is_training,
                                   num_classes,
                                   fc_hyperparams_fn,
@@ -457,6 +684,13 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes,
     config_box_predictor = box_predictor_config.convolutional_box_predictor
     conv_hyperparams_fn = argscope_fn(config_box_predictor.conv_hyperparams,
                                       is_training)
+    # Optionally apply clipping to box encodings, when box_encodings_clip_range
+    # is set.
+    box_encodings_clip_range = None
+    if config_box_predictor.HasField('box_encodings_clip_range'):
+      box_encodings_clip_range = BoxEncodingsClipRange(
+          min=config_box_predictor.box_encodings_clip_range.min,
+          max=config_box_predictor.box_encodings_clip_range.max)
     return build_convolutional_box_predictor(
         is_training=is_training,
         num_classes=num_classes,
@@ -473,7 +707,8 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes,
         apply_sigmoid_to_scores=config_box_predictor.apply_sigmoid_to_scores,
         class_prediction_bias_init=(
             config_box_predictor.class_prediction_bias_init),
-        use_depthwise=config_box_predictor.use_depthwise)
+        use_depthwise=config_box_predictor.use_depthwise,
+        box_encodings_clip_range=box_encodings_clip_range)
 
   if  box_predictor_oneof == 'weight_shared_convolutional_box_predictor':
     config_box_predictor = (
@@ -488,12 +723,11 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes,
         config_box_predictor.score_converter, is_training)
     # Optionally apply clipping to box encodings, when box_encodings_clip_range
     # is set.
-    box_encodings_clip_range = (
-        BoxEncodingsClipRange(
-            min=config_box_predictor.box_encodings_clip_range.min,
-            max=config_box_predictor.box_encodings_clip_range.max)
-        if config_box_predictor.HasField('box_encodings_clip_range') else None)
-
+    box_encodings_clip_range = None
+    if config_box_predictor.HasField('box_encodings_clip_range'):
+      box_encodings_clip_range = BoxEncodingsClipRange(
+          min=config_box_predictor.box_encodings_clip_range.min,
+          max=config_box_predictor.box_encodings_clip_range.max)
     return build_weight_shared_convolutional_box_predictor(
         is_training=is_training,
         num_classes=num_classes,
@@ -514,6 +748,7 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes,
         score_converter_fn=score_converter_fn,
         box_encodings_clip_range=box_encodings_clip_range)
 
+
   if box_predictor_oneof == 'mask_rcnn_box_predictor':
     config_box_predictor = box_predictor_config.mask_rcnn_box_predictor
     fc_hyperparams_fn = argscope_fn(config_box_predictor.fc_hyperparams,
@@ -563,7 +798,7 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes,
   raise ValueError('Unknown box predictor: {}'.format(box_predictor_oneof))
 
 
-def build_keras(conv_hyperparams_fn, freeze_batchnorm, inplace_batchnorm_update,
+def build_keras(hyperparams_fn, freeze_batchnorm, inplace_batchnorm_update,
                 num_predictions_per_location_list, box_predictor_config,
                 is_training, num_classes, add_background_class=True):
   """Builds a Keras-based box predictor based on the configuration.
@@ -573,7 +808,7 @@ def build_keras(conv_hyperparams_fn, freeze_batchnorm, inplace_batchnorm_update,
   for more details.
 
   Args:
-    conv_hyperparams_fn: A function that takes a hyperparams_pb2.Hyperparams
+    hyperparams_fn: A function that takes a hyperparams_pb2.Hyperparams
       proto and returns a `hyperparams_builder.KerasLayerHyperparams`
       for Conv or FC hyperparameters.
     freeze_batchnorm: Whether to freeze batch norm parameters during
@@ -607,8 +842,16 @@ def build_keras(conv_hyperparams_fn, freeze_batchnorm, inplace_batchnorm_update,
 
   if box_predictor_oneof == 'convolutional_box_predictor':
     config_box_predictor = box_predictor_config.convolutional_box_predictor
-    conv_hyperparams = conv_hyperparams_fn(
+    conv_hyperparams = hyperparams_fn(
         config_box_predictor.conv_hyperparams)
+    # Optionally apply clipping to box encodings, when box_encodings_clip_range
+    # is set.
+    box_encodings_clip_range = None
+    if config_box_predictor.HasField('box_encodings_clip_range'):
+      box_encodings_clip_range = BoxEncodingsClipRange(
+          min=config_box_predictor.box_encodings_clip_range.min,
+          max=config_box_predictor.box_encodings_clip_range.max)
+
     return build_convolutional_keras_box_predictor(
         is_training=is_training,
         num_classes=num_classes,
@@ -627,7 +870,97 @@ def build_keras(conv_hyperparams_fn, freeze_batchnorm, inplace_batchnorm_update,
         max_depth=config_box_predictor.max_depth,
         class_prediction_bias_init=(
             config_box_predictor.class_prediction_bias_init),
-        use_depthwise=config_box_predictor.use_depthwise)
+        use_depthwise=config_box_predictor.use_depthwise,
+        box_encodings_clip_range=box_encodings_clip_range)
+
+  if box_predictor_oneof == 'weight_shared_convolutional_box_predictor':
+    config_box_predictor = (
+        box_predictor_config.weight_shared_convolutional_box_predictor)
+    conv_hyperparams = hyperparams_fn(config_box_predictor.conv_hyperparams)
+    apply_batch_norm = config_box_predictor.conv_hyperparams.HasField(
+        'batch_norm')
+    # During training phase, logits are used to compute the loss. Only apply
+    # sigmoid at inference to make the inference graph TPU friendly. This is
+    # required because during TPU inference, model.postprocess is not called.
+    score_converter_fn = build_score_converter(
+        config_box_predictor.score_converter, is_training)
+    # Optionally apply clipping to box encodings, when box_encodings_clip_range
+    # is set.
+    box_encodings_clip_range = None
+    if config_box_predictor.HasField('box_encodings_clip_range'):
+      box_encodings_clip_range = BoxEncodingsClipRange(
+          min=config_box_predictor.box_encodings_clip_range.min,
+          max=config_box_predictor.box_encodings_clip_range.max)
+
+    return build_weight_shared_convolutional_keras_box_predictor(
+        is_training=is_training,
+        num_classes=num_classes,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=freeze_batchnorm,
+        inplace_batchnorm_update=inplace_batchnorm_update,
+        num_predictions_per_location_list=num_predictions_per_location_list,
+        depth=config_box_predictor.depth,
+        num_layers_before_predictor=(
+            config_box_predictor.num_layers_before_predictor),
+        box_code_size=config_box_predictor.box_code_size,
+        kernel_size=config_box_predictor.kernel_size,
+        add_background_class=add_background_class,
+        class_prediction_bias_init=(
+            config_box_predictor.class_prediction_bias_init),
+        use_dropout=config_box_predictor.use_dropout,
+        dropout_keep_prob=config_box_predictor.dropout_keep_probability,
+        share_prediction_tower=config_box_predictor.share_prediction_tower,
+        apply_batch_norm=apply_batch_norm,
+        use_depthwise=config_box_predictor.use_depthwise,
+        score_converter_fn=score_converter_fn,
+        box_encodings_clip_range=box_encodings_clip_range)
+
+  if box_predictor_oneof == 'mask_rcnn_box_predictor':
+    config_box_predictor = box_predictor_config.mask_rcnn_box_predictor
+    fc_hyperparams = hyperparams_fn(config_box_predictor.fc_hyperparams)
+    conv_hyperparams = None
+    if config_box_predictor.HasField('conv_hyperparams'):
+      conv_hyperparams = hyperparams_fn(
+          config_box_predictor.conv_hyperparams)
+    return build_mask_rcnn_keras_box_predictor(
+        is_training=is_training,
+        num_classes=num_classes,
+        add_background_class=add_background_class,
+        fc_hyperparams=fc_hyperparams,
+        freeze_batchnorm=freeze_batchnorm,
+        use_dropout=config_box_predictor.use_dropout,
+        dropout_keep_prob=config_box_predictor.dropout_keep_probability,
+        box_code_size=config_box_predictor.box_code_size,
+        share_box_across_classes=(
+            config_box_predictor.share_box_across_classes),
+        predict_instance_masks=config_box_predictor.predict_instance_masks,
+        conv_hyperparams=conv_hyperparams,
+        mask_height=config_box_predictor.mask_height,
+        mask_width=config_box_predictor.mask_width,
+        mask_prediction_num_conv_layers=(
+            config_box_predictor.mask_prediction_num_conv_layers),
+        mask_prediction_conv_depth=(
+            config_box_predictor.mask_prediction_conv_depth),
+        masks_are_class_agnostic=(
+            config_box_predictor.masks_are_class_agnostic),
+        convolve_then_upsample_masks=(
+            config_box_predictor.convolve_then_upsample_masks))
+
+  if box_predictor_oneof == 'rfcn_box_predictor':
+    config_box_predictor = box_predictor_config.rfcn_box_predictor
+    conv_hyperparams = hyperparams_fn(config_box_predictor.conv_hyperparams)
+    box_predictor_object = rfcn_keras_box_predictor.RfcnKerasBoxPredictor(
+        is_training=is_training,
+        num_classes=num_classes,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=freeze_batchnorm,
+        crop_size=[config_box_predictor.crop_height,
+                   config_box_predictor.crop_width],
+        num_spatial_bins=[config_box_predictor.num_spatial_bins_height,
+                          config_box_predictor.num_spatial_bins_width],
+        depth=config_box_predictor.depth,
+        box_code_size=config_box_predictor.box_code_size)
+    return box_predictor_object
 
   raise ValueError(
       'Unknown box predictor for Keras: {}'.format(box_predictor_oneof))
diff --git a/research/object_detection/builders/box_predictor_builder_test.py b/research/object_detection/builders/box_predictor_builder_test.py
index 08029df7..2f211e2c 100644
--- a/research/object_detection/builders/box_predictor_builder_test.py
+++ b/research/object_detection/builders/box_predictor_builder_test.py
@@ -353,6 +353,8 @@ class WeightSharedConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertEqual(box_predictor._apply_batch_norm, True)
 
 
+
+
 class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
 
   def test_box_predictor_builder_calls_fc_argscope_fn(self):
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index 74c811c4..158abb19 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -56,9 +56,15 @@ def read_dataset(file_read_func, input_files, config):
 
   Returns:
     A tf.data.Dataset of (undecoded) tf-records based on config.
+
+  Raises:
+    RuntimeError: If no files are found at the supplied path(s).
   """
   # Shard, shuffle, and read files.
   filenames = tf.gfile.Glob(input_files)
+  if not filenames:
+    raise RuntimeError('Did not find any input files matching the glob pattern '
+                       '{}'.format(input_files))
   num_readers = config.num_readers
   if num_readers > len(filenames):
     num_readers = len(filenames)
diff --git a/research/object_detection/builders/graph_rewriter_builder.py b/research/object_detection/builders/graph_rewriter_builder.py
index 77e60479..53267f30 100644
--- a/research/object_detection/builders/graph_rewriter_builder.py
+++ b/research/object_detection/builders/graph_rewriter_builder.py
@@ -32,11 +32,14 @@ def build(graph_rewriter_config, is_training):
 
     # Quantize the graph by inserting quantize ops for weights and activations
     if is_training:
-      tf.contrib.quantize.create_training_graph(
+      tf.contrib.quantize.experimental_create_training_graph(
           input_graph=tf.get_default_graph(),
-          quant_delay=graph_rewriter_config.quantization.delay)
+          quant_delay=graph_rewriter_config.quantization.delay
+      )
     else:
-      tf.contrib.quantize.create_eval_graph(input_graph=tf.get_default_graph())
+      tf.contrib.quantize.experimental_create_eval_graph(
+          input_graph=tf.get_default_graph()
+      )
 
     tf.contrib.layers.summarize_collection('quant_vars')
   return graph_rewrite_fn
diff --git a/research/object_detection/builders/graph_rewriter_builder_test.py b/research/object_detection/builders/graph_rewriter_builder_test.py
index 5f38d5a2..72730e72 100644
--- a/research/object_detection/builders/graph_rewriter_builder_test.py
+++ b/research/object_detection/builders/graph_rewriter_builder_test.py
@@ -23,7 +23,8 @@ class QuantizationBuilderTest(tf.test.TestCase):
 
   def testQuantizationBuilderSetsUpCorrectTrainArguments(self):
     with mock.patch.object(
-        tf.contrib.quantize, 'create_training_graph') as mock_quant_fn:
+        tf.contrib.quantize,
+        'experimental_create_training_graph') as mock_quant_fn:
       with mock.patch.object(tf.contrib.layers,
                              'summarize_collection') as mock_summarize_col:
         graph_rewriter_proto = graph_rewriter_pb2.GraphRewriter()
@@ -40,7 +41,7 @@ class QuantizationBuilderTest(tf.test.TestCase):
 
   def testQuantizationBuilderSetsUpCorrectEvalArguments(self):
     with mock.patch.object(tf.contrib.quantize,
-                           'create_eval_graph') as mock_quant_fn:
+                           'experimental_create_eval_graph') as mock_quant_fn:
       with mock.patch.object(tf.contrib.layers,
                              'summarize_collection') as mock_summarize_col:
         graph_rewriter_proto = graph_rewriter_pb2.GraphRewriter()
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index 529065ce..bb24ef8a 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -110,6 +110,32 @@ def build(image_resizer_config):
       else:
         return [image, masks, tf.shape(image)]
     return image_resizer_fn
+  elif image_resizer_oneof == 'conditional_shape_resizer':
+    conditional_shape_resize_config = (
+        image_resizer_config.conditional_shape_resizer)
+    method = _tf_resize_method(conditional_shape_resize_config.resize_method)
+
+    if conditional_shape_resize_config.condition == (
+        image_resizer_pb2.ConditionalShapeResizer.GREATER):
+      image_resizer_fn = functools.partial(
+          preprocessor.resize_to_max_dimension,
+          max_dimension=conditional_shape_resize_config.size_threshold,
+          method=method)
+
+    elif conditional_shape_resize_config.condition == (
+        image_resizer_pb2.ConditionalShapeResizer.SMALLER):
+      image_resizer_fn = functools.partial(
+          preprocessor.resize_to_min_dimension,
+          min_dimension=conditional_shape_resize_config.size_threshold,
+          method=method)
+    else:
+      raise ValueError(
+          'Invalid image resizer condition option for '
+          'ConditionalShapeResizer: \'%s\'.'
+          % conditional_shape_resize_config.condition)
+
+    if not conditional_shape_resize_config.convert_to_grayscale:
+      return image_resizer_fn
   else:
     raise ValueError(
         'Invalid image resizer option: \'%s\'.' % image_resizer_oneof)
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index 54a73c8d..f2d434ae 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -147,6 +147,69 @@ class ImageResizerBuilderTest(tf.test.TestCase):
     self.assertEqual(len(vals), 1)
     self.assertEqual(vals[0], 1)
 
+  def test_build_conditional_shape_resizer_greater_returns_expected_shape(self):
+    image_resizer_text_proto = """
+      conditional_shape_resizer {
+        condition: GREATER
+        size_threshold: 30
+      }
+    """
+    input_shape = (60, 30, 3)
+    expected_output_shape = (30, 15, 3)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
+  def test_build_conditional_shape_resizer_same_shape_with_no_resize(self):
+    image_resizer_text_proto = """
+      conditional_shape_resizer {
+        condition: GREATER
+        size_threshold: 30
+      }
+    """
+    input_shape = (15, 15, 3)
+    expected_output_shape = (15, 15, 3)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
+  def test_build_conditional_shape_resizer_smaller_returns_expected_shape(self):
+    image_resizer_text_proto = """
+      conditional_shape_resizer {
+        condition: SMALLER
+        size_threshold: 30
+      }
+    """
+    input_shape = (30, 15, 3)
+    expected_output_shape = (60, 30, 3)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
+  def test_build_conditional_shape_resizer_grayscale(self):
+    image_resizer_text_proto = """
+      conditional_shape_resizer {
+        condition: GREATER
+        size_threshold: 30
+        convert_to_grayscale: true
+      }
+    """
+    input_shape = (60, 30, 3)
+    expected_output_shape = (30, 15, 1)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
+  def test_build_conditional_shape_resizer_error_on_invalid_condition(self):
+    invalid_image_resizer_text_proto = """
+      conditional_shape_resizer {
+        condition: INVALID
+        size_threshold: 30
+      }
+    """
+    with self.assertRaises(ValueError):
+      image_resizer_builder.build(invalid_image_resizer_text_proto)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index fcb7f215..60eb2b17 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -33,6 +33,7 @@ from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.meta_architectures import rfcn_meta_arch
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
+from object_detection.models import faster_rcnn_inception_resnet_v2_keras_feature_extractor as frcnn_inc_res_keras
 from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
 from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
 from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
@@ -44,13 +45,16 @@ from object_detection.models.ssd_inception_v2_feature_extractor import SSDIncept
 from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
+from object_detection.models.ssd_mobilenet_v1_fpn_keras_feature_extractor import SSDMobileNetV1FpnKerasFeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_keras_feature_extractor import SSDMobileNetV1KerasFeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_fpn_feature_extractor import SSDMobileNetV2FpnFeatureExtractor
+from object_detection.models.ssd_mobilenet_v2_fpn_keras_feature_extractor import SSDMobileNetV2FpnKerasFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_keras_feature_extractor import SSDMobileNetV2KerasFeatureExtractor
 from object_detection.models.ssd_pnasnet_feature_extractor import SSDPNASNetFeatureExtractor
 from object_detection.predictors import rfcn_box_predictor
+from object_detection.predictors import rfcn_keras_box_predictor
 from object_detection.predictors.heads import mask_head
 from object_detection.protos import model_pb2
 from object_detection.utils import ops
@@ -78,7 +82,9 @@ SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
 
 SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
     'ssd_mobilenet_v1_keras': SSDMobileNetV1KerasFeatureExtractor,
-    'ssd_mobilenet_v2_keras': SSDMobileNetV2KerasFeatureExtractor
+    'ssd_mobilenet_v1_fpn_keras': SSDMobileNetV1FpnKerasFeatureExtractor,
+    'ssd_mobilenet_v2_keras': SSDMobileNetV2KerasFeatureExtractor,
+    'ssd_mobilenet_v2_fpn_keras': SSDMobileNetV2FpnKerasFeatureExtractor,
 }
 
 # A map of names to Faster R-CNN feature extractors.
@@ -99,6 +105,11 @@ FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {
     frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor,
 }
 
+FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
+    'faster_rcnn_inception_resnet_v2_keras':
+    frcnn_inc_res_keras.FasterRCNNInceptionResnetV2KerasFeatureExtractor,
+}
+
 
 def build(model_config, is_training, add_summaries=True):
   """Builds a DetectionModel based on the model config.
@@ -253,7 +264,7 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
       ssd_config.anchor_generator)
   if feature_extractor.is_keras_model:
     ssd_box_predictor = box_predictor_builder.build_keras(
-        conv_hyperparams_fn=hyperparams_builder.KerasLayerHyperparams,
+        hyperparams_fn=hyperparams_builder.KerasLayerHyperparams,
         freeze_batchnorm=ssd_config.freeze_batchnorm,
         inplace_batchnorm_update=False,
         num_predictions_per_location_list=anchor_generator
@@ -355,7 +366,45 @@ def _build_faster_rcnn_feature_extractor(
       feature_type]
   return feature_extractor_class(
       is_training, first_stage_features_stride,
-      batch_norm_trainable, reuse_weights)
+      batch_norm_trainable, reuse_weights=reuse_weights)
+
+
+def _build_faster_rcnn_keras_feature_extractor(
+    feature_extractor_config, is_training,
+    inplace_batchnorm_update=False):
+  """Builds a faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor from config.
+
+  Args:
+    feature_extractor_config: A FasterRcnnFeatureExtractor proto config from
+      faster_rcnn.proto.
+    is_training: True if this feature extractor is being built for training.
+    inplace_batchnorm_update: Whether to update batch_norm inplace during
+      training. This is required for batch norm to work correctly on TPUs. When
+      this is false, user must add a control dependency on
+      tf.GraphKeys.UPDATE_OPS for train/loss op in order to update the batch
+      norm moving average parameters.
+
+  Returns:
+    faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor based on config.
+
+  Raises:
+    ValueError: On invalid feature extractor type.
+  """
+  if inplace_batchnorm_update:
+    raise ValueError('inplace batchnorm updates not supported.')
+  feature_type = feature_extractor_config.type
+  first_stage_features_stride = (
+      feature_extractor_config.first_stage_features_stride)
+  batch_norm_trainable = feature_extractor_config.batch_norm_trainable
+
+  if feature_type not in FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP:
+    raise ValueError('Unknown Faster R-CNN feature_extractor: {}'.format(
+        feature_type))
+  feature_extractor_class = FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP[
+      feature_type]
+  return feature_extractor_class(
+      is_training, first_stage_features_stride,
+      batch_norm_trainable)
 
 
 def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
@@ -380,9 +429,17 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
   num_classes = frcnn_config.num_classes
   image_resizer_fn = image_resizer_builder.build(frcnn_config.image_resizer)
 
-  feature_extractor = _build_faster_rcnn_feature_extractor(
-      frcnn_config.feature_extractor, is_training,
-      inplace_batchnorm_update=frcnn_config.inplace_batchnorm_update)
+  is_keras = (frcnn_config.feature_extractor.type in
+              FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP)
+
+  if is_keras:
+    feature_extractor = _build_faster_rcnn_keras_feature_extractor(
+        frcnn_config.feature_extractor, is_training,
+        inplace_batchnorm_update=frcnn_config.inplace_batchnorm_update)
+  else:
+    feature_extractor = _build_faster_rcnn_feature_extractor(
+        frcnn_config.feature_extractor, is_training,
+        inplace_batchnorm_update=frcnn_config.inplace_batchnorm_update)
 
   number_of_stages = frcnn_config.number_of_stages
   first_stage_anchor_generator = anchor_generator_builder.build(
@@ -393,8 +450,13 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       'proposal',
       use_matmul_gather=frcnn_config.use_matmul_gather_in_matcher)
   first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate
-  first_stage_box_predictor_arg_scope_fn = hyperparams_builder.build(
-      frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)
+  if is_keras:
+    first_stage_box_predictor_arg_scope_fn = (
+        hyperparams_builder.KerasLayerHyperparams(
+            frcnn_config.first_stage_box_predictor_conv_hyperparams))
+  else:
+    first_stage_box_predictor_arg_scope_fn = hyperparams_builder.build(
+        frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)
   first_stage_box_predictor_kernel_size = (
       frcnn_config.first_stage_box_predictor_kernel_size)
   first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth
@@ -432,11 +494,21 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       'FasterRCNN',
       'detection',
       use_matmul_gather=frcnn_config.use_matmul_gather_in_matcher)
-  second_stage_box_predictor = box_predictor_builder.build(
-      hyperparams_builder.build,
-      frcnn_config.second_stage_box_predictor,
-      is_training=is_training,
-      num_classes=num_classes)
+  if is_keras:
+    second_stage_box_predictor = box_predictor_builder.build_keras(
+        hyperparams_builder.KerasLayerHyperparams,
+        freeze_batchnorm=False,
+        inplace_batchnorm_update=False,
+        num_predictions_per_location_list=[1],
+        box_predictor_config=frcnn_config.second_stage_box_predictor,
+        is_training=is_training,
+        num_classes=num_classes)
+  else:
+    second_stage_box_predictor = box_predictor_builder.build(
+        hyperparams_builder.build,
+        frcnn_config.second_stage_box_predictor,
+        is_training=is_training,
+        num_classes=num_classes)
   second_stage_batch_size = frcnn_config.second_stage_batch_size
   second_stage_sampler = sampler.BalancedPositiveNegativeSampler(
       positive_fraction=frcnn_config.second_stage_balance_fraction,
@@ -507,8 +579,10 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       'resize_masks': frcnn_config.resize_masks
   }
 
-  if isinstance(second_stage_box_predictor,
-                rfcn_box_predictor.RfcnBoxPredictor):
+  if (isinstance(second_stage_box_predictor,
+                 rfcn_box_predictor.RfcnBoxPredictor) or
+      isinstance(second_stage_box_predictor,
+                 rfcn_keras_box_predictor.RfcnKerasBoxPredictor)):
     return rfcn_meta_arch.RFCNMetaArch(
         second_stage_rfcn_box_predictor=second_stage_box_predictor,
         **common_kwargs)
diff --git a/research/object_detection/core/matcher.py b/research/object_detection/core/matcher.py
index 61b5da61..a454533d 100644
--- a/research/object_detection/core/matcher.py
+++ b/research/object_detection/core/matcher.py
@@ -170,7 +170,13 @@ class Match(object):
       row_indices: int32 tensor of shape [K] with row indices.
     """
     return self._reshape_and_cast(
-        self._gather_op(self._match_results, self.matched_column_indices()))
+        self._gather_op(tf.to_float(self._match_results),
+                        self.matched_column_indices()))
+
+  def num_matched_rows(self):
+    """Returns number (int32 scalar tensor) of matched rows."""
+    unique_rows, _ = tf.unique(self.matched_row_indices())
+    return tf.size(unique_rows)
 
   def _reshape_and_cast(self, t):
     return tf.cast(tf.reshape(t, [-1]), tf.int32)
@@ -199,7 +205,7 @@ class Match(object):
     """
     input_tensor = tf.concat(
         [tf.stack([ignored_value, unmatched_value]),
-         tf.to_float(input_tensor)],
+         input_tensor],
         axis=0)
     gather_indices = tf.maximum(self.match_results + 2, 0)
     gathered_tensor = self._gather_op(input_tensor, gather_indices)
diff --git a/research/object_detection/core/matcher_test.py b/research/object_detection/core/matcher_test.py
index 05607834..1ed32167 100644
--- a/research/object_detection/core/matcher_test.py
+++ b/research/object_detection/core/matcher_test.py
@@ -27,37 +27,42 @@ class MatchTest(tf.test.TestCase):
     match = matcher.Match(match_results)
     expected_column_indices = [0, 1, 3, 5]
     matched_column_indices = match.matched_column_indices()
-    self.assertEquals(matched_column_indices.dtype, tf.int32)
+    self.assertEqual(matched_column_indices.dtype, tf.int32)
     with self.test_session() as sess:
       matched_column_indices = sess.run(matched_column_indices)
       self.assertAllEqual(matched_column_indices, expected_column_indices)
 
   def test_get_correct_counts(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+    match_results = tf.constant([3, 1, -1, 0, -1, 1, -2])
     match = matcher.Match(match_results)
     exp_num_matched_columns = 4
     exp_num_unmatched_columns = 2
     exp_num_ignored_columns = 1
+    exp_num_matched_rows = 3
     num_matched_columns = match.num_matched_columns()
     num_unmatched_columns = match.num_unmatched_columns()
     num_ignored_columns = match.num_ignored_columns()
-    self.assertEquals(num_matched_columns.dtype, tf.int32)
-    self.assertEquals(num_unmatched_columns.dtype, tf.int32)
-    self.assertEquals(num_ignored_columns.dtype, tf.int32)
+    num_matched_rows = match.num_matched_rows()
+    self.assertEqual(num_matched_columns.dtype, tf.int32)
+    self.assertEqual(num_unmatched_columns.dtype, tf.int32)
+    self.assertEqual(num_ignored_columns.dtype, tf.int32)
+    self.assertEqual(num_matched_rows.dtype, tf.int32)
     with self.test_session() as sess:
       (num_matched_columns_out, num_unmatched_columns_out,
-       num_ignored_columns_out) = sess.run(
-           [num_matched_columns, num_unmatched_columns, num_ignored_columns])
+       num_ignored_columns_out, num_matched_rows_out) = sess.run(
+           [num_matched_columns, num_unmatched_columns, num_ignored_columns,
+            num_matched_rows])
       self.assertAllEqual(num_matched_columns_out, exp_num_matched_columns)
       self.assertAllEqual(num_unmatched_columns_out, exp_num_unmatched_columns)
       self.assertAllEqual(num_ignored_columns_out, exp_num_ignored_columns)
+      self.assertAllEqual(num_matched_rows_out, exp_num_matched_rows)
 
   def testGetCorrectUnmatchedColumnIndices(self):
     match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
     match = matcher.Match(match_results)
     expected_column_indices = [2, 4]
     unmatched_column_indices = match.unmatched_column_indices()
-    self.assertEquals(unmatched_column_indices.dtype, tf.int32)
+    self.assertEqual(unmatched_column_indices.dtype, tf.int32)
     with self.test_session() as sess:
       unmatched_column_indices = sess.run(unmatched_column_indices)
       self.assertAllEqual(unmatched_column_indices, expected_column_indices)
@@ -67,7 +72,7 @@ class MatchTest(tf.test.TestCase):
     match = matcher.Match(match_results)
     expected_row_indices = [3, 1, 0, 5]
     matched_row_indices = match.matched_row_indices()
-    self.assertEquals(matched_row_indices.dtype, tf.int32)
+    self.assertEqual(matched_row_indices.dtype, tf.int32)
     with self.test_session() as sess:
       matched_row_inds = sess.run(matched_row_indices)
       self.assertAllEqual(matched_row_inds, expected_row_indices)
@@ -77,7 +82,7 @@ class MatchTest(tf.test.TestCase):
     match = matcher.Match(match_results)
     expected_column_indices = [6]
     ignored_column_indices = match.ignored_column_indices()
-    self.assertEquals(ignored_column_indices.dtype, tf.int32)
+    self.assertEqual(ignored_column_indices.dtype, tf.int32)
     with self.test_session() as sess:
       ignored_column_indices = sess.run(ignored_column_indices)
       self.assertAllEqual(ignored_column_indices, expected_column_indices)
@@ -87,7 +92,7 @@ class MatchTest(tf.test.TestCase):
     match = matcher.Match(match_results)
     expected_column_indicator = [True, True, False, True, False, True, False]
     matched_column_indicator = match.matched_column_indicator()
-    self.assertEquals(matched_column_indicator.dtype, tf.bool)
+    self.assertEqual(matched_column_indicator.dtype, tf.bool)
     with self.test_session() as sess:
       matched_column_indicator = sess.run(matched_column_indicator)
       self.assertAllEqual(matched_column_indicator, expected_column_indicator)
@@ -97,7 +102,7 @@ class MatchTest(tf.test.TestCase):
     match = matcher.Match(match_results)
     expected_column_indicator = [False, False, True, False, True, False, False]
     unmatched_column_indicator = match.unmatched_column_indicator()
-    self.assertEquals(unmatched_column_indicator.dtype, tf.bool)
+    self.assertEqual(unmatched_column_indicator.dtype, tf.bool)
     with self.test_session() as sess:
       unmatched_column_indicator = sess.run(unmatched_column_indicator)
       self.assertAllEqual(unmatched_column_indicator, expected_column_indicator)
@@ -107,7 +112,7 @@ class MatchTest(tf.test.TestCase):
     match = matcher.Match(match_results)
     expected_column_indicator = [False, False, False, False, False, False, True]
     ignored_column_indicator = match.ignored_column_indicator()
-    self.assertEquals(ignored_column_indicator.dtype, tf.bool)
+    self.assertEqual(ignored_column_indicator.dtype, tf.bool)
     with self.test_session() as sess:
       ignored_column_indicator = sess.run(ignored_column_indicator)
       self.assertAllEqual(ignored_column_indicator, expected_column_indicator)
@@ -118,7 +123,7 @@ class MatchTest(tf.test.TestCase):
     expected_column_indices = [2, 4, 6]
     unmatched_ignored_column_indices = (match.
                                         unmatched_or_ignored_column_indices())
-    self.assertEquals(unmatched_ignored_column_indices.dtype, tf.int32)
+    self.assertEqual(unmatched_ignored_column_indices.dtype, tf.int32)
     with self.test_session() as sess:
       unmatched_ignored_column_indices = sess.run(
           unmatched_ignored_column_indices)
@@ -153,7 +158,7 @@ class MatchTest(tf.test.TestCase):
     gathered_tensor = match.gather_based_on_match(input_tensor,
                                                   unmatched_value=100.,
                                                   ignored_value=200.)
-    self.assertEquals(gathered_tensor.dtype, tf.float32)
+    self.assertEqual(gathered_tensor.dtype, tf.float32)
     with self.test_session():
       gathered_tensor_out = gathered_tensor.eval()
     self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
@@ -167,7 +172,7 @@ class MatchTest(tf.test.TestCase):
     gathered_tensor = match.gather_based_on_match(input_tensor,
                                                   unmatched_value=tf.zeros(4),
                                                   ignored_value=tf.zeros(4))
-    self.assertEquals(gathered_tensor.dtype, tf.float32)
+    self.assertEqual(gathered_tensor.dtype, tf.float32)
     with self.test_session():
       gathered_tensor_out = gathered_tensor.eval()
     self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
@@ -181,7 +186,7 @@ class MatchTest(tf.test.TestCase):
     gathered_tensor = match.gather_based_on_match(input_tensor,
                                                   unmatched_value=tf.zeros(4),
                                                   ignored_value=tf.zeros(4))
-    self.assertEquals(gathered_tensor.dtype, tf.float32)
+    self.assertEqual(gathered_tensor.dtype, tf.float32)
     with self.test_session() as sess:
       self.assertTrue(
           all([op.name is not 'Gather' for op in sess.graph.get_operations()]))
diff --git a/research/object_detection/core/post_processing.py b/research/object_detection/core/post_processing.py
index 20775859..5d887a53 100644
--- a/research/object_detection/core/post_processing.py
+++ b/research/object_detection/core/post_processing.py
@@ -12,9 +12,9 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 """Post-processing operations on detected boxes."""
 
+import collections
 import numpy as np
 import tensorflow as tf
 
@@ -271,8 +271,8 @@ def batch_multiclass_non_max_suppression(boxes,
       all images in the batch. If clip_widow is None, all boxes are used to
       perform non-max suppression.
     change_coordinate_frame: Whether to normalize coordinates after clipping
-      relative to clip_window (this can only be set to True if a clip_window
-      is provided)
+      relative to clip_window (this can only be set to True if a clip_window is
+      provided)
     num_valid_boxes: (optional) a Tensor of type `int32`. A 1-D tensor of shape
       [batch_size] representing the number of valid boxes to be considered
       for each image in the batch.  This parameter allows for ignoring zero
@@ -322,7 +322,17 @@ def batch_multiclass_non_max_suppression(boxes,
     raise ValueError('if change_coordinate_frame is True, then a clip_window'
                      'must be specified.')
   original_masks = masks
-  original_additional_fields = additional_fields
+
+  # Create ordered dictionary using the sorted keys from
+  # additional fields to ensure getting the same key value assignment
+  # in _single_image_nms_fn(). The dictionary is thus a sorted version of
+  # additional_fields.
+  if additional_fields is None:
+    ordered_additional_fields = {}
+  else:
+    ordered_additional_fields = collections.OrderedDict(
+        sorted(additional_fields.items(), key=lambda item: item[0]))
+  del additional_fields
   with tf.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):
     boxes_shape = boxes.shape
     batch_size = boxes_shape[0].value
@@ -354,9 +364,6 @@ def batch_multiclass_non_max_suppression(boxes,
     if clip_window.shape.ndims == 1:
       clip_window = tf.tile(tf.expand_dims(clip_window, 0), [batch_size, 1])
 
-    if additional_fields is None:
-      additional_fields = {}
-
     def _single_image_nms_fn(args):
       """Runs NMS on a single image and returns padded output.
 
@@ -403,9 +410,11 @@ def batch_multiclass_non_max_suppression(boxes,
       per_image_scores = args[1]
       per_image_masks = args[2]
       per_image_clip_window = args[3]
+      # Make sure that the order of elements passed in args is aligned with
+      # the iteration order of ordered_additional_fields
       per_image_additional_fields = {
           key: value
-          for key, value in zip(additional_fields, args[4:-1])
+          for key, value in zip(ordered_additional_fields, args[4:-1])
       }
       per_image_num_valid_boxes = args[-1]
       if use_static_shapes:
@@ -459,21 +468,24 @@ def batch_multiclass_non_max_suppression(boxes,
       nmsed_scores = nmsed_boxlist.get_field(fields.BoxListFields.scores)
       nmsed_classes = nmsed_boxlist.get_field(fields.BoxListFields.classes)
       nmsed_masks = nmsed_boxlist.get_field(fields.BoxListFields.masks)
-      nmsed_additional_fields = [
-          nmsed_boxlist.get_field(key) for key in per_image_additional_fields
-      ]
+      nmsed_additional_fields = []
+      # Sorting is needed here to ensure that the values stored in
+      # nmsed_additional_fields are always kept in the same order
+      # across different execution runs.
+      for key in sorted(per_image_additional_fields.keys()):
+        nmsed_additional_fields.append(nmsed_boxlist.get_field(key))
       return ([nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks] +
               nmsed_additional_fields + [num_detections])
 
     num_additional_fields = 0
-    if additional_fields is not None:
-      num_additional_fields = len(additional_fields)
+    if ordered_additional_fields:
+      num_additional_fields = len(ordered_additional_fields)
     num_nmsed_outputs = 4 + num_additional_fields
 
     batch_outputs = shape_utils.static_or_dynamic_map_fn(
         _single_image_nms_fn,
         elems=([boxes, scores, masks, clip_window] +
-               list(additional_fields.values()) + [num_valid_boxes]),
+               list(ordered_additional_fields.values()) + [num_valid_boxes]),
         dtype=(num_nmsed_outputs * [tf.float32] + [tf.int32]),
         parallel_iterations=parallel_iterations)
 
@@ -481,16 +493,23 @@ def batch_multiclass_non_max_suppression(boxes,
     batch_nmsed_scores = batch_outputs[1]
     batch_nmsed_classes = batch_outputs[2]
     batch_nmsed_masks = batch_outputs[3]
-    batch_nmsed_additional_fields = {
-        key: value
-        for key, value in zip(additional_fields, batch_outputs[4:-1])
-    }
+    batch_nmsed_values = batch_outputs[4:-1]
+
+    batch_nmsed_additional_fields = {}
+    if num_additional_fields > 0:
+      # Sort the keys to ensure arranging elements in same order as
+      # in _single_image_nms_fn.
+      batch_nmsed_keys = ordered_additional_fields.keys()
+      for i in range(len(batch_nmsed_keys)):
+        batch_nmsed_additional_fields[
+            batch_nmsed_keys[i]] = batch_nmsed_values[i]
+
     batch_num_detections = batch_outputs[-1]
 
     if original_masks is None:
       batch_nmsed_masks = None
 
-    if original_additional_fields is None:
+    if not ordered_additional_fields:
       batch_nmsed_additional_fields = None
 
     return (batch_nmsed_boxes, batch_nmsed_scores, batch_nmsed_classes,
diff --git a/research/object_detection/core/post_processing_test.py b/research/object_detection/core/post_processing_test.py
index ca8f1fa5..f23886bb 100644
--- a/research/object_detection/core/post_processing_test.py
+++ b/research/object_detection/core/post_processing_test.py
@@ -839,6 +839,9 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
               [[0, 0], [0, 0]]]],
             tf.float32)
     }
+    additional_fields['size'] = tf.constant(
+        [[[[6], [8]], [[0], [2]], [[0], [0]], [[0], [0]]],
+         [[[13], [15]], [[8], [10]], [[10], [12]], [[0], [0]]]], tf.float32)
     score_thresh = 0.1
     iou_thresh = .5
     max_output_size = 4
@@ -865,6 +868,10 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
                                 [[8, 9], [10, 11]],
                                 [[0, 0], [0, 0]]]])
     }
+    exp_nms_additional_fields['size'] = np.array([[[[0], [0]], [[6], [8]],
+                                                   [[0], [0]], [[0], [0]]],
+                                                  [[[10], [12]], [[13], [15]],
+                                                   [[8], [10]], [[0], [0]]]])
 
     (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
      nmsed_additional_fields, num_detections
@@ -1071,6 +1078,11 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
               [[0, 0], [0, 0]]]],
             tf.float32)
     }
+
+    additional_fields['size'] = tf.constant(
+        [[[[7], [9]], [[1], [3]], [[0], [0]], [[0], [0]]],
+         [[[14], [16]], [[9], [11]], [[11], [13]], [[0], [0]]]], tf.float32)
+
     num_valid_boxes = tf.constant([1, 1], tf.int32)
     score_thresh = 0.1
     iou_thresh = .5
@@ -1099,6 +1111,11 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
                                 [[0, 0], [0, 0]]]])
     }
 
+    exp_nms_additional_fields['size'] = np.array([[[[7], [9]], [[0], [0]],
+                                                   [[0], [0]], [[0], [0]]],
+                                                  [[[14], [16]], [[0], [0]],
+                                                   [[0], [0]], [[0], [0]]]])
+
     (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
      nmsed_additional_fields, num_detections
     ) = post_processing.batch_multiclass_non_max_suppression(
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 37c6e889..fc088abc 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -2298,11 +2298,20 @@ def resize_to_range(image,
     return result
 
 
+def _get_image_info(image):
+  """Returns the height, width and number of channels in the image."""
+  image_height = tf.shape(image)[0]
+  image_width = tf.shape(image)[1]
+  num_channels = tf.shape(image)[2]
+  return (image_height, image_width, num_channels)
+
+
 # TODO(alirezafathi): Make sure the static shapes are preserved.
-def resize_to_min_dimension(image, masks=None, min_dimension=600):
+def resize_to_min_dimension(image, masks=None, min_dimension=600,
+                            method=tf.image.ResizeMethod.BILINEAR):
   """Resizes image and masks given the min size maintaining the aspect ratio.
 
-  If one of the image dimensions is smaller that min_dimension, it will scale
+  If one of the image dimensions is smaller than min_dimension, it will scale
   the image such that its smallest dimension is equal to min_dimension.
   Otherwise, will keep the image size as is.
 
@@ -2310,8 +2319,11 @@ def resize_to_min_dimension(image, masks=None, min_dimension=600):
     image: a tensor of size [height, width, channels].
     masks: (optional) a tensors of size [num_instances, height, width].
     min_dimension: minimum image dimension.
+    method: (optional) interpolation method used in resizing. Defaults to
+    BILINEAR.
 
   Returns:
+    An array containing resized_image, resized_masks, and resized_image_shape.
     Note that the position of the resized_image_shape changes based on whether
     masks are present.
     resized_image: A tensor of size [new_height, new_width, channels].
@@ -2327,18 +2339,72 @@ def resize_to_min_dimension(image, masks=None, min_dimension=600):
     raise ValueError('Image should be 3D tensor')
 
   with tf.name_scope('ResizeGivenMinDimension', values=[image, min_dimension]):
-    image_height = tf.shape(image)[0]
-    image_width = tf.shape(image)[1]
-    num_channels = tf.shape(image)[2]
+    (image_height, image_width, num_channels) = _get_image_info(image)
     min_image_dimension = tf.minimum(image_height, image_width)
     min_target_dimension = tf.maximum(min_image_dimension, min_dimension)
     target_ratio = tf.to_float(min_target_dimension) / tf.to_float(
         min_image_dimension)
     target_height = tf.to_int32(tf.to_float(image_height) * target_ratio)
     target_width = tf.to_int32(tf.to_float(image_width) * target_ratio)
-    image = tf.image.resize_bilinear(
-        tf.expand_dims(image, axis=0),
-        size=[target_height, target_width],
+    image = tf.image.resize_images(
+        tf.expand_dims(image, axis=0), size=[target_height, target_width],
+        method=method,
+        align_corners=True)
+    result = [tf.squeeze(image, axis=0)]
+
+    if masks is not None:
+      masks = tf.image.resize_nearest_neighbor(
+          tf.expand_dims(masks, axis=3),
+          size=[target_height, target_width],
+          align_corners=True)
+      result.append(tf.squeeze(masks, axis=3))
+
+    result.append(tf.stack([target_height, target_width, num_channels]))
+    return result
+
+
+def resize_to_max_dimension(image, masks=None, max_dimension=600,
+                            method=tf.image.ResizeMethod.BILINEAR):
+  """Resizes image and masks given the max size maintaining the aspect ratio.
+
+  If one of the image dimensions is greater than max_dimension, it will scale
+  the image such that its largest dimension is equal to max_dimension.
+  Otherwise, will keep the image size as is.
+
+  Args:
+    image: a tensor of size [height, width, channels].
+    masks: (optional) a tensors of size [num_instances, height, width].
+    max_dimension: maximum image dimension.
+    method: (optional) interpolation method used in resizing. Defaults to
+    BILINEAR.
+
+  Returns:
+    An array containing resized_image, resized_masks, and resized_image_shape.
+    Note that the position of the resized_image_shape changes based on whether
+    masks are present.
+    resized_image: A tensor of size [new_height, new_width, channels].
+    resized_masks: If masks is not None, also outputs masks. A 3D tensor of
+      shape [num_instances, new_height, new_width]
+    resized_image_shape: A 1D tensor of shape [3] containing the shape of the
+      resized image.
+
+  Raises:
+    ValueError: if the image is not a 3D tensor.
+  """
+  if len(image.get_shape()) != 3:
+    raise ValueError('Image should be 3D tensor')
+
+  with tf.name_scope('ResizeGivenMaxDimension', values=[image, max_dimension]):
+    (image_height, image_width, num_channels) = _get_image_info(image)
+    max_image_dimension = tf.maximum(image_height, image_width)
+    max_target_dimension = tf.minimum(max_image_dimension, max_dimension)
+    target_ratio = tf.to_float(max_target_dimension) / tf.to_float(
+        max_image_dimension)
+    target_height = tf.to_int32(tf.to_float(image_height) * target_ratio)
+    target_width = tf.to_int32(tf.to_float(image_width) * target_ratio)
+    image = tf.image.resize_images(
+        tf.expand_dims(image, axis=0), size=[target_height, target_width],
+        method=method,
         align_corners=True)
     result = [tf.squeeze(image, axis=0)]
 
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 9bd03e35..030cec59 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -2663,6 +2663,68 @@ class PreprocessorTest(tf.test.TestCase):
         out_image_shape = sess.run(out_image_shape)
         self.assertAllEqual(out_image_shape, expected_shape)
 
+  def testResizeToMaxDimensionTensorShapes(self):
+    """Tests both cases where image should and shouldn't be resized."""
+    in_image_shape_list = [[100, 50, 3], [15, 30, 3]]
+    in_masks_shape_list = [[15, 100, 50], [10, 15, 30]]
+    max_dim = 50
+    expected_image_shape_list = [[50, 25, 3], [15, 30, 3]]
+    expected_masks_shape_list = [[15, 50, 25], [10, 15, 30]]
+
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
+      in_masks = tf.random_uniform(in_masks_shape)
+      out_image, out_masks, _ = preprocessor.resize_to_max_dimension(
+          in_image, in_masks, max_dimension=max_dim)
+      out_image_shape = tf.shape(out_image)
+      out_masks_shape = tf.shape(out_masks)
+
+      with self.test_session() as sess:
+        out_image_shape, out_masks_shape = sess.run(
+            [out_image_shape, out_masks_shape],
+            feed_dict={
+                in_image: np.random.randn(*in_image_shape),
+                in_masks: np.random.randn(*in_masks_shape)
+            })
+        self.assertAllEqual(out_image_shape, expected_image_shape)
+        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+
+  def testResizeToMaxDimensionWithInstanceMasksTensorOfSizeZero(self):
+    """Tests both cases where image should and shouldn't be resized."""
+    in_image_shape_list = [[100, 50, 3], [15, 30, 3]]
+    in_masks_shape_list = [[0, 100, 50], [0, 15, 30]]
+    max_dim = 50
+    expected_image_shape_list = [[50, 25, 3], [15, 30, 3]]
+    expected_masks_shape_list = [[0, 50, 25], [0, 15, 30]]
+
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      in_image = tf.random_uniform(in_image_shape)
+      in_masks = tf.random_uniform(in_masks_shape)
+      out_image, out_masks, _ = preprocessor.resize_to_max_dimension(
+          in_image, in_masks, max_dimension=max_dim)
+      out_image_shape = tf.shape(out_image)
+      out_masks_shape = tf.shape(out_masks)
+
+      with self.test_session() as sess:
+        out_image_shape, out_masks_shape = sess.run(
+            [out_image_shape, out_masks_shape])
+        self.assertAllEqual(out_image_shape, expected_image_shape)
+        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+
+  def testResizeToMaxDimensionRaisesErrorOn4DImage(self):
+    image = tf.random_uniform([1, 200, 300, 3])
+    with self.assertRaises(ValueError):
+      preprocessor.resize_to_max_dimension(image, 500)
+
   def testResizeToMinDimensionTensorShapes(self):
     in_image_shape_list = [[60, 55, 3], [15, 30, 3]]
     in_masks_shape_list = [[15, 60, 55], [10, 15, 30]]
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 664926bc..24254174 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -130,9 +130,13 @@ class TargetAssigner(object):
         representing weights for each element in cls_targets.
       reg_targets: a float32 tensor with shape [num_anchors, box_code_dimension]
       reg_weights: a float32 tensor with shape [num_anchors]
-      match: a matcher.Match object encoding the match between anchors and
-        groundtruth boxes, with rows corresponding to groundtruth boxes
-        and columns corresponding to anchors.
+      match: an int32 tensor of shape [num_anchors] containing result of anchor
+        groundtruth matching. Each position in the tensor indicates an anchor
+        and holds the following meaning:
+        (1) if match[i] >= 0, anchor i is matched with groundtruth match[i].
+        (2) if match[i]=-1, anchor i is marked to be background .
+        (3) if match[i]=-2, anchor i is ignored since it is not background and
+            does not have sufficient overlap to call it a foreground.
 
     Raises:
       ValueError: if anchors or groundtruth_boxes are not of type
@@ -203,7 +207,8 @@ class TargetAssigner(object):
       reg_weights = self._reset_target_shape(reg_weights, num_anchors)
       cls_weights = self._reset_target_shape(cls_weights, num_anchors)
 
-    return cls_targets, cls_weights, reg_targets, reg_weights, match
+    return (cls_targets, cls_weights, reg_targets, reg_weights,
+            match.match_results)
 
   def _reset_target_shape(self, target, num_anchors):
     """Sets the static shape of the target.
@@ -416,12 +421,12 @@ def create_target_assigner(reference, stage=None,
                         negative_class_weight=negative_class_weight)
 
 
-def batch_assign_targets(target_assigner,
-                         anchors_batch,
-                         gt_box_batch,
-                         gt_class_targets_batch,
-                         unmatched_class_label=None,
-                         gt_weights_batch=None):
+def batch_assign(target_assigner,
+                 anchors_batch,
+                 gt_box_batch,
+                 gt_class_targets_batch,
+                 unmatched_class_label=None,
+                 gt_weights_batch=None):
   """Batched assignment of classification and regression targets.
 
   Args:
@@ -450,10 +455,14 @@ def batch_assign_targets(target_assigner,
     batch_reg_targets: a tensor with shape [batch_size, num_anchors,
       box_code_dimension]
     batch_reg_weights: a tensor with shape [batch_size, num_anchors],
-    match_list: a list of matcher.Match objects encoding the match between
-      anchors and groundtruth boxes for each image of the batch,
-      with rows of the Match objects corresponding to groundtruth boxes
-      and columns corresponding to anchors.
+    match: an int32 tensor of shape [batch_size, num_anchors] containing result
+      of anchor groundtruth matching. Each position in the tensor indicates an
+      anchor and holds the following meaning:
+      (1) if match[x, i] >= 0, anchor i is matched with groundtruth match[x, i].
+      (2) if match[x, i]=-1, anchor i is marked to be background .
+      (3) if match[x, i]=-2, anchor i is ignored since it is not background and
+          does not have sufficient overlap to call it a foreground.
+
   Raises:
     ValueError: if input list lengths are inconsistent, i.e.,
       batch_size == len(gt_box_batch) == len(gt_class_targets_batch)
@@ -491,8 +500,55 @@ def batch_assign_targets(target_assigner,
   batch_cls_weights = tf.stack(cls_weights_list)
   batch_reg_targets = tf.stack(reg_targets_list)
   batch_reg_weights = tf.stack(reg_weights_list)
+  batch_match = tf.stack(match_list)
   return (batch_cls_targets, batch_cls_weights, batch_reg_targets,
-          batch_reg_weights, match_list)
+          batch_reg_weights, batch_match)
+
+
+# Assign an alias to avoid large refactor of existing users.
+batch_assign_targets = batch_assign
+
+
+def batch_get_targets(batch_match, groundtruth_tensor_list,
+                      groundtruth_weights_list, unmatched_value,
+                      unmatched_weight):
+  """Returns targets based on anchor-groundtruth box matching results.
+
+  Args:
+    batch_match: An int32 tensor of shape [batch, num_anchors] containing the
+      result of target assignment returned by TargetAssigner.assign(..).
+    groundtruth_tensor_list: A list of groundtruth tensors of shape
+      [num_groundtruth, d_1, d_2, ..., d_k]. The tensors can be of any type.
+    groundtruth_weights_list: A list of weights, one per groundtruth tensor, of
+      shape [num_groundtruth].
+    unmatched_value: A tensor of shape [d_1, d_2, ..., d_k] of the same type as
+      groundtruth tensor containing target value for anchors that remain
+      unmatched.
+    unmatched_weight: Scalar weight to assign to anchors that remain unmatched.
+
+  Returns:
+    targets: A tensor of shape [batch, num_anchors, d_1, d_2, ..., d_k]
+      containing targets for anchors.
+    weights: A float tensor of shape [batch, num_anchors] containing the weights
+      to assign to each target.
+  """
+  match_list = tf.unstack(batch_match)
+  targets_list = []
+  weights_list = []
+  for match_tensor, groundtruth_tensor, groundtruth_weight in zip(
+      match_list, groundtruth_tensor_list, groundtruth_weights_list):
+    match_object = mat.Match(match_tensor)
+    targets = match_object.gather_based_on_match(
+        groundtruth_tensor,
+        unmatched_value=unmatched_value,
+        ignored_value=unmatched_value)
+    targets_list.append(targets)
+    weights = match_object.gather_based_on_match(
+        groundtruth_weight,
+        unmatched_value=unmatched_weight,
+        ignored_value=tf.zeros_like(unmatched_weight))
+    weights_list.append(weights)
+  return tf.stack(targets_list), tf.stack(weights_list)
 
 
 def batch_assign_confidences(target_assigner,
@@ -548,10 +604,13 @@ def batch_assign_confidences(target_assigner,
     batch_reg_targets: a tensor with shape [batch_size, num_anchors,
       box_code_dimension]
     batch_reg_weights: a tensor with shape [batch_size, num_anchors],
-    match_list: a list of matcher.Match objects encoding the match between
-      anchors and groundtruth boxes for each image of the batch,
-      with rows of the Match objects corresponding to groundtruth boxes
-      and columns corresponding to anchors.
+    match: an int32 tensor of shape [batch_size, num_anchors] containing result
+      of anchor groundtruth matching. Each position in the tensor indicates an
+      anchor and holds the following meaning:
+      (1) if match[x, i] >= 0, anchor i is matched with groundtruth match[x, i].
+      (2) if match[x, i]=-1, anchor i is marked to be background .
+      (3) if match[x, i]=-2, anchor i is ignored since it is not background and
+          does not have sufficient overlap to call it a foreground.
 
   Raises:
     ValueError: if input list lengths are inconsistent, i.e.,
@@ -634,5 +693,6 @@ def batch_assign_confidences(target_assigner,
   batch_cls_weights = tf.stack(cls_weights_list)
   batch_reg_targets = tf.stack(reg_targets_list)
   batch_reg_weights = tf.stack(reg_weights_list)
+  batch_match = tf.stack(match_list)
   return (batch_cls_targets, batch_cls_weights, batch_reg_targets,
-          batch_reg_weights, match_list)
+          batch_reg_weights, batch_match)
diff --git a/research/object_detection/core/target_assigner_test.py b/research/object_detection/core/target_assigner_test.py
index 443c33aa..1ac67f2b 100644
--- a/research/object_detection/core/target_assigner_test.py
+++ b/research/object_detection/core/target_assigner_test.py
@@ -713,9 +713,6 @@ class BatchTargetAssignerTest(test_case.TestCase):
     groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],
                                      [0.015789, 0.0985, 0.55789, 0.3842]],
                                     dtype=np.float32)
-    class_targets1 = np.array([[0, 1, 0, 0]], dtype=np.float32)
-    class_targets2 = np.array([[0, 0, 0, 1],
-                               [0, 0, 1, 0]], dtype=np.float32)
     class_targets1 = np.array([[[0, 1, 1],
                                 [1, 1, 0]]], dtype=np.float32)
     class_targets2 = np.array([[[0, 1, 1],
@@ -821,6 +818,63 @@ class BatchTargetAssignerTest(test_case.TestCase):
     self.assertAllClose(reg_weights_out, exp_reg_weights)
 
 
+class BatchGetTargetsTest(test_case.TestCase):
+
+  def test_scalar_targets(self):
+    batch_match = np.array([[1, 0, 1],
+                            [-2, -1, 1]], dtype=np.int32)
+    groundtruth_tensors_list = np.array([[11, 12], [13, 14]], dtype=np.int32)
+    groundtruth_weights_list = np.array([[1.0, 1.0], [1.0, 0.5]],
+                                        dtype=np.float32)
+    unmatched_value = np.array(99, dtype=np.int32)
+    unmatched_weight = np.array(0.0, dtype=np.float32)
+
+    def graph_fn(batch_match, groundtruth_tensors_list,
+                 groundtruth_weights_list, unmatched_value, unmatched_weight):
+      targets, weights = targetassigner.batch_get_targets(
+          batch_match, tf.unstack(groundtruth_tensors_list),
+          tf.unstack(groundtruth_weights_list),
+          unmatched_value, unmatched_weight)
+      return (targets, weights)
+
+    (targets_np, weights_np) = self.execute(graph_fn, [
+        batch_match, groundtruth_tensors_list, groundtruth_weights_list,
+        unmatched_value, unmatched_weight
+    ])
+    self.assertAllEqual([[12, 11, 12],
+                         [99, 99, 14]], targets_np)
+    self.assertAllClose([[1.0, 1.0, 1.0],
+                         [0.0, 0.0, 0.5]], weights_np)
+
+  def test_1d_targets(self):
+    batch_match = np.array([[1, 0, 1],
+                            [-2, -1, 1]], dtype=np.int32)
+    groundtruth_tensors_list = np.array([[[11, 12], [12, 13]],
+                                         [[13, 14], [14, 15]]],
+                                        dtype=np.float32)
+    groundtruth_weights_list = np.array([[1.0, 1.0], [1.0, 0.5]],
+                                        dtype=np.float32)
+    unmatched_value = np.array([99, 99], dtype=np.float32)
+    unmatched_weight = np.array(0.0, dtype=np.float32)
+
+    def graph_fn(batch_match, groundtruth_tensors_list,
+                 groundtruth_weights_list, unmatched_value, unmatched_weight):
+      targets, weights = targetassigner.batch_get_targets(
+          batch_match, tf.unstack(groundtruth_tensors_list),
+          tf.unstack(groundtruth_weights_list),
+          unmatched_value, unmatched_weight)
+      return (targets, weights)
+
+    (targets_np, weights_np) = self.execute(graph_fn, [
+        batch_match, groundtruth_tensors_list, groundtruth_weights_list,
+        unmatched_value, unmatched_weight
+    ])
+    self.assertAllClose([[[12, 13], [11, 12], [12, 13]],
+                         [[99, 99], [99, 99], [14, 15]]], targets_np)
+    self.assertAllClose([[1.0, 1.0, 1.0],
+                         [0.0, 0.0, 0.5]], weights_np)
+
+
 class BatchTargetAssignConfidencesTest(test_case.TestCase):
 
   def _get_target_assigner(self):
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index e761f7f1..f58aa8d7 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -18,7 +18,6 @@ import os
 import numpy as np
 import tensorflow as tf
 
-from tensorflow.python.framework import test_util
 from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
 from object_detection.protos import input_reader_pb2
@@ -257,7 +256,6 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(expected_boxes,
                         tensor_dict[fields.InputDataFields.groundtruth_boxes])
 
-  @test_util.enable_c_shapes
   def testDecodeKeypoint(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -346,7 +344,6 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllClose(tensor_dict[fields.InputDataFields.groundtruth_weights],
                         np.ones(2, dtype=np.float32))
 
-  @test_util.enable_c_shapes
   def testDecodeObjectLabel(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -669,7 +666,6 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual([3, 1],
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
-  @test_util.enable_c_shapes
   def testDecodeObjectArea(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -696,7 +692,6 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(object_area,
                         tensor_dict[fields.InputDataFields.groundtruth_area])
 
-  @test_util.enable_c_shapes
   def testDecodeObjectIsCrowd(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -725,7 +720,6 @@ class TfExampleDecoderTest(tf.test.TestCase):
         [bool(item) for item in object_is_crowd],
         tensor_dict[fields.InputDataFields.groundtruth_is_crowd])
 
-  @test_util.enable_c_shapes
   def testDecodeObjectDifficult(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -754,7 +748,6 @@ class TfExampleDecoderTest(tf.test.TestCase):
         [bool(item) for item in object_difficult],
         tensor_dict[fields.InputDataFields.groundtruth_difficult])
 
-  @test_util.enable_c_shapes
   def testDecodeObjectGroupOf(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -809,7 +802,6 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(object_weights,
                         tensor_dict[fields.InputDataFields.groundtruth_weights])
 
-  @test_util.enable_c_shapes
   def testDecodeInstanceSegmentation(self):
     num_instances = 4
     image_height = 5
diff --git a/research/object_detection/dockerfiles/android/README.md b/research/object_detection/dockerfiles/android/README.md
index 107a0db8..557b1adb 100644
--- a/research/object_detection/dockerfiles/android/README.md
+++ b/research/object_detection/dockerfiles/android/README.md
@@ -14,7 +14,7 @@ A couple words of warning:
    the container. When running through the tutorial,
    **do not close the container**.
 2. To be able to deploy the [Android app](
-   https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app)
+   https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)
    (which you will build at the end of the tutorial),
    you will need to kill any instances of `adb` running on the host machine. You
    can accomplish this by closing all instances of Android Studio, and then
diff --git a/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md b/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
index a86c3792..6f4e3f91 100644
--- a/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
+++ b/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
@@ -94,43 +94,41 @@ bazel run --config=opt tensorflow/lite/toco:toco -- \
 
 # Running our model on Android
 
-To run our TensorFlow Lite model on device, we will need to install the Android
-NDK and SDK. The current recommended Android NDK version is 14b and can be found
-on the [NDK
-Archives](https://developer.android.com/ndk/downloads/older_releases.html#ndk-14b-downloads)
-page. Android SDK and build tools can be [downloaded
-separately](https://developer.android.com/tools/revisions/build-tools.html) or
-used as part of [Android
-Studio](https://developer.android.com/studio/index.html). To build the
-TensorFlow Lite Android demo, build tools require API >= 23 (but it will run on
-devices with API >= 21). Additional details are available on the [TensorFlow
-Lite Android App
-page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo/README.md).
+To run our TensorFlow Lite model on device, we will use Android Studio to build
+and run the TensorFlow Lite detection example with the new model. The example is
+found in the
+[TensorFlow examples repository](https://github.com/tensorflow/examples) under
+`/lite/examples/object_detection`. The example can be built with
+[Android Studio](https://developer.android.com/studio/index.html), and requires
+the
+[Android SDK with build tools](https://developer.android.com/tools/revisions/build-tools.html)
+that support API >= 21. Additional details are available on the
+[TensorFlow Lite example page](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android).
 
 Next we need to point the app to our new detect.tflite file and give it the
 names of our new labels. Specifically, we will copy our TensorFlow Lite
 flatbuffer to the app assets directory with the following command:
 
 ```shell
+mkdir $TF_EXAMPLES/lite/examples/object_detection/android/app/src/main/assets
 cp /tmp/tflite/detect.tflite \
-//tensorflow/lite/examples/android/app/src/main/assets
+  $TF_EXAMPLES/lite/examples/object_detection/android/app/src/main/assets
 ```
 
-You will also need to copy your new labelmap labels_list.txt to the assets
+You will also need to copy your new labelmap labelmap.txt to the assets
 directory.
 
-We will now edit the BUILD file to point to this new model. First, open the
-BUILD file tensorflow/lite/examples/android/BUILD. Then find the assets
-section, and replace the line @tflite_mobilenet_ssd_quant//:detect.tflite
-(which by default points to a COCO pretrained model) with the path to your new
-TFLite model
-//tensorflow/lite/examples/android/app/src/main/assets:detect.tflite.
-Finally, change the last line in assets section to use the new label map as
-well.
-
-We will also need to tell our app to use the new label map. In order to do this,
-open up the
-tensorflow/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java
+We will now edit the gradle build file to use these assets. First, open the
+`build.gradle` file
+`$TF_EXAMPLES/lite/examples/object_detection/android/app/build.gradle`. Comment
+out the model download script to avoid your assets being overwritten: `// apply
+from:'download_model.gradle'` ```
+
+If your model is named `detect.tflite`, and your labels file `labelmap.txt`, the
+example will use them automatically as long as they've been properly copied into
+the base assets directory. If you need to use a custom path or filename, open up
+the
+$TF_EXAMPLES/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java
 file in a text editor and find the definition of TF_OD_API_LABELS_FILE. Update
 this path to point to your new label map file:
 "file:///android_asset/labels_list.txt". Note that if your model is quantized,
@@ -144,20 +142,6 @@ DetectorActivity.java should now look as follows for a quantized model:
   private static final String TF_OD_API_LABELS_FILE = "file:///android_asset/labels_list.txt";
 ```
 
-Once youve copied the TensorFlow Lite file and edited your BUILD and
-DetectorActivity.java files, you can build the demo app, run this bazel command
-from the tensorflow directory:
-
-```shell
- bazel build -c opt --config=android_arm{,64} --cxxopt='--std=c++11'
-"//tensorflow/lite/examples/android:tflite_demo"
-```
-
-Now install the demo on a
-[debug-enabled](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#install)
-Android phone via [Android Debug
-Bridge](https://developer.android.com/studio/command-line/adb) (adb):
-
-```shell
-adb install bazel-bin/tensorflow/lite/examples/android/tflite_demo.apk
-```
+Once youve copied the TensorFlow Lite model and edited the gradle build script
+to not use the downloaded assets, you can build and deploy the app using the
+usual Android Studio build process.
diff --git a/research/object_detection/g3doc/tpu_exporters.md b/research/object_detection/g3doc/tpu_exporters.md
new file mode 100644
index 00000000..03683590
--- /dev/null
+++ b/research/object_detection/g3doc/tpu_exporters.md
@@ -0,0 +1,35 @@
+# Object Detection TPU Inference Exporter
+
+This package contains SavedModel Exporter for TPU Inference of object detection
+models.
+
+## Usage
+
+This Exporter is intended for users who have trained models with CPUs / GPUs,
+but would like to use them for inference on TPU without changing their code or
+re-training their models.
+
+Users are assumed to have:
+
++   `PIPELINE_CONFIG`: A pipeline_pb2.TrainEvalPipelineConfig config file;
++   `CHECKPOINT`: A model checkpoint trained on any device;
+
+and need to correctly set:
+
++   `EXPORT_DIR`: Path to export SavedModel;
++   `INPUT_PLACEHOLDER`: Name of input placeholder in model's signature_def_map;
++   `INPUT_TYPE`: Type of input node, which can be one of 'image_tensor',
+    'encoded_image_string_tensor', or 'tf_example';
++   `USE_BFLOAT16`: Whether to use bfloat16 instead of float32 on TPU.
+
+The model can be exported with:
+
+```
+python object_detection/tpu_exporters/export_saved_model_tpu.py \
+    --pipeline_config_file=<PIPELINE_CONFIG> \
+    --ckpt_path=<CHECKPOINT> \
+    --export_dir=<EXPORT_DIR> \
+    --input_placeholder_name=<INPUT_PLACEHOLDER> \
+    --input_type=<INPUT_TYPE> \
+    --use_bfloat16=<USE_BFLOAT16>
+```
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 916afc70..981913c6 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -43,6 +43,7 @@ SERVING_FED_EXAMPLE_KEY = 'serialized_example'
 # A map of names to methods that help build the input pipeline.
 INPUT_BUILDER_UTIL_MAP = {
     'dataset_build': dataset_builder.build,
+    'model_build': model_builder.build,
 }
 
 
@@ -152,7 +153,7 @@ def transform_input_data(tensor_dict,
   if use_multiclass_scores:
     tensor_dict[fields.InputDataFields.groundtruth_classes] = tensor_dict[
         fields.InputDataFields.multiclass_scores]
-    tensor_dict.pop(fields.InputDataFields.multiclass_scores, None)
+  tensor_dict.pop(fields.InputDataFields.multiclass_scores, None)
 
   if fields.InputDataFields.groundtruth_confidences in tensor_dict:
     groundtruth_confidences = tensor_dict[
@@ -498,11 +499,13 @@ def create_train_input_fn(train_config, train_input_config,
       data_augmentation_fn = functools.partial(
           augment_input_data,
           data_augmentation_options=data_augmentation_options)
-      model = model_builder.build(model_config, is_training=True)
+
+      model_preprocess_fn = INPUT_BUILDER_UTIL_MAP['model_build'](
+          model_config, is_training=True).preprocess
       image_resizer_config = config_util.get_image_resizer_config(model_config)
       image_resizer_fn = image_resizer_builder.build(image_resizer_config)
       transform_data_fn = functools.partial(
-          transform_input_data, model_preprocess_fn=model.preprocess,
+          transform_input_data, model_preprocess_fn=model_preprocess_fn,
           image_resizer_fn=image_resizer_fn,
           num_classes=config_util.get_number_of_classes(model_config),
           data_augmentation_fn=data_augmentation_fn,
@@ -593,12 +596,14 @@ def create_eval_input_fn(eval_config, eval_input_config, model_config):
     def transform_and_pad_input_data_fn(tensor_dict):
       """Combines transform and pad operation."""
       num_classes = config_util.get_number_of_classes(model_config)
-      model = model_builder.build(model_config, is_training=False)
+      model_preprocess_fn = INPUT_BUILDER_UTIL_MAP['model_build'](
+          model_config, is_training=False).preprocess
+
       image_resizer_config = config_util.get_image_resizer_config(model_config)
       image_resizer_fn = image_resizer_builder.build(image_resizer_config)
 
       transform_data_fn = functools.partial(
-          transform_input_data, model_preprocess_fn=model.preprocess,
+          transform_input_data, model_preprocess_fn=model_preprocess_fn,
           image_resizer_fn=image_resizer_fn,
           num_classes=num_classes,
           data_augmentation_fn=None,
@@ -643,12 +648,14 @@ def create_predict_input_fn(model_config, predict_input_config):
     example = tf.placeholder(dtype=tf.string, shape=[], name='tf_example')
 
     num_classes = config_util.get_number_of_classes(model_config)
-    model = model_builder.build(model_config, is_training=False)
+    model_preprocess_fn = INPUT_BUILDER_UTIL_MAP['model_build'](
+        model_config, is_training=False).preprocess
+
     image_resizer_config = config_util.get_image_resizer_config(model_config)
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
 
     transform_fn = functools.partial(
-        transform_input_data, model_preprocess_fn=model.preprocess,
+        transform_input_data, model_preprocess_fn=model_preprocess_fn,
         image_resizer_fn=image_resizer_fn,
         num_classes=num_classes,
         data_augmentation_fn=None)
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 22ece5ae..57958446 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -98,6 +98,7 @@ import tensorflow as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.builders import box_predictor_builder
+from object_detection.builders import hyperparams_builder
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import box_predictor
@@ -110,6 +111,8 @@ from object_detection.utils import shape_utils
 
 slim = tf.contrib.slim
 
+_UNINITIALIZED_FEATURE_EXTRACTOR = '__uninitialized__'
+
 
 class FasterRCNNFeatureExtractor(object):
   """Faster R-CNN Feature Extractor definition."""
@@ -216,6 +219,71 @@ class FasterRCNNFeatureExtractor(object):
     return variables_to_restore
 
 
+class FasterRCNNKerasFeatureExtractor(object):
+  """Keras-based Faster R-CNN Feature Extractor definition."""
+
+  def __init__(self,
+               is_training,
+               first_stage_features_stride,
+               batch_norm_trainable=False,
+               weight_decay=0.0):
+    """Constructor.
+
+    Args:
+      is_training: A boolean indicating whether the training version of the
+        computation graph should be constructed.
+      first_stage_features_stride: Output stride of extracted RPN feature map.
+      batch_norm_trainable: Whether to update batch norm parameters during
+        training or not. When training with a relative large batch size
+        (e.g. 8), it could be desirable to enable batch norm update.
+      weight_decay: float weight decay for feature extractor (default: 0.0).
+    """
+    self._is_training = is_training
+    self._first_stage_features_stride = first_stage_features_stride
+    self._train_batch_norm = (batch_norm_trainable and is_training)
+    self._weight_decay = weight_decay
+
+  @abc.abstractmethod
+  def preprocess(self, resized_inputs):
+    """Feature-extractor specific preprocessing (minus image resizing)."""
+    pass
+
+  @abc.abstractmethod
+  def get_proposal_feature_extractor_model(self, name):
+    """Get model that extracts first stage RPN features, to be overridden."""
+    pass
+
+  @abc.abstractmethod
+  def get_box_classifier_feature_extractor_model(self, name):
+    """Get model that extracts second stage box classifier features."""
+    pass
+
+  def restore_from_classification_checkpoint_fn(
+      self,
+      first_stage_feature_extractor_scope,
+      second_stage_feature_extractor_scope):
+    """Returns a map of variables to load from a foreign checkpoint.
+
+    Args:
+      first_stage_feature_extractor_scope: A scope name for the first stage
+        feature extractor.
+      second_stage_feature_extractor_scope: A scope name for the second stage
+        feature extractor.
+
+    Returns:
+      A dict mapping variable names (to load from a checkpoint) to variables in
+      the model graph.
+    """
+    variables_to_restore = {}
+    for variable in tf.global_variables():
+      for scope_name in [first_stage_feature_extractor_scope,
+                         second_stage_feature_extractor_scope]:
+        if variable.op.name.startswith(scope_name):
+          var_name = variable.op.name.replace(scope_name + '/', '')
+          variables_to_restore[var_name] = variable
+    return variables_to_restore
+
+
 class FasterRCNNMetaArch(model.DetectionModel):
   """Faster R-CNN Meta-architecture definition."""
 
@@ -256,7 +324,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                add_summaries=True,
                clip_anchors_to_image=False,
                use_static_shapes=False,
-               resize_masks=True):
+               resize_masks=True,
+               freeze_batchnorm=False):
     """FasterRCNNMetaArch Constructor.
 
     Args:
@@ -296,9 +365,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
         denser resolutions.  The atrous rate is used to compensate for the
         denser feature maps by using an effectively larger receptive field.
         (This should typically be set to 1).
-      first_stage_box_predictor_arg_scope_fn: A function to construct tf-slim
-        arg_scope for conv2d, separable_conv2d and fully_connected ops for the
-        RPN box predictor.
+      first_stage_box_predictor_arg_scope_fn: Either a
+        Keras layer hyperparams object or a function to construct tf-slim
+        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used
+        for the RPN box predictor. If it is a keras hyperparams object the
+        RPN box predictor will be a Keras model. If it is a function to
+        construct an arg scope it will be a tf-slim box predictor.
       first_stage_box_predictor_kernel_size: Kernel size to use for the
         convolution op just prior to RPN box predictions.
       first_stage_box_predictor_depth: Output depth for the convolution op
@@ -378,6 +450,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
         guarantees.
       resize_masks: Indicates whether the masks presend in the groundtruth
         should be resized in the model with `image_resizer_fn`
+      freeze_batchnorm: Whether to freeze batch norm parameters in the first
+        stage box predictor during training or not. When training with a small
+        batch size (e.g. 1), it is desirable to freeze batch norm update and
+        use pretrained batch norm params.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at
@@ -398,6 +474,19 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._image_resizer_fn = image_resizer_fn
     self._resize_masks = resize_masks
     self._feature_extractor = feature_extractor
+    if isinstance(feature_extractor, FasterRCNNKerasFeatureExtractor):
+      # We delay building the feature extractor until it is used,
+      # to avoid creating the variables when a model is built just for data
+      # preprocessing. (This prevents a subtle bug where variable names are
+      # mismatched across workers, causing only one worker to be able to train)
+      self._feature_extractor_for_proposal_features = (
+          _UNINITIALIZED_FEATURE_EXTRACTOR)
+      self._feature_extractor_for_box_classifier_features = (
+          _UNINITIALIZED_FEATURE_EXTRACTOR)
+    else:
+      self._feature_extractor_for_proposal_features = None
+      self._feature_extractor_for_box_classifier_features = None
+
     self._number_of_stages = number_of_stages
 
     self._proposal_target_assigner = first_stage_target_assigner
@@ -408,25 +497,82 @@ class FasterRCNNMetaArch(model.DetectionModel):
     # (First stage) Region proposal network parameters
     self._first_stage_anchor_generator = first_stage_anchor_generator
     self._first_stage_atrous_rate = first_stage_atrous_rate
-    self._first_stage_box_predictor_arg_scope_fn = (
-        first_stage_box_predictor_arg_scope_fn)
+    self._first_stage_box_predictor_depth = first_stage_box_predictor_depth
     self._first_stage_box_predictor_kernel_size = (
         first_stage_box_predictor_kernel_size)
-    self._first_stage_box_predictor_depth = first_stage_box_predictor_depth
     self._first_stage_minibatch_size = first_stage_minibatch_size
     self._first_stage_sampler = first_stage_sampler
-    self._first_stage_box_predictor = (
-        box_predictor_builder.build_convolutional_box_predictor(
-            is_training=self._is_training,
-            num_classes=1,
-            conv_hyperparams_fn=self._first_stage_box_predictor_arg_scope_fn,
-            use_dropout=False,
-            dropout_keep_prob=1.0,
-            box_code_size=self._box_coder.code_size,
-            kernel_size=1,
-            num_layers_before_predictor=0,
-            min_depth=0,
-            max_depth=0))
+    if isinstance(first_stage_box_predictor_arg_scope_fn,
+                  hyperparams_builder.KerasLayerHyperparams):
+      num_anchors_per_location = (
+          self._first_stage_anchor_generator.num_anchors_per_location())
+      if len(num_anchors_per_location) != 1:
+        raise ValueError('anchor_generator is expected to generate anchors '
+                         'corresponding to a single feature map.')
+      conv_hyperparams = (
+          first_stage_box_predictor_arg_scope_fn)
+      self._first_stage_box_predictor_first_conv = (
+          tf.keras.Sequential([
+              tf.keras.layers.Conv2D(
+                  self._first_stage_box_predictor_depth,
+                  kernel_size=[self._first_stage_box_predictor_kernel_size,
+                               self._first_stage_box_predictor_kernel_size],
+                  dilation_rate=self._first_stage_atrous_rate,
+                  padding='SAME',
+                  name='RPNConv',
+                  **conv_hyperparams.params()),
+              conv_hyperparams.build_batch_norm(
+                  (self._is_training and not freeze_batchnorm),
+                  name='RPNBatchNorm'),
+              tf.keras.layers.Lambda(
+                  tf.nn.relu6,
+                  name='RPNActivation')
+          ], name='FirstStageRPNFeatures'))
+      self._first_stage_box_predictor = (
+          box_predictor_builder.build_convolutional_keras_box_predictor(
+              is_training=self._is_training,
+              num_classes=1,
+              conv_hyperparams=conv_hyperparams,
+              freeze_batchnorm=freeze_batchnorm,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=num_anchors_per_location,
+              use_dropout=False,
+              dropout_keep_prob=1.0,
+              box_code_size=self._box_coder.code_size,
+              kernel_size=1,
+              num_layers_before_predictor=0,
+              min_depth=0,
+              max_depth=0,
+              name=self.first_stage_box_predictor_scope))
+    else:
+      self._first_stage_box_predictor_arg_scope_fn = (
+          first_stage_box_predictor_arg_scope_fn)
+      def rpn_box_predictor_feature_extractor(rpn_features_to_crop):
+        with slim.arg_scope(self._first_stage_box_predictor_arg_scope_fn()):
+          reuse = tf.get_variable_scope().reuse
+          return slim.conv2d(
+              rpn_features_to_crop,
+              self._first_stage_box_predictor_depth,
+              kernel_size=[self._first_stage_box_predictor_kernel_size,
+                           self._first_stage_box_predictor_kernel_size],
+              rate=self._first_stage_atrous_rate,
+              activation_fn=tf.nn.relu6,
+              scope='Conv',
+              reuse=reuse)
+      self._first_stage_box_predictor_first_conv = (
+          rpn_box_predictor_feature_extractor)
+      self._first_stage_box_predictor = (
+          box_predictor_builder.build_convolutional_box_predictor(
+              is_training=self._is_training,
+              num_classes=1,
+              conv_hyperparams_fn=self._first_stage_box_predictor_arg_scope_fn,
+              use_dropout=False,
+              dropout_keep_prob=1.0,
+              box_code_size=self._box_coder.code_size,
+              kernel_size=1,
+              num_layers_before_predictor=0,
+              min_depth=0,
+              max_depth=0))
 
     self._first_stage_nms_fn = first_stage_non_max_suppression_fn
     self._first_stage_max_proposals = first_stage_max_proposals
@@ -444,6 +590,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._initial_crop_size = initial_crop_size
     self._maxpool_kernel_size = maxpool_kernel_size
     self._maxpool_stride = maxpool_stride
+    # If max pooling is to be used, build the layer
+    if maxpool_kernel_size:
+      self._maxpool_layer = tf.keras.layers.MaxPooling2D(
+          [self._maxpool_kernel_size, self._maxpool_kernel_size],
+          strides=self._maxpool_stride,
+          name='MaxPool2D')
 
     self._mask_rcnn_box_predictor = second_stage_mask_rcnn_box_predictor
 
@@ -469,6 +621,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     if self._number_of_stages <= 0 or self._number_of_stages > 3:
       raise ValueError('Number of stages should be a value in {1, 2, 3}.')
+    self._batched_prediction_tensor_names = []
 
   @property
   def first_stage_feature_extractor_scope(self):
@@ -510,6 +663,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
       raise RuntimeError('anchors should be a BoxList object, but is not.')
     return self._anchors
 
+  @property
+  def batched_prediction_tensor_names(self):
+    if not self._batched_prediction_tensor_names:
+      raise RuntimeError('Must call predict() method to get batched prediction '
+                         'tensor names.')
+    return self._batched_prediction_tensor_names
+
   def preprocess(self, inputs):
     """Feature-extractor specific preprocessing.
 
@@ -567,6 +727,17 @@ class FasterRCNNMetaArch(model.DetectionModel):
                                         clip_heights, clip_widths], axis=1))
     return clip_window
 
+  def _proposal_postprocess(self, rpn_box_encodings,
+                            rpn_objectness_predictions_with_background, anchors,
+                            image_shape, true_image_shapes):
+    """Wraps over FasterRCNNMetaArch._postprocess_rpn()."""
+    image_shape_2d = self._image_batch_shape_2d(image_shape)
+    proposal_boxes_normalized, _, num_proposals, _, _ = \
+        self._postprocess_rpn(
+            rpn_box_encodings, rpn_objectness_predictions_with_background,
+            anchors, image_shape_2d, true_image_shapes)
+    return proposal_boxes_normalized, num_proposals
+
   def predict(self, preprocessed_inputs, true_image_shapes):
     """Predicts unpostprocessed tensors from input tensor.
 
@@ -643,6 +814,60 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Raises:
       ValueError: If `predict` is called before `preprocess`.
     """
+    prediction_dict = self._predict_first_stage(preprocessed_inputs)
+
+    if self._number_of_stages >= 2:
+      # If mixed-precision training on TPU is enabled, rpn_box_encodings and
+      # rpn_objectness_predictions_with_background are bfloat16 tensors.
+      # Considered prediction results, they need to be casted to float32
+      # tensors for correct postprocess_rpn computation in predict_second_stage.
+      prediction_dict.update(
+          self._predict_second_stage(
+              tf.to_float(prediction_dict['rpn_box_encodings']),
+              tf.to_float(
+                  prediction_dict['rpn_objectness_predictions_with_background']
+              ), prediction_dict['rpn_features_to_crop'],
+              prediction_dict['anchors'], prediction_dict['image_shape'],
+              true_image_shapes))
+
+    if self._number_of_stages == 3:
+      prediction_dict = self._predict_third_stage(prediction_dict,
+                                                  true_image_shapes)
+
+    self._batched_prediction_tensor_names = [
+        x for x in prediction_dict if x not in ('image_shape', 'anchors')
+    ]
+    return prediction_dict
+
+  def _predict_first_stage(self, preprocessed_inputs):
+    """First stage of prediction.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      prediction_dict: a dictionary holding "raw" prediction tensors:
+        1) rpn_box_predictor_features: A 4-D float32 tensor with shape
+          [batch_size, height, width, depth] to be used for predicting proposal
+          boxes and corresponding objectness scores.
+        2) rpn_features_to_crop: A 4-D float32 tensor with shape
+          [batch_size, height, width, depth] representing image features to crop
+          using the proposal boxes predicted by the RPN.
+        3) image_shape: a 1-D tensor of shape [4] representing the input
+          image shape.
+        4) rpn_box_encodings:  3-D float tensor of shape
+          [batch_size, num_anchors, self._box_coder.code_size] containing
+          predicted boxes.
+        5) rpn_objectness_predictions_with_background: 3-D float tensor of shape
+          [batch_size, num_anchors, 2] containing class
+          predictions (logits) for each of the anchors.  Note that this
+          tensor *includes* background class predictions (at class index 0).
+        6) anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors
+          for the first stage RPN (in absolute coordinates).  Note that
+          `num_anchors` can differ depending on whether the model is created in
+          training or inference mode.
+    """
     (rpn_box_predictor_features, rpn_features_to_crop, anchors_boxlist,
      image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
     (rpn_box_encodings, rpn_objectness_predictions_with_background
@@ -667,30 +892,19 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     self._anchors = anchors_boxlist
     prediction_dict = {
-        'rpn_box_predictor_features': rpn_box_predictor_features,
-        'rpn_features_to_crop': rpn_features_to_crop,
-        'image_shape': image_shape,
-        'rpn_box_encodings': rpn_box_encodings,
+        'rpn_box_predictor_features':
+            rpn_box_predictor_features,
+        'rpn_features_to_crop':
+            rpn_features_to_crop,
+        'image_shape':
+            image_shape,
+        'rpn_box_encodings':
+            rpn_box_encodings,
         'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'anchors': self._anchors.get()
+            rpn_objectness_predictions_with_background,
+        'anchors':
+            anchors_boxlist.data['boxes'],
     }
-
-    if self._number_of_stages >= 2:
-      # If mixed-precision training on TPU is enabled, rpn_box_encodings and
-      # rpn_objectness_predictions_with_background are bfloat16 tensors.
-      # Considered prediction results, they need to be casted to float32
-      # tensors for correct postprocess_rpn computation in predict_second_stage.
-      prediction_dict.update(self._predict_second_stage(
-          tf.to_float(rpn_box_encodings),
-          tf.to_float(rpn_objectness_predictions_with_background),
-          rpn_features_to_crop,
-          self._anchors.get(), image_shape, true_image_shapes))
-
-    if self._number_of_stages == 3:
-      prediction_dict = self._predict_third_stage(
-          prediction_dict, true_image_shapes)
-
     return prediction_dict
 
   def _image_batch_shape_2d(self, image_batch_shape_1d):
@@ -769,11 +983,55 @@ class FasterRCNNMetaArch(model.DetectionModel):
         6) box_classifier_features: a 4-D float32 or bfloat16 tensor
           representing the features for each proposal.
     """
-    image_shape_2d = self._image_batch_shape_2d(image_shape)
-    proposal_boxes_normalized, _, num_proposals, _, _ = self._postprocess_rpn(
-        rpn_box_encodings, rpn_objectness_predictions_with_background,
-        anchors, image_shape_2d, true_image_shapes)
+    proposal_boxes_normalized, num_proposals = self._proposal_postprocess(
+        rpn_box_encodings, rpn_objectness_predictions_with_background, anchors,
+        image_shape, true_image_shapes)
+    prediction_dict = self._box_prediction(rpn_features_to_crop,
+                                           proposal_boxes_normalized,
+                                           image_shape)
+    prediction_dict['num_proposals'] = num_proposals
+    return prediction_dict
+
+  def _box_prediction(self, rpn_features_to_crop, proposal_boxes_normalized,
+                      image_shape):
+    """Predicts the output tensors from second stage of Faster R-CNN.
 
+    Args:
+      rpn_features_to_crop: A 4-D float32 or bfloat16 tensor with shape
+        [batch_size, height, width, depth] representing image features to crop
+        using the proposal boxes predicted by the RPN.
+      proposal_boxes_normalized: A float tensor with shape [batch_size,
+        max_num_proposals, 4] representing the (potentially zero padded)
+        proposal boxes for all images in the batch.  These boxes are represented
+        as normalized coordinates.
+      image_shape: A 1D int32 tensors of size [4] containing the image shape.
+
+    Returns:
+      prediction_dict: a dictionary holding "raw" prediction tensors:
+        1) refined_box_encodings: a 3-D tensor with shape
+          [total_num_proposals, num_classes, self._box_coder.code_size]
+          representing predicted (final) refined box encodings, where
+          total_num_proposals=batch_size*self._max_num_proposals. If using a
+          shared box across classes the shape will instead be
+          [total_num_proposals, 1, self._box_coder.code_size].
+        2) class_predictions_with_background: a 3-D tensor with shape
+          [total_num_proposals, num_classes + 1] containing class
+          predictions (logits) for each of the anchors, where
+          total_num_proposals=batch_size*self._max_num_proposals.
+          Note that this tensor *includes* background class predictions
+          (at class index 0).
+        3) proposal_boxes: A float32 tensor of shape
+          [batch_size, self.max_num_proposals, 4] representing
+          decoded proposal bounding boxes in absolute coordinates.
+        4) proposal_boxes_normalized: A float32 tensor of shape
+          [batch_size, self.max_num_proposals, 4] representing decoded proposal
+          bounding boxes in normalized coordinates. Can be used to override the
+          boxes proposed by the RPN, thus enabling one to extract features and
+          get box classification and prediction for externally selected areas
+          of the image.
+        5) box_classifier_features: a 4-D float32 or bfloat16 tensor
+          representing the features for each proposal.
+    """
     # If mixed-precision training on TPU is enabled, the dtype of
     # rpn_features_to_crop is bfloat16, otherwise it is float32. tf.cast is
     # used to match the dtype of proposal_boxes_normalized to that of
@@ -783,10 +1041,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
             rpn_features_to_crop,
             tf.cast(proposal_boxes_normalized, rpn_features_to_crop.dtype)))
 
-    box_classifier_features = (
-        self._feature_extractor.extract_box_classifier_features(
-            flattened_proposal_feature_maps,
-            scope=self.second_stage_feature_extractor_scope))
+    box_classifier_features = self._extract_box_classifier_features(
+        flattened_proposal_feature_maps)
 
     if self._mask_rcnn_box_predictor.is_keras_model:
       box_predictions = self._mask_rcnn_box_predictor(
@@ -813,7 +1069,6 @@ class FasterRCNNMetaArch(model.DetectionModel):
         'refined_box_encodings': refined_box_encodings,
         'class_predictions_with_background':
         class_predictions_with_background,
-        'num_proposals': num_proposals,
         'proposal_boxes': absolute_proposal_boxes,
         'box_classifier_features': box_classifier_features,
         'proposal_boxes_normalized': proposal_boxes_normalized,
@@ -821,6 +1076,24 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     return prediction_dict
 
+  def _extract_box_classifier_features(self, flattened_feature_maps):
+    if self._feature_extractor_for_box_classifier_features == (
+        _UNINITIALIZED_FEATURE_EXTRACTOR):
+      self._feature_extractor_for_box_classifier_features = (
+          self._feature_extractor.get_box_classifier_feature_extractor_model(
+              name=self.second_stage_feature_extractor_scope))
+
+    if self._feature_extractor_for_box_classifier_features:
+      box_classifier_features = (
+          self._feature_extractor_for_box_classifier_features(
+              flattened_feature_maps))
+    else:
+      box_classifier_features = (
+          self._feature_extractor.extract_box_classifier_features(
+              flattened_feature_maps,
+              scope=self.second_stage_feature_extractor_scope))
+    return box_classifier_features
+
   def _predict_third_stage(self, prediction_dict, image_shapes):
     """Predicts non-box, non-class outputs using refined detections.
 
@@ -896,10 +1169,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       flattened_detected_feature_maps = (
           self._compute_second_stage_input_feature_maps(
               rpn_features_to_crop, detection_boxes))
-      curr_box_classifier_features = (
-          self._feature_extractor.extract_box_classifier_features(
-              flattened_detected_feature_maps,
-              scope=self.second_stage_feature_extractor_scope))
+      curr_box_classifier_features = self._extract_box_classifier_features(
+          flattened_detected_feature_maps)
 
       if self._mask_rcnn_box_predictor.is_keras_model:
         mask_predictions = self._mask_rcnn_box_predictor(
@@ -972,29 +1243,35 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     image_shape = tf.shape(preprocessed_inputs)
 
-    rpn_features_to_crop, self.endpoints = (
-        self._feature_extractor.extract_proposal_features(
-            preprocessed_inputs,
-            scope=self.first_stage_feature_extractor_scope))
+    rpn_features_to_crop, self.endpoints = self._extract_proposal_features(
+        preprocessed_inputs)
 
     feature_map_shape = tf.shape(rpn_features_to_crop)
     anchors = box_list_ops.concatenate(
         self._first_stage_anchor_generator.generate([(feature_map_shape[1],
                                                       feature_map_shape[2])]))
-    with slim.arg_scope(self._first_stage_box_predictor_arg_scope_fn()):
-      kernel_size = self._first_stage_box_predictor_kernel_size
-      reuse = tf.get_variable_scope().reuse
-      rpn_box_predictor_features = slim.conv2d(
-          rpn_features_to_crop,
-          self._first_stage_box_predictor_depth,
-          kernel_size=[kernel_size, kernel_size],
-          rate=self._first_stage_atrous_rate,
-          activation_fn=tf.nn.relu6,
-          scope='Conv',
-          reuse=reuse)
+    rpn_box_predictor_features = (
+        self._first_stage_box_predictor_first_conv(rpn_features_to_crop))
     return (rpn_box_predictor_features, rpn_features_to_crop,
             anchors, image_shape)
 
+  def _extract_proposal_features(self, preprocessed_inputs):
+    if self._feature_extractor_for_proposal_features == (
+        _UNINITIALIZED_FEATURE_EXTRACTOR):
+      self._feature_extractor_for_proposal_features = (
+          self._feature_extractor.get_proposal_feature_extractor_model(
+              name=self.first_stage_feature_extractor_scope))
+    if self._feature_extractor_for_proposal_features:
+      proposal_features = (
+          self._feature_extractor_for_proposal_features(preprocessed_inputs),
+          {})
+    else:
+      proposal_features = (
+          self._feature_extractor.extract_proposal_features(
+              preprocessed_inputs,
+              scope=self.first_stage_feature_extractor_scope))
+    return proposal_features
+
   def _predict_rpn_proposals(self, rpn_box_predictor_features):
     """Adds box predictors to RPN feature map to predict proposals.
 
@@ -1141,6 +1418,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
         detection_classes: [batch, max_detections]
           (this entry is only created if rpn_mode=False)
         num_detections: [batch]
+        raw_detection_boxes: [batch, max_detections, 4]
+        raw_detection_scores: [batch, max_detections, num_classes + 1]
 
     Raises:
       ValueError: If `predict` is called before `preprocess`.
@@ -1210,10 +1489,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       flattened_detected_feature_maps = (
           self._compute_second_stage_input_feature_maps(
               rpn_features_to_crop, detection_boxes))
-      detection_features_unpooled = (
-          self._feature_extractor.extract_box_classifier_features(
-              flattened_detected_feature_maps,
-              scope=self.second_stage_feature_extractor_scope))
+      detection_features_unpooled = self._extract_box_classifier_features(
+          flattened_detected_feature_maps)
 
       batch_size = tf.shape(detection_boxes)[0]
       max_detection = tf.shape(detection_boxes)[1]
@@ -1273,9 +1550,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
         the batch.
       raw_detection_boxes: [batch, total_detections, 4] tensor with decoded
         proposal boxes before Non-Max Suppression.
-      raw_detection_score: [batch, total_detections,
-        num_classes_with_background] tensor of class score logits for
-        raw proposal boxes.
+      raw_detection_scores: [batch, total_detections,
+        num_classes_with_background] tensor of multi-class scores for raw
+        proposal boxes.
     """
     rpn_box_encodings_batch = tf.expand_dims(rpn_box_encodings_batch, axis=2)
     rpn_encodings_shape = shape_utils.combined_static_and_dynamic_shape(
@@ -1285,8 +1562,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
     proposal_boxes = self._batch_decode_boxes(rpn_box_encodings_batch,
                                               tiled_anchor_boxes)
     raw_proposal_boxes = tf.squeeze(proposal_boxes, axis=2)
-    rpn_objectness_softmax_without_background = tf.nn.softmax(
-        rpn_objectness_predictions_with_background_batch)[:, :, 1]
+    rpn_objectness_softmax = tf.nn.softmax(
+        rpn_objectness_predictions_with_background_batch)
+    rpn_objectness_softmax_without_background = rpn_objectness_softmax[:, :, 1]
     clip_window = self._compute_clip_window(image_shapes)
     (proposal_boxes, proposal_scores, _, _, _,
      num_proposals) = self._first_stage_nms_fn(
@@ -1319,8 +1597,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         elems=[raw_proposal_boxes, image_shapes],
         dtype=tf.float32)
     return (normalized_proposal_boxes, proposal_scores, num_proposals,
-            raw_normalized_proposal_boxes,
-            rpn_objectness_predictions_with_background_batch)
+            raw_normalized_proposal_boxes, rpn_objectness_softmax)
 
   def _sample_box_classifier_batch(
       self,
@@ -1547,10 +1824,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         self._crop_and_resize_fn(
             features_to_crop, proposal_boxes_normalized,
             [self._initial_crop_size, self._initial_crop_size]))
-    return slim.max_pool2d(
-        cropped_regions,
-        [self._maxpool_kernel_size, self._maxpool_kernel_size],
-        stride=self._maxpool_stride)
+    return self._maxpool_layer(cropped_regions)
 
   def _postprocess_box_classifier(self,
                                   refined_box_encodings,
@@ -1567,7 +1841,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         representing predicted (final) refined box encodings. If using a shared
         box across classes the shape will instead be
         [total_num_padded_proposals, 1, 4]
-      class_predictions_with_background: a 3-D tensor float with shape
+      class_predictions_with_background: a 2-D tensor float with shape
         [total_num_padded_proposals, num_classes + 1] containing class
         predictions (logits) for each of the proposals.  Note that this tensor
         *includes* background class predictions (at class index 0).
@@ -1594,8 +1868,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
           masks.
         `raw_detection_boxes`: [batch, total_detections, 4] tensor with decoded
           detection boxes before Non-Max Suppression.
-        `raw_detection_score`: [batch, total_detections,
-          num_classes_with_background] tensor of multi-class score logits for
+        `raw_detection_scores`: [batch, total_detections,
+          num_classes_with_background] tensor of multi-class scores for
           raw detection boxes.
     """
     refined_box_encodings_batch = tf.reshape(
@@ -1679,7 +1953,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         fields.DetectionResultFields.raw_detection_boxes:
             raw_normalized_detection_boxes,
         fields.DetectionResultFields.raw_detection_scores:
-            class_predictions_with_background_batch
+            class_predictions_with_background_batch_normalized
     }
     if nmsed_masks is not None:
       detections[fields.DetectionResultFields.detection_masks] = nmsed_masks
@@ -2072,7 +2346,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         # Use normalized proposals to crop mask targets from image masks.
         flat_normalized_proposals = box_list_ops.to_normalized_coordinates(
             box_list.BoxList(tf.reshape(proposal_boxes, [-1, 4])),
-            image_shape[1], image_shape[2]).get()
+            image_shape[1], image_shape[2], check_range=False).get()
 
         flat_cropped_gt_mask = self._crop_and_resize_fn(
             tf.expand_dims(flat_gt_masks, -1),
@@ -2261,7 +2535,32 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Returns:
       A list of regularization loss tensors.
     """
-    return tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
+    all_losses = []
+    slim_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
+    # Copy the slim losses to avoid modifying the collection
+    if slim_losses:
+      all_losses.extend(slim_losses)
+    # TODO(kaftan): Possibly raise an error if the feature extractors are
+    # uninitialized in Keras.
+    if self._feature_extractor_for_proposal_features:
+      if (self._feature_extractor_for_proposal_features !=
+          _UNINITIALIZED_FEATURE_EXTRACTOR):
+        all_losses.extend(self._feature_extractor_for_proposal_features.losses)
+    if isinstance(self._first_stage_box_predictor_first_conv,
+                  tf.keras.Model):
+      all_losses.extend(
+          self._first_stage_box_predictor_first_conv.losses)
+    if self._first_stage_box_predictor.is_keras_model:
+      all_losses.extend(self._first_stage_box_predictor.losses)
+    if self._feature_extractor_for_box_classifier_features:
+      if (self._feature_extractor_for_box_classifier_features !=
+          _UNINITIALIZED_FEATURE_EXTRACTOR):
+        all_losses.extend(
+            self._feature_extractor_for_box_classifier_features.losses)
+    if self._mask_rcnn_box_predictor:
+      if self._mask_rcnn_box_predictor.is_keras_model:
+        all_losses.extend(self._mask_rcnn_box_predictor.losses)
+    return all_losses
 
   def restore_map(self,
                   fine_tune_checkpoint_type='detection',
@@ -2318,4 +2617,55 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Returns:
       A list of update operators.
     """
-    return tf.get_collection(tf.GraphKeys.UPDATE_OPS)
+    update_ops = []
+    slim_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
+    # Copy the slim ops to avoid modifying the collection
+    if slim_update_ops:
+      update_ops.extend(slim_update_ops)
+    # Passing None to get_updates_for grabs updates that should always be
+    # executed and don't depend on any model inputs in the graph.
+    # (E.g. if there was some count that should be incremented every time a
+    # model is run).
+    #
+    # Passing inputs grabs updates that are transitively computed from the
+    # model inputs being passed in.
+    # (E.g. a batchnorm update depends on the observed inputs)
+    if self._feature_extractor_for_proposal_features:
+      if (self._feature_extractor_for_proposal_features !=
+          _UNINITIALIZED_FEATURE_EXTRACTOR):
+        update_ops.extend(
+            self._feature_extractor_for_proposal_features.get_updates_for(None))
+        update_ops.extend(
+            self._feature_extractor_for_proposal_features.get_updates_for(
+                self._feature_extractor_for_proposal_features.inputs))
+    if isinstance(self._first_stage_box_predictor_first_conv,
+                  tf.keras.Model):
+      update_ops.extend(
+          self._first_stage_box_predictor_first_conv.get_updates_for(
+              None))
+      update_ops.extend(
+          self._first_stage_box_predictor_first_conv.get_updates_for(
+              self._first_stage_box_predictor_first_conv.inputs))
+    if self._first_stage_box_predictor.is_keras_model:
+      update_ops.extend(
+          self._first_stage_box_predictor.get_updates_for(None))
+      update_ops.extend(
+          self._first_stage_box_predictor.get_updates_for(
+              self._first_stage_box_predictor.inputs))
+    if self._feature_extractor_for_box_classifier_features:
+      if (self._feature_extractor_for_box_classifier_features !=
+          _UNINITIALIZED_FEATURE_EXTRACTOR):
+        update_ops.extend(
+            self._feature_extractor_for_box_classifier_features.get_updates_for(
+                None))
+        update_ops.extend(
+            self._feature_extractor_for_box_classifier_features.get_updates_for(
+                self._feature_extractor_for_box_classifier_features.inputs))
+    if self._mask_rcnn_box_predictor:
+      if self._mask_rcnn_box_predictor.is_keras_model:
+        update_ops.extend(
+            self._mask_rcnn_box_predictor.get_updates_for(None))
+        update_ops.extend(
+            self._mask_rcnn_box_predictor.get_updates_for(
+                self._mask_rcnn_box_predictor.inputs))
+    return update_ops
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index 810d65a2..c3daa864 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -26,9 +26,15 @@ class FasterRCNNMetaArchTest(
     faster_rcnn_meta_arch_test_lib.FasterRCNNMetaArchTestBase,
     parameterized.TestCase):
 
-  def test_postprocess_second_stage_only_inference_mode_with_masks(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_postprocess_second_stage_only_inference_mode_with_masks(
+      self, use_keras=False):
     model = self._build_model(
-        is_training=False, number_of_stages=2, second_stage_batch_size=6)
+        is_training=False, use_keras=use_keras,
+        number_of_stages=2, second_stage_batch_size=6)
 
     batch_size = 2
     total_num_padded_proposals = batch_size * model.max_num_proposals
@@ -85,9 +91,15 @@ class FasterRCNNMetaArchTest(
       self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
       self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
 
-  def test_postprocess_second_stage_only_inference_mode_with_calibration(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_postprocess_second_stage_only_inference_mode_with_calibration(
+      self, use_keras=False):
     model = self._build_model(
-        is_training=False, number_of_stages=2, second_stage_batch_size=6,
+        is_training=False, use_keras=use_keras,
+        number_of_stages=2, second_stage_batch_size=6,
         calibration_mapping_value=0.5)
 
     batch_size = 2
@@ -147,9 +159,15 @@ class FasterRCNNMetaArchTest(
       self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
       self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
 
-  def test_postprocess_second_stage_only_inference_mode_with_shared_boxes(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_postprocess_second_stage_only_inference_mode_with_shared_boxes(
+      self, use_keras=False):
     model = self._build_model(
-        is_training=False, number_of_stages=2, second_stage_batch_size=6)
+        is_training=False, use_keras=use_keras,
+        number_of_stages=2, second_stage_batch_size=6)
 
     batch_size = 2
     total_num_padded_proposals = batch_size * model.max_num_proposals
@@ -188,11 +206,13 @@ class FasterRCNNMetaArchTest(
       self.assertAllClose(detections_out['num_detections'], [5, 4])
 
   @parameterized.parameters(
-      {'masks_are_class_agnostic': False},
-      {'masks_are_class_agnostic': True},
+      {'masks_are_class_agnostic': False, 'use_keras': True},
+      {'masks_are_class_agnostic': True, 'use_keras': True},
+      {'masks_are_class_agnostic': False, 'use_keras': False},
+      {'masks_are_class_agnostic': True, 'use_keras': False},
   )
   def test_predict_correct_shapes_in_inference_mode_three_stages_with_masks(
-      self, masks_are_class_agnostic):
+      self, masks_are_class_agnostic, use_keras):
     batch_size = 2
     image_size = 10
     max_num_proposals = 8
@@ -232,6 +252,7 @@ class FasterRCNNMetaArchTest(
       with test_graph.as_default():
         model = self._build_model(
             is_training=False,
+            use_keras=use_keras,
             number_of_stages=3,
             second_stage_batch_size=2,
             predict_masks=True,
@@ -267,15 +288,18 @@ class FasterRCNNMetaArchTest(
                           [10, num_classes, 14, 14])
 
   @parameterized.parameters(
-      {'masks_are_class_agnostic': False},
-      {'masks_are_class_agnostic': True},
+      {'masks_are_class_agnostic': False, 'use_keras': True},
+      {'masks_are_class_agnostic': True, 'use_keras': True},
+      {'masks_are_class_agnostic': False, 'use_keras': False},
+      {'masks_are_class_agnostic': True, 'use_keras': False},
   )
   def test_predict_gives_correct_shapes_in_train_mode_both_stages_with_masks(
-      self, masks_are_class_agnostic):
+      self, masks_are_class_agnostic, use_keras):
     test_graph = tf.Graph()
     with test_graph.as_default():
       model = self._build_model(
           is_training=True,
+          use_keras=use_keras,
           number_of_stages=3,
           second_stage_batch_size=7,
           predict_masks=True,
@@ -348,7 +372,11 @@ class FasterRCNNMetaArchTest(
             tensor_dict_out['rpn_objectness_predictions_with_background'].shape,
             (2, num_anchors_out, 2))
 
-  def test_postprocess_third_stage_only_inference_mode(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_postprocess_third_stage_only_inference_mode(self, use_keras=False):
     num_proposals_shapes = [(2), (None)]
     refined_box_encodings_shapes = [(16, 2, 4), (None, 2, 4)]
     class_predictions_with_background_shapes = [(16, 3), (None, 3)]
@@ -364,7 +392,7 @@ class FasterRCNNMetaArchTest(
       tf_graph = tf.Graph()
       with tf_graph.as_default():
         model = self._build_model(
-            is_training=False, number_of_stages=3,
+            is_training=False, use_keras=use_keras, number_of_stages=3,
             second_stage_batch_size=6, predict_masks=True)
         total_num_padded_proposals = batch_size * model.max_num_proposals
         proposal_boxes = np.array(
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 6de390a6..266ce610 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -43,7 +43,7 @@ BOX_CODE_SIZE = 4
 
 class FakeFasterRCNNFeatureExtractor(
     faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):
-  """Fake feature extracture to use in tests."""
+  """Fake feature extractor to use in tests."""
 
   def __init__(self):
     super(FakeFasterRCNNFeatureExtractor, self).__init__(
@@ -67,6 +67,42 @@ class FakeFasterRCNNFeatureExtractor(
                              num_outputs=3, kernel_size=1, scope='layer2')
 
 
+class FakeFasterRCNNKerasFeatureExtractor(
+    faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor):
+  """Fake feature extractor to use in tests."""
+
+  def __init__(self):
+    super(FakeFasterRCNNKerasFeatureExtractor, self).__init__(
+        is_training=False,
+        first_stage_features_stride=32,
+        weight_decay=0.0)
+
+  def preprocess(self, resized_inputs):
+    return tf.identity(resized_inputs)
+
+  def get_proposal_feature_extractor_model(self, name):
+
+    class ProposalFeatureExtractor(tf.keras.Model):
+      """Dummy proposal feature extraction."""
+
+      def __init__(self, name):
+        super(ProposalFeatureExtractor, self).__init__(name=name)
+        self.conv = None
+
+      def build(self, input_shape):
+        self.conv = tf.keras.layers.Conv2D(
+            3, kernel_size=1, padding='SAME', name='layer1')
+
+      def call(self, inputs):
+        return self.conv(inputs)
+
+    return ProposalFeatureExtractor(name=name)
+
+  def get_box_classifier_feature_extractor_model(self, name):
+    return tf.keras.Sequential([tf.keras.layers.Conv2D(
+        3, kernel_size=1, padding='SAME', name=name + '_layer2')])
+
+
 class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
   """Base class to test Faster R-CNN and R-FCN meta architectures."""
 
@@ -77,6 +113,11 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     text_format.Merge(hyperparams_text_proto, hyperparams)
     return hyperparams_builder.build(hyperparams, is_training=is_training)
 
+  def _build_keras_layer_hyperparams(self, hyperparams_text_proto):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(hyperparams)
+
   def _get_second_stage_box_predictor_text_proto(self):
     box_predictor_text_proto = """
       mask_rcnn_box_predictor {
@@ -127,7 +168,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     return box_predictor_text_proto
 
   def _get_second_stage_box_predictor(self, num_classes, is_training,
-                                      predict_masks, masks_are_class_agnostic):
+                                      predict_masks, masks_are_class_agnostic,
+                                      use_keras=False):
     box_predictor_proto = box_predictor_pb2.BoxPredictor()
     text_format.Merge(self._get_second_stage_box_predictor_text_proto(),
                       box_predictor_proto)
@@ -137,13 +179,23 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
               masks_are_class_agnostic),
           box_predictor_proto)
 
-    return box_predictor_builder.build(
-        hyperparams_builder.build,
-        box_predictor_proto,
-        num_classes=num_classes,
-        is_training=is_training)
+    if use_keras:
+      return box_predictor_builder.build_keras(
+          hyperparams_builder.KerasLayerHyperparams,
+          inplace_batchnorm_update=False,
+          freeze_batchnorm=False,
+          box_predictor_config=box_predictor_proto,
+          num_classes=num_classes,
+          num_predictions_per_location_list=None,
+          is_training=is_training)
+    else:
+      return box_predictor_builder.build(
+          hyperparams_builder.build,
+          box_predictor_proto,
+          num_classes=num_classes,
+          is_training=is_training)
 
-  def _get_model(self, box_predictor, **common_kwargs):
+  def _get_model(self, box_predictor, keras_model=False, **common_kwargs):
     return faster_rcnn_meta_arch.FasterRCNNMetaArch(
         initial_crop_size=3,
         maxpool_kernel_size=1,
@@ -155,6 +207,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                    is_training,
                    number_of_stages,
                    second_stage_batch_size,
+                   use_keras=False,
                    first_stage_max_proposals=8,
                    num_classes=2,
                    hard_mining=False,
@@ -204,7 +257,10 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         'proposal',
         use_matmul_gather=use_matmul_gather_in_matcher)
 
-    fake_feature_extractor = FakeFasterRCNNFeatureExtractor()
+    if use_keras:
+      fake_feature_extractor = FakeFasterRCNNKerasFeatureExtractor()
+    else:
+      fake_feature_extractor = FakeFasterRCNNFeatureExtractor()
 
     first_stage_box_predictor_hyperparams_text_proto = """
       op: CONV
@@ -220,9 +276,14 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         }
       }
     """
-    first_stage_box_predictor_arg_scope_fn = (
-        self._build_arg_scope_with_hyperparams(
-            first_stage_box_predictor_hyperparams_text_proto, is_training))
+    if use_keras:
+      first_stage_box_predictor_arg_scope_fn = (
+          self._build_keras_layer_hyperparams(
+              first_stage_box_predictor_hyperparams_text_proto))
+    else:
+      first_stage_box_predictor_arg_scope_fn = (
+          self._build_arg_scope_with_hyperparams(
+              first_stage_box_predictor_hyperparams_text_proto, is_training))
 
     first_stage_box_predictor_kernel_size = 3
     first_stage_atrous_rate = 1
@@ -349,15 +410,18 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         self._get_second_stage_box_predictor(
             num_classes=num_classes,
             is_training=is_training,
+            use_keras=use_keras,
             predict_masks=predict_masks,
             masks_are_class_agnostic=masks_are_class_agnostic), **common_kwargs)
 
   @parameterized.parameters(
-      {'use_static_shapes': False},
-      {'use_static_shapes': True}
+      {'use_static_shapes': False, 'use_keras': True},
+      {'use_static_shapes': False, 'use_keras': False},
+      {'use_static_shapes': True, 'use_keras': True},
+      {'use_static_shapes': True, 'use_keras': False},
   )
   def test_predict_gives_correct_shapes_in_inference_mode_first_stage_only(
-      self, use_static_shapes=False):
+      self, use_static_shapes=False, use_keras=False):
     batch_size = 2
     height = 10
     width = 12
@@ -367,6 +431,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       """Function to construct tf graph for the test."""
       model = self._build_model(
           is_training=False,
+          use_keras=use_keras,
           number_of_stages=1,
           second_stage_batch_size=2,
           clip_anchors_to_image=use_static_shapes,
@@ -423,11 +488,44 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertTrue(np.all(np.less_equal(anchors[:, 2], height)))
     self.assertTrue(np.all(np.less_equal(anchors[:, 3], width)))
 
-  def test_predict_gives_valid_anchors_in_training_mode_first_stage_only(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_regularization_losses(
+      self, use_keras=False):
     test_graph = tf.Graph()
     with test_graph.as_default():
       model = self._build_model(
-          is_training=True, number_of_stages=1, second_stage_batch_size=2)
+          is_training=True, use_keras=use_keras,
+          number_of_stages=1, second_stage_batch_size=2,)
+      batch_size = 2
+      height = 10
+      width = 12
+      input_image_shape = (batch_size, height, width, 3)
+      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
+      preprocessed_inputs = tf.placeholder(
+          dtype=tf.float32, shape=(batch_size, None, None, 3))
+      model.predict(preprocessed_inputs, true_image_shapes)
+
+      reg_losses = tf.math.add_n(model.regularization_losses())
+
+      init_op = tf.global_variables_initializer()
+      with self.test_session(graph=test_graph) as sess:
+        sess.run(init_op)
+        self.assertGreaterEqual(sess.run(reg_losses), 0)
+
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_predict_gives_valid_anchors_in_training_mode_first_stage_only(
+      self, use_keras=False):
+    test_graph = tf.Graph()
+    with test_graph.as_default():
+      model = self._build_model(
+          is_training=True, use_keras=use_keras,
+          number_of_stages=1, second_stage_batch_size=2,)
       batch_size = 2
       height = 10
       width = 12
@@ -478,11 +576,13 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
             (batch_size, num_anchors_out, 2))
 
   @parameterized.parameters(
-      {'use_static_shapes': False},
-      {'use_static_shapes': True}
+      {'use_static_shapes': False, 'use_keras': True},
+      {'use_static_shapes': False, 'use_keras': False},
+      {'use_static_shapes': True, 'use_keras': True},
+      {'use_static_shapes': True, 'use_keras': False},
   )
   def test_predict_correct_shapes_in_inference_mode_two_stages(
-      self, use_static_shapes=False):
+      self, use_static_shapes=False, use_keras=False):
 
     def compare_results(results, expected_output_shapes):
       """Checks if the shape of the predictions are as expected."""
@@ -528,6 +628,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       """Function to construct tf graph for the test."""
       model = self._build_model(
           is_training=False,
+          use_keras=use_keras,
           number_of_stages=2,
           second_stage_batch_size=2,
           predict_masks=False,
@@ -584,6 +685,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         with test_graph.as_default():
           model = self._build_model(
               is_training=False,
+              use_keras=use_keras,
               number_of_stages=2,
               second_stage_batch_size=2,
               predict_masks=False)
@@ -603,12 +705,15 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
 
   @parameterized.parameters(
-      {'use_static_shapes': False},
-      {'use_static_shapes': True}
+      {'use_static_shapes': False, 'use_keras': True},
+      {'use_static_shapes': False, 'use_keras': False},
+      {'use_static_shapes': True, 'use_keras': True},
+      {'use_static_shapes': True, 'use_keras': False},
   )
   def test_predict_gives_correct_shapes_in_train_mode_both_stages(
       self,
-      use_static_shapes=False):
+      use_static_shapes=False,
+      use_keras=False):
     batch_size = 2
     image_size = 10
     max_num_proposals = 7
@@ -619,6 +724,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       """Function to construct tf graph for the test."""
       model = self._build_model(
           is_training=True,
+          use_keras=use_keras,
           number_of_stages=2,
           second_stage_batch_size=7,
           predict_masks=False,
@@ -632,6 +738,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           groundtruth_classes_list=tf.unstack(gt_classes),
           groundtruth_weights_list=tf.unstack(gt_weights))
       result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)
+      updates = model.updates()
       return (result_tensor_dict['refined_box_encodings'],
               result_tensor_dict['class_predictions_with_background'],
               result_tensor_dict['proposal_boxes'],
@@ -641,6 +748,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
               result_tensor_dict['rpn_objectness_predictions_with_background'],
               result_tensor_dict['rpn_features_to_crop'],
               result_tensor_dict['rpn_box_predictor_features'],
+              updates
              )
 
     image_shape = (batch_size, image_size, image_size, 3)
@@ -699,13 +807,27 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                         expected_shapes['rpn_box_predictor_features'])
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'pad_to_max_dimension': None},
-      {'use_static_shapes': True, 'pad_to_max_dimension': None},
-      {'use_static_shapes': False, 'pad_to_max_dimension': 56},
-      {'use_static_shapes': True, 'pad_to_max_dimension': 56}
+      {'use_static_shapes': False, 'pad_to_max_dimension': None,
+       'use_keras': True},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None,
+       'use_keras': True},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
+       'use_keras': True},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
+       'use_keras': True},
+      {'use_static_shapes': False, 'pad_to_max_dimension': None,
+       'use_keras': False},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None,
+       'use_keras': False},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
+       'use_keras': False},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
+       'use_keras': False}
   )
   def test_postprocess_first_stage_only_inference_mode(
-      self, use_static_shapes=False, pad_to_max_dimension=None):
+      self, use_static_shapes=False,
+      pad_to_max_dimension=None,
+      use_keras=False):
     batch_size = 2
     first_stage_max_proposals = 4 if use_static_shapes else 8
 
@@ -716,7 +838,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                  anchors):
       """Function to construct tf graph for the test."""
       model = self._build_model(
-          is_training=False, number_of_stages=1, second_stage_batch_size=6,
+          is_training=False, use_keras=use_keras,
+          number_of_stages=1, second_stage_batch_size=6,
           use_matmul_crop_and_resize=use_static_shapes,
           clip_anchors_to_image=use_static_shapes,
           use_static_shapes=use_static_shapes,
@@ -779,8 +902,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                                     [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]],
                                    [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
                                     [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]]]
-    expected_raw_scores = [[[-10., 13.], [10., -10.], [10., -11.], [-10., 12.]],
-                           [[10., -10.], [-10., 13.], [-10., 12.], [10., -11.]]]
+    expected_raw_scores = [[[0., 1.], [1., 0.], [1., 0.], [0., 1.]],
+                           [[1., 0.], [0., 1.], [0., 1.], [1., 0.]]]
 
     self.assertAllClose(results[0], expected_num_proposals)
     for indx, num_proposals in enumerate(expected_num_proposals):
@@ -792,9 +915,11 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertAllClose(results[4], expected_raw_scores)
 
   def _test_postprocess_first_stage_only_train_mode(self,
+                                                    use_keras=False,
                                                     pad_to_max_dimension=None):
     model = self._build_model(
-        is_training=True, number_of_stages=1, second_stage_batch_size=2,
+        is_training=True, use_keras=use_keras,
+        number_of_stages=1, second_stage_batch_size=2,
         pad_to_max_dimension=pad_to_max_dimension)
     batch_size = 2
     anchors = tf.constant(
@@ -847,9 +972,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                                     [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]],
                                    [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
                                     [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]]]
-    expected_raw_scores = [[[-10., 13.], [-10., 12.], [-10., 11.], [-10., 10.]],
-                           [[-10., 13.], [-10., 12.], [-10., 11.], [-10., 10.]]]
-
+    expected_raw_scores = [[[0., 1.], [0., 1.], [0., 1.], [0., 1.]],
+                           [[0., 1.], [0., 1.], [0., 1.], [0., 1.]]]
     expected_output_keys = set([
         'detection_boxes', 'detection_scores', 'num_detections',
         'raw_detection_boxes', 'raw_detection_scores'
@@ -872,20 +996,44 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertAllClose(proposals_out['raw_detection_scores'],
                         expected_raw_scores)
 
-  def test_postprocess_first_stage_only_train_mode(self):
-    self._test_postprocess_first_stage_only_train_mode()
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_postprocess_first_stage_only_train_mode(self, use_keras=False):
+    self._test_postprocess_first_stage_only_train_mode(use_keras=use_keras)
 
-  def test_postprocess_first_stage_only_train_mode_padded_image(self):
-    self._test_postprocess_first_stage_only_train_mode(pad_to_max_dimension=56)
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_postprocess_first_stage_only_train_mode_padded_image(
+      self, use_keras=False):
+    self._test_postprocess_first_stage_only_train_mode(pad_to_max_dimension=56,
+                                                       use_keras=use_keras)
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'pad_to_max_dimension': None},
-      {'use_static_shapes': True, 'pad_to_max_dimension': None},
-      {'use_static_shapes': False, 'pad_to_max_dimension': 56},
-      {'use_static_shapes': True, 'pad_to_max_dimension': 56}
+      {'use_static_shapes': False, 'pad_to_max_dimension': None,
+       'use_keras': True},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None,
+       'use_keras': True},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
+       'use_keras': True},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
+       'use_keras': True},
+      {'use_static_shapes': False, 'pad_to_max_dimension': None,
+       'use_keras': False},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None,
+       'use_keras': False},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
+       'use_keras': False},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
+       'use_keras': False}
   )
   def test_postprocess_second_stage_only_inference_mode(
-      self, use_static_shapes=False, pad_to_max_dimension=None):
+      self, use_static_shapes=False,
+      pad_to_max_dimension=None,
+      use_keras=False):
     batch_size = 2
     num_classes = 2
     image_shape = np.array((2, 36, 48, 3), dtype=np.int32)
@@ -899,7 +1047,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                  proposal_boxes):
       """Function to construct tf graph for the test."""
       model = self._build_model(
-          is_training=False, number_of_stages=2,
+          is_training=False, use_keras=use_keras,
+          number_of_stages=2,
           second_stage_batch_size=6,
           use_matmul_crop_and_resize=use_static_shapes,
           clip_anchors_to_image=use_static_shapes,
@@ -972,21 +1121,31 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     if not use_static_shapes:
       self.assertAllEqual(results[1].shape, [2, 5, 4])
 
-  def test_preprocess_preserves_input_shapes(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_preprocess_preserves_input_shapes(self, use_keras=False):
     image_shapes = [(3, None, None, 3),
                     (None, 10, 10, 3),
                     (None, None, None, 3)]
     for image_shape in image_shapes:
       model = self._build_model(
-          is_training=False, number_of_stages=2, second_stage_batch_size=6)
+          is_training=False, use_keras=use_keras,
+          number_of_stages=2, second_stage_batch_size=6)
       image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
       preprocessed_inputs, _ = model.preprocess(image_placeholder)
       self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
 
   # TODO(rathodv): Split test into two - with and without masks.
-  def test_loss_first_stage_only_mode(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_loss_first_stage_only_mode(self, use_keras=False):
     model = self._build_model(
-        is_training=True, number_of_stages=1, second_stage_batch_size=6)
+        is_training=True, use_keras=use_keras,
+        number_of_stages=1, second_stage_batch_size=6)
     batch_size = 2
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -1038,9 +1197,14 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                        loss_dict_out)
 
   # TODO(rathodv): Split test into two - with and without masks.
-  def test_loss_full(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_loss_full(self, use_keras=False):
     model = self._build_model(
-        is_training=True, number_of_stages=2, second_stage_batch_size=6)
+        is_training=True, use_keras=use_keras,
+        number_of_stages=2, second_stage_batch_size=6)
     batch_size = 3
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -1153,9 +1317,14 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           'Loss/BoxClassifierLoss/classification_loss'], 0)
       self.assertAllClose(loss_dict_out['Loss/BoxClassifierLoss/mask_loss'], 0)
 
-  def test_loss_full_zero_padded_proposals(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_loss_full_zero_padded_proposals(self, use_keras=False):
     model = self._build_model(
-        is_training=True, number_of_stages=2, second_stage_batch_size=6)
+        is_training=True, use_keras=use_keras,
+        number_of_stages=2, second_stage_batch_size=6)
     batch_size = 1
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -1243,9 +1412,14 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           'Loss/BoxClassifierLoss/classification_loss'], 0)
       self.assertAllClose(loss_dict_out['Loss/BoxClassifierLoss/mask_loss'], 0)
 
-  def test_loss_full_multiple_label_groundtruth(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_loss_full_multiple_label_groundtruth(self, use_keras=False):
     model = self._build_model(
-        is_training=True, number_of_stages=2, second_stage_batch_size=6,
+        is_training=True, use_keras=use_keras,
+        number_of_stages=2, second_stage_batch_size=6,
         softmax_second_stage_classification_loss=False)
     batch_size = 1
     anchors = tf.constant(
@@ -1342,14 +1516,17 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       self.assertAllClose(loss_dict_out['Loss/BoxClassifierLoss/mask_loss'], 0)
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'shared_boxes': False},
-      {'use_static_shapes': False, 'shared_boxes': True},
-
-      {'use_static_shapes': True, 'shared_boxes': False},
-      {'use_static_shapes': True, 'shared_boxes': True},
+      {'use_static_shapes': False, 'shared_boxes': False, 'use_keras': True},
+      {'use_static_shapes': False, 'shared_boxes': True, 'use_keras': True},
+      {'use_static_shapes': True, 'shared_boxes': False, 'use_keras': True},
+      {'use_static_shapes': True, 'shared_boxes': True, 'use_keras': True},
+      {'use_static_shapes': False, 'shared_boxes': False, 'use_keras': False},
+      {'use_static_shapes': False, 'shared_boxes': True, 'use_keras': False},
+      {'use_static_shapes': True, 'shared_boxes': False, 'use_keras': False},
+      {'use_static_shapes': True, 'shared_boxes': True, 'use_keras': False},
   )
   def test_loss_full_zero_padded_proposals_nonzero_loss_with_two_images(
-      self, use_static_shapes=False, shared_boxes=False):
+      self, use_static_shapes=False, shared_boxes=False, use_keras=False):
     batch_size = 2
     first_stage_max_proposals = 8
     second_stage_batch_size = 6
@@ -1361,7 +1538,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                  groundtruth_classes):
       """Function to construct tf graph for the test."""
       model = self._build_model(
-          is_training=True, number_of_stages=2,
+          is_training=True, use_keras=use_keras,
+          number_of_stages=2,
           second_stage_batch_size=second_stage_batch_size,
           first_stage_max_proposals=first_stage_max_proposals,
           num_classes=num_classes,
@@ -1476,8 +1654,13 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertAllClose(results[2], exp_loc_loss, rtol=1e-4, atol=1e-4)
     self.assertAllClose(results[3], 0.0)
 
-  def test_loss_with_hard_mining(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_loss_with_hard_mining(self, use_keras=False):
     model = self._build_model(is_training=True,
+                              use_keras=use_keras,
                               number_of_stages=2,
                               second_stage_batch_size=None,
                               first_stage_max_proposals=6,
@@ -1564,8 +1747,13 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       self.assertAllClose(loss_dict_out[
           'Loss/BoxClassifierLoss/classification_loss'], 0)
 
-  def test_loss_with_hard_mining_and_losses_mask(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_loss_with_hard_mining_and_losses_mask(self, use_keras=False):
     model = self._build_model(is_training=True,
+                              use_keras=use_keras,
                               number_of_stages=2,
                               second_stage_batch_size=None,
                               first_stage_max_proposals=6,
@@ -1678,7 +1866,11 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       self.assertAllClose(loss_dict_out[
           'Loss/BoxClassifierLoss/classification_loss'], 0)
 
-  def test_restore_map_for_classification_ckpt(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_restore_map_for_classification_ckpt(self, use_keras=False):
     # Define mock tensorflow classification graph and save variables.
     test_graph_classification = tf.Graph()
     with test_graph_classification.as_default():
@@ -1699,7 +1891,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
       model = self._build_model(
-          is_training=False, number_of_stages=2, second_stage_batch_size=6)
+          is_training=False, use_keras=use_keras,
+          number_of_stages=2, second_stage_batch_size=6)
 
       inputs_shape = (2, 20, 20, 3)
       inputs = tf.to_float(tf.random_uniform(
@@ -1716,12 +1909,17 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           self.assertNotIn(model.first_stage_feature_extractor_scope, var)
           self.assertNotIn(model.second_stage_feature_extractor_scope, var)
 
-  def test_restore_map_for_detection_ckpt(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_restore_map_for_detection_ckpt(self, use_keras=False):
     # Define first detection graph and save variables.
     test_graph_detection1 = tf.Graph()
     with test_graph_detection1.as_default():
       model = self._build_model(
-          is_training=False, number_of_stages=2, second_stage_batch_size=6)
+          is_training=False, use_keras=use_keras,
+          number_of_stages=2, second_stage_batch_size=6)
       inputs_shape = (2, 20, 20, 3)
       inputs = tf.to_float(tf.random_uniform(
           inputs_shape, minval=0, maxval=255, dtype=tf.int32))
@@ -1739,7 +1937,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     # Define second detection graph and restore variables.
     test_graph_detection2 = tf.Graph()
     with test_graph_detection2.as_default():
-      model2 = self._build_model(is_training=False, number_of_stages=2,
+      model2 = self._build_model(is_training=False, use_keras=use_keras,
+                                 number_of_stages=2,
                                  second_stage_batch_size=6, num_classes=42)
 
       inputs_shape2 = (2, 20, 20, 3)
@@ -1760,11 +1959,16 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           self.assertNotIn(model2.first_stage_feature_extractor_scope, var)
           self.assertNotIn(model2.second_stage_feature_extractor_scope, var)
 
-  def test_load_all_det_checkpoint_vars(self):
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False}
+  )
+  def test_load_all_det_checkpoint_vars(self, use_keras=False):
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
       model = self._build_model(
           is_training=False,
+          use_keras=use_keras,
           number_of_stages=2,
           second_stage_batch_size=6,
           num_classes=42)
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 8dfaa453..4d0a1d69 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -81,7 +81,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                add_summaries=True,
                clip_anchors_to_image=False,
                use_static_shapes=False,
-               resize_masks=False):
+               resize_masks=False,
+               freeze_batchnorm=False):
     """RFCNMetaArch Constructor.
 
     Args:
@@ -110,9 +111,12 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         denser resolutions.  The atrous rate is used to compensate for the
         denser feature maps by using an effectively larger receptive field.
         (This should typically be set to 1).
-      first_stage_box_predictor_arg_scope_fn: A function to generate tf-slim
-        arg_scope for conv2d, separable_conv2d and fully_connected ops for the
-        RPN box predictor.
+      first_stage_box_predictor_arg_scope_fn: Either a
+        Keras layer hyperparams object or a function to construct tf-slim
+        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used
+        for the RPN box predictor. If it is a keras hyperparams object the
+        RPN box predictor will be a Keras model. If it is a function to
+        construct an arg scope it will be a tf-slim box predictor.
       first_stage_box_predictor_kernel_size: Kernel size to use for the
         convolution op just prior to RPN box predictions.
       first_stage_box_predictor_depth: Output depth for the convolution op
@@ -180,6 +184,10 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         guarantees.
       resize_masks: Indicates whether the masks presend in the groundtruth
         should be resized in the model with `image_resizer_fn`
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`
@@ -225,7 +233,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         add_summaries,
         clip_anchors_to_image,
         use_static_shapes,
-        resize_masks)
+        resize_masks,
+        freeze_batchnorm=freeze_batchnorm)
 
     self._rfcn_box_predictor = second_stage_rfcn_box_predictor
 
@@ -293,9 +302,7 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         anchors, image_shape_2d, true_image_shapes)
 
     box_classifier_features = (
-        self._feature_extractor.extract_box_classifier_features(
-            rpn_features,
-            scope=self.second_stage_feature_extractor_scope))
+        self._extract_box_classifier_features(rpn_features))
 
     if self._rfcn_box_predictor.is_keras_model:
       box_predictions = self._rfcn_box_predictor(
@@ -329,3 +336,37 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         'proposal_boxes_normalized': proposal_boxes_normalized,
     }
     return prediction_dict
+
+  def regularization_losses(self):
+    """Returns a list of regularization losses for this model.
+
+    Returns a list of regularization losses for this model that the estimator
+    needs to use during training/optimization.
+
+    Returns:
+      A list of regularization loss tensors.
+    """
+    reg_losses = super(RFCNMetaArch, self).regularization_losses()
+    if self._rfcn_box_predictor.is_keras_model:
+      reg_losses.extend(self._rfcn_box_predictor.losses)
+    return reg_losses
+
+  def updates(self):
+    """Returns a list of update operators for this model.
+
+    Returns a list of update operators for this model that must be executed at
+    each training step. The estimator's train op needs to have a control
+    dependency on these updates.
+
+    Returns:
+      A list of update operators.
+    """
+    update_ops = super(RFCNMetaArch, self).updates()
+
+    if self._rfcn_box_predictor.is_keras_model:
+      update_ops.extend(
+          self._rfcn_box_predictor.get_updates_for(None))
+      update_ops.extend(
+          self._rfcn_box_predictor.get_updates_for(
+              self._rfcn_box_predictor.inputs))
+    return update_ops
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index bcbedd89..4ecdcc52 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -22,6 +22,7 @@ import tensorflow as tf
 
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
+from object_detection.core import matcher
 from object_detection.core import model
 from object_detection.core import standard_fields as fields
 from object_detection.core import target_assigner
@@ -663,8 +664,8 @@ class SSDMetaArch(model.DetectionModel):
         raw_detection_boxes: [batch, total_detections, 4] tensor with decoded
           detection boxes before Non-Max Suppression.
         raw_detection_score: [batch, total_detections,
-          num_classes_with_background] tensor of multi-class score logits for
-          raw detection boxes.
+          num_classes_with_background] tensor of multi-class scores for raw
+          detection boxes.
     Raises:
       ValueError: if prediction_dict does not contain `box_encodings` or
         `class_predictions_with_background` fields.
@@ -672,17 +673,23 @@ class SSDMetaArch(model.DetectionModel):
     if ('box_encodings' not in prediction_dict or
         'class_predictions_with_background' not in prediction_dict):
       raise ValueError('prediction_dict does not contain expected entries.')
+    if 'anchors' not in prediction_dict:
+      prediction_dict['anchors'] = self.anchors.get()
     with tf.name_scope('Postprocessor'):
       preprocessed_images = prediction_dict['preprocessed_inputs']
       box_encodings = prediction_dict['box_encodings']
       box_encodings = tf.identity(box_encodings, 'raw_box_encodings')
-      class_predictions = prediction_dict['class_predictions_with_background']
-      detection_boxes, detection_keypoints = self._batch_decode(box_encodings)
+      class_predictions_with_background = (
+          prediction_dict['class_predictions_with_background'])
+      detection_boxes, detection_keypoints = self._batch_decode(
+          box_encodings, prediction_dict['anchors'])
       detection_boxes = tf.identity(detection_boxes, 'raw_box_locations')
       detection_boxes = tf.expand_dims(detection_boxes, axis=2)
 
-      detection_scores = self._score_conversion_fn(class_predictions)
-      detection_scores = tf.identity(detection_scores, 'raw_box_scores')
+      detection_scores_with_background = self._score_conversion_fn(
+          class_predictions_with_background)
+      detection_scores = tf.identity(detection_scores_with_background,
+                                     'raw_box_scores')
       if self._add_background_class or self._explicit_background_class:
         detection_scores = tf.slice(detection_scores, [0, 0, 1], [-1, -1, -1])
       additional_fields = None
@@ -720,7 +727,7 @@ class SSDMetaArch(model.DetectionModel):
           fields.DetectionResultFields.raw_detection_boxes:
               tf.squeeze(detection_boxes, axis=2),
           fields.DetectionResultFields.raw_detection_scores:
-              class_predictions
+              detection_scores_with_background
       }
       if (nmsed_additional_fields is not None and
           fields.BoxListFields.keypoints in nmsed_additional_fields):
@@ -767,10 +774,11 @@ class SSDMetaArch(model.DetectionModel):
       if self.groundtruth_has_field(fields.BoxListFields.confidences):
         confidences = self.groundtruth_lists(fields.BoxListFields.confidences)
       (batch_cls_targets, batch_cls_weights, batch_reg_targets,
-       batch_reg_weights, match_list) = self._assign_targets(
+       batch_reg_weights, batch_match) = self._assign_targets(
            self.groundtruth_lists(fields.BoxListFields.boxes),
            self.groundtruth_lists(fields.BoxListFields.classes),
            keypoints, weights, confidences)
+      match_list = [matcher.Match(match) for match in tf.unstack(batch_match)]
       if self._add_summaries:
         self._summarize_target_assignment(
             self.groundtruth_lists(fields.BoxListFields.boxes), match_list)
@@ -1017,25 +1025,31 @@ class SSDMetaArch(model.DetectionModel):
         with rows of the Match objects corresponding to groundtruth boxes
         and columns corresponding to anchors.
     """
-    num_boxes_per_image = tf.stack(
-        [tf.shape(x)[0] for x in groundtruth_boxes_list])
-    pos_anchors_per_image = tf.stack(
-        [match.num_matched_columns() for match in match_list])
-    neg_anchors_per_image = tf.stack(
-        [match.num_unmatched_columns() for match in match_list])
-    ignored_anchors_per_image = tf.stack(
-        [match.num_ignored_columns() for match in match_list])
+    avg_num_gt_boxes = tf.reduce_mean(tf.to_float(tf.stack(
+        [tf.shape(x)[0] for x in groundtruth_boxes_list])))
+    avg_num_matched_gt_boxes = tf.reduce_mean(tf.to_float(tf.stack(
+        [match.num_matched_rows() for match in match_list])))
+    avg_pos_anchors = tf.reduce_mean(tf.to_float(tf.stack(
+        [match.num_matched_columns() for match in match_list])))
+    avg_neg_anchors = tf.reduce_mean(tf.to_float(tf.stack(
+        [match.num_unmatched_columns() for match in match_list])))
+    avg_ignored_anchors = tf.reduce_mean(tf.to_float(tf.stack(
+        [match.num_ignored_columns() for match in match_list])))
+    # TODO(rathodv): Add a test for these summaries.
     tf.summary.scalar('AvgNumGroundtruthBoxesPerImage',
-                      tf.reduce_mean(tf.to_float(num_boxes_per_image)),
+                      avg_num_gt_boxes,
+                      family='TargetAssignment')
+    tf.summary.scalar('AvgNumGroundtruthBoxesMatchedPerImage',
+                      avg_num_matched_gt_boxes,
                       family='TargetAssignment')
     tf.summary.scalar('AvgNumPositiveAnchorsPerImage',
-                      tf.reduce_mean(tf.to_float(pos_anchors_per_image)),
+                      avg_pos_anchors,
                       family='TargetAssignment')
     tf.summary.scalar('AvgNumNegativeAnchorsPerImage',
-                      tf.reduce_mean(tf.to_float(neg_anchors_per_image)),
+                      avg_neg_anchors,
                       family='TargetAssignment')
     tf.summary.scalar('AvgNumIgnoredAnchorsPerImage',
-                      tf.reduce_mean(tf.to_float(ignored_anchors_per_image)),
+                      avg_ignored_anchors,
                       family='TargetAssignment')
 
   def _apply_hard_mining(self, location_losses, cls_losses, prediction_dict,
@@ -1054,6 +1068,7 @@ class SSDMetaArch(model.DetectionModel):
           [batch_size, num_anchors, num_classes+1] containing class predictions
           (logits) for each of the anchors.  Note that this tensor *includes*
           background class predictions.
+        3) anchors: (optional) 2-D float tensor of shape [num_anchors, 4].
       match_list: a list of matcher.Match objects encoding the match between
         anchors and groundtruth boxes for each image of the batch,
         with rows of the Match objects corresponding to groundtruth boxes
@@ -1069,7 +1084,10 @@ class SSDMetaArch(model.DetectionModel):
     if self._add_background_class:
       class_predictions = tf.slice(class_predictions, [0, 0, 1], [-1, -1, -1])
 
-    decoded_boxes, _ = self._batch_decode(prediction_dict['box_encodings'])
+    if 'anchors' not in prediction_dict:
+      prediction_dict['anchors'] = self.anchors.get()
+    decoded_boxes, _ = self._batch_decode(prediction_dict['box_encodings'],
+                                          prediction_dict['anchors'])
     decoded_box_tensors_list = tf.unstack(decoded_boxes)
     class_prediction_list = tf.unstack(class_predictions)
     decoded_boxlist_list = []
@@ -1084,12 +1102,13 @@ class SSDMetaArch(model.DetectionModel):
         decoded_boxlist_list=decoded_boxlist_list,
         match_list=match_list)
 
-  def _batch_decode(self, box_encodings):
+  def _batch_decode(self, box_encodings, anchors):
     """Decodes a batch of box encodings with respect to the anchors.
 
     Args:
       box_encodings: A float32 tensor of shape
         [batch_size, num_anchors, box_code_size] containing box encodings.
+      anchors: A tensor of shape [num_anchors, 4].
 
     Returns:
       decoded_boxes: A float32 tensor of shape
@@ -1101,8 +1120,7 @@ class SSDMetaArch(model.DetectionModel):
     combined_shape = shape_utils.combined_static_and_dynamic_shape(
         box_encodings)
     batch_size = combined_shape[0]
-    tiled_anchor_boxes = tf.tile(
-        tf.expand_dims(self.anchors.get(), 0), [batch_size, 1, 1])
+    tiled_anchor_boxes = tf.tile(tf.expand_dims(anchors, 0), [batch_size, 1, 1])
     tiled_anchors_boxlist = box_list.BoxList(
         tf.reshape(tiled_anchor_boxes, [-1, 4]))
     decoded_boxes = self._box_coder.decode(
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index 46d032f9..f5b91c7b 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -311,8 +311,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                             [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]],
                            [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
                             [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]]]
-    raw_detection_scores = [[[0, 0], [0, 0], [0, 0], [0, 0]],
-                            [[0, 0], [0, 0], [0, 0], [0, 0]]]
+    raw_detection_scores = [[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]],
+                            [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]
 
     for input_shape in input_shapes:
       tf_graph = tf.Graph()
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
index 63f789a4..bb98d83c 100644
--- a/research/object_detection/metrics/coco_evaluation.py
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -197,6 +197,7 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
       'PerformanceByCategory' is included in the output regardless of
       all_metrics_per_category.
     """
+    tf.logging.info('Performing evaluation on %d images.', len(self._image_ids))
     groundtruth_dict = {
         'annotations': self._groundtruth_list,
         'images': [{'id': image_id} for image_id in self._image_ids],
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 98dad278..b79320c1 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -24,6 +24,7 @@ import os
 
 import tensorflow as tf
 
+from tensorflow.python.util import function_utils
 from object_detection import eval_util
 from object_detection import exporter as exporter_lib
 from object_detection import inputs
@@ -33,6 +34,7 @@ from object_detection.builders import optimizer_builder
 from object_detection.core import standard_fields as fields
 from object_detection.utils import config_util
 from object_detection.utils import label_map_util
+from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from object_detection.utils import variables_helper
 from object_detection.utils import visualization_utils as vis_utils
@@ -279,9 +281,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
         prediction_dict = detection_model.predict(
             preprocessed_images,
             features[fields.InputDataFields.true_image_shape])
-        for k, v in prediction_dict.items():
-          if v.dtype == tf.bfloat16:
-            prediction_dict[k] = tf.cast(v, tf.float32)
+        prediction_dict = ops.bfloat16_to_float32_nested(prediction_dict)
     else:
       prediction_dict = detection_model.predict(
           preprocessed_images,
@@ -338,6 +338,9 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
       losses = [loss_tensor for loss_tensor in losses_dict.values()]
       if train_config.add_regularization_loss:
         regularization_losses = detection_model.regularization_losses()
+        if use_tpu and train_config.use_bfloat16:
+          regularization_losses = ops.bfloat16_to_float32_nested(
+              regularization_losses)
         if regularization_losses:
           regularization_loss = tf.add_n(
               regularization_losses, name='regularization_loss')
@@ -650,6 +653,11 @@ def create_estimator_and_inputs(run_config,
   model_fn = model_fn_creator(detection_model_fn, configs, hparams, use_tpu,
                               postprocess_on_cpu)
   if use_tpu_estimator:
+    # Multicore inference disabled due to b/129367127
+    tpu_estimator_args = function_utils.fn_args(tf.contrib.tpu.TPUEstimator)
+    kwargs = {}
+    if 'experimental_export_device_assignment' in tpu_estimator_args:
+      kwargs['experimental_export_device_assignment'] = True
     estimator = tf.contrib.tpu.TPUEstimator(
         model_fn=model_fn,
         train_batch_size=train_config.batch_size,
@@ -659,7 +667,8 @@ def create_estimator_and_inputs(run_config,
         config=run_config,
         export_to_tpu=export_to_tpu,
         eval_on_tpu=False,  # Eval runs on CPU, so disable eval on TPU
-        params=params if params else {})
+        params=params if params else {},
+        **kwargs)
   else:
     estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)
 
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
new file mode 100644
index 00000000..a7f97fe9
--- /dev/null
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
@@ -0,0 +1,1078 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Inception Resnet v2 Faster R-CNN implementation in Keras.
+
+See "Inception-v4, Inception-ResNet and the Impact of Residual Connections on
+Learning" by Szegedy et al. (https://arxiv.org/abs/1602.07261)
+as well as
+"Speed/accuracy trade-offs for modern convolutional object detectors" by
+Huang et al. (https://arxiv.org/abs/1611.10012)
+"""
+
+# Skip pylint for this file because it times out
+# pylint: skip-file
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import faster_rcnn_meta_arch
+from object_detection.models.keras_models import inception_resnet_v2
+from object_detection.utils import model_util
+
+
+class FasterRCNNInceptionResnetV2KerasFeatureExtractor(
+    faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor):
+  """Faster R-CNN with Inception Resnet v2 feature extractor implementation."""
+
+  def __init__(self,
+               is_training,
+               first_stage_features_stride,
+               batch_norm_trainable=False,
+               weight_decay=0.0):
+    """Constructor.
+
+    Args:
+      is_training: See base class.
+      first_stage_features_stride: See base class.
+      batch_norm_trainable: See base class.
+      weight_decay: See base class.
+
+    Raises:
+      ValueError: If `first_stage_features_stride` is not 8 or 16.
+    """
+    if first_stage_features_stride != 8 and first_stage_features_stride != 16:
+      raise ValueError('`first_stage_features_stride` must be 8 or 16.')
+    super(FasterRCNNInceptionResnetV2KerasFeatureExtractor, self).__init__(
+        is_training, first_stage_features_stride, batch_norm_trainable,
+        weight_decay)
+
+  def preprocess(self, resized_inputs):
+    """Faster R-CNN with Inception Resnet v2 preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor
+        representing a batch of images with values between 0 and 255.0.
+
+    Returns:
+      preprocessed_inputs: A [batch, height_out, width_out, channels] float32
+        tensor representing a batch of images.
+
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def get_proposal_feature_extractor_model(self, name=None):
+    """Returns a model that extracts first stage RPN features.
+
+    Extracts features using the first half of the Inception Resnet v2 network.
+    We construct the network in `align_feature_maps=True` mode, which means
+    that all VALID paddings in the network are changed to SAME padding so that
+    the feature maps are aligned.
+
+    Args:
+      name: A scope name to construct all variables within.
+
+    Returns:
+      A Keras model that takes preprocessed_inputs:
+        A [batch, height, width, channels] float32 tensor
+        representing a batch of images.
+
+      And returns rpn_feature_map:
+        A tensor with shape [batch, height, width, depth]
+    """
+    with tf.name_scope(name):
+      with tf.name_scope('InceptionResnetV2'):
+        model = inception_resnet_v2.inception_resnet_v2(
+              self._train_batch_norm,
+              output_stride=self._first_stage_features_stride,
+              align_feature_maps=True,
+              weight_decay=self._weight_decay,
+              weights=None,
+              include_top=False)
+        proposal_features = model.get_layer(
+            name='block17_20_ac').output
+        return tf.keras.Model(
+            inputs=model.inputs,
+            outputs=proposal_features)
+
+  def get_box_classifier_feature_extractor_model(self, name=None):
+    """Returns a model that extracts second stage box classifier features.
+
+    This function reconstructs the "second half" of the Inception ResNet v2
+    network after the part defined in `get_proposal_feature_extractor_model`.
+
+    Args:
+      name: A scope name to construct all variables within.
+
+    Returns:
+      A Keras model that takes proposal_feature_maps:
+        A 4-D float tensor with shape
+        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]
+        representing the feature map cropped to each proposal.
+      And returns proposal_classifier_features:
+        A 4-D float tensor with shape
+        [batch_size * self.max_num_proposals, height, width, depth]
+        representing box classifier features for each proposal.
+    """
+    with tf.name_scope(name):
+      with tf.name_scope('InceptionResnetV2'):
+        model = inception_resnet_v2.inception_resnet_v2(
+            self._train_batch_norm,
+            output_stride=16,
+            align_feature_maps=False,
+            weight_decay=self._weight_decay,
+            weights=None,
+            include_top=False)
+
+        proposal_feature_maps = model.get_layer(
+            name='block17_20_ac').output
+        proposal_classifier_features = model.get_layer(
+            name='conv_7b_ac').output
+
+        return model_util.extract_submodel(
+            model=model,
+            inputs=proposal_feature_maps,
+            outputs=proposal_classifier_features)
+
+  def restore_from_classification_checkpoint_fn(
+      self,
+      first_stage_feature_extractor_scope,
+      second_stage_feature_extractor_scope):
+    """Returns a map of variables to load from a foreign checkpoint.
+
+    This uses a hard-coded conversion to load into Keras from a slim-trained
+    inception_resnet_v2 checkpoint.
+    Note that this overrides the default implementation in
+    faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor which does not work
+    for InceptionResnetV2 checkpoints.
+
+    Args:
+      first_stage_feature_extractor_scope: A scope name for the first stage
+        feature extractor.
+      second_stage_feature_extractor_scope: A scope name for the second stage
+        feature extractor.
+
+    Returns:
+      A dict mapping variable names (to load from a checkpoint) to variables in
+      the model graph.
+    """
+
+    keras_to_slim_name_mapping = {
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d/kernel': 'InceptionResnetV2/Conv2d_1a_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm/beta': 'InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm/moving_mean': 'InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm/moving_variance': 'InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_1/kernel': 'InceptionResnetV2/Conv2d_2a_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_1/beta': 'InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_1/moving_mean': 'InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_1/moving_variance': 'InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_2/kernel': 'InceptionResnetV2/Conv2d_2b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_2/beta': 'InceptionResnetV2/Conv2d_2b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_2/moving_mean': 'InceptionResnetV2/Conv2d_2b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_2/moving_variance': 'InceptionResnetV2/Conv2d_2b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_3/kernel': 'InceptionResnetV2/Conv2d_3b_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_3/beta': 'InceptionResnetV2/Conv2d_3b_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_3/moving_mean': 'InceptionResnetV2/Conv2d_3b_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_3/moving_variance': 'InceptionResnetV2/Conv2d_3b_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_4/kernel': 'InceptionResnetV2/Conv2d_4a_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_4/beta': 'InceptionResnetV2/Conv2d_4a_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_4/moving_mean': 'InceptionResnetV2/Conv2d_4a_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_4/moving_variance': 'InceptionResnetV2/Conv2d_4a_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_5/kernel': 'InceptionResnetV2/Mixed_5b/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_5/beta': 'InceptionResnetV2/Mixed_5b/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_5/moving_mean': 'InceptionResnetV2/Mixed_5b/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_5/moving_variance': 'InceptionResnetV2/Mixed_5b/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_6/kernel': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_6/beta': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_6/moving_mean': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_6/moving_variance': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_7/kernel': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_7/beta': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_7/moving_mean': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_7/moving_variance': 'InceptionResnetV2/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_8/kernel': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_8/beta': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_8/moving_mean': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_8/moving_variance': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_9/kernel': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_9/beta': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_9/moving_mean': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_9/moving_variance': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_10/kernel': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_10/beta': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_10/moving_mean': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_10/moving_variance': 'InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_11/kernel': 'InceptionResnetV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_11/beta': 'InceptionResnetV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_11/moving_mean': 'InceptionResnetV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_11/moving_variance': 'InceptionResnetV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_12/kernel': 'InceptionResnetV2/Repeat/block35_1/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_12/beta': 'InceptionResnetV2/Repeat/block35_1/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_12/moving_mean': 'InceptionResnetV2/Repeat/block35_1/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_12/moving_variance': 'InceptionResnetV2/Repeat/block35_1/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_13/kernel': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_13/beta': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_13/moving_mean': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_13/moving_variance': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_14/kernel': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_14/beta': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_14/moving_mean': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_14/moving_variance': 'InceptionResnetV2/Repeat/block35_1/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_15/kernel': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_15/beta': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_15/moving_mean': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_15/moving_variance': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_16/kernel': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_16/beta': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_16/moving_mean': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_16/moving_variance': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_17/kernel': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_17/beta': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_17/moving_mean': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_17/moving_variance': 'InceptionResnetV2/Repeat/block35_1/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_1_conv/kernel': 'InceptionResnetV2/Repeat/block35_1/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_1_conv/bias': 'InceptionResnetV2/Repeat/block35_1/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_18/kernel': 'InceptionResnetV2/Repeat/block35_2/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_18/beta': 'InceptionResnetV2/Repeat/block35_2/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_18/moving_mean': 'InceptionResnetV2/Repeat/block35_2/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_18/moving_variance': 'InceptionResnetV2/Repeat/block35_2/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_19/kernel': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_19/beta': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_19/moving_mean': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_19/moving_variance': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_20/kernel': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_20/beta': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_20/moving_mean': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_20/moving_variance': 'InceptionResnetV2/Repeat/block35_2/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_21/kernel': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_21/beta': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_21/moving_mean': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_21/moving_variance': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_22/kernel': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_22/beta': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_22/moving_mean': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_22/moving_variance': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_23/kernel': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_23/beta': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_23/moving_mean': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_23/moving_variance': 'InceptionResnetV2/Repeat/block35_2/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_2_conv/kernel': 'InceptionResnetV2/Repeat/block35_2/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_2_conv/bias': 'InceptionResnetV2/Repeat/block35_2/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_24/kernel': 'InceptionResnetV2/Repeat/block35_3/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_24/beta': 'InceptionResnetV2/Repeat/block35_3/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_24/moving_mean': 'InceptionResnetV2/Repeat/block35_3/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_24/moving_variance': 'InceptionResnetV2/Repeat/block35_3/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_25/kernel': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_25/beta': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_25/moving_mean': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_25/moving_variance': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_26/kernel': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_26/beta': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_26/moving_mean': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_26/moving_variance': 'InceptionResnetV2/Repeat/block35_3/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_27/kernel': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_27/beta': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_27/moving_mean': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_27/moving_variance': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_28/kernel': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_28/beta': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_28/moving_mean': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_28/moving_variance': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_29/kernel': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_29/beta': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_29/moving_mean': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_29/moving_variance': 'InceptionResnetV2/Repeat/block35_3/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_3_conv/kernel': 'InceptionResnetV2/Repeat/block35_3/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_3_conv/bias': 'InceptionResnetV2/Repeat/block35_3/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_30/kernel': 'InceptionResnetV2/Repeat/block35_4/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_30/beta': 'InceptionResnetV2/Repeat/block35_4/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_30/moving_mean': 'InceptionResnetV2/Repeat/block35_4/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_30/moving_variance': 'InceptionResnetV2/Repeat/block35_4/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_31/kernel': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_31/beta': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_31/moving_mean': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_31/moving_variance': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_32/kernel': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_32/beta': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_32/moving_mean': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_32/moving_variance': 'InceptionResnetV2/Repeat/block35_4/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_33/kernel': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_33/beta': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_33/moving_mean': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_33/moving_variance': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_34/kernel': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_34/beta': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_34/moving_mean': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_34/moving_variance': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_35/kernel': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_35/beta': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_35/moving_mean': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_35/moving_variance': 'InceptionResnetV2/Repeat/block35_4/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_4_conv/kernel': 'InceptionResnetV2/Repeat/block35_4/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_4_conv/bias': 'InceptionResnetV2/Repeat/block35_4/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_36/kernel': 'InceptionResnetV2/Repeat/block35_5/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_36/beta': 'InceptionResnetV2/Repeat/block35_5/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_36/moving_mean': 'InceptionResnetV2/Repeat/block35_5/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_36/moving_variance': 'InceptionResnetV2/Repeat/block35_5/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_37/kernel': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_37/beta': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_37/moving_mean': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_37/moving_variance': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_38/kernel': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_38/beta': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_38/moving_mean': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_38/moving_variance': 'InceptionResnetV2/Repeat/block35_5/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_39/kernel': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_39/beta': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_39/moving_mean': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_39/moving_variance': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_40/kernel': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_40/beta': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_40/moving_mean': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_40/moving_variance': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_41/kernel': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_41/beta': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_41/moving_mean': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_41/moving_variance': 'InceptionResnetV2/Repeat/block35_5/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_5_conv/kernel': 'InceptionResnetV2/Repeat/block35_5/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_5_conv/bias': 'InceptionResnetV2/Repeat/block35_5/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_42/kernel': 'InceptionResnetV2/Repeat/block35_6/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_42/beta': 'InceptionResnetV2/Repeat/block35_6/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_42/moving_mean': 'InceptionResnetV2/Repeat/block35_6/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_42/moving_variance': 'InceptionResnetV2/Repeat/block35_6/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_43/kernel': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_43/beta': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_43/moving_mean': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_43/moving_variance': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_44/kernel': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_44/beta': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_44/moving_mean': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_44/moving_variance': 'InceptionResnetV2/Repeat/block35_6/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_45/kernel': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_45/beta': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_45/moving_mean': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_45/moving_variance': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_46/kernel': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_46/beta': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_46/moving_mean': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_46/moving_variance': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_47/kernel': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_47/beta': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_47/moving_mean': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_47/moving_variance': 'InceptionResnetV2/Repeat/block35_6/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_6_conv/kernel': 'InceptionResnetV2/Repeat/block35_6/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_6_conv/bias': 'InceptionResnetV2/Repeat/block35_6/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_48/kernel': 'InceptionResnetV2/Repeat/block35_7/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_48/beta': 'InceptionResnetV2/Repeat/block35_7/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_48/moving_mean': 'InceptionResnetV2/Repeat/block35_7/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_48/moving_variance': 'InceptionResnetV2/Repeat/block35_7/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_49/kernel': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_49/beta': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_49/moving_mean': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_49/moving_variance': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_50/kernel': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_50/beta': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_50/moving_mean': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_50/moving_variance': 'InceptionResnetV2/Repeat/block35_7/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_51/kernel': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_51/beta': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_51/moving_mean': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_51/moving_variance': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_52/kernel': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_52/beta': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_52/moving_mean': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_52/moving_variance': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_53/kernel': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_53/beta': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_53/moving_mean': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_53/moving_variance': 'InceptionResnetV2/Repeat/block35_7/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_7_conv/kernel': 'InceptionResnetV2/Repeat/block35_7/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_7_conv/bias': 'InceptionResnetV2/Repeat/block35_7/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_54/kernel': 'InceptionResnetV2/Repeat/block35_8/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_54/beta': 'InceptionResnetV2/Repeat/block35_8/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_54/moving_mean': 'InceptionResnetV2/Repeat/block35_8/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_54/moving_variance': 'InceptionResnetV2/Repeat/block35_8/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_55/kernel': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_55/beta': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_55/moving_mean': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_55/moving_variance': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_56/kernel': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_56/beta': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_56/moving_mean': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_56/moving_variance': 'InceptionResnetV2/Repeat/block35_8/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_57/kernel': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_57/beta': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_57/moving_mean': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_57/moving_variance': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_58/kernel': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_58/beta': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_58/moving_mean': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_58/moving_variance': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_59/kernel': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_59/beta': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_59/moving_mean': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_59/moving_variance': 'InceptionResnetV2/Repeat/block35_8/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_8_conv/kernel': 'InceptionResnetV2/Repeat/block35_8/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_8_conv/bias': 'InceptionResnetV2/Repeat/block35_8/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_60/kernel': 'InceptionResnetV2/Repeat/block35_9/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_60/beta': 'InceptionResnetV2/Repeat/block35_9/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_60/moving_mean': 'InceptionResnetV2/Repeat/block35_9/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_60/moving_variance': 'InceptionResnetV2/Repeat/block35_9/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_61/kernel': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_61/beta': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_61/moving_mean': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_61/moving_variance': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_62/kernel': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_62/beta': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_62/moving_mean': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_62/moving_variance': 'InceptionResnetV2/Repeat/block35_9/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_63/kernel': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_63/beta': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_63/moving_mean': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_63/moving_variance': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_64/kernel': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_64/beta': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_64/moving_mean': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_64/moving_variance': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_65/kernel': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_65/beta': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_65/moving_mean': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_65/moving_variance': 'InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_9_conv/kernel': 'InceptionResnetV2/Repeat/block35_9/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_9_conv/bias': 'InceptionResnetV2/Repeat/block35_9/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_66/kernel': 'InceptionResnetV2/Repeat/block35_10/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_66/beta': 'InceptionResnetV2/Repeat/block35_10/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_66/moving_mean': 'InceptionResnetV2/Repeat/block35_10/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_66/moving_variance': 'InceptionResnetV2/Repeat/block35_10/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_67/kernel': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_67/beta': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_67/moving_mean': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_67/moving_variance': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_68/kernel': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_68/beta': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_68/moving_mean': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_68/moving_variance': 'InceptionResnetV2/Repeat/block35_10/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_69/kernel': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_69/beta': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_69/moving_mean': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_69/moving_variance': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_70/kernel': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_70/beta': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_70/moving_mean': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_70/moving_variance': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_71/kernel': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0c_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_71/beta': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0c_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_71/moving_mean': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_71/moving_variance': 'InceptionResnetV2/Repeat/block35_10/Branch_2/Conv2d_0c_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_10_conv/kernel': 'InceptionResnetV2/Repeat/block35_10/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block35_10_conv/bias': 'InceptionResnetV2/Repeat/block35_10/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_72/kernel': 'InceptionResnetV2/Mixed_6a/Branch_0/Conv2d_1a_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_72/beta': 'InceptionResnetV2/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_72/moving_mean': 'InceptionResnetV2/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_72/moving_variance': 'InceptionResnetV2/Mixed_6a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_73/kernel': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_73/beta': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_73/moving_mean': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_73/moving_variance': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_74/kernel': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_74/beta': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_74/moving_mean': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_74/moving_variance': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_75/kernel': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_1a_3x3/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_75/beta': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_75/moving_mean': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_75/moving_variance': 'InceptionResnetV2/Mixed_6a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_76/kernel': 'InceptionResnetV2/Repeat_1/block17_1/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_76/beta': 'InceptionResnetV2/Repeat_1/block17_1/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_76/moving_mean': 'InceptionResnetV2/Repeat_1/block17_1/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_76/moving_variance': 'InceptionResnetV2/Repeat_1/block17_1/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_77/kernel': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_77/beta': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_77/moving_mean': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_77/moving_variance': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_78/kernel': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_78/beta': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_78/moving_mean': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_78/moving_variance': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_79/kernel': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_79/beta': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_79/moving_mean': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_79/moving_variance': 'InceptionResnetV2/Repeat_1/block17_1/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_1_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_1/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_1_conv/bias': 'InceptionResnetV2/Repeat_1/block17_1/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_80/kernel': 'InceptionResnetV2/Repeat_1/block17_2/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_80/beta': 'InceptionResnetV2/Repeat_1/block17_2/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_80/moving_mean': 'InceptionResnetV2/Repeat_1/block17_2/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_80/moving_variance': 'InceptionResnetV2/Repeat_1/block17_2/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_81/kernel': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_81/beta': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_81/moving_mean': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_81/moving_variance': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_82/kernel': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_82/beta': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_82/moving_mean': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_82/moving_variance': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_83/kernel': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_83/beta': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_83/moving_mean': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_83/moving_variance': 'InceptionResnetV2/Repeat_1/block17_2/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_2_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_2/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_2_conv/bias': 'InceptionResnetV2/Repeat_1/block17_2/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_84/kernel': 'InceptionResnetV2/Repeat_1/block17_3/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_84/beta': 'InceptionResnetV2/Repeat_1/block17_3/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_84/moving_mean': 'InceptionResnetV2/Repeat_1/block17_3/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_84/moving_variance': 'InceptionResnetV2/Repeat_1/block17_3/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_85/kernel': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_85/beta': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_85/moving_mean': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_85/moving_variance': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_86/kernel': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_86/beta': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_86/moving_mean': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_86/moving_variance': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_87/kernel': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_87/beta': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_87/moving_mean': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_87/moving_variance': 'InceptionResnetV2/Repeat_1/block17_3/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_3_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_3/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_3_conv/bias': 'InceptionResnetV2/Repeat_1/block17_3/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_88/kernel': 'InceptionResnetV2/Repeat_1/block17_4/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_88/beta': 'InceptionResnetV2/Repeat_1/block17_4/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_88/moving_mean': 'InceptionResnetV2/Repeat_1/block17_4/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_88/moving_variance': 'InceptionResnetV2/Repeat_1/block17_4/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_89/kernel': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_89/beta': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_89/moving_mean': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_89/moving_variance': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_90/kernel': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_90/beta': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_90/moving_mean': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_90/moving_variance': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_91/kernel': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_91/beta': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_91/moving_mean': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_91/moving_variance': 'InceptionResnetV2/Repeat_1/block17_4/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_4_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_4/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_4_conv/bias': 'InceptionResnetV2/Repeat_1/block17_4/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_92/kernel': 'InceptionResnetV2/Repeat_1/block17_5/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_92/beta': 'InceptionResnetV2/Repeat_1/block17_5/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_92/moving_mean': 'InceptionResnetV2/Repeat_1/block17_5/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_92/moving_variance': 'InceptionResnetV2/Repeat_1/block17_5/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_93/kernel': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_93/beta': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_93/moving_mean': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_93/moving_variance': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_94/kernel': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_94/beta': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_94/moving_mean': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_94/moving_variance': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_95/kernel': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_95/beta': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_95/moving_mean': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_95/moving_variance': 'InceptionResnetV2/Repeat_1/block17_5/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_5_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_5/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_5_conv/bias': 'InceptionResnetV2/Repeat_1/block17_5/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_96/kernel': 'InceptionResnetV2/Repeat_1/block17_6/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_96/beta': 'InceptionResnetV2/Repeat_1/block17_6/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_96/moving_mean': 'InceptionResnetV2/Repeat_1/block17_6/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_96/moving_variance': 'InceptionResnetV2/Repeat_1/block17_6/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_97/kernel': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_97/beta': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_97/moving_mean': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_97/moving_variance': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_98/kernel': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_98/beta': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_98/moving_mean': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_98/moving_variance': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_99/kernel': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_99/beta': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_99/moving_mean': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_99/moving_variance': 'InceptionResnetV2/Repeat_1/block17_6/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_6_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_6/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_6_conv/bias': 'InceptionResnetV2/Repeat_1/block17_6/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_100/kernel': 'InceptionResnetV2/Repeat_1/block17_7/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_100/beta': 'InceptionResnetV2/Repeat_1/block17_7/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_100/moving_mean': 'InceptionResnetV2/Repeat_1/block17_7/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_100/moving_variance': 'InceptionResnetV2/Repeat_1/block17_7/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_101/kernel': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_101/beta': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_101/moving_mean': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_101/moving_variance': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_102/kernel': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_102/beta': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_102/moving_mean': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_102/moving_variance': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_103/kernel': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_103/beta': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_103/moving_mean': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_103/moving_variance': 'InceptionResnetV2/Repeat_1/block17_7/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_7_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_7/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_7_conv/bias': 'InceptionResnetV2/Repeat_1/block17_7/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_104/kernel': 'InceptionResnetV2/Repeat_1/block17_8/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_104/beta': 'InceptionResnetV2/Repeat_1/block17_8/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_104/moving_mean': 'InceptionResnetV2/Repeat_1/block17_8/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_104/moving_variance': 'InceptionResnetV2/Repeat_1/block17_8/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_105/kernel': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_105/beta': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_105/moving_mean': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_105/moving_variance': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_106/kernel': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_106/beta': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_106/moving_mean': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_106/moving_variance': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_107/kernel': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_107/beta': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_107/moving_mean': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_107/moving_variance': 'InceptionResnetV2/Repeat_1/block17_8/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_8_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_8/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_8_conv/bias': 'InceptionResnetV2/Repeat_1/block17_8/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_108/kernel': 'InceptionResnetV2/Repeat_1/block17_9/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_108/beta': 'InceptionResnetV2/Repeat_1/block17_9/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_108/moving_mean': 'InceptionResnetV2/Repeat_1/block17_9/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_108/moving_variance': 'InceptionResnetV2/Repeat_1/block17_9/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_109/kernel': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_109/beta': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_109/moving_mean': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_109/moving_variance': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_110/kernel': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_110/beta': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_110/moving_mean': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_110/moving_variance': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_111/kernel': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_111/beta': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_111/moving_mean': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_111/moving_variance': 'InceptionResnetV2/Repeat_1/block17_9/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_9_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_9/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_9_conv/bias': 'InceptionResnetV2/Repeat_1/block17_9/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_112/kernel': 'InceptionResnetV2/Repeat_1/block17_10/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_112/beta': 'InceptionResnetV2/Repeat_1/block17_10/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_112/moving_mean': 'InceptionResnetV2/Repeat_1/block17_10/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_112/moving_variance': 'InceptionResnetV2/Repeat_1/block17_10/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_113/kernel': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_113/beta': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_113/moving_mean': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_113/moving_variance': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_114/kernel': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_114/beta': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_114/moving_mean': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_114/moving_variance': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_115/kernel': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_115/beta': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_115/moving_mean': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_115/moving_variance': 'InceptionResnetV2/Repeat_1/block17_10/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_10_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_10/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_10_conv/bias': 'InceptionResnetV2/Repeat_1/block17_10/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_116/kernel': 'InceptionResnetV2/Repeat_1/block17_11/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_116/beta': 'InceptionResnetV2/Repeat_1/block17_11/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_116/moving_mean': 'InceptionResnetV2/Repeat_1/block17_11/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_116/moving_variance': 'InceptionResnetV2/Repeat_1/block17_11/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_117/kernel': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_117/beta': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_117/moving_mean': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_117/moving_variance': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_118/kernel': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_118/beta': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_118/moving_mean': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_118/moving_variance': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_119/kernel': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_119/beta': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_119/moving_mean': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_119/moving_variance': 'InceptionResnetV2/Repeat_1/block17_11/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_11_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_11/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_11_conv/bias': 'InceptionResnetV2/Repeat_1/block17_11/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_120/kernel': 'InceptionResnetV2/Repeat_1/block17_12/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_120/beta': 'InceptionResnetV2/Repeat_1/block17_12/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_120/moving_mean': 'InceptionResnetV2/Repeat_1/block17_12/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_120/moving_variance': 'InceptionResnetV2/Repeat_1/block17_12/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_121/kernel': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_121/beta': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_121/moving_mean': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_121/moving_variance': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_122/kernel': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_122/beta': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_122/moving_mean': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_122/moving_variance': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_123/kernel': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_123/beta': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_123/moving_mean': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_123/moving_variance': 'InceptionResnetV2/Repeat_1/block17_12/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_12_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_12/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_12_conv/bias': 'InceptionResnetV2/Repeat_1/block17_12/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_124/kernel': 'InceptionResnetV2/Repeat_1/block17_13/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_124/beta': 'InceptionResnetV2/Repeat_1/block17_13/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_124/moving_mean': 'InceptionResnetV2/Repeat_1/block17_13/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_124/moving_variance': 'InceptionResnetV2/Repeat_1/block17_13/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_125/kernel': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_125/beta': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_125/moving_mean': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_125/moving_variance': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_126/kernel': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_126/beta': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_126/moving_mean': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_126/moving_variance': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_127/kernel': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_127/beta': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_127/moving_mean': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_127/moving_variance': 'InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_13_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_13/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_13_conv/bias': 'InceptionResnetV2/Repeat_1/block17_13/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_128/kernel': 'InceptionResnetV2/Repeat_1/block17_14/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_128/beta': 'InceptionResnetV2/Repeat_1/block17_14/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_128/moving_mean': 'InceptionResnetV2/Repeat_1/block17_14/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_128/moving_variance': 'InceptionResnetV2/Repeat_1/block17_14/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_129/kernel': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_129/beta': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_129/moving_mean': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_129/moving_variance': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_130/kernel': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_130/beta': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_130/moving_mean': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_130/moving_variance': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_131/kernel': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_131/beta': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_131/moving_mean': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_131/moving_variance': 'InceptionResnetV2/Repeat_1/block17_14/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_14_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_14/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_14_conv/bias': 'InceptionResnetV2/Repeat_1/block17_14/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_132/kernel': 'InceptionResnetV2/Repeat_1/block17_15/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_132/beta': 'InceptionResnetV2/Repeat_1/block17_15/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_132/moving_mean': 'InceptionResnetV2/Repeat_1/block17_15/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_132/moving_variance': 'InceptionResnetV2/Repeat_1/block17_15/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_133/kernel': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_133/beta': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_133/moving_mean': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_133/moving_variance': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_134/kernel': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_134/beta': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_134/moving_mean': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_134/moving_variance': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_135/kernel': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_135/beta': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_135/moving_mean': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_135/moving_variance': 'InceptionResnetV2/Repeat_1/block17_15/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_15_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_15/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_15_conv/bias': 'InceptionResnetV2/Repeat_1/block17_15/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_136/kernel': 'InceptionResnetV2/Repeat_1/block17_16/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_136/beta': 'InceptionResnetV2/Repeat_1/block17_16/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_136/moving_mean': 'InceptionResnetV2/Repeat_1/block17_16/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_136/moving_variance': 'InceptionResnetV2/Repeat_1/block17_16/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_137/kernel': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_137/beta': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_137/moving_mean': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_137/moving_variance': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_138/kernel': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_138/beta': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_138/moving_mean': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_138/moving_variance': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_139/kernel': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_139/beta': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_139/moving_mean': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_139/moving_variance': 'InceptionResnetV2/Repeat_1/block17_16/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_16_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_16/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_16_conv/bias': 'InceptionResnetV2/Repeat_1/block17_16/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_140/kernel': 'InceptionResnetV2/Repeat_1/block17_17/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_140/beta': 'InceptionResnetV2/Repeat_1/block17_17/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_140/moving_mean': 'InceptionResnetV2/Repeat_1/block17_17/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_140/moving_variance': 'InceptionResnetV2/Repeat_1/block17_17/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_141/kernel': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_141/beta': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_141/moving_mean': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_141/moving_variance': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_142/kernel': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_142/beta': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_142/moving_mean': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_142/moving_variance': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_143/kernel': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_143/beta': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_143/moving_mean': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_143/moving_variance': 'InceptionResnetV2/Repeat_1/block17_17/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_17_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_17/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_17_conv/bias': 'InceptionResnetV2/Repeat_1/block17_17/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_144/kernel': 'InceptionResnetV2/Repeat_1/block17_18/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_144/beta': 'InceptionResnetV2/Repeat_1/block17_18/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_144/moving_mean': 'InceptionResnetV2/Repeat_1/block17_18/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_144/moving_variance': 'InceptionResnetV2/Repeat_1/block17_18/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_145/kernel': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_145/beta': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_145/moving_mean': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_145/moving_variance': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_146/kernel': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_146/beta': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_146/moving_mean': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_146/moving_variance': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_147/kernel': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_147/beta': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_147/moving_mean': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_147/moving_variance': 'InceptionResnetV2/Repeat_1/block17_18/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_18_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_18/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_18_conv/bias': 'InceptionResnetV2/Repeat_1/block17_18/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_148/kernel': 'InceptionResnetV2/Repeat_1/block17_19/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_148/beta': 'InceptionResnetV2/Repeat_1/block17_19/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_148/moving_mean': 'InceptionResnetV2/Repeat_1/block17_19/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_148/moving_variance': 'InceptionResnetV2/Repeat_1/block17_19/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_149/kernel': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_149/beta': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_149/moving_mean': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_149/moving_variance': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_150/kernel': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_150/beta': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_150/moving_mean': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_150/moving_variance': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_151/kernel': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_151/beta': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_151/moving_mean': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_151/moving_variance': 'InceptionResnetV2/Repeat_1/block17_19/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_19_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_19/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_19_conv/bias': 'InceptionResnetV2/Repeat_1/block17_19/Conv2d_1x1/biases',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_152/kernel': 'InceptionResnetV2/Repeat_1/block17_20/Branch_0/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_152/beta': 'InceptionResnetV2/Repeat_1/block17_20/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_152/moving_mean': 'InceptionResnetV2/Repeat_1/block17_20/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_152/moving_variance': 'InceptionResnetV2/Repeat_1/block17_20/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_153/kernel': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0a_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_153/beta': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_153/moving_mean': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_153/moving_variance': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_154/kernel': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0b_1x7/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_154/beta': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0b_1x7/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_154/moving_mean': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_154/moving_variance': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/conv2d_155/kernel': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0c_7x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_155/beta': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0c_7x1/BatchNorm/beta',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_155/moving_mean': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean',
+        'FirstStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_155/moving_variance': 'InceptionResnetV2/Repeat_1/block17_20/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_20_conv/kernel': 'InceptionResnetV2/Repeat_1/block17_20/Conv2d_1x1/weights',
+        'FirstStageFeatureExtractor/InceptionResnetV2/block17_20_conv/bias': 'InceptionResnetV2/Repeat_1/block17_20/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_359/kernel': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_359/beta': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_359/moving_mean': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_359/moving_variance': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_360/kernel': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_360/beta': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_360/moving_mean': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_360/moving_variance': 'InceptionResnetV2/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_361/kernel': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_361/beta': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_361/moving_mean': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_361/moving_variance': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_362/kernel': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_362/beta': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_362/moving_mean': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_362/moving_variance': 'InceptionResnetV2/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_363/kernel': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_363/beta': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_363/moving_mean': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_363/moving_variance': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_364/kernel': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0b_3x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_364/beta': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0b_3x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_364/moving_mean': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_364/moving_variance': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_365/kernel': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_1a_3x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_365/beta': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_1a_3x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_365/moving_mean': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_1a_3x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_365/moving_variance': 'InceptionResnetV2/Mixed_7a/Branch_2/Conv2d_1a_3x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_366/kernel': 'InceptionResnetV2/Repeat_2/block8_1/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_366/beta': 'InceptionResnetV2/Repeat_2/block8_1/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_366/moving_mean': 'InceptionResnetV2/Repeat_2/block8_1/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_366/moving_variance': 'InceptionResnetV2/Repeat_2/block8_1/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_367/kernel': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_367/beta': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_367/moving_mean': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_367/moving_variance': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_368/kernel': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_368/beta': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_368/moving_mean': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_368/moving_variance': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_369/kernel': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_369/beta': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_369/moving_mean': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_369/moving_variance': 'InceptionResnetV2/Repeat_2/block8_1/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_1_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_1/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_1_conv/bias': 'InceptionResnetV2/Repeat_2/block8_1/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_370/kernel': 'InceptionResnetV2/Repeat_2/block8_2/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_370/beta': 'InceptionResnetV2/Repeat_2/block8_2/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_370/moving_mean': 'InceptionResnetV2/Repeat_2/block8_2/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_370/moving_variance': 'InceptionResnetV2/Repeat_2/block8_2/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_371/kernel': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_371/beta': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_371/moving_mean': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_371/moving_variance': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_372/kernel': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_372/beta': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_372/moving_mean': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_372/moving_variance': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_373/kernel': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_373/beta': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_373/moving_mean': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_373/moving_variance': 'InceptionResnetV2/Repeat_2/block8_2/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_2_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_2/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_2_conv/bias': 'InceptionResnetV2/Repeat_2/block8_2/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_374/kernel': 'InceptionResnetV2/Repeat_2/block8_3/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_374/beta': 'InceptionResnetV2/Repeat_2/block8_3/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_374/moving_mean': 'InceptionResnetV2/Repeat_2/block8_3/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_374/moving_variance': 'InceptionResnetV2/Repeat_2/block8_3/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_375/kernel': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_375/beta': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_375/moving_mean': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_375/moving_variance': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_376/kernel': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_376/beta': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_376/moving_mean': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_376/moving_variance': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_377/kernel': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_377/beta': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_377/moving_mean': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_377/moving_variance': 'InceptionResnetV2/Repeat_2/block8_3/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_3_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_3/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_3_conv/bias': 'InceptionResnetV2/Repeat_2/block8_3/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_378/kernel': 'InceptionResnetV2/Repeat_2/block8_4/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_378/beta': 'InceptionResnetV2/Repeat_2/block8_4/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_378/moving_mean': 'InceptionResnetV2/Repeat_2/block8_4/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_378/moving_variance': 'InceptionResnetV2/Repeat_2/block8_4/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_379/kernel': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_379/beta': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_379/moving_mean': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_379/moving_variance': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_380/kernel': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_380/beta': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_380/moving_mean': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_380/moving_variance': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_381/kernel': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_381/beta': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_381/moving_mean': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_381/moving_variance': 'InceptionResnetV2/Repeat_2/block8_4/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_4_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_4/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_4_conv/bias': 'InceptionResnetV2/Repeat_2/block8_4/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_382/kernel': 'InceptionResnetV2/Repeat_2/block8_5/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_382/beta': 'InceptionResnetV2/Repeat_2/block8_5/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_382/moving_mean': 'InceptionResnetV2/Repeat_2/block8_5/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_382/moving_variance': 'InceptionResnetV2/Repeat_2/block8_5/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_383/kernel': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_383/beta': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_383/moving_mean': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_383/moving_variance': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_384/kernel': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_384/beta': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_384/moving_mean': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_384/moving_variance': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_385/kernel': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_385/beta': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_385/moving_mean': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_385/moving_variance': 'InceptionResnetV2/Repeat_2/block8_5/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_5_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_5/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_5_conv/bias': 'InceptionResnetV2/Repeat_2/block8_5/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_386/kernel': 'InceptionResnetV2/Repeat_2/block8_6/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_386/beta': 'InceptionResnetV2/Repeat_2/block8_6/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_386/moving_mean': 'InceptionResnetV2/Repeat_2/block8_6/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_386/moving_variance': 'InceptionResnetV2/Repeat_2/block8_6/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_387/kernel': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_387/beta': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_387/moving_mean': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_387/moving_variance': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_388/kernel': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_388/beta': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_388/moving_mean': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_388/moving_variance': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_389/kernel': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_389/beta': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_389/moving_mean': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_389/moving_variance': 'InceptionResnetV2/Repeat_2/block8_6/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_6_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_6/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_6_conv/bias': 'InceptionResnetV2/Repeat_2/block8_6/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_390/kernel': 'InceptionResnetV2/Repeat_2/block8_7/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_390/beta': 'InceptionResnetV2/Repeat_2/block8_7/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_390/moving_mean': 'InceptionResnetV2/Repeat_2/block8_7/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_390/moving_variance': 'InceptionResnetV2/Repeat_2/block8_7/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_391/kernel': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_391/beta': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_391/moving_mean': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_391/moving_variance': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_392/kernel': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_392/beta': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_392/moving_mean': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_392/moving_variance': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_393/kernel': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_393/beta': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_393/moving_mean': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_393/moving_variance': 'InceptionResnetV2/Repeat_2/block8_7/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_7_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_7/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_7_conv/bias': 'InceptionResnetV2/Repeat_2/block8_7/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_394/kernel': 'InceptionResnetV2/Repeat_2/block8_8/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_394/beta': 'InceptionResnetV2/Repeat_2/block8_8/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_394/moving_mean': 'InceptionResnetV2/Repeat_2/block8_8/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_394/moving_variance': 'InceptionResnetV2/Repeat_2/block8_8/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_395/kernel': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_395/beta': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_395/moving_mean': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_395/moving_variance': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_396/kernel': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_396/beta': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_396/moving_mean': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_396/moving_variance': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_397/kernel': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_397/beta': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_397/moving_mean': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_397/moving_variance': 'InceptionResnetV2/Repeat_2/block8_8/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_8_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_8/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_8_conv/bias': 'InceptionResnetV2/Repeat_2/block8_8/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_398/kernel': 'InceptionResnetV2/Repeat_2/block8_9/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_398/beta': 'InceptionResnetV2/Repeat_2/block8_9/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_398/moving_mean': 'InceptionResnetV2/Repeat_2/block8_9/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_398/moving_variance': 'InceptionResnetV2/Repeat_2/block8_9/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_399/kernel': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_399/beta': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_399/moving_mean': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_399/moving_variance': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_400/kernel': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_400/beta': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_400/moving_mean': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_400/moving_variance': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_401/kernel': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_401/beta': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_401/moving_mean': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_401/moving_variance': 'InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_9_conv/kernel': 'InceptionResnetV2/Repeat_2/block8_9/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_9_conv/bias': 'InceptionResnetV2/Repeat_2/block8_9/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_402/kernel': 'InceptionResnetV2/Block8/Branch_0/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_402/beta': 'InceptionResnetV2/Block8/Branch_0/Conv2d_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_402/moving_mean': 'InceptionResnetV2/Block8/Branch_0/Conv2d_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_402/moving_variance': 'InceptionResnetV2/Block8/Branch_0/Conv2d_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_403/kernel': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0a_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_403/beta': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0a_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_403/moving_mean': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_403/moving_variance': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_404/kernel': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0b_1x3/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_404/beta': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0b_1x3/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_404/moving_mean': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_404/moving_variance': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv2d_405/kernel': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0c_3x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_405/beta': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0c_3x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_405/moving_mean': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/freezable_batch_norm_405/moving_variance': 'InceptionResnetV2/Block8/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_10_conv/kernel': 'InceptionResnetV2/Block8/Conv2d_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/block8_10_conv/bias': 'InceptionResnetV2/Block8/Conv2d_1x1/biases',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv_7b/kernel': 'InceptionResnetV2/Conv2d_7b_1x1/weights',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv_7b_bn/beta': 'InceptionResnetV2/Conv2d_7b_1x1/BatchNorm/beta',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv_7b_bn/moving_mean': 'InceptionResnetV2/Conv2d_7b_1x1/BatchNorm/moving_mean',
+        'SecondStageFeatureExtractor/InceptionResnetV2/conv_7b_bn/moving_variance': 'InceptionResnetV2/Conv2d_7b_1x1/BatchNorm/moving_variance',
+    }
+
+    variables_to_restore = {}
+    for variable in tf.global_variables():
+      var_name = keras_to_slim_name_mapping.get(variable.op.name)
+      if var_name:
+        variables_to_restore[var_name] = variable
+    return variables_to_restore
+
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py
new file mode 100644
index 00000000..f4df2a00
--- /dev/null
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py
@@ -0,0 +1,109 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for models.faster_rcnn_inception_resnet_v2_keras_feature_extractor."""
+
+import tensorflow as tf
+
+from object_detection.models import faster_rcnn_inception_resnet_v2_keras_feature_extractor as frcnn_inc_res
+
+
+class FasterRcnnInceptionResnetV2KerasFeatureExtractorTest(tf.test.TestCase):
+
+  def _build_feature_extractor(self, first_stage_features_stride):
+    return frcnn_inc_res.FasterRCNNInceptionResnetV2KerasFeatureExtractor(
+        is_training=False,
+        first_stage_features_stride=first_stage_features_stride,
+        batch_norm_trainable=False,
+        weight_decay=0.0)
+
+  def test_extract_proposal_features_returns_expected_size(self):
+    feature_extractor = self._build_feature_extractor(
+        first_stage_features_stride=16)
+    preprocessed_inputs = tf.random_uniform(
+        [1, 299, 299, 3], maxval=255, dtype=tf.float32)
+    rpn_feature_map = feature_extractor.get_proposal_feature_extractor_model(
+        name='TestScope')(preprocessed_inputs)
+    features_shape = tf.shape(rpn_feature_map)
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      features_shape_out = sess.run(features_shape)
+      self.assertAllEqual(features_shape_out, [1, 19, 19, 1088])
+
+  def test_extract_proposal_features_stride_eight(self):
+    feature_extractor = self._build_feature_extractor(
+        first_stage_features_stride=8)
+    preprocessed_inputs = tf.random_uniform(
+        [1, 224, 224, 3], maxval=255, dtype=tf.float32)
+    rpn_feature_map = feature_extractor.get_proposal_feature_extractor_model(
+        name='TestScope')(preprocessed_inputs)
+    features_shape = tf.shape(rpn_feature_map)
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      features_shape_out = sess.run(features_shape)
+      self.assertAllEqual(features_shape_out, [1, 28, 28, 1088])
+
+  def test_extract_proposal_features_half_size_input(self):
+    feature_extractor = self._build_feature_extractor(
+        first_stage_features_stride=16)
+    preprocessed_inputs = tf.random_uniform(
+        [1, 112, 112, 3], maxval=255, dtype=tf.float32)
+    rpn_feature_map = feature_extractor.get_proposal_feature_extractor_model(
+        name='TestScope')(preprocessed_inputs)
+    features_shape = tf.shape(rpn_feature_map)
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      features_shape_out = sess.run(features_shape)
+      self.assertAllEqual(features_shape_out, [1, 7, 7, 1088])
+
+  def test_extract_proposal_features_dies_on_invalid_stride(self):
+    with self.assertRaises(ValueError):
+      self._build_feature_extractor(first_stage_features_stride=99)
+
+  def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):
+    feature_extractor = self._build_feature_extractor(
+        first_stage_features_stride=16)
+    preprocessed_inputs = tf.random_uniform(
+        [224, 224, 3], maxval=255, dtype=tf.float32)
+    with self.assertRaises(ValueError):
+      feature_extractor.get_proposal_feature_extractor_model(
+          name='TestScope')(preprocessed_inputs)
+
+  def test_extract_box_classifier_features_returns_expected_size(self):
+    feature_extractor = self._build_feature_extractor(
+        first_stage_features_stride=16)
+    proposal_feature_maps = tf.random_uniform(
+        [2, 17, 17, 1088], maxval=255, dtype=tf.float32)
+    model = feature_extractor.get_box_classifier_feature_extractor_model(
+        name='TestScope')
+    proposal_classifier_features = (
+        model(proposal_feature_maps))
+    features_shape = tf.shape(proposal_classifier_features)
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      features_shape_out = sess.run(features_shape)
+      self.assertAllEqual(features_shape_out, [2, 8, 8, 1536])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index f48a4df9..6e6d3a58 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -51,6 +51,60 @@ def get_depth_fn(depth_multiplier, min_depth):
   return multiply_depth
 
 
+def create_conv_block(
+    use_depthwise, kernel_size, padding, stride, layer_name, conv_hyperparams,
+    is_training, freeze_batchnorm, depth):
+  """Create Keras layers for depthwise & non-depthwise convolutions.
+
+  Args:
+    use_depthwise: Whether to use depthwise separable conv instead of regular
+      conv.
+    kernel_size: A list of length 2: [kernel_height, kernel_width] of the
+      filters. Can be an int if both values are the same.
+    padding: One of 'VALID' or 'SAME'.
+    stride: A list of length 2: [stride_height, stride_width], specifying the
+      convolution stride. Can be an int if both strides are the same.
+    layer_name: String. The name of the layer.
+    conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+      containing hyperparameters for convolution ops.
+    is_training: Indicates whether the feature generator is in training mode.
+    freeze_batchnorm: Bool. Whether to freeze batch norm parameters during
+      training or not. When training with a small batch size (e.g. 1), it is
+      desirable to freeze batch norm update and use pretrained batch norm
+      params.
+    depth: Depth of output feature maps.
+
+  Returns:
+    A list of conv layers.
+  """
+  layers = []
+  if use_depthwise:
+    layers.append(tf.keras.layers.SeparableConv2D(
+        depth,
+        [kernel_size, kernel_size],
+        depth_multiplier=1,
+        padding=padding,
+        strides=stride,
+        name=layer_name + '_depthwise_conv',
+        **conv_hyperparams.params()))
+  else:
+    layers.append(tf.keras.layers.Conv2D(
+        depth,
+        [kernel_size, kernel_size],
+        padding=padding,
+        strides=stride,
+        name=layer_name + '_conv',
+        **conv_hyperparams.params()))
+  layers.append(
+      conv_hyperparams.build_batch_norm(
+          training=(is_training and not freeze_batchnorm),
+          name=layer_name + '_batchnorm'))
+  layers.append(
+      conv_hyperparams.build_activation_layer(
+          name=layer_name))
+  return layers
+
+
 class KerasMultiResolutionFeatureMaps(tf.keras.Model):
   """Generates multi resolution feature maps from input image features.
 
@@ -419,6 +473,180 @@ def multi_resolution_feature_maps(feature_map_layout, depth_multiplier,
       [(x, y) for (x, y) in zip(feature_map_keys, feature_maps)])
 
 
+class KerasFpnTopDownFeatureMaps(tf.keras.Model):
+  """Generates Keras based `top-down` feature maps for Feature Pyramid Networks.
+
+  See https://arxiv.org/abs/1612.03144 for details.
+  """
+
+  def __init__(self,
+               num_levels,
+               depth,
+               is_training,
+               conv_hyperparams,
+               freeze_batchnorm,
+               use_depthwise=False,
+               use_explicit_padding=False,
+               use_bounded_activations=False,
+               use_native_resize_op=False,
+               scope=None,
+               name=None):
+    """Constructor.
+
+    Args:
+      num_levels: the number of image features.
+      depth: depth of output feature maps.
+      is_training: Indicates whether the feature generator is in training mode.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops.
+      freeze_batchnorm: Bool. Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      use_depthwise: whether to use depthwise separable conv instead of regular
+        conv.
+      use_explicit_padding: whether to use explicit padding.
+      use_bounded_activations: Whether or not to clip activations to range
+        [-ACTIVATION_BOUND, ACTIVATION_BOUND]. Bounded activations better lend
+        themselves to quantized inference.
+      use_native_resize_op: If True, uses tf.image.resize_nearest_neighbor op
+        for the upsampling process instead of reshape and broadcasting
+        implementation.
+      scope: A scope name to wrap this op under.
+      name: A string name scope to assign to the model. If 'None', Keras
+        will auto-generate one from the class name.
+    """
+    super(KerasFpnTopDownFeatureMaps, self).__init__(name=name)
+
+    self.scope = scope if scope else 'top_down'
+    self.top_layers = []
+    self.residual_blocks = []
+    self.top_down_blocks = []
+    self.reshape_blocks = []
+    self.conv_layers = []
+
+    padding = 'VALID' if use_explicit_padding else 'SAME'
+    stride = 1
+    kernel_size = 3
+    def clip_by_value(features):
+      return tf.clip_by_value(features, -ACTIVATION_BOUND, ACTIVATION_BOUND)
+
+    # top layers
+    self.top_layers.append(tf.keras.layers.Conv2D(
+        depth, [1, 1], strides=stride, padding=padding,
+        name='projection_%d' % num_levels,
+        **conv_hyperparams.params(use_bias=True)))
+    if use_bounded_activations:
+      self.top_layers.append(tf.keras.layers.Lambda(
+          clip_by_value, name='clip_by_value'))
+
+    for level in reversed(range(num_levels - 1)):
+      # to generate residual from image features
+      residual_net = []
+      # to preprocess top_down (the image feature map from last layer)
+      top_down_net = []
+      # to reshape top_down according to residual if necessary
+      reshaped_residual = []
+      # to apply convolution layers to feature map
+      conv_net = []
+
+      # residual block
+      residual_net.append(tf.keras.layers.Conv2D(
+          depth, [1, 1], padding=padding, strides=1,
+          name='projection_%d' % (level + 1),
+          **conv_hyperparams.params(use_bias=True)))
+      if use_bounded_activations:
+        residual_net.append(tf.keras.layers.Lambda(
+            clip_by_value, name='clip_by_value'))
+
+      # top-down block
+      # TODO (b/128922690): clean-up of ops.nearest_neighbor_upsampling
+      if use_native_resize_op:
+        def resize_nearest_neighbor(image):
+          image_shape = image.shape.as_list()
+          return tf.image.resize_nearest_neighbor(
+              image, [image_shape[1] * 2, image_shape[2] * 2])
+        top_down_net.append(tf.keras.layers.Lambda(
+            resize_nearest_neighbor, name='nearest_neighbor_upsampling'))
+      else:
+        def nearest_neighbor_upsampling(image):
+          return ops.nearest_neighbor_upsampling(image, scale=2)
+        top_down_net.append(tf.keras.layers.Lambda(
+            nearest_neighbor_upsampling, name='nearest_neighbor_upsampling'))
+
+      # reshape block
+      if use_explicit_padding:
+        def reshape(inputs):
+          residual_shape = tf.shape(inputs[0])
+          return inputs[1][:, :residual_shape[1], :residual_shape[2], :]
+        reshaped_residual.append(
+            tf.keras.layers.Lambda(reshape, name='reshape'))
+
+      # down layers
+      if use_bounded_activations:
+        conv_net.append(tf.keras.layers.Lambda(
+            clip_by_value, name='clip_by_value'))
+
+      if use_explicit_padding:
+        def fixed_padding(features, kernel_size=kernel_size):
+          return ops.fixed_padding(features, kernel_size)
+        conv_net.append(tf.keras.layers.Lambda(
+            fixed_padding, name='fixed_padding'))
+
+      layer_name = 'smoothing_%d' % (level + 1)
+      conv_block = create_conv_block(
+          use_depthwise, kernel_size, padding, stride, layer_name,
+          conv_hyperparams, is_training, freeze_batchnorm, depth)
+      conv_net.extend(conv_block)
+
+      self.residual_blocks.append(residual_net)
+      self.top_down_blocks.append(top_down_net)
+      self.reshape_blocks.append(reshaped_residual)
+      self.conv_layers.append(conv_net)
+
+  def call(self, image_features):
+    """Generate the multi-resolution feature maps.
+
+    Executed when calling the `.__call__` method on input.
+
+    Args:
+      image_features: list of tuples of (tensor_name, image_feature_tensor).
+        Spatial resolutions of succesive tensors must reduce exactly by a factor
+        of 2.
+
+    Returns:
+      feature_maps: an OrderedDict mapping keys (feature map names) to
+        tensors where each tensor has shape [batch, height_i, width_i, depth_i].
+    """
+    output_feature_maps_list = []
+    output_feature_map_keys = []
+
+    with tf.name_scope(self.scope):
+      top_down = image_features[-1][1]
+      for layer in self.top_layers:
+        top_down = layer(top_down)
+      output_feature_maps_list.append(top_down)
+      output_feature_map_keys.append('top_down_%s' % image_features[-1][0])
+
+      num_levels = len(image_features)
+      for index, level in enumerate(reversed(range(num_levels - 1))):
+        residual = image_features[level][1]
+        top_down = output_feature_maps_list[-1]
+        for layer in self.residual_blocks[index]:
+          residual = layer(residual)
+        for layer in self.top_down_blocks[index]:
+          top_down = layer(top_down)
+        for layer in self.reshape_blocks[index]:
+          top_down = layer([residual, top_down])
+        top_down += residual
+        for layer in self.conv_layers[index]:
+          top_down = layer(top_down)
+        output_feature_maps_list.append(top_down)
+        output_feature_map_keys.append('top_down_%s' % image_features[level][0])
+    return collections.OrderedDict(reversed(
+        list(zip(output_feature_map_keys, output_feature_maps_list))))
+
+
 def fpn_top_down_feature_maps(image_features,
                               depth,
                               use_depthwise=False,
diff --git a/research/object_detection/models/feature_map_generators_test.py b/research/object_detection/models/feature_map_generators_test.py
index dabb918a..30a68188 100644
--- a/research/object_detection/models/feature_map_generators_test.py
+++ b/research/object_detection/models/feature_map_generators_test.py
@@ -403,21 +403,101 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
         self.assertSetEqual(expected_slim_variables, actual_variable_set)
 
 
-@parameterized.parameters({'use_native_resize_op': True},
-                          {'use_native_resize_op': False})
+@parameterized.parameters({'use_native_resize_op': True, 'use_keras': False},
+                          {'use_native_resize_op': False, 'use_keras': False},
+                          {'use_native_resize_op': True, 'use_keras': True},
+                          {'use_native_resize_op': False, 'use_keras': True})
 class FPNFeatureMapGeneratorTest(tf.test.TestCase, parameterized.TestCase):
 
-  def test_get_expected_feature_map_shapes(self, use_native_resize_op):
+  def _build_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  def _build_feature_map_generator(
+      self, image_features, depth, use_keras, use_bounded_activations=False,
+      use_native_resize_op=False, use_explicit_padding=False,
+      use_depthwise=False):
+    if use_keras:
+      return feature_map_generators.KerasFpnTopDownFeatureMaps(
+          num_levels=len(image_features),
+          depth=depth,
+          is_training=True,
+          conv_hyperparams=self._build_conv_hyperparams(),
+          freeze_batchnorm=False,
+          use_depthwise=use_depthwise,
+          use_explicit_padding=use_explicit_padding,
+          use_bounded_activations=use_bounded_activations,
+          use_native_resize_op=use_native_resize_op,
+          scope=None,
+          name='FeatureMaps',
+      )
+    else:
+      def feature_map_generator(image_features):
+        return feature_map_generators.fpn_top_down_feature_maps(
+            image_features=image_features,
+            depth=depth,
+            use_depthwise=use_depthwise,
+            use_explicit_padding=use_explicit_padding,
+            use_bounded_activations=use_bounded_activations,
+            use_native_resize_op=use_native_resize_op)
+      return feature_map_generator
+
+  def test_get_expected_feature_map_shapes(
+      self, use_native_resize_op, use_keras):
+    image_features = [
+        ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
+        ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
+        ('block4', tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
+        ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
+    ]
+    feature_map_generator = self._build_feature_map_generator(
+        image_features=image_features,
+        depth=128,
+        use_keras=use_keras,
+        use_native_resize_op=use_native_resize_op)
+    feature_maps = feature_map_generator(image_features)
+
+    expected_feature_map_shapes = {
+        'top_down_block2': (4, 8, 8, 128),
+        'top_down_block3': (4, 4, 4, 128),
+        'top_down_block4': (4, 2, 2, 128),
+        'top_down_block5': (4, 1, 1, 128)
+    }
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      out_feature_maps = sess.run(feature_maps)
+      out_feature_map_shapes = {key: value.shape
+                                for key, value in out_feature_maps.items()}
+      self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)
+
+  def test_get_expected_feature_map_shapes_with_explicit_padding(
+      self, use_native_resize_op, use_keras):
     image_features = [
         ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
         ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
         ('block4', tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
         ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
     ]
-    feature_maps = feature_map_generators.fpn_top_down_feature_maps(
+    feature_map_generator = self._build_feature_map_generator(
         image_features=image_features,
         depth=128,
+        use_keras=use_keras,
+        use_explicit_padding=True,
         use_native_resize_op=use_native_resize_op)
+    feature_maps = feature_map_generator(image_features)
 
     expected_feature_map_shapes = {
         'top_down_block2': (4, 8, 8, 128),
@@ -434,7 +514,8 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase, parameterized.TestCase):
                                 for key, value in out_feature_maps.items()}
       self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)
 
-  def test_use_bounded_activations_add_operations(self, use_native_resize_op):
+  def test_use_bounded_activations_add_operations(
+      self, use_native_resize_op, use_keras):
     tf_graph = tf.Graph()
     with tf_graph.as_default():
       image_features = [('block2',
@@ -445,22 +526,37 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase, parameterized.TestCase):
                          tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
                         ('block5',
                          tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))]
-      feature_map_generators.fpn_top_down_feature_maps(
+      feature_map_generator = self._build_feature_map_generator(
           image_features=image_features,
           depth=128,
+          use_keras=use_keras,
           use_bounded_activations=True,
           use_native_resize_op=use_native_resize_op)
+      feature_map_generator(image_features)
+
+      if use_keras:
+        expected_added_operations = dict.fromkeys([
+            'FeatureMaps/top_down/clip_by_value/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_1/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_2/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_3/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_4/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_5/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_6/clip_by_value',
+        ])
+      else:
+        expected_added_operations = dict.fromkeys([
+            'top_down/clip_by_value', 'top_down/clip_by_value_1',
+            'top_down/clip_by_value_2', 'top_down/clip_by_value_3',
+            'top_down/clip_by_value_4', 'top_down/clip_by_value_5',
+            'top_down/clip_by_value_6'
+        ])
 
-      expected_added_operations = dict.fromkeys([
-          'top_down/clip_by_value', 'top_down/clip_by_value_1',
-          'top_down/clip_by_value_2', 'top_down/clip_by_value_3',
-          'top_down/clip_by_value_4', 'top_down/clip_by_value_5',
-          'top_down/clip_by_value_6'
-      ])
       op_names = {op.name: None for op in tf_graph.get_operations()}
       self.assertDictContainsSubset(expected_added_operations, op_names)
 
-  def test_use_bounded_activations_clip_value(self, use_native_resize_op):
+  def test_use_bounded_activations_clip_value(
+      self, use_native_resize_op, use_keras):
     tf_graph = tf.Graph()
     with tf_graph.as_default():
       image_features = [
@@ -469,18 +565,31 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase, parameterized.TestCase):
           ('block4', 255 * tf.ones([4, 2, 2, 256], dtype=tf.float32)),
           ('block5', 255 * tf.ones([4, 1, 1, 256], dtype=tf.float32))
       ]
-      feature_map_generators.fpn_top_down_feature_maps(
+      feature_map_generator = self._build_feature_map_generator(
           image_features=image_features,
           depth=128,
+          use_keras=use_keras,
           use_bounded_activations=True,
           use_native_resize_op=use_native_resize_op)
+      feature_map_generator(image_features)
 
-      expected_clip_by_value_ops = [
-          'top_down/clip_by_value', 'top_down/clip_by_value_1',
-          'top_down/clip_by_value_2', 'top_down/clip_by_value_3',
-          'top_down/clip_by_value_4', 'top_down/clip_by_value_5',
-          'top_down/clip_by_value_6'
-      ]
+      if use_keras:
+        expected_clip_by_value_ops = dict.fromkeys([
+            'FeatureMaps/top_down/clip_by_value/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_1/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_2/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_3/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_4/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_5/clip_by_value',
+            'FeatureMaps/top_down/clip_by_value_6/clip_by_value',
+        ])
+      else:
+        expected_clip_by_value_ops = [
+            'top_down/clip_by_value', 'top_down/clip_by_value_1',
+            'top_down/clip_by_value_2', 'top_down/clip_by_value_3',
+            'top_down/clip_by_value_4', 'top_down/clip_by_value_5',
+            'top_down/clip_by_value_6'
+        ]
 
       # Gathers activation tensors before and after clip_by_value operations.
       activations = {}
@@ -522,18 +631,20 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase, parameterized.TestCase):
           self.assertLessEqual(after_clipping_upper_bound, expected_upper_bound)
 
   def test_get_expected_feature_map_shapes_with_depthwise(
-      self, use_native_resize_op):
+      self, use_native_resize_op, use_keras):
     image_features = [
         ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
         ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
         ('block4', tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
         ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
     ]
-    feature_maps = feature_map_generators.fpn_top_down_feature_maps(
+    feature_map_generator = self._build_feature_map_generator(
         image_features=image_features,
         depth=128,
+        use_keras=use_keras,
         use_depthwise=True,
         use_native_resize_op=use_native_resize_op)
+    feature_maps = feature_map_generator(image_features)
 
     expected_feature_map_shapes = {
         'top_down_block2': (4, 8, 8, 128),
@@ -550,6 +661,131 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase, parameterized.TestCase):
                                 for key, value in out_feature_maps.items()}
       self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)
 
+  def test_get_expected_variable_names(
+      self, use_native_resize_op, use_keras):
+    image_features = [
+        ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
+        ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
+        ('block4', tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
+        ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
+    ]
+    feature_map_generator = self._build_feature_map_generator(
+        image_features=image_features,
+        depth=128,
+        use_keras=use_keras,
+        use_native_resize_op=use_native_resize_op)
+    feature_maps = feature_map_generator(image_features)
+
+    expected_slim_variables = set([
+        'projection_1/weights',
+        'projection_1/biases',
+        'projection_2/weights',
+        'projection_2/biases',
+        'projection_3/weights',
+        'projection_3/biases',
+        'projection_4/weights',
+        'projection_4/biases',
+        'smoothing_1/weights',
+        'smoothing_1/biases',
+        'smoothing_2/weights',
+        'smoothing_2/biases',
+        'smoothing_3/weights',
+        'smoothing_3/biases',
+    ])
+
+    expected_keras_variables = set([
+        'FeatureMaps/top_down/projection_1/kernel',
+        'FeatureMaps/top_down/projection_1/bias',
+        'FeatureMaps/top_down/projection_2/kernel',
+        'FeatureMaps/top_down/projection_2/bias',
+        'FeatureMaps/top_down/projection_3/kernel',
+        'FeatureMaps/top_down/projection_3/bias',
+        'FeatureMaps/top_down/projection_4/kernel',
+        'FeatureMaps/top_down/projection_4/bias',
+        'FeatureMaps/top_down/smoothing_1_conv/kernel',
+        'FeatureMaps/top_down/smoothing_1_conv/bias',
+        'FeatureMaps/top_down/smoothing_2_conv/kernel',
+        'FeatureMaps/top_down/smoothing_2_conv/bias',
+        'FeatureMaps/top_down/smoothing_3_conv/kernel',
+        'FeatureMaps/top_down/smoothing_3_conv/bias'
+    ])
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      sess.run(feature_maps)
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+      if use_keras:
+        self.assertSetEqual(expected_keras_variables, actual_variable_set)
+      else:
+        self.assertSetEqual(expected_slim_variables, actual_variable_set)
+
+  def test_get_expected_variable_names_with_depthwise(
+      self, use_native_resize_op, use_keras):
+    image_features = [
+        ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
+        ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
+        ('block4', tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
+        ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
+    ]
+    feature_map_generator = self._build_feature_map_generator(
+        image_features=image_features,
+        depth=128,
+        use_keras=use_keras,
+        use_depthwise=True,
+        use_native_resize_op=use_native_resize_op)
+    feature_maps = feature_map_generator(image_features)
+
+    expected_slim_variables = set([
+        'projection_1/weights',
+        'projection_1/biases',
+        'projection_2/weights',
+        'projection_2/biases',
+        'projection_3/weights',
+        'projection_3/biases',
+        'projection_4/weights',
+        'projection_4/biases',
+        'smoothing_1/depthwise_weights',
+        'smoothing_1/pointwise_weights',
+        'smoothing_1/biases',
+        'smoothing_2/depthwise_weights',
+        'smoothing_2/pointwise_weights',
+        'smoothing_2/biases',
+        'smoothing_3/depthwise_weights',
+        'smoothing_3/pointwise_weights',
+        'smoothing_3/biases',
+    ])
+
+    expected_keras_variables = set([
+        'FeatureMaps/top_down/projection_1/kernel',
+        'FeatureMaps/top_down/projection_1/bias',
+        'FeatureMaps/top_down/projection_2/kernel',
+        'FeatureMaps/top_down/projection_2/bias',
+        'FeatureMaps/top_down/projection_3/kernel',
+        'FeatureMaps/top_down/projection_3/bias',
+        'FeatureMaps/top_down/projection_4/kernel',
+        'FeatureMaps/top_down/projection_4/bias',
+        'FeatureMaps/top_down/smoothing_1_depthwise_conv/depthwise_kernel',
+        'FeatureMaps/top_down/smoothing_1_depthwise_conv/pointwise_kernel',
+        'FeatureMaps/top_down/smoothing_1_depthwise_conv/bias',
+        'FeatureMaps/top_down/smoothing_2_depthwise_conv/depthwise_kernel',
+        'FeatureMaps/top_down/smoothing_2_depthwise_conv/pointwise_kernel',
+        'FeatureMaps/top_down/smoothing_2_depthwise_conv/bias',
+        'FeatureMaps/top_down/smoothing_3_depthwise_conv/depthwise_kernel',
+        'FeatureMaps/top_down/smoothing_3_depthwise_conv/pointwise_kernel',
+        'FeatureMaps/top_down/smoothing_3_depthwise_conv/bias'
+    ])
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      sess.run(feature_maps)
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+      if use_keras:
+        self.assertSetEqual(expected_keras_variables, actual_variable_set)
+      else:
+        self.assertSetEqual(expected_slim_variables, actual_variable_set)
+
 
 class GetDepthFunctionTest(tf.test.TestCase):
 
diff --git a/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py b/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
new file mode 100644
index 00000000..28e5b69c
--- /dev/null
+++ b/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
@@ -0,0 +1,479 @@
+
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""MobileNet v2 models for Keras.
+
+MobileNetV2 is a general architecture and can be used for multiple use cases.
+Depending on the use case, it can use different input layer size and
+different width factors. This allows different width models to reduce
+the number of multiply-adds and thereby
+reduce inference cost on mobile devices.
+
+MobileNetV2 is very similar to the original MobileNet,
+except that it uses inverted residual blocks with
+bottlenecking features. It has a drastically lower
+parameter count than the original MobileNet.
+MobileNets support any input size greater
+than 32 x 32, with larger image sizes
+offering better performance.
+
+The number of parameters and number of multiply-adds
+can be modified by using the `alpha` parameter,
+which increases/decreases the number of filters in each layer.
+By altering the image size and `alpha` parameter,
+all 22 models from the paper can be built, with ImageNet weights provided.
+
+The paper demonstrates the performance of MobileNets using `alpha` values of
+1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4
+
+For each of these `alpha` values, weights for 5 different input image sizes
+are provided (224, 192, 160, 128, and 96).
+
+
+The following table describes the performance of
+MobileNet on various input sizes:
+------------------------------------------------------------------------
+MACs stands for Multiply Adds
+
+ Classification Checkpoint| MACs (M)   | Parameters (M)| Top 1 Acc| Top 5 Acc
+--------------------------|------------|---------------|---------|----|-------
+| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |
+| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |
+| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |
+| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |
+| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |
+| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |
+| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |
+| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |
+| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |
+| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |
+| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |
+| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |
+| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |
+| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |
+| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |
+| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |
+| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |
+| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |
+| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |
+| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |
+| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |
+| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |
+
+The weights for all 16 models are obtained and translated from the Tensorflow
+checkpoints from TensorFlow checkpoints found at
+https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md
+
+# Reference
+This file contains building code for MobileNetV2, based on
+[MobileNetV2: Inverted Residuals and Linear Bottlenecks]
+(https://arxiv.org/abs/1801.04381)
+
+Tests comparing this model to the existing Tensorflow model can be
+found at
+[mobilenet_v2_keras](https://github.com/JonathanCMitchell/mobilenet_v2_keras)
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import warnings
+import numpy as np
+import tensorflow as tf
+
+Model = tf.keras.Model
+Input = tf.keras.layers.Input
+Activation = tf.keras.layers.Activation
+BatchNormalization = tf.keras.layers.BatchNormalization
+Conv2D = tf.keras.layers.Conv2D
+DepthwiseConv2D = tf.keras.layers.DepthwiseConv2D
+GlobalAveragePooling2D = tf.keras.layers.GlobalAveragePooling2D
+Add = tf.keras.layers.Add
+Dense = tf.keras.layers.Dense
+K = tf.keras.Backend
+
+
+def relu6(x):
+  return K.relu(x, max_value=6)
+
+
+def _obtain_input_shape(
+    input_shape,
+    default_size,
+    min_size,
+    data_format,
+    require_flatten):
+  """Internal utility to compute/validate an ImageNet model's input shape.
+
+  Arguments:
+      input_shape: either None (will return the default network input shape),
+          or a user-provided shape to be validated.
+      default_size: default input width/height for the model.
+      min_size: minimum input width/height accepted by the model.
+      data_format: image data format to use.
+      require_flatten: whether the model is expected to
+          be linked to a classifier via a Flatten layer.
+
+  Returns:
+      An integer shape tuple (may include None entries).
+
+  Raises:
+      ValueError: in case of invalid argument values.
+  """
+  if input_shape and len(input_shape) == 3:
+    if data_format == 'channels_first':
+      if input_shape[0] not in {1, 3}:
+        warnings.warn(
+            'This model usually expects 1 or 3 input channels. '
+            'However, it was passed an input_shape with ' +
+            str(input_shape[0]) + ' input channels.')
+      default_shape = (input_shape[0], default_size, default_size)
+    else:
+      if input_shape[-1] not in {1, 3}:
+        warnings.warn(
+            'This model usually expects 1 or 3 input channels. '
+            'However, it was passed an input_shape with ' +
+            str(input_shape[-1]) + ' input channels.')
+      default_shape = (default_size, default_size, input_shape[-1])
+  else:
+    if data_format == 'channels_first':
+      default_shape = (3, default_size, default_size)
+    else:
+      default_shape = (default_size, default_size, 3)
+  if input_shape:
+    if data_format == 'channels_first':
+      if input_shape is not None:
+        if len(input_shape) != 3:
+          raise ValueError(
+              '`input_shape` must be a tuple of three integers.')
+        if ((input_shape[1] is not None and input_shape[1] < min_size) or
+            (input_shape[2] is not None and input_shape[2] < min_size)):
+          raise ValueError('Input size must be at least ' +
+                           str(min_size) + 'x' + str(min_size) +
+                           '; got `input_shape=' +
+                           str(input_shape) + '`')
+    else:
+      if input_shape is not None:
+        if len(input_shape) != 3:
+          raise ValueError(
+              '`input_shape` must be a tuple of three integers.')
+        if ((input_shape[0] is not None and input_shape[0] < min_size) or
+            (input_shape[1] is not None and input_shape[1] < min_size)):
+          raise ValueError('Input size must be at least ' +
+                           str(min_size) + 'x' + str(min_size) +
+                           '; got `input_shape=' +
+                           str(input_shape) + '`')
+  else:
+    if require_flatten:
+      input_shape = default_shape
+    else:
+      if data_format == 'channels_first':
+        input_shape = (3, None, None)
+      else:
+        input_shape = (None, None, 3)
+  if require_flatten:
+    if None in input_shape:
+      raise ValueError('If `include_top` is True, '
+                       'you should specify a static `input_shape`. '
+                       'Got `input_shape=' + str(input_shape) + '`')
+  return input_shape
+
+
+def preprocess_input(x):
+  """Preprocesses a numpy array encoding a batch of images.
+
+  This function applies the "Inception" preprocessing which converts
+  the RGB values from [0, 255] to [-1, 1]. Note that this preprocessing
+  function is different from `imagenet_utils.preprocess_input()`.
+
+  Arguments:
+    x: a 4D numpy array consists of RGB values within [0, 255].
+
+  Returns:
+    Preprocessed array.
+  """
+  x /= 128.
+  x -= 1.
+  return x.astype(np.float32)
+
+
+# This function is taken from the original tf repo.
+# It ensures that all layers have a channel number that is divisible by 8
+# It can be seen here:
+# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
+
+
+def _make_divisible(v, divisor, min_value=None):
+  if min_value is None:
+    min_value = divisor
+  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
+  # Make sure that round down does not go down by more than 10%.
+  if new_v < 0.9 * v:
+    new_v += divisor
+  return new_v
+
+
+def mobilenet_v2(input_shape=None,
+                 alpha=1.0,
+                 include_top=True,
+                 classes=1000):
+  """Instantiates the MobileNetV2 architecture.
+
+  To load a MobileNetV2 model via `load_model`, import the custom
+  objects `relu6` and pass them to the `custom_objects` parameter.
+  E.g.
+  model = load_model('mobilenet.h5', custom_objects={
+                     'relu6': mobilenet.relu6})
+
+  Arguments:
+    input_shape: optional shape tuple, to be specified if you would
+      like to use a model with an input img resolution that is not
+      (224, 224, 3).
+      It should have exactly 3 inputs channels (224, 224, 3).
+      You can also omit this option if you would like
+      to infer input_shape from an input_tensor.
+      If you choose to include both input_tensor and input_shape then
+      input_shape will be used if they match, if the shapes
+      do not match then we will throw an error.
+      E.g. `(160, 160, 3)` would be one valid value.
+    alpha: controls the width of the network. This is known as the
+    width multiplier in the MobileNetV2 paper.
+      - If `alpha` < 1.0, proportionally decreases the number
+          of filters in each layer.
+      - If `alpha` > 1.0, proportionally increases the number
+          of filters in each layer.
+      - If `alpha` = 1, default number of filters from the paper
+           are used at each layer.
+    include_top: whether to include the fully-connected
+      layer at the top of the network.
+    classes: optional number of classes to classify images
+      into, only to be specified if `include_top` is True, and
+      if no `weights` argument is specified.
+
+  Returns:
+    A Keras model instance.
+
+  Raises:
+    ValueError: in case of invalid argument for `weights`,
+        or invalid input shape or invalid depth_multiplier, alpha,
+        rows when weights='imagenet'
+  """
+
+  # Determine proper input shape and default size.
+  # If input_shape is None and no input_tensor
+  if input_shape is None:
+    default_size = 224
+
+  # If input_shape is not None, assume default size
+  else:
+    if K.image_data_format() == 'channels_first':
+      rows = input_shape[1]
+      cols = input_shape[2]
+    else:
+      rows = input_shape[0]
+      cols = input_shape[1]
+
+    if rows == cols and rows in [96, 128, 160, 192, 224]:
+      default_size = rows
+    else:
+      default_size = 224
+
+  input_shape = _obtain_input_shape(input_shape,
+                                    default_size=default_size,
+                                    min_size=32,
+                                    data_format=K.image_data_format(),
+                                    require_flatten=include_top)
+
+  if K.image_data_format() == 'channels_last':
+    row_axis, col_axis = (0, 1)
+  else:
+    row_axis, col_axis = (1, 2)
+  rows = input_shape[row_axis]
+  cols = input_shape[col_axis]
+
+  if K.image_data_format() != 'channels_last':
+    warnings.warn('The MobileNet family of models is only available '
+                  'for the input data format "channels_last" '
+                  '(width, height, channels). '
+                  'However your settings specify the default '
+                  'data format "channels_first" (channels, width, height).'
+                  ' You should set `image_data_format="channels_last"` '
+                  'in your Keras config located at ~/.keras/keras.json. '
+                  'The model being returned right now will expect inputs '
+                  'to follow the "channels_last" data format.')
+    K.set_image_data_format('channels_last')
+    old_data_format = 'channels_first'
+  else:
+    old_data_format = None
+
+  img_input = Input(shape=input_shape)
+
+  first_block_filters = _make_divisible(32 * alpha, 8)
+  x = Conv2D(first_block_filters,
+             kernel_size=3,
+             strides=(2, 2), padding='same',
+             use_bias=False, name='Conv1')(img_input)
+  x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='bn_Conv1')(x)
+  x = Activation(relu6, name='Conv1_relu')(x)
+
+  x = _first_inverted_res_block(x,
+                                filters=16,
+                                alpha=alpha,
+                                stride=1,
+                                block_id=0)
+
+  x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,
+                          expansion=6, block_id=1)
+  x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,
+                          expansion=6, block_id=2)
+
+  x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,
+                          expansion=6, block_id=3)
+  x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,
+                          expansion=6, block_id=4)
+  x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,
+                          expansion=6, block_id=5)
+
+  x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,
+                          expansion=6, block_id=6)
+  x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,
+                          expansion=6, block_id=7)
+  x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,
+                          expansion=6, block_id=8)
+  x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,
+                          expansion=6, block_id=9)
+
+  x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,
+                          expansion=6, block_id=10)
+  x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,
+                          expansion=6, block_id=11)
+  x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,
+                          expansion=6, block_id=12)
+
+  x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,
+                          expansion=6, block_id=13)
+  x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,
+                          expansion=6, block_id=14)
+  x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,
+                          expansion=6, block_id=15)
+
+  x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,
+                          expansion=6, block_id=16)
+
+  # no alpha applied to last conv as stated in the paper:
+  # if the width multiplier is greater than 1 we
+  # increase the number of output channels
+  if alpha > 1.0:
+    last_block_filters = _make_divisible(1280 * alpha, 8)
+  else:
+    last_block_filters = 1280
+
+  x = Conv2D(last_block_filters,
+             kernel_size=1,
+             use_bias=False,
+             name='Conv_1')(x)
+  x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='Conv_1_bn')(x)
+  x = Activation(relu6, name='out_relu')(x)
+
+  if include_top:
+    x = GlobalAveragePooling2D()(x)
+    x = Dense(classes, activation='softmax',
+              use_bias=True, name='Logits')(x)
+
+  # Ensure that the model takes into account
+  # any potential predecessors of `input_tensor`.
+  inputs = img_input
+
+  # Create model.
+  model = Model(inputs, x, name='mobilenetv2_%0.2f_%s' % (alpha, rows))
+
+  if old_data_format:
+    K.set_image_data_format(old_data_format)
+  return model
+
+
+def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):
+  """Build an inverted res block."""
+  in_channels = int(inputs.shape[-1])
+  pointwise_conv_filters = int(filters * alpha)
+  pointwise_filters = _make_divisible(pointwise_conv_filters, 8)
+  # Expand
+
+  x = Conv2D(expansion * in_channels, kernel_size=1, padding='same',
+             use_bias=False, activation=None,
+             name='mobl%d_conv_expand' % block_id)(inputs)
+  x = BatchNormalization(epsilon=1e-3, momentum=0.999,
+                         name='bn%d_conv_bn_expand' %
+                         block_id)(x)
+  x = Activation(relu6, name='conv_%d_relu' % block_id)(x)
+
+  # Depthwise
+  x = DepthwiseConv2D(kernel_size=3, strides=stride, activation=None,
+                      use_bias=False, padding='same',
+                      name='mobl%d_conv_depthwise' % block_id)(x)
+  x = BatchNormalization(epsilon=1e-3, momentum=0.999,
+                         name='bn%d_conv_depthwise' % block_id)(x)
+
+  x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)
+
+  # Project
+  x = Conv2D(pointwise_filters,
+             kernel_size=1, padding='same', use_bias=False, activation=None,
+             name='mobl%d_conv_project' % block_id)(x)
+  x = BatchNormalization(epsilon=1e-3, momentum=0.999,
+                         name='bn%d_conv_bn_project' % block_id)(x)
+
+  if in_channels == pointwise_filters and stride == 1:
+    return Add(name='res_connect_' + str(block_id))([inputs, x])
+
+  return x
+
+
+def _first_inverted_res_block(inputs,
+                              stride,
+                              alpha, filters, block_id):
+  """Build the first inverted res block."""
+  in_channels = int(inputs.shape[-1])
+  pointwise_conv_filters = int(filters * alpha)
+  pointwise_filters = _make_divisible(pointwise_conv_filters, 8)
+
+  # Depthwise
+  x = DepthwiseConv2D(kernel_size=3,
+                      strides=stride, activation=None,
+                      use_bias=False, padding='same',
+                      name='mobl%d_conv_depthwise' %
+                      block_id)(inputs)
+  x = BatchNormalization(epsilon=1e-3, momentum=0.999,
+                         name='bn%d_conv_depthwise' %
+                         block_id)(x)
+  x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)
+
+  # Project
+  x = Conv2D(pointwise_filters,
+             kernel_size=1,
+             padding='same',
+             use_bias=False,
+             activation=None,
+             name='mobl%d_conv_project' %
+             block_id)(x)
+  x = BatchNormalization(epsilon=1e-3, momentum=0.999,
+                         name='bn%d_conv_project' %
+                         block_id)(x)
+
+  if in_channels == pointwise_filters and stride == 1:
+    return Add(name='res_connect_' + str(block_id))([inputs, x])
+
+  return x
diff --git a/research/object_detection/models/keras_models/inception_resnet_v2.py b/research/object_detection/models/keras_models/inception_resnet_v2.py
new file mode 100644
index 00000000..ec99e3e9
--- /dev/null
+++ b/research/object_detection/models/keras_models/inception_resnet_v2.py
@@ -0,0 +1,244 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""A wrapper around the Keras InceptionResnetV2 models for object detection."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from object_detection.core import freezable_batch_norm
+
+
+class _LayersOverride(object):
+  """Alternative Keras layers interface for the Keras InceptionResNetV2."""
+
+  def __init__(self,
+               batchnorm_training,
+               output_stride=16,
+               align_feature_maps=False,
+               batchnorm_scale=False,
+               default_batchnorm_momentum=0.999,
+               default_batchnorm_epsilon=1e-3,
+               weight_decay=0.00004):
+    """Alternative tf.keras.layers interface, for use by InceptionResNetV2.
+
+    It is used by the Keras applications kwargs injection API to
+    modify the Inception Resnet V2 Keras application with changes required by
+    the Object Detection API.
+
+    These injected interfaces make the following changes to the network:
+
+    - Supports freezing batch norm layers
+    - Adds support for feature map alignment (like in the Slim model)
+    - Adds support for changing the output stride (like in the Slim model)
+    - Adds support for overriding various batch norm hyperparameters
+
+    Because the Keras inception resnet v2 application does not assign explicit
+    names to most individual layers, the injection of output stride support
+    works by identifying convolution layers according to their filter counts
+    and pre-feature-map-alignment padding arguments.
+
+    Args:
+      batchnorm_training: Bool. Assigned to Batch norm layer `training` param
+        when constructing `freezable_batch_norm.FreezableBatchNorm` layers.
+      output_stride: A scalar that specifies the requested ratio of input to
+        output spatial resolution. Only supports 8 and 16.
+      align_feature_maps: When true, changes all the VALID paddings in the
+        network to SAME padding so that the feature maps are aligned.
+      batchnorm_scale: If True, uses an explicit `gamma` multiplier to scale the
+        activations in the batch normalization layer.
+      default_batchnorm_momentum: Float. Batch norm layers will be constructed
+        using this value as the momentum.
+      default_batchnorm_epsilon: small float added to variance to avoid
+        dividing by zero.
+      weight_decay: the l2 regularization weight decay for weights variables.
+        (gets multiplied by 0.5 to map from slim l2 regularization weight to
+        Keras l2 regularization weight).
+    """
+    self._use_atrous = output_stride == 8
+    self._align_feature_maps = align_feature_maps
+    self._batchnorm_training = batchnorm_training
+    self._batchnorm_scale = batchnorm_scale
+    self._default_batchnorm_momentum = default_batchnorm_momentum
+    self._default_batchnorm_epsilon = default_batchnorm_epsilon
+    self.regularizer = tf.keras.regularizers.l2(weight_decay * 0.5)
+
+  def Conv2D(self, filters, kernel_size, **kwargs):
+    """Builds a Conv2D layer according to the current Object Detection config.
+
+    Overrides the Keras InceptionResnetV2 application's convolutions with ones
+    that follow the spec specified by the Object Detection hyperparameters.
+
+    If feature map alignment is enabled, the padding will be forced to 'same'.
+    If output_stride is 8, some conv2d layers will be matched according to
+    their name or filter counts or pre-alignment padding parameters, and will
+    have the correct 'dilation rate' or 'strides' set.
+
+    Args:
+      filters: The number of filters to use for the convolution.
+      kernel_size: The kernel size to specify the height and width of the 2D
+        convolution window.
+      **kwargs: Keyword args specified by the Keras application for
+        constructing the convolution.
+
+    Returns:
+      A Keras Conv2D layer specified by the Object Detection hyperparameter
+      configurations.
+    """
+    kwargs['kernel_regularizer'] = self.regularizer
+    kwargs['bias_regularizer'] = self.regularizer
+
+    # Because the Keras application does not set explicit names for most layers,
+    # (instead allowing names to auto-increment), we must match individual
+    # layers in the model according to their filter count, name, or
+    # pre-alignment mapping. This means we can only align the feature maps
+    # after we have applied our updates in cases where output_stride=8.
+    if self._use_atrous and (filters == 384):
+      kwargs['strides'] = 1
+
+    name = kwargs.get('name')
+    if self._use_atrous and (
+        (name and 'block17' in name) or
+        (filters == 128 or filters == 160 or
+         (filters == 192 and kwargs.get('padding', '').lower() != 'valid'))):
+      kwargs['dilation_rate'] = 2
+
+    if self._align_feature_maps:
+      kwargs['padding'] = 'same'
+
+    return tf.keras.layers.Conv2D(filters, kernel_size, **kwargs)
+
+  def MaxPooling2D(self, pool_size, strides, **kwargs):
+    """Builds a pooling layer according to the current Object Detection config.
+
+    Overrides the Keras InceptionResnetV2 application's MaxPooling2D layers with
+    ones that follow the spec specified by the Object Detection hyperparameters.
+
+    If feature map alignment is enabled, the padding will be forced to 'same'.
+    If output_stride is 8, some pooling layers will be matched according to
+    their pre-alignment padding parameters, and will have their 'strides'
+    argument overridden.
+
+    Args:
+      pool_size: The pool size specified by the Keras application.
+      strides: The strides specified by the unwrapped Keras application.
+      **kwargs: Keyword args specified by the Keras application for
+        constructing the max pooling layer.
+
+    Returns:
+      A MaxPool2D layer specified by the Object Detection hyperparameter
+      configurations.
+    """
+    if self._use_atrous and kwargs.get('padding', '').lower() == 'valid':
+      strides = 1
+
+    if self._align_feature_maps:
+      kwargs['padding'] = 'same'
+
+    return tf.keras.layers.MaxPool2D(pool_size, strides=strides, **kwargs)
+
+  # We alias MaxPool2D because Keras has that alias
+  MaxPool2D = MaxPooling2D  # pylint: disable=invalid-name
+
+  def BatchNormalization(self, **kwargs):
+    """Builds a normalization layer.
+
+    Overrides the Keras application batch norm with the norm specified by the
+    Object Detection configuration.
+
+    Args:
+      **kwargs: Keyword arguments from the `layers.BatchNormalization` calls in
+        the Keras application.
+
+    Returns:
+      A normalization layer specified by the Object Detection hyperparameter
+      configurations.
+    """
+    kwargs['scale'] = self._batchnorm_scale
+    return freezable_batch_norm.FreezableBatchNorm(
+        training=self._batchnorm_training,
+        epsilon=self._default_batchnorm_epsilon,
+        momentum=self._default_batchnorm_momentum,
+        **kwargs)
+
+  # Forward all non-overridden methods to the keras layers
+  def __getattr__(self, item):
+    return getattr(tf.keras.layers, item)
+
+
+# pylint: disable=invalid-name
+def inception_resnet_v2(
+    batchnorm_training,
+    output_stride=16,
+    align_feature_maps=False,
+    batchnorm_scale=False,
+    weight_decay=0.00004,
+    default_batchnorm_momentum=0.9997,
+    default_batchnorm_epsilon=0.001,
+    **kwargs):
+  """Instantiates the InceptionResnetV2 architecture.
+
+  (Modified for object detection)
+
+  This wraps the InceptionResnetV2 tensorflow Keras application, but uses the
+  Keras application's kwargs-based monkey-patching API to override the Keras
+  architecture with the following changes:
+
+  - Supports freezing batch norm layers with FreezableBatchNorms
+  - Adds support for feature map alignment (like in the Slim model)
+  - Adds support for changing the output stride (like in the Slim model)
+  - Changes the default batchnorm momentum to 0.9997
+  - Adds support for overriding various batchnorm hyperparameters
+
+  Args:
+      batchnorm_training: Bool. Assigned to Batch norm layer `training` param
+        when constructing `freezable_batch_norm.FreezableBatchNorm` layers.
+      output_stride: A scalar that specifies the requested ratio of input to
+        output spatial resolution. Only supports 8 and 16.
+      align_feature_maps: When true, changes all the VALID paddings in the
+        network to SAME padding so that the feature maps are aligned.
+      batchnorm_scale: If True, uses an explicit `gamma` multiplier to scale the
+        activations in the batch normalization layer.
+      weight_decay: the l2 regularization weight decay for weights variables.
+        (gets multiplied by 0.5 to map from slim l2 regularization weight to
+        Keras l2 regularization weight).
+      default_batchnorm_momentum: Float. Batch norm layers will be constructed
+        using this value as the momentum.
+      default_batchnorm_epsilon: small float added to variance to avoid
+        dividing by zero.
+      **kwargs: Keyword arguments forwarded directly to the
+        `tf.keras.applications.InceptionResNetV2` method that constructs the
+        Keras model.
+
+  Returns:
+      A Keras model instance.
+  """
+  if output_stride != 8 and output_stride != 16:
+    raise ValueError('output_stride must be 8 or 16.')
+
+  layers_override = _LayersOverride(
+      batchnorm_training,
+      output_stride,
+      align_feature_maps=align_feature_maps,
+      batchnorm_scale=batchnorm_scale,
+      default_batchnorm_momentum=default_batchnorm_momentum,
+      default_batchnorm_epsilon=default_batchnorm_epsilon,
+      weight_decay=weight_decay)
+  return tf.keras.applications.InceptionResNetV2(
+      layers=layers_override, **kwargs)
+# pylint: enable=invalid-name
diff --git a/research/object_detection/models/keras_models/inception_resnet_v2_test.py b/research/object_detection/models/keras_models/inception_resnet_v2_test.py
new file mode 100644
index 00000000..18801b3b
--- /dev/null
+++ b/research/object_detection/models/keras_models/inception_resnet_v2_test.py
@@ -0,0 +1,223 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for inception_resnet_v2.py.
+
+This test mainly focuses on comparing slim inception resnet v2 and Keras
+inception resnet v2 for object detection. To verify the consistency of the two
+models, we compare:
+  1. Output shape of each layer given different inputs
+  2. Number of global variables
+
+We also visualize the model structure via Tensorboard, and compare the model
+layout and the parameters of each Op to make sure the two implementations are
+consistent.
+"""
+
+import itertools
+import numpy as np
+import tensorflow as tf
+
+from object_detection.models.keras_models import inception_resnet_v2
+from object_detection.utils import test_case
+
+_KERAS_TO_SLIM_ENDPOINT_NAMES = {
+    'activation': 'Conv2d_1a_3x3',
+    'activation_1': 'Conv2d_2a_3x3',
+    'activation_2': 'Conv2d_2b_3x3',
+    'activation_3': 'Conv2d_3b_1x1',
+    'activation_4': 'Conv2d_4a_3x3',
+    'max_pooling2d': 'MaxPool_3a_3x3',
+    'max_pooling2d_1': 'MaxPool_5a_3x3',
+    'mixed_5b': 'Mixed_5b',
+    'mixed_6a': 'Mixed_6a',
+    'block17_20_ac': 'PreAuxLogits',
+    'mixed_7a': 'Mixed_7a',
+    'conv_7b_ac': 'Conv2d_7b_1x1',
+}
+
+_SLIM_ENDPOINT_SHAPES_128 = {
+    'Conv2d_1a_3x3': (2, 64, 64, 32),
+    'Conv2d_2a_3x3': (2, 64, 64, 32),
+    'Conv2d_2b_3x3': (2, 64, 64, 64),
+    'Conv2d_3b_1x1': (2, 32, 32, 80),
+    'Conv2d_4a_3x3': (2, 32, 32, 192),
+    'Conv2d_7b_1x1': (2, 4, 4, 1536),
+    'MaxPool_3a_3x3': (2, 32, 32, 64),
+    'MaxPool_5a_3x3': (2, 16, 16, 192),
+    'Mixed_5b': (2, 16, 16, 320),
+    'Mixed_6a': (2, 8, 8, 1088),
+    'Mixed_7a': (2, 4, 4, 2080),
+    'PreAuxLogits': (2, 8, 8, 1088)}
+_SLIM_ENDPOINT_SHAPES_128_STRIDE_8 = {
+    'Conv2d_1a_3x3': (2, 64, 64, 32),
+    'Conv2d_2a_3x3': (2, 64, 64, 32),
+    'Conv2d_2b_3x3': (2, 64, 64, 64),
+    'Conv2d_3b_1x1': (2, 32, 32, 80),
+    'Conv2d_4a_3x3': (2, 32, 32, 192),
+    'MaxPool_3a_3x3': (2, 32, 32, 64),
+    'MaxPool_5a_3x3': (2, 16, 16, 192),
+    'Mixed_5b': (2, 16, 16, 320),
+    'Mixed_6a': (2, 16, 16, 1088),
+    'PreAuxLogits': (2, 16, 16, 1088)}
+_SLIM_ENDPOINT_SHAPES_128_ALIGN_FEATURE_MAPS_FALSE = {
+    'Conv2d_1a_3x3': (2, 63, 63, 32),
+    'Conv2d_2a_3x3': (2, 61, 61, 32),
+    'Conv2d_2b_3x3': (2, 61, 61, 64),
+    'Conv2d_3b_1x1': (2, 30, 30, 80),
+    'Conv2d_4a_3x3': (2, 28, 28, 192),
+    'Conv2d_7b_1x1': (2, 2, 2, 1536),
+    'MaxPool_3a_3x3': (2, 30, 30, 64),
+    'MaxPool_5a_3x3': (2, 13, 13, 192),
+    'Mixed_5b': (2, 13, 13, 320),
+    'Mixed_6a': (2, 6, 6, 1088),
+    'Mixed_7a': (2, 2, 2, 2080),
+    'PreAuxLogits': (2, 6, 6, 1088)}
+_SLIM_ENDPOINT_SHAPES_299 = {}
+_SLIM_ENDPOINT_SHAPES_299_STRIDE_8 = {}
+_SLIM_ENDPOINT_SHAPES_299_ALIGN_FEATURE_MAPS_FALSE = {}
+
+_KERAS_LAYERS_TO_CHECK = list(_KERAS_TO_SLIM_ENDPOINT_NAMES.keys())
+
+_NUM_CHANNELS = 3
+_BATCH_SIZE = 2
+
+
+class InceptionResnetV2Test(test_case.TestCase):
+
+  def _create_application_with_layer_outputs(
+      self, layer_names, batchnorm_training,
+      output_stride=16,
+      align_feature_maps=False,
+      batchnorm_scale=False,
+      weight_decay=0.00004,
+      default_batchnorm_momentum=0.9997,
+      default_batchnorm_epsilon=0.001,):
+    """Constructs Keras inception_resnet_v2 that extracts layer outputs."""
+    # Have to clear the Keras backend to ensure isolation in layer naming
+    tf.keras.backend.clear_session()
+    if not layer_names:
+      layer_names = _KERAS_LAYERS_TO_CHECK
+    full_model = inception_resnet_v2.inception_resnet_v2(
+        batchnorm_training=batchnorm_training,
+        output_stride=output_stride,
+        align_feature_maps=align_feature_maps,
+        weights=None,
+        batchnorm_scale=batchnorm_scale,
+        weight_decay=weight_decay,
+        default_batchnorm_momentum=default_batchnorm_momentum,
+        default_batchnorm_epsilon=default_batchnorm_epsilon,
+        include_top=False)
+    layer_outputs = [full_model.get_layer(name=layer).output
+                     for layer in layer_names]
+    return tf.keras.Model(
+        inputs=full_model.inputs,
+        outputs=layer_outputs)
+
+  def _check_returns_correct_shape(
+      self, image_height, image_width,
+      expected_feature_map_shape, layer_names=None, batchnorm_training=True,
+      output_stride=16,
+      align_feature_maps=False,
+      batchnorm_scale=False,
+      weight_decay=0.00004,
+      default_batchnorm_momentum=0.9997,
+      default_batchnorm_epsilon=0.001,):
+    if not layer_names:
+      layer_names = _KERAS_LAYERS_TO_CHECK
+    model = self._create_application_with_layer_outputs(
+        layer_names=layer_names,
+        batchnorm_training=batchnorm_training,
+        output_stride=output_stride,
+        align_feature_maps=align_feature_maps,
+        batchnorm_scale=batchnorm_scale,
+        weight_decay=weight_decay,
+        default_batchnorm_momentum=default_batchnorm_momentum,
+        default_batchnorm_epsilon=default_batchnorm_epsilon)
+
+    image_tensor = np.random.rand(_BATCH_SIZE, image_height, image_width,
+                                  _NUM_CHANNELS).astype(np.float32)
+    feature_maps = model(image_tensor)
+
+    for feature_map, layer_name in itertools.izip(
+        feature_maps, layer_names):
+      endpoint_name = _KERAS_TO_SLIM_ENDPOINT_NAMES[layer_name]
+      expected_shape = expected_feature_map_shape[endpoint_name]
+      self.assertAllEqual(feature_map.shape, expected_shape)
+
+  def _get_variables(self, layer_names=None):
+    tf.keras.backend.clear_session()
+    model = self._create_application_with_layer_outputs(
+        layer_names=layer_names,
+        batchnorm_training=False)
+    preprocessed_inputs = tf.placeholder(
+        tf.float32, (4, None, None, _NUM_CHANNELS))
+    model(preprocessed_inputs)
+    return model.variables
+
+  def test_returns_correct_shapes_128(self):
+    image_height = 128
+    image_width = 128
+    expected_feature_map_shape = (
+        _SLIM_ENDPOINT_SHAPES_128)
+    self._check_returns_correct_shape(
+        image_height, image_width, expected_feature_map_shape,
+        align_feature_maps=True)
+
+  def test_returns_correct_shapes_128_output_stride_8(self):
+    image_height = 128
+    image_width = 128
+    expected_feature_map_shape = (
+        _SLIM_ENDPOINT_SHAPES_128_STRIDE_8)
+
+    # Output stride of 8 not defined beyond 'block17_20_ac', which is
+    # PreAuxLogits in slim. So, we exclude those layers in our Keras vs Slim
+    # comparison.
+    excluded_layers = {'mixed_7a', 'conv_7b_ac'}
+    layer_names = [l for l in _KERAS_LAYERS_TO_CHECK
+                   if l not in excluded_layers]
+    self._check_returns_correct_shape(
+        image_height, image_width, expected_feature_map_shape,
+        layer_names=layer_names, output_stride=8, align_feature_maps=True)
+
+  def test_returns_correct_shapes_128_align_feature_maps_false(
+      self):
+    image_height = 128
+    image_width = 128
+    expected_feature_map_shape = (
+        _SLIM_ENDPOINT_SHAPES_128_ALIGN_FEATURE_MAPS_FALSE)
+    self._check_returns_correct_shape(
+        image_height, image_width, expected_feature_map_shape,
+        align_feature_maps=False)
+
+  def test_hyperparam_override(self):
+    model = inception_resnet_v2.inception_resnet_v2(
+        batchnorm_training=True,
+        default_batchnorm_momentum=0.2,
+        default_batchnorm_epsilon=0.1,
+        weights=None,
+        include_top=False)
+    bn_layer = model.get_layer(name='freezable_batch_norm')
+    self.assertAllClose(bn_layer.momentum, 0.2)
+    self.assertAllClose(bn_layer.epsilon, 0.1)
+
+  def test_variable_count(self):
+    variables = self._get_variables()
+    # 896 is the number of variables from slim inception resnet v2 model.
+    self.assertEqual(len(variables), 896)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/keras_models/mobilenet_v1.py b/research/object_detection/models/keras_models/mobilenet_v1.py
index 841cfe6e..f9783396 100644
--- a/research/object_detection/models/keras_models/mobilenet_v1.py
+++ b/research/object_detection/models/keras_models/mobilenet_v1.py
@@ -22,6 +22,7 @@ from __future__ import print_function
 import tensorflow as tf
 
 from object_detection.core import freezable_batch_norm
+from object_detection.models.keras_models import model_utils
 
 
 def _fixed_padding(inputs, kernel_size, rate=1):  # pylint: disable=invalid-name
@@ -59,7 +60,8 @@ class _LayersOverride(object):
                conv_hyperparams=None,
                use_explicit_padding=False,
                alpha=1.0,
-               min_depth=None):
+               min_depth=None,
+               conv_defs=None):
     """Alternative tf.keras.layers interface, for use by the Keras MobileNetV1.
 
     It is used by the Keras applications kwargs injection API to
@@ -90,6 +92,8 @@ class _LayersOverride(object):
         modifies the number of filters in each convolutional layer. It's called
         depth multiplier in Keras application MobilenetV1.
       min_depth: Minimum number of filters in the convolutional layers.
+      conv_defs: Network layout to specify the mobilenet_v1 body. Default is
+        `None` to use the default mobilenet_v1 network layout.
     """
     self._alpha = alpha
     self._batchnorm_training = batchnorm_training
@@ -97,6 +101,7 @@ class _LayersOverride(object):
     self._conv_hyperparams = conv_hyperparams
     self._use_explicit_padding = use_explicit_padding
     self._min_depth = min_depth
+    self._conv_defs = conv_defs
     self.regularizer = tf.keras.regularizers.l2(0.00004 * 0.5)
     self.initializer = tf.truncated_normal_initializer(stddev=0.09)
 
@@ -122,6 +127,11 @@ class _LayersOverride(object):
       the input argument, or that will first pad the input then apply a Conv2D
       layer.
     """
+    layer_name = kwargs['name']
+    if self._conv_defs:
+      conv_filters = model_utils.get_conv_def(self._conv_defs, layer_name)
+      if conv_filters:
+        filters = conv_filters
     # Apply the width multiplier and the minimum depth to the convolution layers
     filters = int(filters * self._alpha)
     if self._min_depth and filters < self._min_depth:
@@ -163,7 +173,12 @@ class _LayersOverride(object):
     """
     if self._conv_hyperparams:
       kwargs = self._conv_hyperparams.params(**kwargs)
+      # Both regularizer and initializaer also applies to depthwise layer in
+      # MobilenetV1, so we remap the kernel_* to depthwise_* here.
+      kwargs['depthwise_regularizer'] = kwargs['kernel_regularizer']
+      kwargs['depthwise_initializer'] = kwargs['kernel_initializer']
     else:
+      kwargs['depthwise_regularizer'] = self.regularizer
       kwargs['depthwise_initializer'] = self.initializer
 
     kwargs['padding'] = 'same'
@@ -278,6 +293,7 @@ def mobilenet_v1(batchnorm_training,
                  use_explicit_padding=False,
                  alpha=1.0,
                  min_depth=None,
+                 conv_defs=None,
                  **kwargs):
   """Instantiates the MobileNetV1 architecture, modified for object detection.
 
@@ -309,6 +325,8 @@ def mobilenet_v1(batchnorm_training,
       alpha: The width multiplier referenced in the MobileNetV1 paper. It
         modifies the number of filters in each convolutional layer.
       min_depth: Minimum number of filters in the convolutional layers.
+      conv_defs: Network layout to specify the mobilenet_v1 body. Default is
+        `None` to use the default mobilenet_v1 network layout.
       **kwargs: Keyword arguments forwarded directly to the
         `tf.keras.applications.Mobilenet` method that constructs the Keras
         model.
@@ -322,7 +340,8 @@ def mobilenet_v1(batchnorm_training,
       conv_hyperparams=conv_hyperparams,
       use_explicit_padding=use_explicit_padding,
       min_depth=min_depth,
-      alpha=alpha)
+      alpha=alpha,
+      conv_defs=conv_defs)
   return tf.keras.applications.MobileNet(
       alpha=alpha, layers=layers_override, **kwargs)
 # pylint: enable=invalid-name
diff --git a/research/object_detection/models/keras_models/mobilenet_v1_test.py b/research/object_detection/models/keras_models/mobilenet_v1_test.py
index a326c46c..9e1d3498 100644
--- a/research/object_detection/models/keras_models/mobilenet_v1_test.py
+++ b/research/object_detection/models/keras_models/mobilenet_v1_test.py
@@ -33,6 +33,7 @@ from google.protobuf import text_format
 
 from object_detection.builders import hyperparams_builder
 from object_detection.models.keras_models import mobilenet_v1
+from object_detection.models.keras_models import model_utils
 from object_detection.models.keras_models import test_utils
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import test_case
@@ -88,7 +89,8 @@ class MobilenetV1Test(test_case.TestCase):
       conv_hyperparams=None,
       use_explicit_padding=False,
       alpha=1.0,
-      min_depth=None):
+      min_depth=None,
+      conv_defs=None):
     """Constructs Keras MobilenetV1 that extracts intermediate layer outputs."""
     if not layer_names:
       layer_names = _KERAS_LAYERS_TO_CHECK
@@ -99,6 +101,7 @@ class MobilenetV1Test(test_case.TestCase):
         use_explicit_padding=use_explicit_padding,
         alpha=alpha,
         min_depth=min_depth,
+        conv_defs=conv_defs,
         include_top=False)
     layer_outputs = [full_model.get_layer(name=layer).output
                      for layer in layer_names]
@@ -109,14 +112,15 @@ class MobilenetV1Test(test_case.TestCase):
   def _check_returns_correct_shape(
       self, image_height, image_width, depth_multiplier,
       expected_feature_map_shape, use_explicit_padding=False, min_depth=8,
-      layer_names=None):
+      layer_names=None, conv_defs=None):
     def graph_fn(image_tensor):
       model = self._create_application_with_layer_outputs(
           layer_names=layer_names,
           batchnorm_training=False,
           use_explicit_padding=use_explicit_padding,
           min_depth=min_depth,
-          alpha=depth_multiplier)
+          alpha=depth_multiplier,
+          conv_defs=conv_defs)
       return model(image_tensor)
 
     image_tensor = np.random.rand(_BATCH_SIZE, image_height, image_width,
@@ -211,6 +215,23 @@ class MobilenetV1Test(test_case.TestCase):
     self._check_returns_correct_shape(
         image_height, image_width, depth_multiplier, expected_feature_map_shape)
 
+  def test_returns_correct_shapes_with_conv_defs(
+      self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    conv_def_block_12 = model_utils.ConvDefs(
+        conv_name='conv_pw_12', filters=512)
+    conv_def_block_13 = model_utils.ConvDefs(
+        conv_name='conv_pw_13', filters=256)
+    conv_defs = [conv_def_block_12, conv_def_block_13]
+
+    expected_feature_map_shape = (
+        test_utils.moblenet_v1_expected_feature_map_shape_with_conv_defs)
+    self._check_returns_correct_shape(
+        image_height, image_width, depth_multiplier, expected_feature_map_shape,
+        conv_defs=conv_defs)
+
   def test_hyperparam_override(self):
     hyperparams = self._build_conv_hyperparams()
     model = mobilenet_v1.mobilenet_v1(
diff --git a/research/object_detection/models/keras_models/mobilenet_v2.py b/research/object_detection/models/keras_models/mobilenet_v2.py
index 5969b23d..cc093ece 100644
--- a/research/object_detection/models/keras_models/mobilenet_v2.py
+++ b/research/object_detection/models/keras_models/mobilenet_v2.py
@@ -21,6 +21,7 @@ from __future__ import print_function
 import tensorflow as tf
 
 from object_detection.core import freezable_batch_norm
+from object_detection.models.keras_models import model_utils
 from object_detection.utils import ops
 
 
@@ -45,7 +46,8 @@ class _LayersOverride(object):
                conv_hyperparams=None,
                use_explicit_padding=False,
                alpha=1.0,
-               min_depth=None):
+               min_depth=None,
+               conv_defs=None):
     """Alternative tf.keras.layers interface, for use by the Keras MobileNetV2.
 
     It is used by the Keras applications kwargs injection API to
@@ -75,6 +77,8 @@ class _LayersOverride(object):
       alpha: The width multiplier referenced in the MobileNetV2 paper. It
         modifies the number of filters in each convolutional layer.
       min_depth: Minimum number of filters in the convolutional layers.
+      conv_defs: Network layout to specify the mobilenet_v2 body. Default is
+        `None` to use the default mobilenet_v2 network layout.
     """
     self._alpha = alpha
     self._batchnorm_training = batchnorm_training
@@ -82,6 +86,7 @@ class _LayersOverride(object):
     self._conv_hyperparams = conv_hyperparams
     self._use_explicit_padding = use_explicit_padding
     self._min_depth = min_depth
+    self._conv_defs = conv_defs
     self.regularizer = tf.keras.regularizers.l2(0.00004 * 0.5)
     self.initializer = tf.truncated_normal_initializer(stddev=0.09)
 
@@ -106,8 +111,14 @@ class _LayersOverride(object):
     """
     # Make sure 'alpha' is always applied to the last convolution block's size
     # (This overrides the Keras application's functionality)
-    if kwargs.get('name') == 'Conv_1' and self._alpha < 1.0:
-      filters = _make_divisible(1280 * self._alpha, 8)
+    layer_name = kwargs.get('name')
+    if layer_name == 'Conv_1':
+      if self._conv_defs:
+        filters = model_utils.get_conv_def(self._conv_defs, 'Conv_1')
+      else:
+        filters = 1280
+      if self._alpha < 1.0:
+        filters = _make_divisible(filters * self._alpha, 8)
 
     # Apply the minimum depth to the convolution layers
     if (self._min_depth and (filters < self._min_depth)
@@ -263,6 +274,7 @@ def mobilenet_v2(batchnorm_training,
                  use_explicit_padding=False,
                  alpha=1.0,
                  min_depth=None,
+                 conv_defs=None,
                  **kwargs):
   """Instantiates the MobileNetV2 architecture, modified for object detection.
 
@@ -294,6 +306,8 @@ def mobilenet_v2(batchnorm_training,
       alpha: The width multiplier referenced in the MobileNetV2 paper. It
         modifies the number of filters in each convolutional layer.
       min_depth: Minimum number of filters in the convolutional layers.
+      conv_defs: Network layout to specify the mobilenet_v2 body. Default is
+        `None` to use the default mobilenet_v2 network layout.
       **kwargs: Keyword arguments forwarded directly to the
         `tf.keras.applications.MobilenetV2` method that constructs the Keras
         model.
@@ -307,7 +321,8 @@ def mobilenet_v2(batchnorm_training,
       conv_hyperparams=conv_hyperparams,
       use_explicit_padding=use_explicit_padding,
       min_depth=min_depth,
-      alpha=alpha)
+      alpha=alpha,
+      conv_defs=conv_defs)
   return tf.keras.applications.MobileNetV2(alpha=alpha,
                                            layers=layers_override,
                                            **kwargs)
diff --git a/research/object_detection/models/keras_models/mobilenet_v2_test.py b/research/object_detection/models/keras_models/mobilenet_v2_test.py
index 74d26f71..5ec8aae5 100644
--- a/research/object_detection/models/keras_models/mobilenet_v2_test.py
+++ b/research/object_detection/models/keras_models/mobilenet_v2_test.py
@@ -22,6 +22,7 @@ from google.protobuf import text_format
 
 from object_detection.builders import hyperparams_builder
 from object_detection.models.keras_models import mobilenet_v2
+from object_detection.models.keras_models import model_utils
 from object_detection.models.keras_models import test_utils
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import test_case
@@ -77,7 +78,8 @@ class MobilenetV2Test(test_case.TestCase):
       conv_hyperparams=None,
       use_explicit_padding=False,
       alpha=1.0,
-      min_depth=None):
+      min_depth=None,
+      conv_defs=None):
     """Constructs Keras mobilenetv2 that extracts intermediate layer outputs."""
     if not layer_names:
       layer_names = _layers_to_check
@@ -88,7 +90,8 @@ class MobilenetV2Test(test_case.TestCase):
         use_explicit_padding=use_explicit_padding,
         alpha=alpha,
         min_depth=min_depth,
-        include_top=False)
+        include_top=False,
+        conv_defs=conv_defs)
     layer_outputs = [full_model.get_layer(name=layer).output
                      for layer in layer_names]
     return tf.keras.Model(
@@ -98,13 +101,15 @@ class MobilenetV2Test(test_case.TestCase):
   def _check_returns_correct_shape(
       self, batch_size, image_height, image_width, depth_multiplier,
       expected_feature_map_shapes, use_explicit_padding=False, min_depth=None,
-      layer_names=None):
+      layer_names=None, conv_defs=None):
     def graph_fn(image_tensor):
       model = self._create_application_with_layer_outputs(
           layer_names=layer_names,
-          batchnorm_training=False, use_explicit_padding=use_explicit_padding,
+          batchnorm_training=False,
+          use_explicit_padding=use_explicit_padding,
           min_depth=min_depth,
-          alpha=depth_multiplier)
+          alpha=depth_multiplier,
+          conv_defs=conv_defs)
       return model(image_tensor)
 
     image_tensor = np.random.rand(batch_size, image_height, image_width,
@@ -202,6 +207,21 @@ class MobilenetV2Test(test_case.TestCase):
         2, image_height, image_width, depth_multiplier,
         expected_feature_map_shape, min_depth=32)
 
+  def test_returns_correct_shapes_with_conv_defs(
+      self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    conv_1 = model_utils.ConvDefs(
+        conv_name='Conv_1', filters=256)
+    conv_defs = [conv_1]
+
+    expected_feature_map_shape = (
+        test_utils.moblenet_v2_expected_feature_map_shape_with_conv_defs)
+    self._check_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier,
+        expected_feature_map_shape, conv_defs=conv_defs)
+
   def test_hyperparam_override(self):
     hyperparams = self._build_conv_hyperparams()
     model = mobilenet_v2.mobilenet_v2(
diff --git a/research/object_detection/models/keras_models/model_utils.py b/research/object_detection/models/keras_models/model_utils.py
new file mode 100644
index 00000000..981b298d
--- /dev/null
+++ b/research/object_detection/models/keras_models/model_utils.py
@@ -0,0 +1,45 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Utils for Keras models."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+
+# This is to specify the custom config of model structures. For example,
+# ConvDefs(conv_name='conv_pw_12', filters=512) for Mobilenet V1 is to specify
+# the filters of the conv layer with name 'conv_pw_12' as 512.s
+ConvDefs = collections.namedtuple('ConvDefs', ['conv_name', 'filters'])
+
+
+def get_conv_def(conv_defs, layer_name):
+  """Get the custom config for some layer of the model structure.
+
+  Args:
+    conv_defs: A named tuple to specify the custom config of the model
+      network. See `ConvDefs` for details.
+    layer_name: A string, the name of the layer to be customized.
+
+  Returns:
+    The number of filters for the layer, or `None` if there is no custom
+    config for the requested layer.
+  """
+  for conv_def in conv_defs:
+    if layer_name == conv_def.conv_name:
+      return conv_def.filters
+  return None
diff --git a/research/object_detection/models/keras_models/original_mobilenet_v2.py b/research/object_detection/models/keras_models/original_mobilenet_v2.py
deleted file mode 100644
index d0a219e3..00000000
--- a/research/object_detection/models/keras_models/original_mobilenet_v2.py
+++ /dev/null
@@ -1,542 +0,0 @@
-  
-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-"""MobileNet v2 models for Keras.
-
-MobileNetV2 is a general architecture and can be used for multiple use cases.
-Depending on the use case, it can use different input layer size and
-different width factors. This allows different width models to reduce
-the number of multiply-adds and thereby
-reduce inference cost on mobile devices.
-
-MobileNetV2 is very similar to the original MobileNet,
-except that it uses inverted residual blocks with
-bottlenecking features. It has a drastically lower
-parameter count than the original MobileNet.
-MobileNets support any input size greater
-than 32 x 32, with larger image sizes
-offering better performance.
-
-The number of parameters and number of multiply-adds
-can be modified by using the `alpha` parameter,
-which increases/decreases the number of filters in each layer.
-By altering the image size and `alpha` parameter,
-all 22 models from the paper can be built, with ImageNet weights provided.
-
-The paper demonstrates the performance of MobileNets using `alpha` values of
-1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4
-
-For each of these `alpha` values, weights for 5 different input image sizes
-are provided (224, 192, 160, 128, and 96).
-
-
-The following table describes the performance of
-MobileNet on various input sizes:
-------------------------------------------------------------------------
-MACs stands for Multiply Adds
-
- Classification Checkpoint| MACs (M)   | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy
---------------------------|------------|---------------|---------|----|-------------
-| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |
-| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |
-| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |
-| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |
-| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |
-| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |
-| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |
-| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |
-| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |
-| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |
-| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |
-| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |
-| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |
-| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |
-| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |
-| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |
-| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |
-| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |
-| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |
-| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |
-| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |
-| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |
-
-The weights for all 16 models are obtained and translated from the Tensorflow checkpoints
-from TensorFlow checkpoints found at
-https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md
-
-# Reference
-This file contains building code for MobileNetV2, based on
-[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)
-
-Tests comparing this model to the existing Tensorflow model can be
-found at [mobilenet_v2_keras](https://github.com/JonathanCMitchell/mobilenet_v2_keras)
-"""
-from __future__ import print_function
-from __future__ import absolute_import
-from __future__ import division
-
-import os
-import warnings
-import h5py
-import numpy as np
-
-from ..models import Model
-from ..layers import Input
-from ..layers import Activation
-from ..layers import Dropout
-from ..layers import Reshape
-from ..layers import BatchNormalization
-from ..layers import Conv2D
-from ..layers import DepthwiseConv2D
-from ..layers import GlobalAveragePooling2D
-from ..layers import Add
-from ..layers import Flatten
-from ..layers import Dense
-from .. import initializers
-from .. import regularizers
-from .. import constraints
-from ..utils import conv_utils
-from ..utils.data_utils import get_file
-from ..engine import get_source_inputs
-from ..engine.base_layer import InputSpec
-from . import imagenet_utils
-from .imagenet_utils import _obtain_input_shape
-from .imagenet_utils import decode_predictions
-from .. import backend as K
-
-# TODO Change path to v1.1
-BASE_WEIGHT_PATH = 'https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/'
-
-
-def relu6(x):
-    return K.relu(x, max_value=6)
-
-
-def preprocess_input(x):
-    """Preprocesses a numpy array encoding a batch of images.
-
-    This function applies the "Inception" preprocessing which converts
-    the RGB values from [0, 255] to [-1, 1]. Note that this preprocessing
-    function is different from `imagenet_utils.preprocess_input()`.
-
-    # Arguments
-        x: a 4D numpy array consists of RGB values within [0, 255].
-
-    # Returns
-        Preprocessed array.
-    """
-    x /= 128.
-    x -= 1.
-    return x.astype(np.float32)
-
-
-# This function is taken from the original tf repo.
-# It ensures that all layers have a channel number that is divisible by 8
-# It can be seen here:
-# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
-
-
-def _make_divisible(v, divisor, min_value=None):
-    if min_value is None:
-        min_value = divisor
-    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
-    # Make sure that round down does not go down by more than 10%.
-    if new_v < 0.9 * v:
-        new_v += divisor
-    return new_v
-
-
-def MobileNetV2(input_shape=None,
-                alpha=1.0,
-                depth_multiplier=1,
-                dropout=1e-3,
-                include_top=True,
-                weights='imagenet',
-                input_tensor=None,
-                classes=1000):
-    """Instantiates the MobileNetV2 architecture.
-
-    To load a MobileNetV2 model via `load_model`, import the custom
-    objects `relu6` and pass them to the `custom_objects` parameter.
-    E.g.
-    model = load_model('mobilenet.h5', custom_objects={
-                       'relu6': mobilenet.relu6})
-
-    # Arguments
-        input_shape: optional shape tuple, to be specified if you would
-            like to use a model with an input img resolution that is not
-            (224, 224, 3).
-            It should have exactly 3 inputs channels (224, 224, 3).
-            You can also omit this option if you would like
-            to infer input_shape from an input_tensor.
-            If you choose to include both input_tensor and input_shape then
-            input_shape will be used if they match, if the shapes
-            do not match then we will throw an error.
-            E.g. `(160, 160, 3)` would be one valid value.
-        alpha: controls the width of the network. This is known as the
-        width multiplier in the MobileNetV2 paper.
-            - If `alpha` < 1.0, proportionally decreases the number
-                of filters in each layer.
-            - If `alpha` > 1.0, proportionally increases the number
-                of filters in each layer.
-            - If `alpha` = 1, default number of filters from the paper
-                 are used at each layer.
-        depth_multiplier: depth multiplier for depthwise convolution
-            (also called the resolution multiplier)
-        dropout: dropout rate, dropout is currently not in use
-        include_top: whether to include the fully-connected
-            layer at the top of the network.
-        weights: one of `None` (random initialization),
-              'imagenet' (pre-training on ImageNet),
-              or the path to the weights file to be loaded.
-        input_tensor: optional Keras tensor (i.e. output of
-            `layers.Input()`)
-            to use as image input for the model.
-        classes: optional number of classes to classify images
-            into, only to be specified if `include_top` is True, and
-            if no `weights` argument is specified.
-
-    # Returns
-        A Keras model instance.
-
-    # Raises
-        ValueError: in case of invalid argument for `weights`,
-            or invalid input shape or invalid depth_multiplier, alpha,
-            rows when weights='imagenet'
-    """
-
-    if not (weights in {'imagenet', None} or os.path.exists(weights)):
-        raise ValueError('The `weights` argument should be either '
-                         '`None` (random initialization), `imagenet` '
-                         '(pre-training on ImageNet), '
-                         'or the path to the weights file to be loaded.')
-
-    if weights == 'imagenet' and include_top and classes != 1000:
-        raise ValueError('If using `weights` as ImageNet with `include_top` '
-                         'as true, `classes` should be 1000')
-
-    # Determine proper input shape and default size.
-    # If both input_shape and input_tensor are used, they should match
-    if input_shape is not None and input_tensor is not None:
-        try:
-            is_input_t_tensor = K.is_keras_tensor(input_tensor)
-        except ValueError:
-            try:
-                is_input_t_tensor = K.is_keras_tensor(
-                    get_source_inputs(input_tensor))
-            except ValueError:
-                raise ValueError('input_tensor: ', input_tensor,
-                                 'is not type input_tensor')
-        if is_input_t_tensor:
-            if K.image_data_format == 'channels_first':
-                if input_tensor._keras_shape[1] != input_shape[1]:
-                    raise ValueError('input_shape: ', input_shape,
-                                     'and input_tensor: ', input_tensor,
-                                     'do not meet the same shape requirements')
-            else:
-                if input_tensor._keras_shape[2] != input_shape[1]:
-                    raise ValueError('input_shape: ', input_shape,
-                                     'and input_tensor: ', input_tensor,
-                                     'do not meet the same shape requirements')
-        else:
-            raise ValueError('input_tensor specified: ', input_tensor,
-                             'is not a keras tensor')
-
-    # If input_shape is None, infer shape from input_tensor
-    if input_shape is None and input_tensor is not None:
-
-        try:
-            K.is_keras_tensor(input_tensor)
-        except ValueError:
-            raise ValueError('input_tensor: ', input_tensor,
-                             'is type: ', type(input_tensor),
-                             'which is not a valid type')
-
-        if input_shape is None and not K.is_keras_tensor(input_tensor):
-            default_size = 224
-        elif input_shape is None and K.is_keras_tensor(input_tensor):
-            if K.image_data_format() == 'channels_first':
-                rows = input_tensor._keras_shape[2]
-                cols = input_tensor._keras_shape[3]
-            else:
-                rows = input_tensor._keras_shape[1]
-                cols = input_tensor._keras_shape[2]
-
-            if rows == cols and rows in [96, 128, 160, 192, 224]:
-                default_size = rows
-            else:
-                default_size = 224
-
-    # If input_shape is None and no input_tensor
-    elif input_shape is None:
-        default_size = 224
-
-    # If input_shape is not None, assume default size
-    else:
-        if K.image_data_format() == 'channels_first':
-            rows = input_shape[1]
-            cols = input_shape[2]
-        else:
-            rows = input_shape[0]
-            cols = input_shape[1]
-
-        if rows == cols and rows in [96, 128, 160, 192, 224]:
-            default_size = rows
-        else:
-            default_size = 224
-
-    input_shape = _obtain_input_shape(input_shape,
-                                      default_size=default_size,
-                                      min_size=32,
-                                      data_format=K.image_data_format(),
-                                      require_flatten=include_top,
-                                      weights=weights)
-
-    if K.image_data_format() == 'channels_last':
-        row_axis, col_axis = (0, 1)
-    else:
-        row_axis, col_axis = (1, 2)
-    rows = input_shape[row_axis]
-    cols = input_shape[col_axis]
-
-    if weights == 'imagenet':
-        if depth_multiplier != 1:
-            raise ValueError('If imagenet weights are being loaded, '
-                             'depth multiplier must be 1')
-
-        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:
-            raise ValueError('If imagenet weights are being loaded, '
-                             'alpha can be one of'
-                             '`0.25`, `0.50`, `0.75` or `1.0` only.')
-
-        if rows != cols or rows not in [96, 128, 160, 192, 224]:
-            if rows is None:
-                rows = 224
-                warnings.warn('MobileNet shape is undefined.'
-                              ' Weights for input shape'
-                              '(224, 224) will be loaded.')
-            else:
-                raise ValueError('If imagenet weights are being loaded, '
-                                 'input must have a static square shape'
-                                 '(one of (96, 96), (128, 128), (160, 160),'
-                                 '(192, 192), or (224, 224)).'
-                                 'Input shape provided = %s' % (input_shape,))
-
-    if K.image_data_format() != 'channels_last':
-        warnings.warn('The MobileNet family of models is only available '
-                      'for the input data format "channels_last" '
-                      '(width, height, channels). '
-                      'However your settings specify the default '
-                      'data format "channels_first" (channels, width, height).'
-                      ' You should set `image_data_format="channels_last"` '
-                      'in your Keras config located at ~/.keras/keras.json. '
-                      'The model being returned right now will expect inputs '
-                      'to follow the "channels_last" data format.')
-        K.set_image_data_format('channels_last')
-        old_data_format = 'channels_first'
-    else:
-        old_data_format = None
-
-    if input_tensor is None:
-        img_input = Input(shape=input_shape)
-    else:
-        if not K.is_keras_tensor(input_tensor):
-            img_input = Input(tensor=input_tensor, shape=input_shape)
-        else:
-            img_input = input_tensor
-
-    first_block_filters = _make_divisible(32 * alpha, 8)
-    x = Conv2D(first_block_filters,
-               kernel_size=3,
-               strides=(2, 2), padding='same',
-               use_bias=False, name='Conv1')(img_input)
-    x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='bn_Conv1')(x)
-    x = Activation(relu6, name='Conv1_relu')(x)
-
-    x = _first_inverted_res_block(x,
-                                  filters=16,
-                                  alpha=alpha,
-                                  stride=1,
-                                  expansion=1,
-                                  block_id=0)
-
-    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,
-                            expansion=6, block_id=1)
-    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,
-                            expansion=6, block_id=2)
-
-    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,
-                            expansion=6, block_id=3)
-    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,
-                            expansion=6, block_id=4)
-    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,
-                            expansion=6, block_id=5)
-
-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,
-                            expansion=6, block_id=6)
-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,
-                            expansion=6, block_id=7)
-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,
-                            expansion=6, block_id=8)
-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,
-                            expansion=6, block_id=9)
-
-    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,
-                            expansion=6, block_id=10)
-    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,
-                            expansion=6, block_id=11)
-    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,
-                            expansion=6, block_id=12)
-
-    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,
-                            expansion=6, block_id=13)
-    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,
-                            expansion=6, block_id=14)
-    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,
-                            expansion=6, block_id=15)
-
-    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,
-                            expansion=6, block_id=16)
-
-    # no alpha applied to last conv as stated in the paper:
-    # if the width multiplier is greater than 1 we
-    # increase the number of output channels
-    if alpha > 1.0:
-        last_block_filters = _make_divisible(1280 * alpha, 8)
-    else:
-        last_block_filters = 1280
-
-    x = Conv2D(last_block_filters,
-               kernel_size=1,
-               use_bias=False,
-               name='Conv_1')(x)
-    x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='Conv_1_bn')(x)
-    x = Activation(relu6, name='out_relu')(x)
-
-    if include_top:
-        x = GlobalAveragePooling2D()(x)
-        x = Dense(classes, activation='softmax',
-                  use_bias=True, name='Logits')(x)
-
-    # Ensure that the model takes into account
-    # any potential predecessors of `input_tensor`.
-    if input_tensor is not None:
-        inputs = get_source_inputs(input_tensor)
-    else:
-        inputs = img_input
-
-    # Create model.
-    model = Model(inputs, x, name='mobilenetv2_%0.2f_%s' % (alpha, rows))
-
-    # load weights
-    if weights == 'imagenet':
-        if K.image_data_format() == 'channels_first':
-            raise ValueError('Weights for "channels_first" format '
-                             'are not available.')
-
-        if include_top:
-            model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + \
-                str(alpha) + '_' + str(rows) + '.h5'
-            weigh_path = BASE_WEIGHT_PATH + model_name
-            weights_path = get_file(model_name, weigh_path,
-                                    cache_subdir='models')
-        else:
-            model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + \
-                str(alpha) + '_' + str(rows) + '_no_top' + '.h5'
-            weigh_path = BASE_WEIGHT_PATH + model_name
-            weights_path = get_file(model_name, weigh_path,
-                                    cache_subdir='models')
-        model.load_weights(weights_path)
-    elif weights is not None:
-        model.load_weights(weights)
-
-    if old_data_format:
-        K.set_image_data_format(old_data_format)
-    return model
-
-
-def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):
-    in_channels = inputs._keras_shape[-1]
-    prefix = 'features.' + str(block_id) + '.conv.'
-    pointwise_conv_filters = int(filters * alpha)
-    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)
-    # Expand
-
-    x = Conv2D(expansion * in_channels, kernel_size=1, padding='same',
-               use_bias=False, activation=None,
-               name='mobl%d_conv_expand' % block_id)(inputs)
-    x = BatchNormalization(epsilon=1e-3, momentum=0.999,
-                           name='bn%d_conv_bn_expand' %
-                           block_id)(x)
-    x = Activation(relu6, name='conv_%d_relu' % block_id)(x)
-
-    # Depthwise
-    x = DepthwiseConv2D(kernel_size=3, strides=stride, activation=None,
-                        use_bias=False, padding='same',
-                        name='mobl%d_conv_depthwise' % block_id)(x)
-    x = BatchNormalization(epsilon=1e-3, momentum=0.999,
-                           name='bn%d_conv_depthwise' % block_id)(x)
-
-    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)
-
-    # Project
-    x = Conv2D(pointwise_filters,
-               kernel_size=1, padding='same', use_bias=False, activation=None,
-               name='mobl%d_conv_project' % block_id)(x)
-    x = BatchNormalization(epsilon=1e-3, momentum=0.999,
-                           name='bn%d_conv_bn_project' % block_id)(x)
-
-    if in_channels == pointwise_filters and stride == 1:
-        return Add(name='res_connect_' + str(block_id))([inputs, x])
-
-    return x
-
-
-def _first_inverted_res_block(inputs,
-                              expansion, stride,
-                              alpha, filters, block_id):
-    in_channels = inputs._keras_shape[-1]
-    prefix = 'features.' + str(block_id) + '.conv.'
-    pointwise_conv_filters = int(filters * alpha)
-    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)
-
-    # Depthwise
-    x = DepthwiseConv2D(kernel_size=3,
-                        strides=stride, activation=None,
-                        use_bias=False, padding='same',
-                        name='mobl%d_conv_depthwise' %
-                        block_id)(inputs)
-    x = BatchNormalization(epsilon=1e-3, momentum=0.999,
-                           name='bn%d_conv_depthwise' %
-                           block_id)(x)
-    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)
-
-    # Project
-    x = Conv2D(pointwise_filters,
-               kernel_size=1,
-               padding='same',
-               use_bias=False,
-               activation=None,
-               name='mobl%d_conv_project' %
-               block_id)(x)
-    x = BatchNormalization(epsilon=1e-3, momentum=0.999,
-                           name='bn%d_conv_project' %
-                           block_id)(x)
-
-    if in_channels == pointwise_filters and stride == 1:
-        return Add(name='res_connect_' + str(block_id))([inputs, x])
-
-    return x
diff --git a/research/object_detection/models/keras_models/test_utils.py b/research/object_detection/models/keras_models/test_utils.py
index b7aa503e..0669b6c6 100644
--- a/research/object_detection/models/keras_models/test_utils.py
+++ b/research/object_detection/models/keras_models/test_utils.py
@@ -106,6 +106,16 @@ moblenet_v1_expected_feature_map_shape_enforcing_min_depth = [
     (2, 10, 10, 8), (2, 10, 10, 8), (2, 10, 10, 8),
 ]
 
+moblenet_v1_expected_feature_map_shape_with_conv_defs = [
+    (2, 150, 150, 32), (2, 150, 150, 32), (2, 150, 150, 64), (2, 75, 75, 64),
+    (2, 75, 75, 128), (2, 75, 75, 128), (2, 75, 75, 128), (2, 38, 38, 128),
+    (2, 38, 38, 256), (2, 38, 38, 256), (2, 38, 38, 256), (2, 19, 19, 256),
+    (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512),
+    (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512),
+    (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512), (2, 10, 10, 512),
+    (2, 10, 10, 512), (2, 10, 10, 512), (2, 10, 10, 256),
+]
+
 # For Mobilenet V2
 moblenet_v2_expected_feature_map_shape_128 = [
     (2, 64, 64, 32), (2, 64, 64, 96), (2, 32, 32, 96), (2, 32, 32, 24),
@@ -187,3 +197,18 @@ moblenet_v2_expected_feature_map_shape_enforcing_min_depth = [
     (2, 10, 10, 32), (2, 10, 10, 32)
 ]
 
+moblenet_v2_expected_feature_map_shape_with_conv_defs = [
+    (2, 150, 150, 32), (2, 150, 150, 96), (2, 75, 75, 96), (2, 75, 75, 24),
+    (2, 75, 75, 144), (2, 75, 75, 144), (2, 75, 75, 24), (2, 75, 75, 144),
+    (2, 38, 38, 144), (2, 38, 38, 32), (2, 38, 38, 192), (2, 38, 38, 192),
+    (2, 38, 38, 32), (2, 38, 38, 192), (2, 38, 38, 192), (2, 38, 38, 32),
+    (2, 38, 38, 192), (2, 19, 19, 192), (2, 19, 19, 64), (2, 19, 19, 384),
+    (2, 19, 19, 384), (2, 19, 19, 64), (2, 19, 19, 384), (2, 19, 19, 384),
+    (2, 19, 19, 64), (2, 19, 19, 384), (2, 19, 19, 384), (2, 19, 19, 64),
+    (2, 19, 19, 384), (2, 19, 19, 384), (2, 19, 19, 96), (2, 19, 19, 576),
+    (2, 19, 19, 576), (2, 19, 19, 96), (2, 19, 19, 576), (2, 19, 19, 576),
+    (2, 19, 19, 96), (2, 19, 19, 576), (2, 10, 10, 576), (2, 10, 10, 160),
+    (2, 10, 10, 960), (2, 10, 10, 960), (2, 10, 10, 160), (2, 10, 10, 960),
+    (2, 10, 10, 960), (2, 10, 10, 160), (2, 10, 10, 960), (2, 10, 10, 960),
+    (2, 10, 10, 320), (2, 10, 10, 256)
+]
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
index 72771ceb..6ee6dcab 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
@@ -13,21 +13,32 @@
 # limitations under the License.
 # ==============================================================================
 
-"""Tests for ssd_mobilenet_v1_fpn_feature_extractor."""
+"""Tests for ssd_mobilenet_v1_fpn_feature_extractor.
+
+By using parameterized test decorator, this test serves for both Slim-based and
+Keras-based Mobilenet V1 FPN feature extractors in SSD.
+"""
+from absl.testing import parameterized
 import numpy as np
 import tensorflow as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v1_fpn_feature_extractor
+from object_detection.models import ssd_mobilenet_v1_fpn_keras_feature_extractor
 
 slim = tf.contrib.slim
 
 
+@parameterized.parameters(
+    {'use_keras': False},
+    {'use_keras': True},
+)
 class SsdMobilenetV1FpnFeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-                                is_training=True, use_explicit_padding=False):
+                                is_training=True, use_explicit_padding=False,
+                                use_keras=False):
     """Constructs a new feature extractor.
 
     Args:
@@ -38,20 +49,38 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
       use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
         inputs so that the output dimensions are the same as if 'SAME' padding
         were used.
+      use_keras: if True builds a keras-based feature extractor, if False builds
+        a slim-based one.
     Returns:
       an ssd_meta_arch.SSDFeatureExtractor object.
     """
     min_depth = 32
-    return (ssd_mobilenet_v1_fpn_feature_extractor.
-            SSDMobileNetV1FpnFeatureExtractor(
-                is_training,
-                depth_multiplier,
-                min_depth,
-                pad_to_multiple,
-                self.conv_hyperparams_fn,
-                use_explicit_padding=use_explicit_padding))
-
-  def test_extract_features_returns_correct_shapes_256(self):
+    if use_keras:
+      return (ssd_mobilenet_v1_fpn_keras_feature_extractor.
+              SSDMobileNetV1FpnKerasFeatureExtractor(
+                  is_training=is_training,
+                  depth_multiplier=depth_multiplier,
+                  min_depth=min_depth,
+                  pad_to_multiple=pad_to_multiple,
+                  conv_hyperparams=self._build_conv_hyperparams(
+                      add_batch_norm=False),
+                  freeze_batchnorm=False,
+                  inplace_batchnorm_update=False,
+                  use_explicit_padding=use_explicit_padding,
+                  use_depthwise=True,
+                  name='MobilenetV1_FPN'))
+    else:
+      return (ssd_mobilenet_v1_fpn_feature_extractor.
+              SSDMobileNetV1FpnFeatureExtractor(
+                  is_training,
+                  depth_multiplier,
+                  min_depth,
+                  pad_to_multiple,
+                  self.conv_hyperparams_fn,
+                  use_depthwise=True,
+                  use_explicit_padding=use_explicit_padding))
+
+  def test_extract_features_returns_correct_shapes_256(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1.0
@@ -61,12 +90,14 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
                                   (2, 2, 2, 256)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_384(self):
+  def test_extract_features_returns_correct_shapes_384(self, use_keras):
     image_height = 320
     image_width = 320
     depth_multiplier = 1.0
@@ -76,12 +107,14 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
                                   (2, 3, 3, 256)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_with_dynamic_image_shape(self):
+  def test_extract_features_with_dynamic_image_shape(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1.0
@@ -91,12 +124,15 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
                                   (2, 2, 2, 256)]
     self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
+  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(
+      self, use_keras):
     image_height = 299
     image_width = 299
     depth_multiplier = 1.0
@@ -106,12 +142,15 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
                                   (2, 3, 3, 256)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
+  def test_extract_features_returns_correct_shapes_enforcing_min_depth(
+      self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 0.5**12
@@ -121,38 +160,50 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
                                   (2, 2, 2, 32)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_raises_error_with_invalid_image_size(self):
+  def test_extract_features_raises_error_with_invalid_image_size(
+      self, use_keras):
     image_height = 32
     image_width = 32
     depth_multiplier = 1.0
     pad_to_multiple = 1
     self.check_extract_features_raises_error_with_invalid_image_size(
-        image_height, image_width, depth_multiplier, pad_to_multiple)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
+        use_keras=use_keras)
 
-  def test_preprocess_returns_correct_value_range(self):
+  def test_preprocess_returns_correct_value_range(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1
     pad_to_multiple = 1
     test_image = np.random.rand(2, image_height, image_width, 3)
     feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       pad_to_multiple)
+                                                       pad_to_multiple,
+                                                       use_keras=use_keras)
     preprocessed_image = feature_extractor.preprocess(test_image)
     self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))
 
-  def test_variables_only_created_in_scope(self):
+  def test_variables_only_created_in_scope(self, use_keras):
     depth_multiplier = 1
     pad_to_multiple = 1
     scope_name = 'MobilenetV1'
     self.check_feature_extractor_variables_under_scope(
-        depth_multiplier, pad_to_multiple, scope_name)
+        depth_multiplier, pad_to_multiple, scope_name, use_keras=use_keras)
 
-  def test_fused_batchnorm(self):
+  def test_variable_count(self, use_keras):
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    variables = self.get_feature_extractor_variables(
+        depth_multiplier, pad_to_multiple, use_keras=use_keras)
+    self.assertEqual(len(variables), 153)
+
+  def test_fused_batchnorm(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1
@@ -160,9 +211,14 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
     image_placeholder = tf.placeholder(tf.float32,
                                        [1, image_height, image_width, 3])
     feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       pad_to_multiple)
+                                                       pad_to_multiple,
+                                                       use_keras=use_keras)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
-    _ = feature_extractor.extract_features(preprocessed_image)
+    if use_keras:
+      _ = feature_extractor(preprocessed_image)
+    else:
+      _ = feature_extractor.extract_features(preprocessed_image)
+
     self.assertTrue(
         any(op.type == 'FusedBatchNorm'
             for op in tf.get_default_graph().get_operations()))
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
new file mode 100644
index 00000000..b8090893
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
@@ -0,0 +1,234 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""SSD Keras-based MobilenetV1 FPN Feature Extractor."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.models.keras_models import mobilenet_v1
+from object_detection.models.keras_models import model_utils
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+
+
+# A modified config of mobilenet v1 that makes it more detection friendly.
+def _create_modified_mobilenet_config():
+  conv_def_block_12 = model_utils.ConvDefs(conv_name='conv_pw_12', filters=512)
+  conv_def_block_13 = model_utils.ConvDefs(conv_name='conv_pw_13', filters=256)
+  return [conv_def_block_12, conv_def_block_13]
+
+
+class SSDMobileNetV1FpnKerasFeatureExtractor(
+    ssd_meta_arch.SSDKerasFeatureExtractor):
+  """SSD Feature Extractor using Keras-based MobilenetV1 FPN features."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams,
+               freeze_batchnorm,
+               inplace_batchnorm_update,
+               fpn_min_level=3,
+               fpn_max_level=7,
+               additional_layer_depth=256,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False,
+               name=None):
+    """SSD Keras based FPN feature extractor Mobilenet v1 architecture.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams: a `hyperparams_builder.KerasLayerHyperparams` object
+        containing convolution hyperparameters for the layers added on top of
+        the base feature extractor.
+      freeze_batchnorm: whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      inplace_batchnorm_update: whether to update batch norm moving average
+        values inplace. When this is false train op must add a control
+        dependency on tf.graphkeys.UPDATE_OPS collection in order to update
+        batch norm statistics.
+      fpn_min_level: the highest resolution feature map to use in FPN. The valid
+        values are {2, 3, 4, 5} which map to MobileNet v1 layers
+        {Conv2d_3_pointwise, Conv2d_5_pointwise, Conv2d_11_pointwise,
+        Conv2d_13_pointwise}, respectively.
+      fpn_max_level: the smallest resolution feature map to construct or use in
+        FPN. FPN constructions uses features maps starting from fpn_min_level
+        upto the fpn_max_level. In the case that there are not enough feature
+        maps in the backbone network, additional feature maps are created by
+        applying stride 2 convolutions until we get the desired number of fpn
+        levels.
+      additional_layer_depth: additional feature map layer channel depth.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams`.
+      name: a string name scope to assign to the model. If 'None', Keras
+        will auto-generate one from the class name.
+    """
+    super(SSDMobileNetV1FpnKerasFeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=freeze_batchnorm,
+        inplace_batchnorm_update=inplace_batchnorm_update,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams,
+        name=name)
+    self._fpn_min_level = fpn_min_level
+    self._fpn_max_level = fpn_max_level
+    self._additional_layer_depth = additional_layer_depth
+    self._conv_defs = None
+    if self._use_depthwise:
+      self._conv_defs = _create_modified_mobilenet_config()
+    self._feature_blocks = [
+        'Conv2d_3_pointwise', 'Conv2d_5_pointwise', 'Conv2d_11_pointwise',
+        'Conv2d_13_pointwise'
+    ]
+    self._mobilenet_v1 = None
+    self._fpn_features_generator = None
+    self._coarse_feature_layers = []
+
+  def build(self, input_shape):
+    full_mobilenet_v1 = mobilenet_v1.mobilenet_v1(
+        batchnorm_training=(self._is_training and not self._freeze_batchnorm),
+        conv_hyperparams=(self._conv_hyperparams
+                          if self._override_base_feature_extractor_hyperparams
+                          else None),
+        weights=None,
+        use_explicit_padding=self._use_explicit_padding,
+        alpha=self._depth_multiplier,
+        min_depth=self._min_depth,
+        conv_defs=self._conv_defs,
+        include_top=False)
+    conv2d_3_pointwise = full_mobilenet_v1.get_layer(
+        name='conv_pw_3_relu').output
+    conv2d_5_pointwise = full_mobilenet_v1.get_layer(
+        name='conv_pw_5_relu').output
+    conv2d_11_pointwise = full_mobilenet_v1.get_layer(
+        name='conv_pw_11_relu').output
+    conv2d_13_pointwise = full_mobilenet_v1.get_layer(
+        name='conv_pw_13_relu').output
+    self._mobilenet_v1 = tf.keras.Model(
+        inputs=full_mobilenet_v1.inputs,
+        outputs=[conv2d_3_pointwise, conv2d_5_pointwise,
+                 conv2d_11_pointwise, conv2d_13_pointwise]
+    )
+    # pylint:disable=g-long-lambda
+    self._depth_fn = lambda d: max(
+        int(d * self._depth_multiplier), self._min_depth)
+    self._base_fpn_max_level = min(self._fpn_max_level, 5)
+    self._num_levels = self._base_fpn_max_level + 1 - self._fpn_min_level
+    self._fpn_features_generator = (
+        feature_map_generators.KerasFpnTopDownFeatureMaps(
+            num_levels=self._num_levels,
+            depth=self._depth_fn(self._additional_layer_depth),
+            use_depthwise=self._use_depthwise,
+            use_explicit_padding=self._use_explicit_padding,
+            is_training=self._is_training,
+            conv_hyperparams=self._conv_hyperparams,
+            freeze_batchnorm=self._freeze_batchnorm,
+            name='FeatureMaps'))
+    # Construct coarse feature layers
+    padding = 'VALID' if self._use_explicit_padding else 'SAME'
+    kernel_size = 3
+    stride = 2
+    for i in range(self._base_fpn_max_level + 1, self._fpn_max_level + 1):
+      coarse_feature_layers = []
+      if self._use_explicit_padding:
+        def fixed_padding(features, kernel_size=kernel_size):
+          return ops.fixed_padding(features, kernel_size)
+        coarse_feature_layers.append(tf.keras.layers.Lambda(
+            fixed_padding, name='fixed_padding'))
+      layer_name = 'bottom_up_Conv2d_{}'.format(
+          i - self._base_fpn_max_level + 13)
+      conv_block = feature_map_generators.create_conv_block(
+          self._use_depthwise, kernel_size, padding, stride, layer_name,
+          self._conv_hyperparams, self._is_training, self._freeze_batchnorm,
+          self._depth_fn(self._additional_layer_depth))
+      coarse_feature_layers.extend(conv_block)
+      self._coarse_feature_layers.append(coarse_feature_layers)
+    self.built = True
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def _extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+
+    image_features = self._mobilenet_v1(
+        ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple))
+
+    feature_block_list = []
+    for level in range(self._fpn_min_level, self._base_fpn_max_level + 1):
+      feature_block_list.append(self._feature_blocks[level - 2])
+
+    feature_start_index = len(self._feature_blocks) - self._num_levels
+    fpn_input_image_features = [
+        (key, image_features[feature_start_index + index])
+        for index, key in enumerate(feature_block_list)]
+    fpn_features = self._fpn_features_generator(fpn_input_image_features)
+
+    feature_maps = []
+    for level in range(self._fpn_min_level, self._base_fpn_max_level + 1):
+      feature_maps.append(fpn_features['top_down_{}'.format(
+          self._feature_blocks[level - 2])])
+    last_feature_map = fpn_features['top_down_{}'.format(
+        self._feature_blocks[self._base_fpn_max_level - 2])]
+
+    for coarse_feature_layers in self._coarse_feature_layers:
+      for layer in coarse_feature_layers:
+        last_feature_map = layer(last_feature_map)
+      feature_maps.append(last_feature_map)
+    return feature_maps
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py
index c37b06b3..1e402785 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py
@@ -13,21 +13,32 @@
 # limitations under the License.
 # ==============================================================================
 
-"""Tests for ssd_mobilenet_v2_fpn_feature_extractor."""
+"""Tests for ssd_mobilenet_v2_fpn_feature_extractor.
+
+By using parameterized test decorator, this test serves for both Slim-based and
+Keras-based Mobilenet V2 FPN feature extractors in SSD.
+"""
+from absl.testing import parameterized
 import numpy as np
 import tensorflow as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v2_fpn_feature_extractor
+from object_detection.models import ssd_mobilenet_v2_fpn_keras_feature_extractor
 
 slim = tf.contrib.slim
 
 
+@parameterized.parameters(
+    {'use_keras': False},
+    {'use_keras': True},
+)
 class SsdMobilenetV2FpnFeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-                                is_training=True, use_explicit_padding=False):
+                                is_training=True, use_explicit_padding=False,
+                                use_keras=False):
     """Constructs a new feature extractor.
 
     Args:
@@ -38,20 +49,36 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
       use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
         inputs so that the output dimensions are the same as if 'SAME' padding
         were used.
+      use_keras: if True builds a keras-based feature extractor, if False builds
+        a slim-based one.
     Returns:
       an ssd_meta_arch.SSDFeatureExtractor object.
     """
     min_depth = 32
-    return (ssd_mobilenet_v2_fpn_feature_extractor.
-            SSDMobileNetV2FpnFeatureExtractor(
-                is_training,
-                depth_multiplier,
-                min_depth,
-                pad_to_multiple,
-                self.conv_hyperparams_fn,
-                use_explicit_padding=use_explicit_padding))
-
-  def test_extract_features_returns_correct_shapes_256(self):
+    if use_keras:
+      return (ssd_mobilenet_v2_fpn_keras_feature_extractor.
+              SSDMobileNetV2FpnKerasFeatureExtractor(
+                  is_training=is_training,
+                  depth_multiplier=depth_multiplier,
+                  min_depth=min_depth,
+                  pad_to_multiple=pad_to_multiple,
+                  conv_hyperparams=self._build_conv_hyperparams(
+                      add_batch_norm=False),
+                  freeze_batchnorm=False,
+                  inplace_batchnorm_update=False,
+                  use_explicit_padding=use_explicit_padding,
+                  name='MobilenetV2_FPN'))
+    else:
+      return (ssd_mobilenet_v2_fpn_feature_extractor.
+              SSDMobileNetV2FpnFeatureExtractor(
+                  is_training,
+                  depth_multiplier,
+                  min_depth,
+                  pad_to_multiple,
+                  self.conv_hyperparams_fn,
+                  use_explicit_padding=use_explicit_padding))
+
+  def test_extract_features_returns_correct_shapes_256(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1.0
@@ -61,12 +88,14 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
                                   (2, 2, 2, 256)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_384(self):
+  def test_extract_features_returns_correct_shapes_384(self, use_keras):
     image_height = 320
     image_width = 320
     depth_multiplier = 1.0
@@ -76,12 +105,14 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
                                   (2, 3, 3, 256)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_with_dynamic_image_shape(self):
+  def test_extract_features_with_dynamic_image_shape(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1.0
@@ -91,12 +122,15 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
                                   (2, 2, 2, 256)]
     self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
+  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(
+      self, use_keras):
     image_height = 299
     image_width = 299
     depth_multiplier = 1.0
@@ -106,12 +140,15 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
                                   (2, 3, 3, 256)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
+  def test_extract_features_returns_correct_shapes_enforcing_min_depth(
+      self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 0.5**12
@@ -121,38 +158,43 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
                                   (2, 2, 2, 32)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_raises_error_with_invalid_image_size(self):
+  def test_extract_features_raises_error_with_invalid_image_size(
+      self, use_keras):
     image_height = 32
     image_width = 32
     depth_multiplier = 1.0
     pad_to_multiple = 1
     self.check_extract_features_raises_error_with_invalid_image_size(
-        image_height, image_width, depth_multiplier, pad_to_multiple)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
+        use_keras=use_keras)
 
-  def test_preprocess_returns_correct_value_range(self):
+  def test_preprocess_returns_correct_value_range(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1
     pad_to_multiple = 1
     test_image = np.random.rand(2, image_height, image_width, 3)
     feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       pad_to_multiple)
+                                                       pad_to_multiple,
+                                                       use_keras=use_keras)
     preprocessed_image = feature_extractor.preprocess(test_image)
     self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))
 
-  def test_variables_only_created_in_scope(self):
+  def test_variables_only_created_in_scope(self, use_keras):
     depth_multiplier = 1
     pad_to_multiple = 1
     scope_name = 'MobilenetV2'
     self.check_feature_extractor_variables_under_scope(
-        depth_multiplier, pad_to_multiple, scope_name)
+        depth_multiplier, pad_to_multiple, scope_name, use_keras=use_keras)
 
-  def test_fused_batchnorm(self):
+  def test_fused_batchnorm(self, use_keras):
     image_height = 256
     image_width = 256
     depth_multiplier = 1
@@ -160,19 +202,30 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
     image_placeholder = tf.placeholder(tf.float32,
                                        [1, image_height, image_width, 3])
     feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       pad_to_multiple)
+                                                       pad_to_multiple,
+                                                       use_keras=use_keras)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
-    _ = feature_extractor.extract_features(preprocessed_image)
+    if use_keras:
+      _ = feature_extractor(preprocessed_image)
+    else:
+      _ = feature_extractor.extract_features(preprocessed_image)
     self.assertTrue(
         any(op.type == 'FusedBatchNorm'
             for op in tf.get_default_graph().get_operations()))
 
-  def test_get_expected_feature_map_variable_names(self):
+  def test_variable_count(self, use_keras):
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    variables = self.get_feature_extractor_variables(
+        depth_multiplier, pad_to_multiple, use_keras=use_keras)
+    self.assertEqual(len(variables), 274)
+
+  def test_get_expected_feature_map_variable_names(self, use_keras):
     depth_multiplier = 1.0
     pad_to_multiple = 1
 
-    expected_feature_maps_variables = set([
-        # Mobilenet V2 feature maps
+    slim_expected_feature_maps_variables = set([
+        # Slim Mobilenet V2 feature maps
         'MobilenetV2/expanded_conv_4/depthwise/depthwise_weights',
         'MobilenetV2/expanded_conv_7/depthwise/depthwise_weights',
         'MobilenetV2/expanded_conv_14/depthwise/depthwise_weights',
@@ -186,13 +239,32 @@ class SsdMobilenetV2FpnFeatureExtractorTest(
         'MobilenetV2/fpn/projection_2/weights',
         'MobilenetV2/fpn/projection_3/weights',
     ])
-
+    keras_expected_feature_maps_variables = set([
+        # Keras Mobilenet V2 feature maps
+        'MobilenetV2_FPN/block_4_depthwise/depthwise_kernel',
+        'MobilenetV2_FPN/block_7_depthwise/depthwise_kernel',
+        'MobilenetV2_FPN/block_14_depthwise/depthwise_kernel',
+        'MobilenetV2_FPN/Conv_1/kernel',
+        # FPN layers
+        'MobilenetV2_FPN/bottom_up_Conv2d_20_conv/kernel',
+        'MobilenetV2_FPN/bottom_up_Conv2d_21_conv/kernel',
+        'MobilenetV2_FPN/FeatureMaps/top_down/smoothing_1_conv/kernel',
+        'MobilenetV2_FPN/FeatureMaps/top_down/smoothing_2_conv/kernel',
+        'MobilenetV2_FPN/FeatureMaps/top_down/projection_1/kernel',
+        'MobilenetV2_FPN/FeatureMaps/top_down/projection_2/kernel',
+        'MobilenetV2_FPN/FeatureMaps/top_down/projection_3/kernel'
+    ])
     g = tf.Graph()
     with g.as_default():
       preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
       feature_extractor = self._create_feature_extractor(
-          depth_multiplier, pad_to_multiple)
-      feature_extractor.extract_features(preprocessed_inputs)
+          depth_multiplier, pad_to_multiple, use_keras=use_keras)
+      if use_keras:
+        feature_extractor(preprocessed_inputs)
+        expected_feature_maps_variables = keras_expected_feature_maps_variables
+      else:
+        feature_extractor.extract_features(preprocessed_inputs)
+        expected_feature_maps_variables = slim_expected_feature_maps_variables
       actual_variable_set = set([
           var.op.name for var in g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
       ])
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
new file mode 100644
index 00000000..f20ca2d0
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
@@ -0,0 +1,231 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""SSD Keras-based MobilenetV2 FPN Feature Extractor."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.models.keras_models import mobilenet_v2
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+
+# Total number of blocks in Mobilenet_V2 base network.
+NUM_LAYERS = 19
+
+
+# A modified config of mobilenet v2 that makes it more detection friendly.
+def _create_modified_mobilenet_config():
+  last_conv = mobilenet_v2.ConvDefs(conv_name='Conv_1', filters=256)
+  return [last_conv]
+
+
+class SSDMobileNetV2FpnKerasFeatureExtractor(
+    ssd_meta_arch.SSDKerasFeatureExtractor):
+  """SSD Feature Extractor using Keras-based MobilenetV2 FPN features."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams,
+               freeze_batchnorm,
+               inplace_batchnorm_update,
+               fpn_min_level=3,
+               fpn_max_level=7,
+               additional_layer_depth=256,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False,
+               name=None):
+    """SSD Keras based FPN feature extractor Mobilenet v2 architecture.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams: a `hyperparams_builder.KerasLayerHyperparams` object
+        containing convolution hyperparameters for the layers added on top of
+        the base feature extractor.
+      freeze_batchnorm: whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      inplace_batchnorm_update: whether to update batch norm moving average
+        values inplace. When this is false train op must add a control
+        dependency on tf.graphkeys.UPDATE_OPS collection in order to update
+        batch norm statistics.
+      fpn_min_level: the highest resolution feature map to use in FPN. The valid
+        values are {2, 3, 4, 5} which map to MobileNet v2 layers
+        {layer_4, layer_7, layer_14, layer_19}, respectively.
+      fpn_max_level: the smallest resolution feature map to construct or use in
+        FPN. FPN constructions uses features maps starting from fpn_min_level
+        upto the fpn_max_level. In the case that there are not enough feature
+        maps in the backbone network, additional feature maps are created by
+        applying stride 2 convolutions until we get the desired number of fpn
+        levels.
+      additional_layer_depth: additional feature map layer channel depth.
+      reuse_weights: whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams`.
+      name: a string name scope to assign to the model. If 'None', Keras
+        will auto-generate one from the class name.
+    """
+    super(SSDMobileNetV2FpnKerasFeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=freeze_batchnorm,
+        inplace_batchnorm_update=inplace_batchnorm_update,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams,
+        name=name)
+    self._fpn_min_level = fpn_min_level
+    self._fpn_max_level = fpn_max_level
+    self._additional_layer_depth = additional_layer_depth
+    self._conv_defs = None
+    if self._use_depthwise:
+      self._conv_defs = _create_modified_mobilenet_config()
+    self._feature_blocks = ['layer_4', 'layer_7', 'layer_14', 'layer_19']
+    self._mobilenet_v2 = None
+    self._fpn_features_generator = None
+    self._coarse_feature_layers = []
+
+  def build(self, input_shape):
+    full_mobilenet_v2 = mobilenet_v2.mobilenet_v2(
+        batchnorm_training=(self._is_training and not self._freeze_batchnorm),
+        conv_hyperparams=(self._conv_hyperparams
+                          if self._override_base_feature_extractor_hyperparams
+                          else None),
+        weights=None,
+        use_explicit_padding=self._use_explicit_padding,
+        alpha=self._depth_multiplier,
+        min_depth=self._min_depth,
+        include_top=False)
+    layer_names = [layer.name for layer in full_mobilenet_v2.layers]
+    outputs = []
+    for layer_idx in [4, 7, 14]:
+      add_name = 'block_{}_add'.format(layer_idx - 2)
+      project_name = 'block_{}_project_BN'.format(layer_idx - 2)
+      output_layer_name = add_name if add_name in layer_names else project_name
+      outputs.append(full_mobilenet_v2.get_layer(output_layer_name).output)
+    layer_19 = full_mobilenet_v2.get_layer(name='out_relu').output
+    outputs.append(layer_19)
+    self._mobilenet_v2 = tf.keras.Model(
+        inputs=full_mobilenet_v2.inputs,
+        outputs=outputs)
+    # pylint:disable=g-long-lambda
+    self._depth_fn = lambda d: max(
+        int(d * self._depth_multiplier), self._min_depth)
+    self._base_fpn_max_level = min(self._fpn_max_level, 5)
+    self._num_levels = self._base_fpn_max_level + 1 - self._fpn_min_level
+    self._fpn_features_generator = (
+        feature_map_generators.KerasFpnTopDownFeatureMaps(
+            num_levels=self._num_levels,
+            depth=self._depth_fn(self._additional_layer_depth),
+            use_depthwise=self._use_depthwise,
+            use_explicit_padding=self._use_explicit_padding,
+            is_training=self._is_training,
+            conv_hyperparams=self._conv_hyperparams,
+            freeze_batchnorm=self._freeze_batchnorm,
+            name='FeatureMaps'))
+    # Construct coarse feature layers
+    padding = 'VALID' if self._use_explicit_padding else 'SAME'
+    kernel_size = 3
+    stride = 2
+    for i in range(self._base_fpn_max_level + 1, self._fpn_max_level + 1):
+      coarse_feature_layers = []
+      if self._use_explicit_padding:
+        def fixed_padding(features, kernel_size=kernel_size):
+          return ops.fixed_padding(features, kernel_size)
+        coarse_feature_layers.append(tf.keras.layers.Lambda(
+            fixed_padding, name='fixed_padding'))
+      layer_name = 'bottom_up_Conv2d_{}'.format(
+          i - self._base_fpn_max_level + NUM_LAYERS)
+      conv_block = feature_map_generators.create_conv_block(
+          self._use_depthwise, kernel_size, padding, stride, layer_name,
+          self._conv_hyperparams, self._is_training, self._freeze_batchnorm,
+          self._depth_fn(self._additional_layer_depth))
+      coarse_feature_layers.extend(conv_block)
+      self._coarse_feature_layers.append(coarse_feature_layers)
+    self.built = True
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def _extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+
+    image_features = self._mobilenet_v2(
+        ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple))
+
+    feature_block_list = []
+    for level in range(self._fpn_min_level, self._base_fpn_max_level + 1):
+      feature_block_list.append(self._feature_blocks[level - 2])
+
+    feature_start_index = len(self._feature_blocks) - self._num_levels
+    fpn_input_image_features = [
+        (key, image_features[feature_start_index + index])
+        for index, key in enumerate(feature_block_list)]
+    fpn_features = self._fpn_features_generator(fpn_input_image_features)
+
+    feature_maps = []
+    for level in range(self._fpn_min_level, self._base_fpn_max_level + 1):
+      feature_maps.append(fpn_features['top_down_{}'.format(
+          self._feature_blocks[level - 2])])
+    last_feature_map = fpn_features['top_down_{}'.format(
+        self._feature_blocks[self._base_fpn_max_level - 2])]
+
+    for coarse_feature_layers in self._coarse_feature_layers:
+      for layer in coarse_feature_layers:
+        last_feature_map = layer(last_feature_map)
+      feature_maps.append(last_feature_map)
+    return feature_maps
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index 33cedcce..74434af6 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -29,7 +29,7 @@ from nets import resnet_v1
 slim = tf.contrib.slim
 
 
-class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
+class SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD FPN feature extractor based on Resnet v1 architecture."""
 
   def __init__(self,
@@ -84,7 +84,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     Raises:
       ValueError: On supplying invalid arguments for unused arguments.
     """
-    super(_SSDResnetV1FpnFeatureExtractor, self).__init__(
+    super(SSDResnetV1FpnFeatureExtractor, self).__init__(
         is_training=is_training,
         depth_multiplier=depth_multiplier,
         min_depth=min_depth,
@@ -198,7 +198,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     return feature_maps
 
 
-class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+class SSDResnet50V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
   """SSD Resnet50 V1 FPN feature extractor."""
 
   def __init__(self,
@@ -255,7 +255,7 @@ class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         override_base_feature_extractor_hyperparams)
 
 
-class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+class SSDResnet101V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
   """SSD Resnet101 V1 FPN feature extractor."""
 
   def __init__(self,
@@ -312,7 +312,7 @@ class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         override_base_feature_extractor_hyperparams)
 
 
-class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+class SSDResnet152V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
   """SSD Resnet152 V1 FPN feature extractor."""
 
   def __init__(self,
diff --git a/research/object_detection/predictors/convolutional_box_predictor.py b/research/object_detection/predictors/convolutional_box_predictor.py
index 8762412a..ca6f3686 100644
--- a/research/object_detection/predictors/convolutional_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_box_predictor.py
@@ -357,9 +357,9 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
       inserted_layer_counter = 0
       target_channel = max(set(feature_channels), key=feature_channels.count)
       tf.logging.info('Not all feature maps have the same number of '
-                      'channels, found: {}, addition project layers '
-                      'to bring all feature maps to uniform channels '
-                      'of {}'.format(feature_channels, target_channel))
+                      'channels, found: {}, appending additional projection '
+                      'layers to bring all feature maps to uniformly have {} '
+                      'channels.'.format(feature_channels, target_channel))
     else:
       # Place holder variables if has_different_feature_channels is False.
       target_channel = -1
@@ -377,36 +377,38 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
       with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
                              reuse=tf.AUTO_REUSE):
         with slim.arg_scope(self._conv_hyperparams_fn()):
-          (image_feature,
-           inserted_layer_counter) = self._insert_additional_projection_layer(
-               image_feature, inserted_layer_counter, target_channel)
-          if self._share_prediction_tower:
-            box_tower_scope = 'PredictionTower'
-          else:
-            box_tower_scope = 'BoxPredictionTower'
-          box_tower_feature = self._compute_base_tower(
-              tower_name_scope=box_tower_scope,
-              image_feature=image_feature,
-              feature_index=feature_index)
-          box_encodings = self._box_prediction_head.predict(
-              features=box_tower_feature,
-              num_predictions_per_location=num_predictions_per_location)
-          predictions[BOX_ENCODINGS].append(box_encodings)
-          sorted_keys = sorted(self._other_heads.keys())
-          sorted_keys.append(CLASS_PREDICTIONS_WITH_BACKGROUND)
-          for head_name in sorted_keys:
-            if head_name == CLASS_PREDICTIONS_WITH_BACKGROUND:
-              head_obj = self._class_prediction_head
+          # TODO(wangjiang) Pass is_training to the head class directly.
+          with slim.arg_scope([slim.dropout], is_training=self._is_training):
+            (image_feature,
+             inserted_layer_counter) = self._insert_additional_projection_layer(
+                 image_feature, inserted_layer_counter, target_channel)
+            if self._share_prediction_tower:
+              box_tower_scope = 'PredictionTower'
             else:
-              head_obj = self._other_heads[head_name]
-            prediction = self._predict_head(
-                head_name=head_name,
-                head_obj=head_obj,
+              box_tower_scope = 'BoxPredictionTower'
+            box_tower_feature = self._compute_base_tower(
+                tower_name_scope=box_tower_scope,
                 image_feature=image_feature,
-                box_tower_feature=box_tower_feature,
-                feature_index=feature_index,
+                feature_index=feature_index)
+            box_encodings = self._box_prediction_head.predict(
+                features=box_tower_feature,
                 num_predictions_per_location=num_predictions_per_location)
-            predictions[head_name].append(prediction)
+            predictions[BOX_ENCODINGS].append(box_encodings)
+            sorted_keys = sorted(self._other_heads.keys())
+            sorted_keys.append(CLASS_PREDICTIONS_WITH_BACKGROUND)
+            for head_name in sorted_keys:
+              if head_name == CLASS_PREDICTIONS_WITH_BACKGROUND:
+                head_obj = self._class_prediction_head
+              else:
+                head_obj = self._other_heads[head_name]
+              prediction = self._predict_head(
+                  head_name=head_name,
+                  head_obj=head_obj,
+                  image_feature=image_feature,
+                  box_tower_feature=box_tower_feature,
+                  feature_index=feature_index,
+                  num_predictions_per_location=num_predictions_per_location)
+              predictions[head_name].append(prediction)
     return predictions
 
 
diff --git a/research/object_detection/predictors/convolutional_keras_box_predictor.py b/research/object_detection/predictors/convolutional_keras_box_predictor.py
index f8e981a3..90fcb0dd 100644
--- a/research/object_detection/predictors/convolutional_keras_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_keras_box_predictor.py
@@ -197,3 +197,272 @@ class ConvolutionalBoxPredictor(box_predictor.KerasBoxPredictor):
         predictions[head_name].append(prediction)
 
     return predictions
+
+
+class WeightSharedConvolutionalBoxPredictor(box_predictor.KerasBoxPredictor):
+  """Convolutional Box Predictor with weight sharing based on Keras.
+
+  Defines the box predictor as defined in
+  https://arxiv.org/abs/1708.02002. This class differs from
+  ConvolutionalBoxPredictor in that it shares weights and biases while
+  predicting from different feature maps. However, batch_norm parameters are not
+  shared because the statistics of the activations vary among the different
+  feature maps.
+
+  Also note that separate multi-layer towers are constructed for the box
+  encoding and class predictors respectively.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               box_prediction_head,
+               class_prediction_head,
+               other_heads,
+               conv_hyperparams,
+               depth,
+               num_layers_before_predictor,
+               freeze_batchnorm,
+               inplace_batchnorm_update,
+               kernel_size=3,
+               apply_batch_norm=False,
+               share_prediction_tower=False,
+               use_depthwise=False,
+               name=None):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      box_prediction_head: The head that predicts the boxes.
+      class_prediction_head: The head that predicts the classes.
+      other_heads: A dictionary mapping head names to convolutional
+        head classes.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops.
+      depth: depth of conv layers.
+      num_layers_before_predictor: Number of the additional conv layers before
+        the predictor.
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      inplace_batchnorm_update: Whether to update batch norm moving average
+        values inplace. When this is false train op must add a control
+        dependency on tf.graphkeys.UPDATE_OPS collection in order to update
+        batch norm statistics.
+      kernel_size: Size of final convolution kernel.
+      apply_batch_norm: Whether to apply batch normalization to conv layers in
+        this predictor.
+      share_prediction_tower: Whether to share the multi-layer tower among box
+        prediction head, class prediction head and other heads.
+      use_depthwise: Whether to use depthwise separable conv2d instead of
+       regular conv2d.
+      name: A string name scope to assign to the model. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(WeightSharedConvolutionalBoxPredictor, self).__init__(
+        is_training, num_classes, freeze_batchnorm=freeze_batchnorm,
+        inplace_batchnorm_update=inplace_batchnorm_update,
+        name=name)
+
+    self._box_prediction_head = box_prediction_head
+    self._prediction_heads = {
+        CLASS_PREDICTIONS_WITH_BACKGROUND: class_prediction_head,
+    }
+    if other_heads:
+      self._prediction_heads.update(other_heads)
+    # We generate a consistent ordering for the prediction head names,
+    # so that all workers build the model in the exact same order.
+    self._sorted_head_names = sorted(self._prediction_heads.keys())
+
+    self._conv_hyperparams = conv_hyperparams
+    self._depth = depth
+    self._num_layers_before_predictor = num_layers_before_predictor
+    self._kernel_size = kernel_size
+    self._apply_batch_norm = apply_batch_norm
+    self._share_prediction_tower = share_prediction_tower
+    self._use_depthwise = use_depthwise
+
+    # Additional projection layers to bring all feature maps to uniform
+    # channels.
+    self._additional_projection_layers = []
+    # The base tower layers for each head.
+    self._base_tower_layers_for_heads = {
+        BOX_ENCODINGS: [],
+        CLASS_PREDICTIONS_WITH_BACKGROUND: [],
+    }
+    for head_name in other_heads.keys():
+      self._base_tower_layers_for_heads[head_name] = []
+
+    # A dict maps the tower_name_scope of each head to the shared conv layers in
+    # the base tower for different feature map levels.
+    self._head_scope_conv_layers = {}
+
+  def _insert_additional_projection_layer(
+      self, inserted_layer_counter, target_channel):
+    projection_layers = []
+    if inserted_layer_counter >= 0:
+      use_bias = False if self._apply_batch_norm else True
+      projection_layers.append(keras.Conv2D(
+          target_channel, [1, 1], strides=1, padding='SAME',
+          name='ProjectionLayer/conv2d_{}'.format(inserted_layer_counter),
+          **self._conv_hyperparams.params(use_bias=use_bias)))
+      if self._apply_batch_norm:
+        projection_layers.append(self._conv_hyperparams.build_batch_norm(
+            training=(self._is_training and not self._freeze_batchnorm),
+            name='ProjectionLayer/conv2d_{}/BatchNorm'.format(
+                inserted_layer_counter)))
+      inserted_layer_counter += 1
+    return inserted_layer_counter, projection_layers
+
+  def _compute_base_tower(self, tower_name_scope, feature_index):
+    conv_layers = []
+    batch_norm_layers = []
+    activation_layers = []
+    use_bias = False if self._apply_batch_norm else True
+    for additional_conv_layer_idx in range(self._num_layers_before_predictor):
+      layer_name = '{}/conv2d_{}'.format(
+          tower_name_scope, additional_conv_layer_idx)
+      if tower_name_scope not in self._head_scope_conv_layers:
+        if self._use_depthwise:
+          conv_layers.append(
+              tf.keras.layers.SeparableConv2D(
+                  self._depth,
+                  [self._kernel_size, self._kernel_size],
+                  padding='SAME',
+                  name=layer_name,
+                  **self._conv_hyperparams.params(use_bias=use_bias)))
+        else:
+          conv_layers.append(
+              tf.keras.layers.Conv2D(
+                  self._depth,
+                  [self._kernel_size, self._kernel_size],
+                  padding='SAME',
+                  name=layer_name,
+                  **self._conv_hyperparams.params(use_bias=use_bias)))
+      # Each feature gets a separate batchnorm parameter even though they share
+      # the same convolution weights.
+      if self._apply_batch_norm:
+        batch_norm_layers.append(self._conv_hyperparams.build_batch_norm(
+            training=(self._is_training and not self._freeze_batchnorm),
+            name='{}/conv2d_{}/BatchNorm/feature_{}'.format(
+                tower_name_scope, additional_conv_layer_idx, feature_index)))
+      activation_layers.append(tf.keras.layers.Lambda(tf.nn.relu6))
+
+    # Set conv layers as the shared conv layers for different feature maps with
+    # the same tower_name_scope.
+    if tower_name_scope in self._head_scope_conv_layers:
+      conv_layers = self._head_scope_conv_layers[tower_name_scope]
+
+    # Stack the base_tower_layers in the order of conv_layer, batch_norm_layer
+    # and activation_layer
+    base_tower_layers = []
+    for i in range(self._num_layers_before_predictor):
+      base_tower_layers.extend([conv_layers[i]])
+      if self._apply_batch_norm:
+        base_tower_layers.extend([batch_norm_layers[i]])
+      base_tower_layers.extend([activation_layers[i]])
+    return conv_layers, base_tower_layers
+
+  def build(self, input_shapes):
+    """Creates the variables of the layer."""
+    feature_channels = [
+        input_shape[3].value for input_shape in input_shapes
+    ]
+    has_different_feature_channels = len(set(feature_channels)) > 1
+    if has_different_feature_channels:
+      inserted_layer_counter = 0
+      target_channel = max(set(feature_channels), key=feature_channels.count)
+      tf.logging.info('Not all feature maps have the same number of '
+                      'channels, found: {}, appending additional projection '
+                      'layers to bring all feature maps to uniformly have {} '
+                      'channels.'.format(feature_channels, target_channel))
+    else:
+      # Place holder variables if has_different_feature_channels is False.
+      target_channel = -1
+      inserted_layer_counter = -1
+
+    def _build_layers(tower_name_scope, feature_index):
+      conv_layers, base_tower_layers = self._compute_base_tower(
+          tower_name_scope=tower_name_scope, feature_index=feature_index)
+      if tower_name_scope not in self._head_scope_conv_layers:
+        self._head_scope_conv_layers[tower_name_scope] = conv_layers
+      return base_tower_layers
+
+    for feature_index, input_shape in enumerate(input_shapes):
+      # Additional projection layers should not be shared as input channels
+      # (and thus weight shapes) are different
+      inserted_layer_counter, projection_layers = (
+          self._insert_additional_projection_layer(
+              inserted_layer_counter, target_channel))
+      self._additional_projection_layers.append(projection_layers)
+
+      if self._share_prediction_tower:
+        box_tower_scope = 'PredictionTower'
+      else:
+        box_tower_scope = 'BoxPredictionTower'
+      # For box tower base
+      box_tower_layers = _build_layers(box_tower_scope, feature_index)
+      self._base_tower_layers_for_heads[BOX_ENCODINGS].append(box_tower_layers)
+
+      for head_name in self._sorted_head_names:
+        if head_name == CLASS_PREDICTIONS_WITH_BACKGROUND:
+          tower_name_scope = 'ClassPredictionTower'
+        else:
+          tower_name_scope = '{}PredictionTower'.format(head_name)
+        box_tower_layers = _build_layers(tower_name_scope, feature_index)
+        self._base_tower_layers_for_heads[head_name].append(box_tower_layers)
+
+    self.built = True
+
+  def _predict(self, image_features):
+    """Computes encoded object locations and corresponding confidences.
+
+    Args:
+      image_features: A list of float tensors of shape [batch_size, height_i,
+        width_i, channels_i] containing features for a batch of images.
+
+    Returns:
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes. Each entry in the
+        list corresponds to a feature map in the input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
+    """
+    predictions = collections.defaultdict(list)
+
+    def _apply_layers(base_tower_layers, image_feature):
+      for layer in base_tower_layers:
+        image_feature = layer(image_feature)
+      return image_feature
+
+    for (index, image_feature) in enumerate(image_features):
+      # Apply additional projection layers to image features
+      for layer in self._additional_projection_layers[index]:
+        image_feature = layer(image_feature)
+
+      # Apply box tower layers.
+      box_tower_feature = _apply_layers(
+          self._base_tower_layers_for_heads[BOX_ENCODINGS][index],
+          image_feature)
+      box_encodings = self._box_prediction_head(box_tower_feature)
+      predictions[BOX_ENCODINGS].append(box_encodings)
+
+      for head_name in self._sorted_head_names:
+        head_obj = self._prediction_heads[head_name]
+        if self._share_prediction_tower:
+          head_tower_feature = box_tower_feature
+        else:
+          head_tower_feature = _apply_layers(
+              self._base_tower_layers_for_heads[head_name][index],
+              image_feature)
+        prediction = head_obj(head_tower_feature)
+        predictions[head_name].append(prediction)
+    return predictions
diff --git a/research/object_detection/predictors/convolutional_keras_box_predictor_test.py b/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
index 48f0009e..1de42d9f 100644
--- a/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
@@ -21,6 +21,9 @@ from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
 from object_detection.builders import hyperparams_builder
 from object_detection.predictors import convolutional_keras_box_predictor as box_predictor
+from object_detection.predictors.heads import keras_box_head
+from object_detection.predictors.heads import keras_class_head
+from object_detection.predictors.heads import keras_mask_head
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import test_case
 
@@ -255,5 +258,651 @@ class ConvolutionalKerasBoxPredictorTest(test_case.TestCase):
     self.assertEqual(conv_box_predictor._sorted_head_names,
                      ['box_encodings', 'class_predictions_with_background'])
 
+
+class WeightSharedConvolutionalKerasBoxPredictorTest(test_case.TestCase):
+
+  def _build_conv_hyperparams(self, add_batch_norm=True):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      activation: RELU_6
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+          mean: 0.0
+        }
+      }
+    """
+    if add_batch_norm:
+      batch_norm_proto = """
+        batch_norm {
+          train: true,
+        }
+      """
+      conv_hyperparams_text_proto += batch_norm_proto
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  # pylint: disable=line-too-long
+  def test_get_boxes_for_five_aspect_ratios_per_location(self):
+
+    def graph_fn(image_features):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=0,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5],
+              depth=32,
+              num_layers_before_predictor=1,
+              box_code_size=4))
+      box_predictions = conv_box_predictor([image_features])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      objectness_predictions = tf.concat(box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
+      return (box_encodings, objectness_predictions)
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, objectness_predictions) = self.execute(
+        graph_fn, [image_features])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 4])
+    self.assertAllEqual(objectness_predictions.shape, [4, 320, 1])
+
+  def test_bias_predictions_to_background_with_sigmoid_score_conversion(self):
+
+    def graph_fn(image_features):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=True,
+              num_classes=2,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5],
+              depth=32,
+              num_layers_before_predictor=1,
+              class_prediction_bias_init=-4.6,
+              box_code_size=4))
+      box_predictions = conv_box_predictor([image_features])
+      class_predictions = tf.concat(box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
+      return (tf.nn.sigmoid(class_predictions),)
+
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    class_predictions = self.execute(graph_fn, [image_features])
+    self.assertAlmostEqual(np.mean(class_predictions), 0.01, places=3)
+
+  def test_get_multi_class_predictions_for_five_aspect_ratios_per_location(
+      self):
+
+    num_classes_without_background = 6
+    def graph_fn(image_features):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5],
+              depth=32,
+              num_layers_before_predictor=1,
+              box_code_size=4))
+      box_predictions = conv_box_predictor([image_features])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 4])
+    self.assertAllEqual(class_predictions_with_background.shape,
+                        [4, 320, num_classes_without_background+1])
+
+  def test_get_multi_class_predictions_from_two_feature_maps(
+      self):
+
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5],
+              depth=32,
+              num_layers_before_predictor=1,
+              box_code_size=4))
+      box_predictions = conv_box_predictor([image_features1, image_features2])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    image_features1 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    image_features2 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features1, image_features2])
+    self.assertAllEqual(box_encodings.shape, [4, 640, 4])
+    self.assertAllEqual(class_predictions_with_background.shape,
+                        [4, 640, num_classes_without_background+1])
+
+  def test_get_multi_class_predictions_from_feature_maps_of_different_depth(
+      self):
+
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2, image_features3):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5, 5],
+              depth=32,
+              num_layers_before_predictor=1,
+              box_code_size=4))
+      box_predictions = conv_box_predictor(
+          [image_features1, image_features2, image_features3])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    image_features1 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    image_features2 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    image_features3 = np.random.rand(4, 8, 8, 32).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features1, image_features2, image_features3])
+    self.assertAllEqual(box_encodings.shape, [4, 960, 4])
+    self.assertAllEqual(class_predictions_with_background.shape,
+                        [4, 960, num_classes_without_background+1])
+
+  def test_predictions_multiple_feature_maps_share_weights_separate_batchnorm(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5],
+              depth=32,
+              num_layers_before_predictor=2,
+              box_code_size=4))
+      box_predictions = conv_box_predictor([image_features1, image_features2])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Box prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/BatchNorm/feature_0/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/BatchNorm/feature_1/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/BatchNorm/feature_0/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/BatchNorm/feature_1/beta'),
+        # Box prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/bias'),
+        # Class prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/BatchNorm/feature_0/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/BatchNorm/feature_1/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/BatchNorm/feature_0/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/BatchNorm/feature_1/beta'),
+        # Class prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/bias')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_predictions_multiple_feature_maps_share_weights_without_batchnorm(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5],
+              depth=32,
+              num_layers_before_predictor=2,
+              box_code_size=4,
+              apply_batch_norm=False))
+      box_predictions = conv_box_predictor([image_features1, image_features2])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Box prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/bias'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/bias'),
+        # Box prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/bias'),
+        # Class prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/bias'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/bias'),
+        # Class prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/bias')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_predictions_multiple_feature_maps_share_weights_with_depthwise(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(
+                  add_batch_norm=False),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5],
+              depth=32,
+              num_layers_before_predictor=2,
+              box_code_size=4,
+              apply_batch_norm=False,
+              use_depthwise=True))
+      box_predictions = conv_box_predictor([image_features1, image_features2])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Box prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/depthwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/pointwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/bias'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/depthwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/pointwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/bias'),
+        # Box prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/depthwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/pointwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/bias'),
+        # Class prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/depthwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/pointwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/bias'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/depthwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/pointwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/bias'),
+        # Class prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/depthwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/pointwise_kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/bias')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_no_batchnorm_params_when_batchnorm_is_not_configured(self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(
+                  add_batch_norm=False),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5],
+              depth=32,
+              num_layers_before_predictor=2,
+              box_code_size=4,
+              apply_batch_norm=False))
+      box_predictions = conv_box_predictor(
+          [image_features1, image_features2])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Box prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/bias'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/bias'),
+        # Box prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/bias'),
+        # Class prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/bias'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/bias'),
+        # Class prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/bias')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_predictions_share_weights_share_tower_separate_batchnorm(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5],
+              depth=32,
+              num_layers_before_predictor=2,
+              box_code_size=4,
+              share_prediction_tower=True))
+      box_predictions = conv_box_predictor(
+          [image_features1, image_features2])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Shared prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/BatchNorm/feature_0/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/BatchNorm/feature_1/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/BatchNorm/feature_0/beta'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/BatchNorm/feature_1/beta'),
+        # Box prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/bias'),
+        # Class prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/bias')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_predictions_share_weights_share_tower_without_batchnorm(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = (
+          box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+              is_training=False,
+              num_classes=num_classes_without_background,
+              conv_hyperparams=self._build_conv_hyperparams(
+                  add_batch_norm=False),
+              freeze_batchnorm=False,
+              inplace_batchnorm_update=False,
+              num_predictions_per_location_list=[5, 5],
+              depth=32,
+              num_layers_before_predictor=2,
+              box_code_size=4,
+              share_prediction_tower=True,
+              apply_batch_norm=False))
+      box_predictions = conv_box_predictor(
+          [image_features1, image_features2])
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Shared prediction tower
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/bias'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/bias'),
+        # Box prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalBoxHead/BoxPredictor/bias'),
+        # Class prediction head
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/kernel'),
+        ('WeightSharedConvolutionalBoxPredictor/'
+         'WeightSharedConvolutionalClassHead/ClassPredictor/bias')])
+
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_get_predictions_with_feature_maps_of_dynamic_shape(
+      self):
+    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
+    conv_box_predictor = (
+        box_predictor_builder.build_weight_shared_convolutional_keras_box_predictor(
+            is_training=False,
+            num_classes=0,
+            conv_hyperparams=self._build_conv_hyperparams(),
+            freeze_batchnorm=False,
+            inplace_batchnorm_update=False,
+            num_predictions_per_location_list=[5],
+            depth=32,
+            num_layers_before_predictor=1,
+            box_code_size=4))
+    box_predictions = conv_box_predictor([image_features])
+    box_encodings = tf.concat(box_predictions[box_predictor.BOX_ENCODINGS],
+                              axis=1)
+    objectness_predictions = tf.concat(box_predictions[
+        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
+    init_op = tf.global_variables_initializer()
+
+    resolution = 32
+    expected_num_anchors = resolution*resolution*5
+    with self.test_session() as sess:
+      sess.run(init_op)
+      (box_encodings_shape,
+       objectness_predictions_shape) = sess.run(
+           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
+           feed_dict={image_features:
+                      np.random.rand(4, resolution, resolution, 64)})
+      self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 4])
+      self.assertAllEqual(objectness_predictions_shape,
+                          [4, expected_num_anchors, 1])
+
+  def test_other_heads_predictions(self):
+    box_code_size = 4
+    num_classes_without_background = 3
+    other_head_name = 'Mask'
+    mask_height = 5
+    mask_width = 5
+    num_predictions_per_location = 5
+
+    def graph_fn(image_features):
+      box_prediction_head = keras_box_head.WeightSharedConvolutionalBoxHead(
+          box_code_size=box_code_size,
+          conv_hyperparams=self._build_conv_hyperparams(),
+          num_predictions_per_location=num_predictions_per_location)
+      class_prediction_head = keras_class_head.WeightSharedConvolutionalClassHead(
+          num_class_slots=num_classes_without_background + 1,
+          conv_hyperparams=self._build_conv_hyperparams(),
+          num_predictions_per_location=num_predictions_per_location)
+      other_heads = {
+          other_head_name:
+              keras_mask_head.WeightSharedConvolutionalMaskHead(
+                  num_classes=num_classes_without_background,
+                  conv_hyperparams=self._build_conv_hyperparams(),
+                  num_predictions_per_location=num_predictions_per_location,
+                  mask_height=mask_height,
+                  mask_width=mask_width)
+      }
+
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          box_prediction_head=box_prediction_head,
+          class_prediction_head=class_prediction_head,
+          other_heads=other_heads,
+          conv_hyperparams=self._build_conv_hyperparams(),
+          freeze_batchnorm=False,
+          inplace_batchnorm_update=False,
+          depth=32,
+          num_layers_before_predictor=2)
+      box_predictions = conv_box_predictor([image_features])
+      for key, value in box_predictions.items():
+        box_predictions[key] = tf.concat(value, axis=1)
+      assert len(box_predictions) == 3
+      return (box_predictions[box_predictor.BOX_ENCODINGS],
+              box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+              box_predictions[other_head_name])
+
+    batch_size = 4
+    feature_ht = 8
+    feature_wt = 8
+    image_features = np.random.rand(batch_size, feature_ht, feature_wt,
+                                    64).astype(np.float32)
+    (box_encodings, class_predictions, other_head_predictions) = self.execute(
+        graph_fn, [image_features])
+    num_anchors = feature_ht * feature_wt * num_predictions_per_location
+    self.assertAllEqual(box_encodings.shape,
+                        [batch_size, num_anchors, box_code_size])
+    self.assertAllEqual(
+        class_predictions.shape,
+        [batch_size, num_anchors, num_classes_without_background + 1])
+    self.assertAllEqual(other_head_predictions.shape, [
+        batch_size, num_anchors, num_classes_without_background, mask_height,
+        mask_width
+    ])
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/heads/box_head.py b/research/object_detection/predictors/heads/box_head.py
index 3adfecb7..7ff2ed1e 100644
--- a/research/object_detection/predictors/heads/box_head.py
+++ b/research/object_detection/predictors/heads/box_head.py
@@ -120,7 +120,8 @@ class ConvolutionalBoxHead(head.Head):
                is_training,
                box_code_size,
                kernel_size,
-               use_depthwise=False):
+               use_depthwise=False,
+               box_encodings_clip_range=None):
     """Constructor.
 
     Args:
@@ -132,6 +133,7 @@ class ConvolutionalBoxHead(head.Head):
         min(feature_width, feature_height).
       use_depthwise: Whether to use depthwise convolutions for prediction
         steps. Default is False.
+      box_encodings_clip_range: Min and max values for clipping box_encodings.
 
     Raises:
       ValueError: if min_depth > max_depth.
@@ -141,6 +143,7 @@ class ConvolutionalBoxHead(head.Head):
     self._box_code_size = box_code_size
     self._kernel_size = kernel_size
     self._use_depthwise = use_depthwise
+    self._box_encodings_clip_range = box_encodings_clip_range
 
   def predict(self, features, num_predictions_per_location):
     """Predicts boxes.
@@ -180,6 +183,11 @@ class ConvolutionalBoxHead(head.Head):
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
       batch_size = tf.shape(features)[0]
+    # Clipping the box encodings to make the inference graph TPU friendly.
+    if self._box_encodings_clip_range is not None:
+      box_encodings = tf.clip_by_value(
+          box_encodings, self._box_encodings_clip_range.min,
+          self._box_encodings_clip_range.max)
     box_encodings = tf.reshape(box_encodings,
                                [batch_size, -1, 1, self._box_code_size])
     return box_encodings
@@ -198,7 +206,8 @@ class WeightSharedConvolutionalBoxHead(head.Head):
                box_code_size,
                kernel_size=3,
                use_depthwise=False,
-               box_encodings_clip_range=None):
+               box_encodings_clip_range=None,
+               return_flat_predictions=True):
     """Constructor.
 
     Args:
@@ -207,12 +216,18 @@ class WeightSharedConvolutionalBoxHead(head.Head):
       use_depthwise: Whether to use depthwise convolutions for prediction steps.
         Default is False.
       box_encodings_clip_range: Min and max values for clipping box_encodings.
+      return_flat_predictions: If true, returns flattened prediction tensor
+        of shape [batch, height * width * num_predictions_per_location,
+        box_coder]. Otherwise returns the prediction tensor before reshaping,
+        whose shape is [batch, height, width, num_predictions_per_location *
+        num_class_slots].
     """
     super(WeightSharedConvolutionalBoxHead, self).__init__()
     self._box_code_size = box_code_size
     self._kernel_size = kernel_size
     self._use_depthwise = use_depthwise
     self._box_encodings_clip_range = box_encodings_clip_range
+    self._return_flat_predictions = return_flat_predictions
 
   def predict(self, features, num_predictions_per_location):
     """Predicts boxes.
@@ -226,7 +241,9 @@ class WeightSharedConvolutionalBoxHead(head.Head):
     Returns:
       box_encodings: A float tensor of shape
         [batch_size, num_anchors, code_size] representing the location of
-        the objects.
+        the objects, or a float tensor of shape [batch, height, width,
+        num_predictions_per_location * box_code_size] representing grid box
+        location predictions if self._return_flat_predictions is False.
     """
     box_encodings_net = features
     if self._use_depthwise:
@@ -248,6 +265,7 @@ class WeightSharedConvolutionalBoxHead(head.Head):
       box_encodings = tf.clip_by_value(
           box_encodings, self._box_encodings_clip_range.min,
           self._box_encodings_clip_range.max)
-    box_encodings = tf.reshape(box_encodings,
-                               [batch_size, -1, self._box_code_size])
+    if self._return_flat_predictions:
+      box_encodings = tf.reshape(box_encodings,
+                                 [batch_size, -1, self._box_code_size])
     return box_encodings
diff --git a/research/object_detection/predictors/heads/class_head.py b/research/object_detection/predictors/heads/class_head.py
index ad41203b..d0d56677 100644
--- a/research/object_detection/predictors/heads/class_head.py
+++ b/research/object_detection/predictors/heads/class_head.py
@@ -39,7 +39,8 @@ class MaskRCNNClassHead(head.Head):
                num_class_slots,
                fc_hyperparams_fn,
                use_dropout,
-               dropout_keep_prob):
+               dropout_keep_prob,
+               scope='ClassPredictor'):
     """Constructor.
 
     Args:
@@ -53,6 +54,7 @@ class MaskRCNNClassHead(head.Head):
         in contrast to the ConvolutionalBoxPredictor below.
       dropout_keep_prob: Keep probability for dropout.
         This is only used if use_dropout is True.
+      scope: Scope name for the convolution operation.
     """
     super(MaskRCNNClassHead, self).__init__()
     self._is_training = is_training
@@ -60,6 +62,7 @@ class MaskRCNNClassHead(head.Head):
     self._fc_hyperparams_fn = fc_hyperparams_fn
     self._use_dropout = use_dropout
     self._dropout_keep_prob = dropout_keep_prob
+    self._scope = scope
 
   def predict(self, features, num_predictions_per_location=1):
     """Predicts boxes and class scores.
@@ -95,7 +98,7 @@ class MaskRCNNClassHead(head.Head):
           flattened_roi_pooled_features,
           self._num_class_slots,
           activation_fn=None,
-          scope='ClassPredictor')
+          scope=self._scope)
     class_predictions_with_background = tf.reshape(
         class_predictions_with_background,
         [-1, 1, self._num_class_slots])
@@ -113,7 +116,8 @@ class ConvolutionalClassHead(head.Head):
                kernel_size,
                apply_sigmoid_to_scores=False,
                class_prediction_bias_init=0.0,
-               use_depthwise=False):
+               use_depthwise=False,
+               scope='ClassPredictor'):
     """Constructor.
 
     Args:
@@ -135,6 +139,7 @@ class ConvolutionalClassHead(head.Head):
         conv2d layer before class prediction.
       use_depthwise: Whether to use depthwise convolutions for prediction
         steps. Default is False.
+      scope: Scope name for the convolution operation.
 
     Raises:
       ValueError: if min_depth > max_depth.
@@ -148,6 +153,7 @@ class ConvolutionalClassHead(head.Head):
     self._apply_sigmoid_to_scores = apply_sigmoid_to_scores
     self._class_prediction_bias_init = class_prediction_bias_init
     self._use_depthwise = use_depthwise
+    self._scope = scope
 
   def predict(self, features, num_predictions_per_location):
     """Predicts boxes.
@@ -167,17 +173,18 @@ class ConvolutionalClassHead(head.Head):
     if self._use_dropout:
       net = slim.dropout(net, keep_prob=self._dropout_keep_prob)
     if self._use_depthwise:
+      depthwise_scope = self._scope + '_depthwise'
       class_predictions_with_background = slim.separable_conv2d(
           net, None, [self._kernel_size, self._kernel_size],
           padding='SAME', depth_multiplier=1, stride=1,
-          rate=1, scope='ClassPredictor_depthwise')
+          rate=1, scope=depthwise_scope)
       class_predictions_with_background = slim.conv2d(
           class_predictions_with_background,
           num_predictions_per_location * self._num_class_slots, [1, 1],
           activation_fn=None,
           normalizer_fn=None,
           normalizer_params=None,
-          scope='ClassPredictor')
+          scope=self._scope)
     else:
       class_predictions_with_background = slim.conv2d(
           net,
@@ -186,7 +193,7 @@ class ConvolutionalClassHead(head.Head):
           activation_fn=None,
           normalizer_fn=None,
           normalizer_params=None,
-          scope='ClassPredictor',
+          scope=self._scope,
           biases_initializer=tf.constant_initializer(
               self._class_prediction_bias_init))
     if self._apply_sigmoid_to_scores:
@@ -217,7 +224,9 @@ class WeightSharedConvolutionalClassHead(head.Head):
                use_dropout=False,
                dropout_keep_prob=0.8,
                use_depthwise=False,
-               score_converter_fn=tf.identity):
+               score_converter_fn=tf.identity,
+               return_flat_predictions=True,
+               scope='ClassPredictor'):
     """Constructor.
 
     Args:
@@ -232,6 +241,12 @@ class WeightSharedConvolutionalClassHead(head.Head):
         steps. Default is False.
       score_converter_fn: Callable elementwise nonlinearity (that takes tensors
         as inputs and returns tensors).
+      return_flat_predictions: If true, returns flattened prediction tensor
+        of shape [batch, height * width * num_predictions_per_location,
+        box_coder]. Otherwise returns the prediction tensor before reshaping,
+        whose shape is [batch, height, width, num_predictions_per_location *
+        num_class_slots].
+      scope: Scope name for the convolution operation.
     """
     super(WeightSharedConvolutionalClassHead, self).__init__()
     self._num_class_slots = num_class_slots
@@ -241,6 +256,8 @@ class WeightSharedConvolutionalClassHead(head.Head):
     self._dropout_keep_prob = dropout_keep_prob
     self._use_depthwise = use_depthwise
     self._score_converter_fn = score_converter_fn
+    self._return_flat_predictions = return_flat_predictions
+    self._scope = scope
 
   def predict(self, features, num_predictions_per_location):
     """Predicts boxes.
@@ -254,7 +271,10 @@ class WeightSharedConvolutionalClassHead(head.Head):
     Returns:
       class_predictions_with_background: A tensor of shape
         [batch_size, num_anchors, num_class_slots] representing the class
-        predictions for the proposals.
+        predictions for the proposals, or a tensor of shape [batch, height,
+        width, num_predictions_per_location * num_class_slots] representing
+        class predictions before reshaping if self._return_flat_predictions is
+        False.
     """
     class_predictions_net = features
     if self._use_dropout:
@@ -272,13 +292,15 @@ class WeightSharedConvolutionalClassHead(head.Head):
         normalizer_fn=None,
         biases_initializer=tf.constant_initializer(
             self._class_prediction_bias_init),
-        scope='ClassPredictor')
+        scope=self._scope)
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
       batch_size = tf.shape(features)[0]
     class_predictions_with_background = self._score_converter_fn(
         class_predictions_with_background)
-    class_predictions_with_background = tf.reshape(
-        class_predictions_with_background,
-        [batch_size, -1, self._num_class_slots])
+    if self._return_flat_predictions:
+      class_predictions_with_background = tf.reshape(
+          class_predictions_with_background,
+          [batch_size, -1, self._num_class_slots])
     return class_predictions_with_background
+
diff --git a/research/object_detection/predictors/heads/class_head_test.py b/research/object_detection/predictors/heads/class_head_test.py
index 270dc5d1..4680524e 100644
--- a/research/object_detection/predictors/heads/class_head_test.py
+++ b/research/object_detection/predictors/heads/class_head_test.py
@@ -56,6 +56,30 @@ class MaskRCNNClassHeadTest(test_case.TestCase):
         features=roi_pooled_features, num_predictions_per_location=1)
     self.assertAllEqual([64, 1, 20], prediction.get_shape().as_list())
 
+  def test_scope_name(self):
+    expected_var_names = set([
+        """ClassPredictor/weights""",
+        """ClassPredictor/biases"""
+    ])
+
+    g = tf.Graph()
+    with g.as_default():
+      class_prediction_head = class_head.MaskRCNNClassHead(
+          is_training=True,
+          num_class_slots=20,
+          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+          use_dropout=True,
+          dropout_keep_prob=0.5)
+      image_feature = tf.random_uniform(
+          [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+      class_prediction_head.predict(
+          features=image_feature,
+          num_predictions_per_location=1)
+      actual_variable_set = set([
+          var.op.name for var in g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+      ])
+      self.assertSetEqual(expected_var_names, actual_variable_set)
+
 
 class ConvolutionalClassPredictorTest(test_case.TestCase):
 
@@ -92,6 +116,29 @@ class ConvolutionalClassPredictorTest(test_case.TestCase):
     self.assertAllEqual([64, 323, 20],
                         class_predictions.get_shape().as_list())
 
+  def test_scope_name(self):
+    expected_var_names = set([
+        """ClassPredictor/weights""",
+        """ClassPredictor/biases"""
+    ])
+    g = tf.Graph()
+    with g.as_default():
+      class_prediction_head = class_head.ConvolutionalClassHead(
+          is_training=True,
+          num_class_slots=20,
+          use_dropout=True,
+          dropout_keep_prob=0.5,
+          kernel_size=3)
+      image_feature = tf.random_uniform(
+          [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+      class_prediction_head.predict(
+          features=image_feature,
+          num_predictions_per_location=1)
+      actual_variable_set = set([
+          var.op.name for var in g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+      ])
+      self.assertSetEqual(expected_var_names, actual_variable_set)
+
 
 class WeightSharedConvolutionalClassPredictorTest(test_case.TestCase):
 
@@ -123,6 +170,25 @@ class WeightSharedConvolutionalClassPredictorTest(test_case.TestCase):
         num_predictions_per_location=1)
     self.assertAllEqual([64, 323, 20], class_predictions.get_shape().as_list())
 
+  def test_scope_name(self):
+    expected_var_names = set([
+        """ClassPredictor/weights""",
+        """ClassPredictor/biases"""
+    ])
+    g = tf.Graph()
+    with g.as_default():
+      class_prediction_head = class_head.WeightSharedConvolutionalClassHead(
+          num_class_slots=20)
+      image_feature = tf.random_uniform(
+          [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+      class_prediction_head.predict(
+          features=image_feature,
+          num_predictions_per_location=1)
+      actual_variable_set = set([
+          var.op.name for var in g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+      ])
+      self.assertSetEqual(expected_var_names, actual_variable_set)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/heads/keras_box_head.py b/research/object_detection/predictors/heads/keras_box_head.py
index da311478..eb88a89b 100644
--- a/research/object_detection/predictors/heads/keras_box_head.py
+++ b/research/object_detection/predictors/heads/keras_box_head.py
@@ -34,7 +34,8 @@ class ConvolutionalBoxHead(head.KerasHead):
                num_predictions_per_location,
                conv_hyperparams,
                freeze_batchnorm,
-               use_depthwise=True,
+               use_depthwise=False,
+               box_encodings_clip_range=None,
                name=None):
     """Constructor.
 
@@ -55,6 +56,7 @@ class ConvolutionalBoxHead(head.KerasHead):
         params.
       use_depthwise: Whether to use depthwise convolutions for prediction
         steps. Default is False.
+      box_encodings_clip_range: Min and max values for clipping box_encodings.
       name: A string name scope to assign to the model. If `None`, Keras
         will auto-generate one from the class name.
 
@@ -67,6 +69,7 @@ class ConvolutionalBoxHead(head.KerasHead):
     self._kernel_size = kernel_size
     self._num_predictions_per_location = num_predictions_per_location
     self._use_depthwise = use_depthwise
+    self._box_encodings_clip_range = box_encodings_clip_range
 
     self._box_encoder_layers = []
 
@@ -119,6 +122,202 @@ class ConvolutionalBoxHead(head.KerasHead):
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
       batch_size = tf.shape(features)[0]
+    # Clipping the box encodings to make the inference graph TPU friendly.
+    if self._box_encodings_clip_range is not None:
+      box_encodings = tf.clip_by_value(
+          box_encodings, self._box_encodings_clip_range.min,
+          self._box_encodings_clip_range.max)
     box_encodings = tf.reshape(box_encodings,
                                [batch_size, -1, 1, self._box_code_size])
     return box_encodings
+
+
+class MaskRCNNBoxHead(head.KerasHead):
+  """Box prediction head.
+
+  This is a piece of Mask RCNN which is responsible for predicting
+  just the box encodings.
+
+  Please refer to Mask RCNN paper:
+  https://arxiv.org/abs/1703.06870
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               fc_hyperparams,
+               freeze_batchnorm,
+               use_dropout,
+               dropout_keep_prob,
+               box_code_size,
+               share_box_across_classes=False,
+               name=None):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      fc_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for fully connected dense ops.
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      use_dropout: Option to use dropout or not.  Note that a single dropout
+        op is applied here prior to both box and class predictions, which stands
+        in contrast to the ConvolutionalBoxPredictor below.
+      dropout_keep_prob: Keep probability for dropout.
+        This is only used if use_dropout is True.
+      box_code_size: Size of encoding for each box.
+      share_box_across_classes: Whether to share boxes across classes rather
+        than use a different box for each class.
+      name: A string name scope to assign to the box head. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(MaskRCNNBoxHead, self).__init__(name=name)
+    self._is_training = is_training
+    self._num_classes = num_classes
+    self._fc_hyperparams = fc_hyperparams
+    self._freeze_batchnorm = freeze_batchnorm
+    self._use_dropout = use_dropout
+    self._dropout_keep_prob = dropout_keep_prob
+    self._box_code_size = box_code_size
+    self._share_box_across_classes = share_box_across_classes
+
+    self._box_encoder_layers = [tf.keras.layers.Flatten()]
+
+    if self._use_dropout:
+      self._box_encoder_layers.append(
+          tf.keras.layers.Dropout(rate=1.0 - self._dropout_keep_prob))
+
+    self._number_of_boxes = 1
+    if not self._share_box_across_classes:
+      self._number_of_boxes = self._num_classes
+
+    self._box_encoder_layers.append(
+        tf.keras.layers.Dense(self._number_of_boxes * self._box_code_size,
+                              name='BoxEncodingPredictor_dense'))
+    self._box_encoder_layers.append(
+        fc_hyperparams.build_batch_norm(training=(is_training and
+                                                  not freeze_batchnorm),
+                                        name='BoxEncodingPredictor_batchnorm'))
+
+  def _predict(self, features):
+    """Predicts box encodings.
+
+    Args:
+      features: A float tensor of shape [batch_size, height, width,
+        channels] containing features for a batch of images.
+
+    Returns:
+      box_encodings: A float tensor of shape
+        [batch_size, 1, num_classes, code_size] representing the location of the
+        objects.
+    """
+    spatial_averaged_roi_pooled_features = tf.reduce_mean(
+        features, [1, 2], keep_dims=True, name='AvgPool')
+    net = spatial_averaged_roi_pooled_features
+    for layer in self._box_encoder_layers:
+      net = layer(net)
+    box_encodings = tf.reshape(net,
+                               [-1, 1,
+                                self._number_of_boxes,
+                                self._box_code_size])
+    return box_encodings
+
+
+# TODO(b/128922690): Unify the implementations of ConvolutionalBoxHead
+# and WeightSharedConvolutionalBoxHead
+class WeightSharedConvolutionalBoxHead(head.KerasHead):
+  """Weight shared convolutional box prediction head based on Keras.
+
+  This head allows sharing the same set of parameters (weights) when called more
+  then once on different feature maps.
+  """
+
+  def __init__(self,
+               box_code_size,
+               num_predictions_per_location,
+               conv_hyperparams,
+               kernel_size=3,
+               use_depthwise=False,
+               box_encodings_clip_range=None,
+               return_flat_predictions=True,
+               name=None):
+    """Constructor.
+
+    Args:
+      box_code_size: Size of encoding for each box.
+      num_predictions_per_location: Number of box predictions to be made per
+        spatial location. Int specifying number of boxes per location.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops.
+      kernel_size: Size of final convolution kernel.
+      use_depthwise: Whether to use depthwise convolutions for prediction steps.
+        Default is False.
+      box_encodings_clip_range: Min and max values for clipping box_encodings.
+      return_flat_predictions: If true, returns flattened prediction tensor
+        of shape [batch, height * width * num_predictions_per_location,
+        box_coder]. Otherwise returns the prediction tensor before reshaping,
+        whose shape is [batch, height, width, num_predictions_per_location *
+        num_class_slots].
+      name: A string name scope to assign to the model. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(WeightSharedConvolutionalBoxHead, self).__init__(name=name)
+    self._box_code_size = box_code_size
+    self._kernel_size = kernel_size
+    self._num_predictions_per_location = num_predictions_per_location
+    self._use_depthwise = use_depthwise
+    self._box_encodings_clip_range = box_encodings_clip_range
+    self._return_flat_predictions = return_flat_predictions
+
+    self._box_encoder_layers = []
+
+    if self._use_depthwise:
+      self._box_encoder_layers.append(
+          tf.keras.layers.SeparableConv2D(
+              num_predictions_per_location * self._box_code_size,
+              [self._kernel_size, self._kernel_size],
+              padding='SAME',
+              name='BoxPredictor',
+              **conv_hyperparams.params(use_bias=True)))
+    else:
+      self._box_encoder_layers.append(
+          tf.keras.layers.Conv2D(
+              num_predictions_per_location * self._box_code_size,
+              [self._kernel_size, self._kernel_size],
+              padding='SAME',
+              name='BoxPredictor',
+              **conv_hyperparams.params(use_bias=True)))
+
+  def _predict(self, features):
+    """Predicts boxes.
+
+    Args:
+      features: A float tensor of shape [batch_size, height, width, channels]
+        containing image features.
+
+    Returns:
+      box_encodings: A float tensor of shape
+        [batch_size, num_anchors, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes.
+    """
+    box_encodings = features
+    for layer in self._box_encoder_layers:
+      box_encodings = layer(box_encodings)
+    batch_size = features.get_shape().as_list()[0]
+    if batch_size is None:
+      batch_size = tf.shape(features)[0]
+    # Clipping the box encodings to make the inference graph TPU friendly.
+    if self._box_encodings_clip_range is not None:
+      box_encodings = tf.clip_by_value(
+          box_encodings, self._box_encodings_clip_range.min,
+          self._box_encodings_clip_range.max)
+    if self._return_flat_predictions:
+      box_encodings = tf.reshape(box_encodings,
+                                 [batch_size, -1, self._box_code_size])
+    return box_encodings
diff --git a/research/object_detection/predictors/heads/keras_box_head_test.py b/research/object_detection/predictors/heads/keras_box_head_test.py
index c03d7313..929b5f97 100644
--- a/research/object_detection/predictors/heads/keras_box_head_test.py
+++ b/research/object_detection/predictors/heads/keras_box_head_test.py
@@ -71,5 +71,114 @@ class ConvolutionalKerasBoxHeadTest(test_case.TestCase):
     box_encodings = box_prediction_head(image_feature)
     self.assertAllEqual([64, 323, 1, 4], box_encodings.get_shape().as_list())
 
+
+class MaskRCNNKerasBoxHeadTest(test_case.TestCase):
+
+  def _build_fc_hyperparams(
+      self, op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.KerasLayerHyperparams(hyperparams)
+
+  def test_prediction_size(self):
+    box_prediction_head = keras_box_head.MaskRCNNBoxHead(
+        is_training=False,
+        num_classes=20,
+        fc_hyperparams=self._build_fc_hyperparams(),
+        freeze_batchnorm=False,
+        use_dropout=True,
+        dropout_keep_prob=0.5,
+        box_code_size=4,
+        share_box_across_classes=False)
+    roi_pooled_features = tf.random_uniform(
+        [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    prediction = box_prediction_head(roi_pooled_features)
+    self.assertAllEqual([64, 1, 20, 4], prediction.get_shape().as_list())
+
+
+class WeightSharedConvolutionalKerasBoxHead(test_case.TestCase):
+
+  def _build_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+    activation: NONE
+    regularizer {
+      l2_regularizer {
+      }
+    }
+    initializer {
+      truncated_normal_initializer {
+      }
+    }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  def test_prediction_size_depthwise_false(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    box_prediction_head = keras_box_head.WeightSharedConvolutionalBoxHead(
+        box_code_size=4,
+        conv_hyperparams=conv_hyperparams,
+        num_predictions_per_location=1,
+        use_depthwise=False)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    box_encodings = box_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 4], box_encodings.get_shape().as_list())
+
+  def test_prediction_size_depthwise_true(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    box_prediction_head = keras_box_head.WeightSharedConvolutionalBoxHead(
+        box_code_size=4,
+        conv_hyperparams=conv_hyperparams,
+        num_predictions_per_location=1,
+        use_depthwise=True)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    box_encodings = box_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 4], box_encodings.get_shape().as_list())
+
+  def test_variable_count_depth_wise_true(self):
+    g = tf.Graph()
+    with g.as_default():
+      conv_hyperparams = self._build_conv_hyperparams()
+      box_prediction_head = keras_box_head.WeightSharedConvolutionalBoxHead(
+          box_code_size=4,
+          conv_hyperparams=conv_hyperparams,
+          num_predictions_per_location=1,
+          use_depthwise=True)
+      image_feature = tf.random_uniform(
+          [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+      _ = box_prediction_head(image_feature)
+      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+    self.assertEqual(len(variables), 3)
+
+  def test_variable_count_depth_wise_False(self):
+    g = tf.Graph()
+    with g.as_default():
+      conv_hyperparams = self._build_conv_hyperparams()
+      box_prediction_head = keras_box_head.WeightSharedConvolutionalBoxHead(
+          box_code_size=4,
+          conv_hyperparams=conv_hyperparams,
+          num_predictions_per_location=1,
+          use_depthwise=False)
+      image_feature = tf.random_uniform(
+          [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+      _ = box_prediction_head(image_feature)
+      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+    self.assertEqual(len(variables), 2)
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/heads/keras_class_head.py b/research/object_detection/predictors/heads/keras_class_head.py
index f157254e..061c706d 100644
--- a/research/object_detection/predictors/heads/keras_class_head.py
+++ b/research/object_detection/predictors/heads/keras_class_head.py
@@ -134,7 +134,6 @@ class ConvolutionalClassHead(head.KerasHead):
         [batch_size, num_anchors, num_class_slots] representing the class
         predictions for the proposals.
     """
-    # Add a slot for the background class.
     class_predictions_with_background = features
     for layer in self._class_predictor_layers:
       class_predictions_with_background = layer(
@@ -146,3 +145,197 @@ class ConvolutionalClassHead(head.KerasHead):
         class_predictions_with_background,
         [batch_size, -1, self._num_class_slots])
     return class_predictions_with_background
+
+
+class MaskRCNNClassHead(head.KerasHead):
+  """Mask RCNN class prediction head.
+
+  This is a piece of Mask RCNN which is responsible for predicting
+  just the class scores of boxes.
+
+  Please refer to Mask RCNN paper:
+  https://arxiv.org/abs/1703.06870
+  """
+
+  def __init__(self,
+               is_training,
+               num_class_slots,
+               fc_hyperparams,
+               freeze_batchnorm,
+               use_dropout,
+               dropout_keep_prob,
+               name=None):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_class_slots: number of class slots. Note that num_class_slots may or
+        may not include an implicit background category.
+      fc_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for fully connected dense ops.
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      use_dropout: Option to use dropout or not.  Note that a single dropout
+        op is applied here prior to both box and class predictions, which stands
+        in contrast to the ConvolutionalBoxPredictor below.
+      dropout_keep_prob: Keep probability for dropout.
+        This is only used if use_dropout is True.
+      name: A string name scope to assign to the class head. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(MaskRCNNClassHead, self).__init__(name=name)
+    self._is_training = is_training
+    self._freeze_batchnorm = freeze_batchnorm
+    self._num_class_slots = num_class_slots
+    self._fc_hyperparams = fc_hyperparams
+    self._use_dropout = use_dropout
+    self._dropout_keep_prob = dropout_keep_prob
+
+    self._class_predictor_layers = [tf.keras.layers.Flatten()]
+
+    if self._use_dropout:
+      self._class_predictor_layers.append(
+          tf.keras.layers.Dropout(rate=1.0 - self._dropout_keep_prob))
+
+    self._class_predictor_layers.append(
+        tf.keras.layers.Dense(self._num_class_slots,
+                              name='ClassPredictor_dense'))
+    self._class_predictor_layers.append(
+        fc_hyperparams.build_batch_norm(training=(is_training and
+                                                  not freeze_batchnorm),
+                                        name='ClassPredictor_batchnorm'))
+
+  def _predict(self, features):
+    """Predicts the class scores for boxes.
+
+    Args:
+      features: A float tensor of shape [batch_size, height, width, channels]
+        containing features for a batch of images.
+
+    Returns:
+      class_predictions_with_background: A float tensor of shape
+        [batch_size, 1, num_class_slots] representing the class predictions for
+        the proposals.
+    """
+    spatial_averaged_roi_pooled_features = tf.reduce_mean(
+        features, [1, 2], keep_dims=True, name='AvgPool')
+    net = spatial_averaged_roi_pooled_features
+    for layer in self._class_predictor_layers:
+      net = layer(net)
+    class_predictions_with_background = tf.reshape(
+        net,
+        [-1, 1, self._num_class_slots])
+    return class_predictions_with_background
+
+
+class WeightSharedConvolutionalClassHead(head.KerasHead):
+  """Weight shared convolutional class prediction head.
+
+  This head allows sharing the same set of parameters (weights) when called more
+  then once on different feature maps.
+  """
+
+  def __init__(self,
+               num_class_slots,
+               num_predictions_per_location,
+               conv_hyperparams,
+               kernel_size=3,
+               class_prediction_bias_init=0.0,
+               use_dropout=False,
+               dropout_keep_prob=0.8,
+               use_depthwise=False,
+               score_converter_fn=tf.identity,
+               return_flat_predictions=True,
+               name=None):
+    """Constructor.
+
+    Args:
+      num_class_slots: number of class slots. Note that num_class_slots may or
+        may not include an implicit background category.
+      num_predictions_per_location: Number of box predictions to be made per
+        spatial location. Int specifying number of boxes per location.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops.
+      kernel_size: Size of final convolution kernel.
+      class_prediction_bias_init: constant value to initialize bias of the last
+        conv2d layer before class prediction.
+      use_dropout: Whether to apply dropout to class prediction head.
+      dropout_keep_prob: Probability of keeping activiations.
+      use_depthwise: Whether to use depthwise convolutions for prediction
+        steps. Default is False.
+      score_converter_fn: Callable elementwise nonlinearity (that takes tensors
+        as inputs and returns tensors).
+      return_flat_predictions: If true, returns flattened prediction tensor
+        of shape [batch, height * width * num_predictions_per_location,
+        box_coder]. Otherwise returns the prediction tensor before reshaping,
+        whose shape is [batch, height, width, num_predictions_per_location *
+        num_class_slots].
+      name: A string name scope to assign to the model. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(WeightSharedConvolutionalClassHead, self).__init__(name=name)
+    self._num_class_slots = num_class_slots
+    self._kernel_size = kernel_size
+    self._class_prediction_bias_init = class_prediction_bias_init
+    self._use_dropout = use_dropout
+    self._dropout_keep_prob = dropout_keep_prob
+    self._use_depthwise = use_depthwise
+    self._score_converter_fn = score_converter_fn
+    self._return_flat_predictions = return_flat_predictions
+
+    self._class_predictor_layers = []
+
+    if self._use_dropout:
+      self._class_predictor_layers.append(
+          tf.keras.layers.Dropout(rate=1.0 - self._dropout_keep_prob))
+    if self._use_depthwise:
+      self._class_predictor_layers.append(
+          tf.keras.layers.SeparableConv2D(
+              num_predictions_per_location * self._num_class_slots,
+              [self._kernel_size, self._kernel_size],
+              padding='SAME',
+              depth_multiplier=1,
+              strides=1,
+              name='ClassPredictor',
+              bias_initializer=tf.constant_initializer(
+                  self._class_prediction_bias_init),
+              **conv_hyperparams.params(use_bias=True)))
+    else:
+      self._class_predictor_layers.append(
+          tf.keras.layers.Conv2D(
+              num_predictions_per_location * self._num_class_slots,
+              [self._kernel_size, self._kernel_size],
+              padding='SAME',
+              name='ClassPredictor',
+              bias_initializer=tf.constant_initializer(
+                  self._class_prediction_bias_init),
+              **conv_hyperparams.params(use_bias=True)))
+
+  def _predict(self, features):
+    """Predicts boxes.
+
+    Args:
+      features: A float tensor of shape [batch_size, height, width, channels]
+        containing image features.
+
+    Returns:
+      class_predictions_with_background: A float tensor of shape
+        [batch_size, num_anchors, num_class_slots] representing the class
+        predictions for the proposals.
+    """
+    class_predictions_with_background = features
+    for layer in self._class_predictor_layers:
+      class_predictions_with_background = layer(
+          class_predictions_with_background)
+    batch_size = features.get_shape().as_list()[0]
+    if batch_size is None:
+      batch_size = tf.shape(features)[0]
+    class_predictions_with_background = self._score_converter_fn(
+        class_predictions_with_background)
+    if self._return_flat_predictions:
+      class_predictions_with_background = tf.reshape(
+          class_predictions_with_background,
+          [batch_size, -1, self._num_class_slots])
+    return class_predictions_with_background
diff --git a/research/object_detection/predictors/heads/keras_class_head_test.py b/research/object_detection/predictors/heads/keras_class_head_test.py
index 8a49b7ab..1c339ec8 100644
--- a/research/object_detection/predictors/heads/keras_class_head_test.py
+++ b/research/object_detection/predictors/heads/keras_class_head_test.py
@@ -77,5 +77,115 @@ class ConvolutionalKerasClassPredictorTest(test_case.TestCase):
     self.assertAllEqual([64, 323, 20],
                         class_predictions.get_shape().as_list())
 
+
+class MaskRCNNClassHeadTest(test_case.TestCase):
+
+  def _build_fc_hyperparams(self,
+                            op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.KerasLayerHyperparams(hyperparams)
+
+  def test_prediction_size(self):
+    class_prediction_head = keras_class_head.MaskRCNNClassHead(
+        is_training=False,
+        num_class_slots=20,
+        fc_hyperparams=self._build_fc_hyperparams(),
+        freeze_batchnorm=False,
+        use_dropout=True,
+        dropout_keep_prob=0.5)
+    roi_pooled_features = tf.random_uniform(
+        [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    prediction = class_prediction_head(roi_pooled_features)
+    self.assertAllEqual([64, 1, 20], prediction.get_shape().as_list())
+
+
+class WeightSharedConvolutionalKerasClassPredictorTest(test_case.TestCase):
+
+  def _build_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+    activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  def test_prediction_size_depthwise_false(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    class_prediction_head = keras_class_head.WeightSharedConvolutionalClassHead(
+        num_class_slots=20,
+        conv_hyperparams=conv_hyperparams,
+        num_predictions_per_location=1,
+        use_depthwise=False)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    class_predictions = class_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 20], class_predictions.get_shape().as_list())
+
+  def test_prediction_size_depthwise_true(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    class_prediction_head = keras_class_head.WeightSharedConvolutionalClassHead(
+        num_class_slots=20,
+        conv_hyperparams=conv_hyperparams,
+        num_predictions_per_location=1,
+        use_depthwise=True)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    class_predictions = class_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 20], class_predictions.get_shape().as_list())
+
+  def test_variable_count_depth_wise_true(self):
+    g = tf.Graph()
+    with g.as_default():
+      conv_hyperparams = self._build_conv_hyperparams()
+      class_prediction_head = (
+          keras_class_head.WeightSharedConvolutionalClassHead(
+              num_class_slots=20,
+              conv_hyperparams=conv_hyperparams,
+              num_predictions_per_location=1,
+              use_depthwise=True))
+      image_feature = tf.random_uniform(
+          [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+      _ = class_prediction_head(image_feature)
+      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+    self.assertEqual(len(variables), 3)
+
+  def test_variable_count_depth_wise_False(self):
+    g = tf.Graph()
+    with g.as_default():
+      conv_hyperparams = self._build_conv_hyperparams()
+      class_prediction_head = (
+          keras_class_head.WeightSharedConvolutionalClassHead(
+              num_class_slots=20,
+              conv_hyperparams=conv_hyperparams,
+              num_predictions_per_location=1,
+              use_depthwise=False))
+      image_feature = tf.random_uniform(
+          [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+      _ = class_prediction_head(image_feature)
+      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+    self.assertEqual(len(variables), 2)
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/heads/keras_mask_head.py b/research/object_detection/predictors/heads/keras_mask_head.py
index fa4b1d1e..5e6b6c2e 100644
--- a/research/object_detection/predictors/heads/keras_mask_head.py
+++ b/research/object_detection/predictors/heads/keras_mask_head.py
@@ -19,9 +19,11 @@ Contains Mask prediction head classes for different meta architectures.
 All the mask prediction heads have a predict function that receives the
 `features` as the first argument and returns `mask_predictions`.
 """
+import math
 import tensorflow as tf
 
 from object_detection.predictors.heads import head
+from object_detection.utils import ops
 
 
 class ConvolutionalMaskHead(head.KerasHead):
@@ -156,3 +158,281 @@ class ConvolutionalMaskHead(head.KerasHead):
         mask_predictions,
         [batch_size, -1, self._num_masks, self._mask_height, self._mask_width])
     return mask_predictions
+
+
+class MaskRCNNMaskHead(head.KerasHead):
+  """Mask RCNN mask prediction head.
+
+  This is a piece of Mask RCNN which is responsible for predicting
+  just the pixelwise foreground scores for regions within the boxes.
+
+  Please refer to Mask RCNN paper:
+  https://arxiv.org/abs/1703.06870
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               freeze_batchnorm,
+               conv_hyperparams,
+               mask_height=14,
+               mask_width=14,
+               mask_prediction_num_conv_layers=2,
+               mask_prediction_conv_depth=256,
+               masks_are_class_agnostic=False,
+               convolve_then_upsample=False,
+               name=None):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the Mask head is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops.
+      mask_height: Desired output mask height. The default value is 14.
+      mask_width: Desired output mask width. The default value is 14.
+      mask_prediction_num_conv_layers: Number of convolution layers applied to
+        the image_features in mask prediction branch.
+      mask_prediction_conv_depth: The depth for the first conv2d_transpose op
+        applied to the image_features in the mask prediction branch. If set
+        to 0, the depth of the convolution layers will be automatically chosen
+        based on the number of object classes and the number of channels in the
+        image features.
+      masks_are_class_agnostic: Boolean determining if the mask-head is
+        class-agnostic or not.
+      convolve_then_upsample: Whether to apply convolutions on mask features
+        before upsampling using nearest neighbor resizing. Otherwise, mask
+        features are resized to [`mask_height`, `mask_width`] using bilinear
+        resizing before applying convolutions.
+      name: A string name scope to assign to the mask head. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(MaskRCNNMaskHead, self).__init__(name=name)
+    self._is_training = is_training
+    self._freeze_batchnorm = freeze_batchnorm
+    self._num_classes = num_classes
+    self._conv_hyperparams = conv_hyperparams
+    self._mask_height = mask_height
+    self._mask_width = mask_width
+    self._mask_prediction_num_conv_layers = mask_prediction_num_conv_layers
+    self._mask_prediction_conv_depth = mask_prediction_conv_depth
+    self._masks_are_class_agnostic = masks_are_class_agnostic
+    self._convolve_then_upsample = convolve_then_upsample
+
+    self._mask_predictor_layers = []
+
+  def build(self, input_shapes):
+    num_conv_channels = self._mask_prediction_conv_depth
+    if num_conv_channels == 0:
+      num_feature_channels = input_shapes.as_list()[3]
+      num_conv_channels = self._get_mask_predictor_conv_depth(
+          num_feature_channels, self._num_classes)
+
+    for i in range(self._mask_prediction_num_conv_layers - 1):
+      self._mask_predictor_layers.append(
+          tf.keras.layers.Conv2D(
+              num_conv_channels,
+              [3, 3],
+              padding='SAME',
+              name='MaskPredictor_conv2d_{}'.format(i),
+              **self._conv_hyperparams.params()))
+      self._mask_predictor_layers.append(
+          self._conv_hyperparams.build_batch_norm(
+              training=(self._is_training and not self._freeze_batchnorm),
+              name='MaskPredictor_batchnorm_{}'.format(i)))
+      self._mask_predictor_layers.append(
+          self._conv_hyperparams.build_activation_layer(
+              name='MaskPredictor_activation_{}'.format(i)))
+
+    if self._convolve_then_upsample:
+      # Replace Transposed Convolution with a Nearest Neighbor upsampling step
+      # followed by 3x3 convolution.
+      height_scale = self._mask_height / input_shapes[1].value
+      width_scale = self._mask_width / input_shapes[2].value
+      # pylint: disable=g-long-lambda
+      self._mask_predictor_layers.append(tf.keras.layers.Lambda(
+          lambda features: ops.nearest_neighbor_upsampling(
+              features, height_scale=height_scale, width_scale=width_scale)
+      ))
+      # pylint: enable=g-long-lambda
+      self._mask_predictor_layers.append(
+          tf.keras.layers.Conv2D(
+              num_conv_channels,
+              [3, 3],
+              padding='SAME',
+              name='MaskPredictor_upsample_conv2d',
+              **self._conv_hyperparams.params()))
+      self._mask_predictor_layers.append(
+          self._conv_hyperparams.build_batch_norm(
+              training=(self._is_training and not self._freeze_batchnorm),
+              name='MaskPredictor_upsample_batchnorm'))
+      self._mask_predictor_layers.append(
+          self._conv_hyperparams.build_activation_layer(
+              name='MaskPredictor_upsample_activation'))
+
+    num_masks = 1 if self._masks_are_class_agnostic else self._num_classes
+    self._mask_predictor_layers.append(
+        tf.keras.layers.Conv2D(
+            num_masks,
+            [3, 3],
+            padding='SAME',
+            name='MaskPredictor_last_conv2d',
+            **self._conv_hyperparams.params(use_bias=True)))
+
+    self.built = True
+
+  def _get_mask_predictor_conv_depth(self,
+                                     num_feature_channels,
+                                     num_classes,
+                                     class_weight=3.0,
+                                     feature_weight=2.0):
+    """Computes the depth of the mask predictor convolutions.
+
+    Computes the depth of the mask predictor convolutions given feature channels
+    and number of classes by performing a weighted average of the two in
+    log space to compute the number of convolution channels. The weights that
+    are used for computing the weighted average do not need to sum to 1.
+
+    Args:
+      num_feature_channels: An integer containing the number of feature
+        channels.
+      num_classes: An integer containing the number of classes.
+      class_weight: Class weight used in computing the weighted average.
+      feature_weight: Feature weight used in computing the weighted average.
+
+    Returns:
+      An integer containing the number of convolution channels used by mask
+        predictor.
+    """
+    num_feature_channels_log = math.log(float(num_feature_channels), 2.0)
+    num_classes_log = math.log(float(num_classes), 2.0)
+    weighted_num_feature_channels_log = (
+        num_feature_channels_log * feature_weight)
+    weighted_num_classes_log = num_classes_log * class_weight
+    total_weight = feature_weight + class_weight
+    num_conv_channels_log = round(
+        (weighted_num_feature_channels_log + weighted_num_classes_log) /
+        total_weight)
+    return int(math.pow(2.0, num_conv_channels_log))
+
+  def _predict(self, features):
+    """Predicts pixelwise foreground scores for regions within the boxes.
+
+    Args:
+      features: A float tensor of shape [batch_size, height, width, channels]
+        containing features for a batch of images.
+
+    Returns:
+      instance_masks: A float tensor of shape
+          [batch_size, 1, num_classes, mask_height, mask_width].
+    """
+    if not self._convolve_then_upsample:
+      features = tf.image.resize_bilinear(
+          features, [self._mask_height, self._mask_width],
+          align_corners=True)
+
+    mask_predictions = features
+    for layer in self._mask_predictor_layers:
+      mask_predictions = layer(mask_predictions)
+    return tf.expand_dims(
+        tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),
+        axis=1,
+        name='MaskPredictor')
+
+
+class WeightSharedConvolutionalMaskHead(head.KerasHead):
+  """Weight shared convolutional mask prediction head based on Keras."""
+
+  def __init__(self,
+               num_classes,
+               num_predictions_per_location,
+               conv_hyperparams,
+               kernel_size=3,
+               use_dropout=False,
+               dropout_keep_prob=0.8,
+               mask_height=7,
+               mask_width=7,
+               masks_are_class_agnostic=False,
+               name=None):
+    """Constructor.
+
+    Args:
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      num_predictions_per_location: Number of box predictions to be made per
+        spatial location. Int specifying number of boxes per location.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops.
+      kernel_size: Size of final convolution kernel.
+      use_dropout: Whether to apply dropout to class prediction head.
+      dropout_keep_prob: Probability of keeping activiations.
+      mask_height: Desired output mask height. The default value is 7.
+      mask_width: Desired output mask width. The default value is 7.
+      masks_are_class_agnostic: Boolean determining if the mask-head is
+        class-agnostic or not.
+      name: A string name scope to assign to the model. If `None`, Keras
+        will auto-generate one from the class name.
+
+    Raises:
+      ValueError: if min_depth > max_depth.
+    """
+    super(WeightSharedConvolutionalMaskHead, self).__init__(name=name)
+    self._num_classes = num_classes
+    self._num_predictions_per_location = num_predictions_per_location
+    self._kernel_size = kernel_size
+    self._use_dropout = use_dropout
+    self._dropout_keep_prob = dropout_keep_prob
+    self._mask_height = mask_height
+    self._mask_width = mask_width
+    self._masks_are_class_agnostic = masks_are_class_agnostic
+
+    self._mask_predictor_layers = []
+
+    if self._masks_are_class_agnostic:
+      self._num_masks = 1
+    else:
+      self._num_masks = self._num_classes
+    num_mask_channels = self._num_masks * self._mask_height * self._mask_width
+
+    if self._use_dropout:
+      self._mask_predictor_layers.append(
+          tf.keras.layers.Dropout(rate=1.0 - self._dropout_keep_prob))
+    self._mask_predictor_layers.append(
+        tf.keras.layers.Conv2D(
+            num_predictions_per_location * num_mask_channels,
+            [self._kernel_size, self._kernel_size],
+            padding='SAME',
+            name='MaskPredictor',
+            **conv_hyperparams.params(use_bias=True)))
+
+  def _predict(self, features):
+    """Predicts boxes.
+
+    Args:
+      features: A float tensor of shape [batch_size, height, width, channels]
+        containing image features.
+
+    Returns:
+      mask_predictions: A tensor of shape
+        [batch_size, num_anchors, num_classes, mask_height, mask_width]
+        representing the mask predictions for the proposals.
+    """
+    mask_predictions = features
+    for layer in self._mask_predictor_layers:
+      mask_predictions = layer(mask_predictions)
+    batch_size = features.get_shape().as_list()[0]
+    if batch_size is None:
+      batch_size = tf.shape(features)[0]
+    mask_predictions = tf.reshape(
+        mask_predictions,
+        [batch_size, -1, self._num_masks, self._mask_height, self._mask_width])
+    return mask_predictions
diff --git a/research/object_detection/predictors/heads/keras_mask_head_test.py b/research/object_detection/predictors/heads/keras_mask_head_test.py
index 2177c334..46baeb17 100644
--- a/research/object_detection/predictors/heads/keras_mask_head_test.py
+++ b/research/object_detection/predictors/heads/keras_mask_head_test.py
@@ -123,5 +123,107 @@ class ConvolutionalMaskPredictorTest(test_case.TestCase):
     self.assertAllEqual([64, 323, 1, 7, 7],
                         mask_predictions.get_shape().as_list())
 
+
+class MaskRCNNMaskHeadTest(test_case.TestCase):
+
+  def _build_conv_hyperparams(self,
+                              op_type=hyperparams_pb2.Hyperparams.CONV):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.KerasLayerHyperparams(hyperparams)
+
+  def test_prediction_size(self):
+    mask_prediction_head = keras_mask_head.MaskRCNNMaskHead(
+        is_training=True,
+        num_classes=20,
+        conv_hyperparams=self._build_conv_hyperparams(),
+        freeze_batchnorm=False,
+        mask_height=14,
+        mask_width=14,
+        mask_prediction_num_conv_layers=2,
+        mask_prediction_conv_depth=256,
+        masks_are_class_agnostic=False)
+    roi_pooled_features = tf.random_uniform(
+        [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    prediction = mask_prediction_head(roi_pooled_features)
+    self.assertAllEqual([64, 1, 20, 14, 14], prediction.get_shape().as_list())
+
+  def test_prediction_size_with_convolve_then_upsample(self):
+    mask_prediction_head = keras_mask_head.MaskRCNNMaskHead(
+        is_training=True,
+        num_classes=20,
+        conv_hyperparams=self._build_conv_hyperparams(),
+        freeze_batchnorm=False,
+        mask_height=28,
+        mask_width=28,
+        mask_prediction_num_conv_layers=2,
+        mask_prediction_conv_depth=256,
+        masks_are_class_agnostic=True,
+        convolve_then_upsample=True)
+    roi_pooled_features = tf.random_uniform(
+        [64, 14, 14, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    prediction = mask_prediction_head(roi_pooled_features)
+    self.assertAllEqual([64, 1, 1, 28, 28], prediction.get_shape().as_list())
+
+
+class WeightSharedConvolutionalMaskPredictorTest(test_case.TestCase):
+
+  def _build_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+    activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  def test_prediction_size(self):
+    mask_prediction_head = (
+        keras_mask_head.WeightSharedConvolutionalMaskHead(
+            num_classes=20,
+            num_predictions_per_location=1,
+            conv_hyperparams=self._build_conv_hyperparams(),
+            mask_height=7,
+            mask_width=7))
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    mask_predictions = mask_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 20, 7, 7],
+                        mask_predictions.get_shape().as_list())
+
+  def test_class_agnostic_prediction_size(self):
+    mask_prediction_head = (
+        keras_mask_head.WeightSharedConvolutionalMaskHead(
+            num_classes=20,
+            num_predictions_per_location=1,
+            conv_hyperparams=self._build_conv_hyperparams(),
+            mask_height=7,
+            mask_width=7,
+            masks_are_class_agnostic=True))
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    mask_predictions = mask_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 1, 7, 7],
+                        mask_predictions.get_shape().as_list())
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/mask_rcnn_keras_box_predictor.py b/research/object_detection/predictors/mask_rcnn_keras_box_predictor.py
new file mode 100644
index 00000000..f418c65f
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_keras_box_predictor.py
@@ -0,0 +1,137 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Mask R-CNN Box Predictor."""
+from object_detection.core import box_predictor
+
+
+BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
+CLASS_PREDICTIONS_WITH_BACKGROUND = (
+    box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND)
+MASK_PREDICTIONS = box_predictor.MASK_PREDICTIONS
+
+
+class MaskRCNNKerasBoxPredictor(box_predictor.KerasBoxPredictor):
+  """Mask R-CNN Box Predictor.
+
+  See Mask R-CNN: He, K., Gkioxari, G., Dollar, P., & Girshick, R. (2017).
+  Mask R-CNN. arXiv preprint arXiv:1703.06870.
+
+  This is used for the second stage of the Mask R-CNN detector where proposals
+  cropped from an image are arranged along the batch dimension of the input
+  image_features tensor. Notice that locations are *not* shared across classes,
+  thus for each anchor, a separate prediction is made for each class.
+
+  In addition to predicting boxes and classes, optionally this class allows
+  predicting masks and/or keypoints inside detection boxes.
+
+  Currently this box predictor makes per-class predictions; that is, each
+  anchor makes a separate box prediction for each class.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               freeze_batchnorm,
+               box_prediction_head,
+               class_prediction_head,
+               third_stage_heads,
+               name=None):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      box_prediction_head: The head that predicts the boxes in second stage.
+      class_prediction_head: The head that predicts the classes in second stage.
+      third_stage_heads: A dictionary mapping head names to mask rcnn head
+        classes.
+      name: A string name scope to assign to the model. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(MaskRCNNKerasBoxPredictor, self).__init__(
+        is_training, num_classes, freeze_batchnorm=freeze_batchnorm,
+        inplace_batchnorm_update=False, name=name)
+    self._box_prediction_head = box_prediction_head
+    self._class_prediction_head = class_prediction_head
+    self._third_stage_heads = third_stage_heads
+
+  @property
+  def num_classes(self):
+    return self._num_classes
+
+  def get_second_stage_prediction_heads(self):
+    return BOX_ENCODINGS, CLASS_PREDICTIONS_WITH_BACKGROUND
+
+  def get_third_stage_prediction_heads(self):
+    return sorted(self._third_stage_heads.keys())
+
+  def _predict(self,
+               image_features,
+               prediction_stage=2):
+    """Optionally computes encoded object locations, confidences, and masks.
+
+    Predicts the heads belonging to the given prediction stage.
+
+    Args:
+      image_features: A list of float tensors of shape
+        [batch_size, height_i, width_i, channels_i] containing roi pooled
+        features for each image. The length of the list should be 1 otherwise
+        a ValueError will be raised.
+      prediction_stage: Prediction stage. Acceptable values are 2 and 3.
+
+    Returns:
+      A dictionary containing the predicted tensors that are listed in
+      self._prediction_heads. A subset of the following keys will exist in the
+      dictionary:
+        BOX_ENCODINGS: A float tensor of shape
+          [batch_size, 1, num_classes, code_size] representing the
+          location of the objects.
+        CLASS_PREDICTIONS_WITH_BACKGROUND: A float tensor of shape
+          [batch_size, 1, num_classes + 1] representing the class
+          predictions for the proposals.
+        MASK_PREDICTIONS: A float tensor of shape
+          [batch_size, 1, num_classes, image_height, image_width]
+
+    Raises:
+      ValueError: If num_predictions_per_location is not 1 or if
+        len(image_features) is not 1.
+      ValueError: if prediction_stage is not 2 or 3.
+    """
+    if len(image_features) != 1:
+      raise ValueError('length of `image_features` must be 1. Found {}'.format(
+          len(image_features)))
+    image_feature = image_features[0]
+    predictions_dict = {}
+
+    if prediction_stage == 2:
+      predictions_dict[BOX_ENCODINGS] = self._box_prediction_head(image_feature)
+      predictions_dict[CLASS_PREDICTIONS_WITH_BACKGROUND] = (
+          self._class_prediction_head(image_feature))
+    elif prediction_stage == 3:
+      for prediction_head in self.get_third_stage_prediction_heads():
+        head_object = self._third_stage_heads[prediction_head]
+        predictions_dict[prediction_head] = head_object(image_feature)
+    else:
+      raise ValueError('prediction_stage should be either 2 or 3.')
+
+    return predictions_dict
diff --git a/research/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py b/research/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py
new file mode 100644
index 00000000..03cad615
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py
@@ -0,0 +1,140 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.mask_rcnn_box_predictor."""
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import box_predictor_builder
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors import mask_rcnn_keras_box_predictor as box_predictor
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class MaskRCNNKerasBoxPredictorTest(test_case.TestCase):
+
+  def _build_hyperparams(self,
+                         op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.KerasLayerHyperparams(hyperparams)
+
+  def test_get_boxes_with_five_classes(self):
+    def graph_fn(image_features):
+      mask_box_predictor = (
+          box_predictor_builder.build_mask_rcnn_keras_box_predictor(
+              is_training=False,
+              num_classes=5,
+              fc_hyperparams=self._build_hyperparams(),
+              freeze_batchnorm=False,
+              use_dropout=False,
+              dropout_keep_prob=0.5,
+              box_code_size=4,
+          ))
+      box_predictions = mask_box_predictor(
+          [image_features],
+          prediction_stage=2)
+      return (box_predictions[box_predictor.BOX_ENCODINGS],
+              box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND])
+    image_features = np.random.rand(2, 7, 7, 3).astype(np.float32)
+    (box_encodings,
+     class_predictions_with_background) = self.execute(graph_fn,
+                                                       [image_features])
+    self.assertAllEqual(box_encodings.shape, [2, 1, 5, 4])
+    self.assertAllEqual(class_predictions_with_background.shape, [2, 1, 6])
+
+  def test_get_boxes_with_five_classes_share_box_across_classes(self):
+    def graph_fn(image_features):
+      mask_box_predictor = (
+          box_predictor_builder.build_mask_rcnn_keras_box_predictor(
+              is_training=False,
+              num_classes=5,
+              fc_hyperparams=self._build_hyperparams(),
+              freeze_batchnorm=False,
+              use_dropout=False,
+              dropout_keep_prob=0.5,
+              box_code_size=4,
+              share_box_across_classes=True
+          ))
+      box_predictions = mask_box_predictor(
+          [image_features],
+          prediction_stage=2)
+      return (box_predictions[box_predictor.BOX_ENCODINGS],
+              box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND])
+    image_features = np.random.rand(2, 7, 7, 3).astype(np.float32)
+    (box_encodings,
+     class_predictions_with_background) = self.execute(graph_fn,
+                                                       [image_features])
+    self.assertAllEqual(box_encodings.shape, [2, 1, 1, 4])
+    self.assertAllEqual(class_predictions_with_background.shape, [2, 1, 6])
+
+  def test_get_instance_masks(self):
+    def graph_fn(image_features):
+      mask_box_predictor = (
+          box_predictor_builder.build_mask_rcnn_keras_box_predictor(
+              is_training=False,
+              num_classes=5,
+              fc_hyperparams=self._build_hyperparams(),
+              freeze_batchnorm=False,
+              use_dropout=False,
+              dropout_keep_prob=0.5,
+              box_code_size=4,
+              conv_hyperparams=self._build_hyperparams(
+                  op_type=hyperparams_pb2.Hyperparams.CONV),
+              predict_instance_masks=True))
+      box_predictions = mask_box_predictor(
+          [image_features],
+          prediction_stage=3)
+      return (box_predictions[box_predictor.MASK_PREDICTIONS],)
+    image_features = np.random.rand(2, 7, 7, 3).astype(np.float32)
+    mask_predictions = self.execute(graph_fn, [image_features])
+    self.assertAllEqual(mask_predictions.shape, [2, 1, 5, 14, 14])
+
+  def test_do_not_return_instance_masks_without_request(self):
+    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
+    mask_box_predictor = (
+        box_predictor_builder.build_mask_rcnn_keras_box_predictor(
+            is_training=False,
+            num_classes=5,
+            fc_hyperparams=self._build_hyperparams(),
+            freeze_batchnorm=False,
+            use_dropout=False,
+            dropout_keep_prob=0.5,
+            box_code_size=4))
+    box_predictions = mask_box_predictor(
+        [image_features],
+        prediction_stage=2)
+    self.assertEqual(len(box_predictions), 2)
+    self.assertTrue(box_predictor.BOX_ENCODINGS in box_predictions)
+    self.assertTrue(box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND
+                    in box_predictions)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/predictors/rfcn_keras_box_predictor.py b/research/object_detection/predictors/rfcn_keras_box_predictor.py
new file mode 100644
index 00000000..09582fa5
--- /dev/null
+++ b/research/object_detection/predictors/rfcn_keras_box_predictor.py
@@ -0,0 +1,203 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""RFCN Box Predictor."""
+import tensorflow as tf
+from object_detection.core import box_predictor
+from object_detection.utils import ops
+
+BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
+CLASS_PREDICTIONS_WITH_BACKGROUND = (
+    box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND)
+MASK_PREDICTIONS = box_predictor.MASK_PREDICTIONS
+
+
+class RfcnKerasBoxPredictor(box_predictor.KerasBoxPredictor):
+  """RFCN Box Predictor.
+
+  Applies a position sensitive ROI pooling on position sensitive feature maps to
+  predict classes and refined locations. See https://arxiv.org/abs/1605.06409
+  for details.
+
+  This is used for the second stage of the RFCN meta architecture. Notice that
+  locations are *not* shared across classes, thus for each anchor, a separate
+  prediction is made for each class.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               conv_hyperparams,
+               freeze_batchnorm,
+               num_spatial_bins,
+               depth,
+               crop_size,
+               box_code_size,
+               name=None):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops.
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      num_spatial_bins: A list of two integers `[spatial_bins_y,
+        spatial_bins_x]`.
+      depth: Target depth to reduce the input feature maps to.
+      crop_size: A list of two integers `[crop_height, crop_width]`.
+      box_code_size: Size of encoding for each box.
+      name: A string name scope to assign to the box predictor. If `None`, Keras
+        will auto-generate one from the class name.
+    """
+    super(RfcnKerasBoxPredictor, self).__init__(
+        is_training, num_classes, freeze_batchnorm=freeze_batchnorm,
+        inplace_batchnorm_update=False, name=name)
+    self._freeze_batchnorm = freeze_batchnorm
+    self._conv_hyperparams = conv_hyperparams
+    self._num_spatial_bins = num_spatial_bins
+    self._depth = depth
+    self._crop_size = crop_size
+    self._box_code_size = box_code_size
+
+    # Build the shared layers used for both heads
+    self._shared_conv_layers = []
+    self._shared_conv_layers.append(
+        tf.keras.layers.Conv2D(
+            self._depth,
+            [1, 1],
+            padding='SAME',
+            name='reduce_depth_conv',
+            **self._conv_hyperparams.params()))
+    self._shared_conv_layers.append(
+        self._conv_hyperparams.build_batch_norm(
+            training=(self._is_training and not self._freeze_batchnorm),
+            name='reduce_depth_batchnorm'))
+    self._shared_conv_layers.append(
+        self._conv_hyperparams.build_activation_layer(
+            name='reduce_depth_activation'))
+
+    self._box_encoder_layers = []
+    location_feature_map_depth = (self._num_spatial_bins[0] *
+                                  self._num_spatial_bins[1] *
+                                  self.num_classes *
+                                  self._box_code_size)
+    self._box_encoder_layers.append(
+        tf.keras.layers.Conv2D(
+            location_feature_map_depth,
+            [1, 1],
+            padding='SAME',
+            name='refined_locations_conv',
+            **self._conv_hyperparams.params()))
+    self._box_encoder_layers.append(
+        self._conv_hyperparams.build_batch_norm(
+            training=(self._is_training and not self._freeze_batchnorm),
+            name='refined_locations_batchnorm'))
+
+    self._class_predictor_layers = []
+    self._total_classes = self.num_classes + 1  # Account for background class.
+    class_feature_map_depth = (self._num_spatial_bins[0] *
+                               self._num_spatial_bins[1] *
+                               self._total_classes)
+    self._class_predictor_layers.append(
+        tf.keras.layers.Conv2D(
+            class_feature_map_depth,
+            [1, 1],
+            padding='SAME',
+            name='class_predictions_conv',
+            **self._conv_hyperparams.params()))
+    self._class_predictor_layers.append(
+        self._conv_hyperparams.build_batch_norm(
+            training=(self._is_training and not self._freeze_batchnorm),
+            name='class_predictions_batchnorm'))
+
+  @property
+  def num_classes(self):
+    return self._num_classes
+
+  def _predict(self, image_features, proposal_boxes):
+    """Computes encoded object locations and corresponding confidences.
+
+    Args:
+      image_features: A list of float tensors of shape [batch_size, height_i,
+      width_i, channels_i] containing features for a batch of images.
+      proposal_boxes: A float tensor of shape [batch_size, num_proposals,
+        box_code_size].
+
+    Returns:
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes. Each entry in the
+        list corresponds to a feature map in the input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
+
+    Raises:
+      ValueError: if num_predictions_per_location is not 1 or if
+        len(image_features) is not 1.
+    """
+    if len(image_features) != 1:
+      raise ValueError('length of `image_features` must be 1. Found {}'.
+                       format(len(image_features)))
+    image_feature = image_features[0]
+    batch_size = tf.shape(proposal_boxes)[0]
+    num_boxes = tf.shape(proposal_boxes)[1]
+    net = image_feature
+    for layer in self._shared_conv_layers:
+      net = layer(net)
+
+    # Location predictions.
+    box_net = net
+    for layer in self._box_encoder_layers:
+      box_net = layer(box_net)
+    box_encodings = ops.batch_position_sensitive_crop_regions(
+        box_net,
+        boxes=proposal_boxes,
+        crop_size=self._crop_size,
+        num_spatial_bins=self._num_spatial_bins,
+        global_pool=True)
+    box_encodings = tf.squeeze(box_encodings, squeeze_dims=[2, 3])
+    box_encodings = tf.reshape(box_encodings,
+                               [batch_size * num_boxes, 1, self.num_classes,
+                                self._box_code_size])
+
+    # Class predictions.
+    class_net = net
+    for layer in self._class_predictor_layers:
+      class_net = layer(class_net)
+    class_predictions_with_background = (
+        ops.batch_position_sensitive_crop_regions(
+            class_net,
+            boxes=proposal_boxes,
+            crop_size=self._crop_size,
+            num_spatial_bins=self._num_spatial_bins,
+            global_pool=True))
+    class_predictions_with_background = tf.squeeze(
+        class_predictions_with_background, squeeze_dims=[2, 3])
+    class_predictions_with_background = tf.reshape(
+        class_predictions_with_background,
+        [batch_size * num_boxes, 1, self._total_classes])
+
+    return {BOX_ENCODINGS: [box_encodings],
+            CLASS_PREDICTIONS_WITH_BACKGROUND:
+            [class_predictions_with_background]}
diff --git a/research/object_detection/predictors/rfcn_keras_box_predictor_test.py b/research/object_detection/predictors/rfcn_keras_box_predictor_test.py
new file mode 100644
index 00000000..30a5bb59
--- /dev/null
+++ b/research/object_detection/predictors/rfcn_keras_box_predictor_test.py
@@ -0,0 +1,77 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.rfcn_box_predictor."""
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors import rfcn_keras_box_predictor as box_predictor
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class RfcnKerasBoxPredictorTest(test_case.TestCase):
+
+  def _build_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  def test_get_correct_box_encoding_and_class_prediction_shapes(self):
+
+    def graph_fn(image_features, proposal_boxes):
+      rfcn_box_predictor = box_predictor.RfcnKerasBoxPredictor(
+          is_training=False,
+          num_classes=2,
+          conv_hyperparams=self._build_conv_hyperparams(),
+          freeze_batchnorm=False,
+          num_spatial_bins=[3, 3],
+          depth=4,
+          crop_size=[12, 12],
+          box_code_size=4
+      )
+      box_predictions = rfcn_box_predictor(
+          [image_features],
+          proposal_boxes=proposal_boxes)
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    proposal_boxes = np.random.rand(4, 2, 4).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features, proposal_boxes])
+
+    self.assertAllEqual(box_encodings.shape, [8, 1, 2, 4])
+    self.assertAllEqual(class_predictions_with_background.shape, [8, 1, 3])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/protos/anchor_generator.proto b/research/object_detection/protos/anchor_generator.proto
index c47b558f..9608ca48 100644
--- a/research/object_detection/protos/anchor_generator.proto
+++ b/research/object_detection/protos/anchor_generator.proto
@@ -2,9 +2,10 @@ syntax = "proto2";
 
 package object_detection.protos;
 
+import "object_detection/protos/flexible_grid_anchor_generator.proto";
 import "object_detection/protos/grid_anchor_generator.proto";
-import "object_detection/protos/ssd_anchor_generator.proto";
 import "object_detection/protos/multiscale_anchor_generator.proto";
+import "object_detection/protos/ssd_anchor_generator.proto";
 
 // Configuration proto for the anchor generator to use in the object detection
 // pipeline. See core/anchor_generator.py for details.
@@ -13,5 +14,6 @@ message AnchorGenerator {
     GridAnchorGenerator grid_anchor_generator = 1;
     SsdAnchorGenerator ssd_anchor_generator = 2;
     MultiscaleAnchorGenerator multiscale_anchor_generator = 3;
+    FlexibleGridAnchorGenerator flexible_grid_anchor_generator = 4;
   }
 }
diff --git a/research/object_detection/protos/box_predictor.proto b/research/object_detection/protos/box_predictor.proto
index 25d5b1fc..75a38bff 100644
--- a/research/object_detection/protos/box_predictor.proto
+++ b/research/object_detection/protos/box_predictor.proto
@@ -15,7 +15,6 @@ message BoxPredictor {
   }
 }
 
-
 // Configuration proto for Convolutional box predictor.
 // Next id: 13
 message ConvolutionalBoxPredictor {
@@ -57,6 +56,13 @@ message ConvolutionalBoxPredictor {
 
   // Whether to use depthwise separable convolution for box predictor layers.
   optional bool use_depthwise = 11 [default = false];
+
+  // If specified, apply clipping to box encodings.
+  message BoxEncodingsClipRange {
+    optional float min = 1;
+    optional float max = 2;
+  }
+  optional BoxEncodingsClipRange box_encodings_clip_range = 12;
 }
 
 // Configuration proto for weight shared convolutional box predictor.
@@ -118,6 +124,8 @@ message WeightSharedConvolutionalBoxPredictor {
   optional BoxEncodingsClipRange box_encodings_clip_range = 17;
 }
 
+
+
 // TODO(alirezafathi): Refactor the proto file to be able to configure mask rcnn
 // head easily.
 // Next id: 15
diff --git a/research/object_detection/protos/flexible_grid_anchor_generator.proto b/research/object_detection/protos/flexible_grid_anchor_generator.proto
new file mode 100644
index 00000000..8e778355
--- /dev/null
+++ b/research/object_detection/protos/flexible_grid_anchor_generator.proto
@@ -0,0 +1,30 @@
+syntax = "proto2";
+
+package object_detection.protos;
+
+message FlexibleGridAnchorGenerator {
+  repeated AnchorGrid anchor_grid = 1;
+
+  // Whether to produce anchors in normalized coordinates.
+  optional bool normalize_coordinates = 2 [default = true];
+}
+
+message AnchorGrid {
+  // The base sizes in pixels for each anchor in this anchor layer.
+  repeated float base_sizes = 1;
+
+  // The aspect ratios for each anchor in this anchor layer.
+  repeated float aspect_ratios = 2;
+
+  // The anchor height stride in pixels.
+  optional uint32 height_stride = 3;
+
+  // The anchor width stride in pixels.
+  optional uint32 width_stride = 4;
+
+  // The anchor height offset in pixels.
+  optional uint32 height_offset = 5 [default = 0];
+
+  // The anchor width offset in pixels.
+  optional uint32 width_offset = 6 [default = 0];
+}
diff --git a/research/object_detection/protos/graph_rewriter.proto b/research/object_detection/protos/graph_rewriter.proto
index 8828f9f0..671d2346 100644
--- a/research/object_detection/protos/graph_rewriter.proto
+++ b/research/object_detection/protos/graph_rewriter.proto
@@ -20,4 +20,7 @@ message Quantization {
   // Number of bits to use for quantizing activations.
   // Only 8 bit is supported for now.
   optional int32 activation_bits = 3 [default = 8];
+
+  // Whether to use symmetric weight quantization.
+  optional bool symmetric = 4 [default = false];
 }
diff --git a/research/object_detection/protos/image_resizer.proto b/research/object_detection/protos/image_resizer.proto
index 031aeae0..8132d9cf 100644
--- a/research/object_detection/protos/image_resizer.proto
+++ b/research/object_detection/protos/image_resizer.proto
@@ -9,6 +9,7 @@ message ImageResizer {
     KeepAspectRatioResizer keep_aspect_ratio_resizer = 1;
     FixedShapeResizer fixed_shape_resizer = 2;
     IdentityResizer identity_resizer = 3;
+    ConditionalShapeResizer conditional_shape_resizer = 4;
   }
 }
 
@@ -61,3 +62,31 @@ message FixedShapeResizer {
   // Whether to also resize the image channels from 3 to 1 (RGB to grayscale).
   optional bool convert_to_grayscale = 4 [default = false];
 }
+
+// Configuration proto for image resizer that resizes only if input image height
+// or width is greater or smaller than a certain size.
+// Aspect ratio is maintained.
+message ConditionalShapeResizer {
+
+  // Enumeration for the condition on which to resize an image.
+  enum ResizeCondition {
+    INVALID = 0; // Default value.
+    GREATER = 1; // Resizes image if a dimension is greater than specified size.
+    SMALLER = 2; // Resizes image if a dimension is smaller than specified size.
+  }
+
+  // Condition which must be true to resize the image.
+  optional ResizeCondition condition = 1 [default = GREATER];
+
+  // Threshold for the image size. If any image dimension is above or below this
+  // (as specified by condition) the image will be resized so that it meets the
+  // threshold.
+  optional int32 size_threshold = 2 [default = 300];
+
+  // Desired method when resizing image.
+  optional ResizeType resize_method = 3 [default = BILINEAR];
+
+  // Whether to also resize the image channels from 3 to 1 (RGB to grayscale).
+  optional bool convert_to_grayscale = 4 [default = false];
+
+}
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index 77f6118f..6c0082fd 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -1,4 +1,5 @@
 syntax = "proto2";
+
 package object_detection.protos;
 
 import "object_detection/protos/anchor_generator.proto";
@@ -6,15 +7,14 @@ import "object_detection/protos/box_coder.proto";
 import "object_detection/protos/box_predictor.proto";
 import "object_detection/protos/hyperparams.proto";
 import "object_detection/protos/image_resizer.proto";
-import "object_detection/protos/matcher.proto";
 import "object_detection/protos/losses.proto";
+import "object_detection/protos/matcher.proto";
 import "object_detection/protos/post_processing.proto";
 import "object_detection/protos/region_similarity_calculator.proto";
 
 // Configuration for Single Shot Detection (SSD) models.
 // Next id: 26
 message Ssd {
-
   // Number of classes to predict.
   optional int32 num_classes = 1;
 
@@ -114,8 +114,8 @@ message Ssd {
     // features and the number of classes.
     optional int32 mask_prediction_conv_depth = 4 [default = 256];
 
-    // The number of convolutions applied to image_features in the mask prediction
-    // branch.
+    // The number of convolutions applied to image_features in the mask
+    // prediction branch.
     optional int32 mask_prediction_num_conv_layers = 5 [default = 2];
 
     // Whether to apply convolutions on mask features before upsampling using
@@ -125,10 +125,10 @@ message Ssd {
     optional bool convolve_then_upsample_masks = 6 [default = false];
 
     // Mask loss weight.
-    optional float mask_loss_weight = 7 [default=5.0];
+    optional float mask_loss_weight = 7 [default = 5.0];
 
     // Number of boxes to be generated at training time for computing mask loss.
-    optional int32 mask_loss_sample_size = 8 [default=16];
+    optional int32 mask_loss_sample_size = 8 [default = 16];
 
     // Hyperparameters for convolution ops used in the box predictor.
     optional Hyperparams conv_hyperparams = 9;
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config
new file mode 100644
index 00000000..7a90d50a
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config
@@ -0,0 +1,192 @@
+# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+# TPU-compatible for both training and inference
+
+model {
+  ssd {
+    num_classes: 37
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.9997,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.9997,
+          epsilon: 0.001,
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+        use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 24
+  optimizer {
+    rms_prop_optimizer: {
+      learning_rate: {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.004
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  max_number_of_boxes: 50
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config b/research/object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config
new file mode 100644
index 00000000..fdeb853c
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config
@@ -0,0 +1,207 @@
+# SSD with Mobilenet v2 configuration for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  ssd {
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+        height_stride: 16
+        height_stride: 32
+        height_stride: 64
+        height_stride: 128
+        height_stride: 256
+        height_stride: 512
+        width_stride: 16
+        width_stride: 32
+        width_stride: 64
+        width_stride: 128
+        width_stride: 256
+        width_stride: 512
+      }
+    }
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 320
+        max_dimension: 640
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.9997,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v2'
+      min_depth: 16
+      depth_multiplier: 1.0
+      use_explicit_padding: true
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.9997,
+          epsilon: 0.001,
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid {
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      hard_example_miner {
+        num_hard_examples: 3000
+        iou_threshold: 0.99
+        loss_type: CLASSIFICATION
+        max_negatives_per_positive: 3
+        min_negatives_per_image: 3
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 24
+  optimizer {
+    rms_prop_optimizer: {
+      learning_rate: {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.004
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  fine_tune_checkpoint_type:  "detection"
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop_fixed_aspect_ratio {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/tpu_exporters/__init__.py b/research/object_detection/tpu_exporters/__init__.py
new file mode 100644
index 00000000..8b137891
--- /dev/null
+++ b/research/object_detection/tpu_exporters/__init__.py
@@ -0,0 +1 @@
+
diff --git a/research/object_detection/tpu_exporters/export_saved_model_tpu.py b/research/object_detection/tpu_exporters/export_saved_model_tpu.py
new file mode 100644
index 00000000..018461af
--- /dev/null
+++ b/research/object_detection/tpu_exporters/export_saved_model_tpu.py
@@ -0,0 +1,54 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Python binary for exporting SavedModel, tailored for TPU inference."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from object_detection.tpu_exporters import export_saved_model_tpu_lib
+
+flags = tf.app.flags
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('pipeline_config_file', None,
+                    'A pipeline_pb2.TrainEvalPipelineConfig config file.')
+flags.DEFINE_string(
+    'ckpt_path', None, 'Path to trained checkpoint, typically of the form '
+    'path/to/model.ckpt')
+flags.DEFINE_string('export_dir', None, 'Path to export SavedModel.')
+flags.DEFINE_string('input_placeholder_name', 'placeholder_tensor',
+                    'Name of input placeholder in model\'s signature_def_map.')
+flags.DEFINE_string(
+    'input_type', 'tf_example', 'Type of input node. Can be '
+    'one of [`image_tensor`, `encoded_image_string_tensor`, '
+    '`tf_example`]')
+flags.DEFINE_boolean('use_bfloat16', False, 'If true, use tf.bfloat16 on TPU.')
+
+
+def main(argv):
+  if len(argv) > 1:
+    raise tf.app.UsageError('Too many command-line arguments.')
+  export_saved_model_tpu_lib.export(FLAGS.pipeline_config_file, FLAGS.ckpt_path,
+                                    FLAGS.export_dir,
+                                    FLAGS.input_placeholder_name,
+                                    FLAGS.input_type, FLAGS.use_bfloat16)
+
+
+if __name__ == '__main__':
+  tf.app.flags.mark_flag_as_required('pipeline_config_file')
+  tf.app.flags.mark_flag_as_required('ckpt_path')
+  tf.app.flags.mark_flag_as_required('export_dir')
+  tf.app.run()
diff --git a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
new file mode 100644
index 00000000..bb3218cb
--- /dev/null
+++ b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
@@ -0,0 +1,215 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Python library for exporting SavedModel, tailored for TPU inference."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from google.protobuf import text_format
+# pylint: disable=g-direct-tensorflow-import
+from tensorflow.python.saved_model import loader
+from tensorflow.python.saved_model import signature_constants
+from tensorflow.python.saved_model import tag_constants
+# pylint: enable=g-direct-tensorflow-import
+from object_detection.protos import pipeline_pb2
+from object_detection.tpu_exporters import faster_rcnn
+from object_detection.tpu_exporters import ssd
+
+model_map = {
+    'faster_rcnn': faster_rcnn,
+    'ssd': ssd,
+}
+
+
+def parse_pipeline_config(pipeline_config_file):
+  """Returns pipeline config and meta architecture name."""
+  with tf.gfile.GFile(pipeline_config_file, 'r') as config_file:
+    config_str = config_file.read()
+  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+  text_format.Merge(config_str, pipeline_config)
+  meta_arch = pipeline_config.model.WhichOneof('model')
+
+  return pipeline_config, meta_arch
+
+
+def export(pipeline_config_file,
+           ckpt_path,
+           export_dir,
+           input_placeholder_name='placeholder_tensor',
+           input_type='encoded_image_string_tensor',
+           use_bfloat16=False):
+  """Exports as SavedModel.
+
+  Args:
+    pipeline_config_file: Pipeline config file name.
+    ckpt_path: Training checkpoint path.
+    export_dir: Directory to export SavedModel.
+    input_placeholder_name: input placeholder's name in SavedModel signature.
+    input_type: One of
+                'encoded_image_string_tensor': a 1d tensor with dtype=tf.string
+                'image_tensor': a 4d tensor with dtype=tf.uint8
+                'tf_example': a 1d tensor with dtype=tf.string
+    use_bfloat16: If true, use tf.bfloat16 on TPU.
+  """
+  pipeline_config, meta_arch = parse_pipeline_config(pipeline_config_file)
+
+  shapes_info = model_map[meta_arch].get_prediction_tensor_shapes(
+      pipeline_config)
+
+  with tf.Graph().as_default(), tf.Session() as sess:
+    placeholder_tensor, result_tensor_dict = model_map[meta_arch].build_graph(
+        pipeline_config, shapes_info, input_type, use_bfloat16)
+
+    saver = tf.train.Saver()
+    init_op = tf.global_variables_initializer()
+
+    sess.run(init_op)
+    if ckpt_path is not None:
+      saver.restore(sess, ckpt_path)
+
+    # export saved model
+    builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
+    tensor_info_inputs = {
+        input_placeholder_name:
+            tf.saved_model.utils.build_tensor_info(placeholder_tensor)
+    }
+    tensor_info_outputs = {
+        k: tf.saved_model.utils.build_tensor_info(v)
+        for k, v in result_tensor_dict.items()
+    }
+    detection_signature = (
+        tf.saved_model.signature_def_utils.build_signature_def(
+            inputs=tensor_info_inputs,
+            outputs=tensor_info_outputs,
+            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))
+
+    tf.logging.info('Inputs:\n{}\nOutputs:{}\nPredict method name:{}'.format(
+        tensor_info_inputs, tensor_info_outputs,
+        tf.saved_model.signature_constants.PREDICT_METHOD_NAME))
+    # Graph for TPU.
+    builder.add_meta_graph_and_variables(
+        sess, [
+            tf.saved_model.tag_constants.SERVING,
+            tf.saved_model.tag_constants.TPU
+        ],
+        signature_def_map={
+            tf.saved_model.signature_constants
+            .DEFAULT_SERVING_SIGNATURE_DEF_KEY:
+                detection_signature,
+        },
+        strip_default_attrs=True)
+    # Graph for CPU, this is for passing infra validation.
+    builder.add_meta_graph(
+        [tf.saved_model.tag_constants.SERVING],
+        signature_def_map={
+            tf.saved_model.signature_constants
+            .DEFAULT_SERVING_SIGNATURE_DEF_KEY:
+                detection_signature,
+        },
+        strip_default_attrs=True)
+    builder.save(as_text=False)
+    tf.logging.info('Model saved to {}'.format(export_dir))
+
+
+def run_inference(inputs,
+                  pipeline_config_file,
+                  ckpt_path,
+                  input_type='encoded_image_string_tensor',
+                  use_bfloat16=False,
+                  repeat=1):
+  """Runs inference on TPU.
+
+  Args:
+    inputs: Input image with the same type as `input_type`
+    pipeline_config_file: Pipeline config file name.
+    ckpt_path: Training checkpoint path.
+    input_type: One of
+                'encoded_image_string_tensor': a 1d tensor with dtype=tf.string
+                'image_tensor': a 4d tensor with dtype=tf.uint8
+                'tf_example': a 1d tensor with dtype=tf.string
+    use_bfloat16: If true, use tf.bfloat16 on TPU.
+    repeat: Number of times to repeat running the provided input for profiling.
+
+  Returns:
+    A dict of resulting tensors.
+  """
+
+  pipeline_config, meta_arch = parse_pipeline_config(pipeline_config_file)
+
+  shapes_info = model_map[meta_arch].get_prediction_tensor_shapes(
+      pipeline_config)
+
+  with tf.Graph().as_default(), tf.Session() as sess:
+    placeholder_tensor, result_tensor_dict = model_map[meta_arch].build_graph(
+        pipeline_config, shapes_info, input_type, use_bfloat16)
+
+    saver = tf.train.Saver()
+    init_op = tf.global_variables_initializer()
+
+    sess.run(tf.contrib.tpu.initialize_system())
+
+    sess.run(init_op)
+    if ckpt_path is not None:
+      saver.restore(sess, ckpt_path)
+
+    for _ in range(repeat):
+      tensor_dict_out = sess.run(
+          result_tensor_dict, feed_dict={placeholder_tensor: [inputs]})
+
+    sess.run(tf.contrib.tpu.shutdown_system())
+
+    return tensor_dict_out
+
+
+def run_inference_from_saved_model(inputs,
+                                   saved_model_dir,
+                                   input_placeholder_name='placeholder_tensor',
+                                   repeat=1):
+  """Loads saved model and run inference on TPU.
+
+  Args:
+    inputs: Input image with the same type as `input_type`
+    saved_model_dir: The directory SavedModel being exported to.
+    input_placeholder_name: input placeholder's name in SavedModel signature.
+    repeat: Number of times to repeat running the provided input for profiling.
+
+  Returns:
+    A dict of resulting tensors.
+  """
+  with tf.Graph().as_default(), tf.Session() as sess:
+    meta_graph = loader.load(sess, [tag_constants.SERVING, tag_constants.TPU],
+                             saved_model_dir)
+
+    sess.run(tf.contrib.tpu.initialize_system())
+
+    key_prediction = signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
+
+    tensor_name_input = (
+        meta_graph.signature_def[key_prediction].inputs[input_placeholder_name]
+        .name)
+    tensor_name_output = {
+        k: v.name
+        for k, v in (meta_graph.signature_def[key_prediction].outputs.items())
+    }
+
+    for _ in range(repeat):
+      tensor_dict_out = sess.run(
+          tensor_name_output, feed_dict={tensor_name_input: [inputs]})
+
+    sess.run(tf.contrib.tpu.shutdown_system())
+
+    return tensor_dict_out
diff --git a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py
new file mode 100644
index 00000000..1d696036
--- /dev/null
+++ b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py
@@ -0,0 +1,68 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Test for object detection's TPU exporter."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+
+from absl.testing import parameterized
+import numpy as np
+import tensorflow as tf
+
+from object_detection.tpu_exporters import export_saved_model_tpu_lib
+
+flags = tf.app.flags
+FLAGS = flags.FLAGS
+
+
+def get_path(path_suffix):
+  return os.path.join(tf.resource_loader.get_data_files_path(), 'testdata',
+                      path_suffix)
+
+
+class ExportSavedModelTPUTest(tf.test.TestCase, parameterized.TestCase):
+
+  @parameterized.named_parameters(
+      ('ssd', get_path('ssd/ssd_pipeline.config'), 'image_tensor', True, 20),
+      ('faster_rcnn',
+       get_path('faster_rcnn/faster_rcnn_resnet101_atrous_coco.config'),
+       'image_tensor', True, 20))
+  def testExportAndLoad(self,
+                        pipeline_config_file,
+                        input_type='image_tensor',
+                        use_bfloat16=False,
+                        repeat=1):
+
+    input_placeholder_name = 'placeholder_tensor'
+    export_dir = os.path.join(FLAGS.test_tmpdir, 'tpu_saved_model')
+    if tf.gfile.Exists(export_dir):
+      tf.gfile.DeleteRecursively(export_dir)
+    ckpt_path = None
+    export_saved_model_tpu_lib.export(pipeline_config_file, ckpt_path,
+                                      export_dir, input_placeholder_name,
+                                      input_type, use_bfloat16)
+
+    inputs = np.random.rand(256, 256, 3)
+    tensor_dict_out = export_saved_model_tpu_lib.run_inference_from_saved_model(
+        inputs, export_dir, input_placeholder_name, repeat)
+    for k, v in tensor_dict_out.items():
+      tf.logging.info('{}: {}'.format(k, v))
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/tpu_exporters/faster_rcnn.py b/research/object_detection/tpu_exporters/faster_rcnn.py
new file mode 100644
index 00000000..caddd26e
--- /dev/null
+++ b/research/object_detection/tpu_exporters/faster_rcnn.py
@@ -0,0 +1,295 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Python library for faster_rcnn model, tailored for TPU inference."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# pylint: disable=protected-access
+import tensorflow as tf
+
+# pylint: disable=g-import-not-at-top
+# Checking TF version, because this module relies on TPUPartitionedCall
+# in tensorflow.python.tpu, which is not available until TF r1.14.
+major, minor, _ = tf.__version__.split('.')  # pylint: disable=protected-access
+if int(major) < 1 or (int(major == 1) and int(minor) < 14):
+  raise RuntimeError(
+      'TensorFlow version >= 1.14 is required. Found ({}).'.format(
+          tf.__version__))
+
+from tensorflow.python.framework import function
+from tensorflow.python.tpu import functional as tpu_functional
+from tensorflow.python.tpu.ops import tpu_ops
+from object_detection import exporter
+from object_detection.builders import model_builder
+from object_detection.tpu_exporters import utils
+
+ANCHORS = 'anchors'
+BOX_CLASSIFIER_FEATURES = 'box_classifier_features'
+BOX_ENCODINGS = 'box_encodings'
+CLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'
+IMAGE_SHAPE = 'image_shape'
+NUM_PROPOSALS = 'num_proposals'
+PROPOSAL_BOXES = 'proposal_boxes'
+PROPOSAL_BOXES_NORMALIZED = 'proposal_boxes_normalized'
+REFINED_BOX_ENCODINGS = 'refined_box_encodings'
+RPN_BOX_ENCODINGS = 'rpn_box_encodings'
+RPN_BOX_PREDICTOR_FEATURES = 'rpn_box_predictor_features'
+RPN_FEATURES_TO_CROP = 'rpn_features_to_crop'
+RPN_OBJECTNESS_PREDICTIONS_WITH_BACKGROUND = \
+    'rpn_objectness_predictions_with_background'
+
+
+def modify_config(pipeline_config):
+  """Modifies pipeline config to build the correct graph for TPU."""
+  # faster_rcnn.use_static_shapes and faster_rcnn.use_static_shapes_for_eval
+  # are set to True in order for detection_model.use_static_shapes to be True.
+  # We need to set this so that clip_to_window in _predict_first_stage
+  # can work on TPU. However as a side-effect, the flag forces the use of
+  # padded version of NMS.
+  pipeline_config.model.faster_rcnn.use_static_shapes = True
+  pipeline_config.model.faster_rcnn.use_static_shapes_for_eval = True
+  pipeline_config.model.faster_rcnn.use_matmul_crop_and_resize = True
+  pipeline_config.model.faster_rcnn.clip_anchors_to_image = True
+  return pipeline_config
+
+
+def get_prediction_tensor_shapes(pipeline_config):
+  """Gets static shapes of tensors by building the graph on CPU.
+
+  This function builds the graph on CPU and obtain static shapes of output
+  tensors from TPUPartitionedCall. Shapes information are later used for setting
+  shapes of tensors when TPU graphs are built. This is necessary because tensors
+  coming out of TPUPartitionedCall lose their shape information, which are
+  needed for a lot of CPU operations later.
+
+  Args:
+    pipeline_config: A TrainEvalPipelineConfig proto.
+
+  Returns:
+    A python dict of tensors' names and their shapes.
+  """
+  pipeline_config = modify_config(pipeline_config)
+  detection_model = model_builder.build(
+      pipeline_config.model, is_training=False)
+
+  _, input_tensors = exporter.input_placeholder_fn_map['image_tensor']()
+
+  inputs = tf.to_float(input_tensors)
+  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
+
+  prediction_dict = detection_model.predict(preprocessed_inputs,
+                                            true_image_shapes)
+
+  shapes_info = {k: v.shape.as_list() for k, v in prediction_dict.items()}
+  return shapes_info
+
+
+def build_graph(pipeline_config,
+                shapes_info,
+                input_type='encoded_image_string_tensor',
+                use_bfloat16=True):
+  """Builds serving graph of faster_rcnn to be exported.
+
+  Args:
+    pipeline_config: A TrainEvalPipelineConfig proto.
+    shapes_info: A python dict of tensors' names and their shapes, returned by
+      `get_prediction_tensor_shapes()`.
+    input_type: One of
+                'encoded_image_string_tensor': a 1d tensor with dtype=tf.string
+                'image_tensor': a 4d tensor with dtype=tf.uint8
+                'tf_example': a 1d tensor with dtype=tf.string
+    use_bfloat16: If true, use tf.bfloat16 on TPU.
+
+  Returns:
+    placeholder_tensor: A placeholder tensor, type determined by `input_type`.
+    result_tensor_dict: A python dict of tensors' names and tensors.
+  """
+  pipeline_config = modify_config(pipeline_config)
+  detection_model = model_builder.build(
+      pipeline_config.model, is_training=False)
+
+  placeholder_tensor, input_tensors = \
+      exporter.input_placeholder_fn_map[input_type]()
+
+  # CPU pre-processing
+  inputs = tf.to_float(input_tensors)
+  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
+
+  # Dimshuffle: [b, h, w, c] -> [b, c, h, w]
+  preprocessed_inputs = tf.transpose(preprocessed_inputs, perm=[0, 3, 1, 2])
+  if use_bfloat16:
+    preprocessed_inputs = tf.cast(preprocessed_inputs, dtype=tf.bfloat16)
+
+  # TPU feature extraction
+  def tpu_subgraph_first_stage_fn(preprocessed_inputs):
+    """Defines the first part of graph on TPU."""
+    # [b, c, h, w] -> [b, h, w, c]
+    preprocessed_inputs = tf.transpose(preprocessed_inputs, perm=[0, 2, 3, 1])
+
+    prediction_dict = detection_model._predict_first_stage(preprocessed_inputs)
+
+    # [b, h, w, c] -> [b, c, h, w]
+    rpn_box_predictor_features = tf.transpose(
+        prediction_dict[RPN_BOX_PREDICTOR_FEATURES], perm=[0, 3, 1, 2])
+    # [b, h, w, c] -> [b, c, h, w]
+    rpn_features_to_crop = tf.transpose(
+        prediction_dict[RPN_FEATURES_TO_CROP], perm=[0, 3, 1, 2])
+    # [batch, anchor, depth] -> [depth, batch, anchor]
+    rpn_box_encodings = tf.transpose(
+        prediction_dict[RPN_BOX_ENCODINGS], perm=[2, 0, 1])
+    # [batch, anchor, depth] -> [depth, batch, anchor]
+    rpn_objectness_predictions_with_background = tf.transpose(
+        prediction_dict[RPN_OBJECTNESS_PREDICTIONS_WITH_BACKGROUND],
+        perm=[2, 0, 1])
+    # [anchors, depth]
+    anchors = tf.transpose(prediction_dict[ANCHORS], perm=[1, 0])
+
+    return (rpn_box_predictor_features, rpn_features_to_crop,
+            prediction_dict['image_shape'], rpn_box_encodings,
+            rpn_objectness_predictions_with_background, anchors)
+
+  @function.Defun(capture_resource_var_by_value=False)
+  def tpu_subgraph_first_stage():
+    if use_bfloat16:
+      with tf.contrib.tpu.bfloat16_scope():
+        return tf.contrib.tpu.rewrite(tpu_subgraph_first_stage_fn,
+                                      [preprocessed_inputs])
+    else:
+      return tf.contrib.tpu.rewrite(tpu_subgraph_first_stage_fn,
+                                    [preprocessed_inputs])
+
+  (rpn_box_predictor_features, rpn_features_to_crop, image_shape,
+   rpn_box_encodings, rpn_objectness_predictions_with_background,
+   anchors) = \
+      tpu_functional.TPUPartitionedCall(
+          args=tpu_subgraph_first_stage.captured_inputs,
+          device_ordinal=tpu_ops.tpu_ordinal_selector(),
+          Tout=[
+              o.type
+              for o in tpu_subgraph_first_stage.definition.signature.output_arg
+          ],
+          f=tpu_subgraph_first_stage)
+
+  prediction_dict = {
+      RPN_BOX_PREDICTOR_FEATURES:
+          tf.transpose(rpn_box_predictor_features, perm=[0, 2, 3, 1]),
+      RPN_FEATURES_TO_CROP:
+          tf.transpose(rpn_features_to_crop, perm=[0, 2, 3, 1]),
+      IMAGE_SHAPE:
+          image_shape,
+      RPN_BOX_ENCODINGS:
+          tf.transpose(rpn_box_encodings, perm=[1, 2, 0]),
+      RPN_OBJECTNESS_PREDICTIONS_WITH_BACKGROUND:
+          tf.transpose(
+              rpn_objectness_predictions_with_background, perm=[1, 2, 0]),
+      ANCHORS:
+          tf.transpose(anchors, perm=[1, 0]),
+  }
+
+  for k in prediction_dict:
+    prediction_dict[k].set_shape(shapes_info[k])
+
+  if use_bfloat16:
+    prediction_dict = utils.bfloat16_to_float32_nested(prediction_dict)
+
+  # CPU region proposal (NMS)
+  proposal_boxes_normalized, num_proposals = \
+      detection_model._proposal_postprocess(
+          tf.cast(prediction_dict[RPN_BOX_ENCODINGS], dtype=tf.float32),
+          tf.cast(
+              prediction_dict[RPN_OBJECTNESS_PREDICTIONS_WITH_BACKGROUND],
+              dtype=tf.float32), prediction_dict[ANCHORS],
+          prediction_dict[IMAGE_SHAPE], true_image_shapes)
+  prediction_dict[NUM_PROPOSALS] = num_proposals
+
+  # [b, h, w, c] -> [b, c, h, w]
+  prediction_dict[RPN_FEATURES_TO_CROP] = tf.transpose(
+      prediction_dict[RPN_FEATURES_TO_CROP], perm=[0, 3, 1, 2])
+
+  if use_bfloat16:
+    prediction_dict[RPN_FEATURES_TO_CROP] = tf.cast(
+        prediction_dict[RPN_FEATURES_TO_CROP], dtype=tf.bfloat16)
+    proposal_boxes_normalized = tf.cast(
+        proposal_boxes_normalized, dtype=tf.bfloat16)
+
+  # TPU box prediction
+  def tpu_subgraph_second_stage_fn(rpn_features_to_crop,
+                                   proposal_boxes_normalized, image_shape):
+    """Defines the second part of graph on TPU."""
+    rpn_features_to_crop = tf.transpose(rpn_features_to_crop, perm=[0, 2, 3, 1])
+
+    output_dict = detection_model._box_prediction(
+        rpn_features_to_crop, proposal_boxes_normalized, image_shape)
+
+    return [
+        output_dict[REFINED_BOX_ENCODINGS],
+        output_dict[CLASS_PREDICTIONS_WITH_BACKGROUND],
+        output_dict[PROPOSAL_BOXES], output_dict[BOX_CLASSIFIER_FEATURES]
+    ]
+
+  @function.Defun(capture_resource_var_by_value=False)
+  def tpu_subgraph_second_stage():
+    """TPU subgraph 2 wrapper."""
+    if use_bfloat16:
+      with tf.contrib.tpu.bfloat16_scope():
+        return tf.contrib.tpu.rewrite(tpu_subgraph_second_stage_fn, [
+            prediction_dict[RPN_FEATURES_TO_CROP],
+            proposal_boxes_normalized,
+            prediction_dict[IMAGE_SHAPE],
+        ])
+    else:
+      return tf.contrib.tpu.rewrite(tpu_subgraph_second_stage_fn, [
+          prediction_dict[RPN_FEATURES_TO_CROP],
+          proposal_boxes_normalized,
+          prediction_dict[IMAGE_SHAPE],
+      ])
+
+  (refined_box_encodings, class_predictions_with_background, proposal_boxes,
+   box_classifier_features) = tpu_functional.TPUPartitionedCall(
+       args=tpu_subgraph_second_stage.captured_inputs,
+       device_ordinal=tpu_ops.tpu_ordinal_selector(),
+       Tout=[
+           o.type
+           for o in tpu_subgraph_second_stage.definition.signature.output_arg
+       ],
+       f=tpu_subgraph_second_stage)
+
+  prediction_dict[RPN_FEATURES_TO_CROP] = tf.transpose(
+      prediction_dict[RPN_FEATURES_TO_CROP], perm=[0, 2, 3, 1])
+
+  prediction_dict_updater = {
+      REFINED_BOX_ENCODINGS: refined_box_encodings,
+      CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_with_background,
+      PROPOSAL_BOXES: proposal_boxes,
+      BOX_CLASSIFIER_FEATURES: box_classifier_features,
+      PROPOSAL_BOXES_NORMALIZED: proposal_boxes_normalized,
+  }
+
+  for k in prediction_dict_updater:
+    prediction_dict_updater[k].set_shape(shapes_info[k])
+
+  prediction_dict.update(prediction_dict_updater)
+
+  if use_bfloat16:
+    prediction_dict = utils.bfloat16_to_float32_nested(prediction_dict)
+
+  # CPU post-processing (NMS)
+  postprocessed_tensors = detection_model.postprocess(prediction_dict,
+                                                      true_image_shapes)
+  result_tensor_dict = exporter.add_output_tensor_nodes(postprocessed_tensors,
+                                                        'inference_op')
+
+  return placeholder_tensor, result_tensor_dict
diff --git a/research/object_detection/tpu_exporters/ssd.py b/research/object_detection/tpu_exporters/ssd.py
new file mode 100644
index 00000000..2dcfd130
--- /dev/null
+++ b/research/object_detection/tpu_exporters/ssd.py
@@ -0,0 +1,219 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Python library for ssd model, tailored for TPU inference."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+# pylint: disable=g-import-not-at-top
+# Checking TF version, because this module relies on TPUPartitionedCall
+# in tensorflow.python.tpu, which is not available until TF r1.14.
+major, minor, _ = tf.__version__.split('.')  # pylint: disable=protected-access
+if int(major) < 1 or (int(major == 1) and int(minor) < 14):
+  raise RuntimeError(
+      'TensorFlow version >= 1.14 is required. Found ({}).'.format(
+          tf.__version__))  # pylint: disable=protected-access
+
+from tensorflow.python.framework import function
+from tensorflow.python.tpu import functional as tpu_functional
+from tensorflow.python.tpu.ops import tpu_ops
+from object_detection import exporter
+from object_detection.builders import model_builder
+from object_detection.tpu_exporters import utils
+
+ANCHORS = 'anchors'
+BOX_ENCODINGS = 'box_encodings'
+CLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'
+
+
+def get_prediction_tensor_shapes(pipeline_config):
+  """Gets static shapes of tensors by building the graph on CPU.
+
+  This function builds the graph on CPU and obtain static shapes of output
+  tensors from TPUPartitionedCall. Shapes information are later used for setting
+  shapes of tensors when TPU graphs are built. This is necessary because tensors
+  coming out of TPUPartitionedCall lose their shape information, which are
+  needed for a lot of CPU operations later.
+  Args:
+    pipeline_config: A TrainEvalPipelineConfig proto.
+
+  Returns:
+    A python dict of tensors' names and their shapes.
+  """
+  detection_model = model_builder.build(
+      pipeline_config.model, is_training=False)
+  _, input_tensors = exporter.input_placeholder_fn_map['image_tensor']()
+  inputs = tf.to_float(input_tensors)
+  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
+  prediction_dict = detection_model.predict(preprocessed_inputs,
+                                            true_image_shapes)
+
+  return {
+      BOX_ENCODINGS:
+          prediction_dict[BOX_ENCODINGS].shape.as_list(),
+      CLASS_PREDICTIONS_WITH_BACKGROUND:
+          prediction_dict[CLASS_PREDICTIONS_WITH_BACKGROUND].shape.as_list(),
+      ANCHORS:
+          prediction_dict[ANCHORS].shape.as_list(),
+  }
+
+
+def recover_shape(preprocessed_inputs, prediction_outputs, shapes_info):
+  """Recovers shape from TPUPartitionedCall.
+
+  Args:
+    preprocessed_inputs: 4D tensor, shaped (batch, channels, height, width)
+    prediction_outputs: Python list of tensors, in the following order -
+      box_encodings - 3D tensor, shaped (code_size, batch, num_anchors);
+      class_predictions_with_background - 3D tensor, shaped (num_classes + 1,
+      batch, num_anchors); anchors - 2D tensor, shaped (4, num_anchors)
+    shapes_info: Python dict of tensor shapes as lists.
+
+  Returns:
+    preprocessed_inputs: 4D tensor, shaped (batch, height, width, channels)
+    box_encodings: 3D tensor, shaped (batch, num_anchors, code_size)
+    class_predictions_with_background: 3D tensor,
+        shaped (batch, num_anchors, num_classes + 1)
+    anchors: 2D tensor, shaped (num_anchors, 4)
+  """
+  # Dimshuffle: (b, c, h, w) -> (b, h, w, c)
+  preprocessed_inputs = tf.transpose(preprocessed_inputs, perm=[0, 2, 3, 1])
+
+  box_encodings = tf.transpose(prediction_outputs[0], perm=[1, 2, 0])
+  # [None, None, detection_model._box_coder.code_size]
+  box_encodings.set_shape(shapes_info[BOX_ENCODINGS])
+
+  class_predictions_with_background = tf.transpose(
+      prediction_outputs[1], perm=[1, 2, 0])
+  # [None, None, num_classes + 1]
+  class_predictions_with_background.set_shape(
+      shapes_info[CLASS_PREDICTIONS_WITH_BACKGROUND])
+
+  anchors = tf.transpose(prediction_outputs[2], perm=[1, 0])
+  # [None, 4]
+  anchors.set_shape(shapes_info[ANCHORS])
+
+  return (preprocessed_inputs, box_encodings, class_predictions_with_background,
+          anchors)
+
+
+def build_graph(pipeline_config,
+                shapes_info,
+                input_type='encoded_image_string_tensor',
+                use_bfloat16=False):
+  """Builds TPU serving graph of ssd to be exported.
+
+  Args:
+    pipeline_config: A TrainEvalPipelineConfig proto.
+    shapes_info: A python dict of tensors' names and their shapes, returned by
+      `get_prediction_tensor_shapes()`.
+    input_type: One of
+                'encoded_image_string_tensor': a 1d tensor with dtype=tf.string
+                'image_tensor': a 4d tensor with dtype=tf.uint8
+                'tf_example': a 1d tensor with dtype=tf.string
+    use_bfloat16: If true, use tf.bfloat16 on TPU.
+
+  Returns:
+    placeholder_tensor: A placeholder tensor, type determined by `input_type`.
+    result_tensor_dict: A python dict of tensors' names and tensors.
+  """
+
+  detection_model = model_builder.build(
+      pipeline_config.model, is_training=False)
+
+  placeholder_tensor, input_tensors = \
+      exporter.input_placeholder_fn_map[input_type]()
+
+  inputs = tf.to_float(input_tensors)
+  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
+
+  # Dimshuffle: (b, h, w, c) -> (b, c, h, w)
+  # This is to avoid extra padding due to TPU memory layout:
+  # We swap larger dimensions in and smaller dimensions out, so that small
+  # dimensions don't get padded tens / hundreds times of its own size.
+  # This trick is applied to other similar tensors below.
+  preprocessed_inputs = tf.transpose(preprocessed_inputs, perm=[0, 3, 1, 2])
+  if use_bfloat16:
+    preprocessed_inputs = tf.cast(preprocessed_inputs, dtype=tf.bfloat16)
+
+  def predict_tpu_subgraph(preprocessed_inputs, true_image_shapes):
+    """Wraps over the CPU version of `predict()`.
+
+    This builds a same graph as the original `predict()`, manipulates
+    result tensors' dimensions to be memory efficient on TPU, and
+    returns them as list of tensors.
+
+    Args:
+      preprocessed_inputs: A 4D tensor of shape (batch, channels, height, width)
+      true_image_shapes: True image shapes tensor.
+
+    Returns:
+      A Python list of tensors:
+        box_encodings: 3D tensor of shape (code_size, batch_size, num_anchors)
+        class_predictions_with_background: 3D tensor,
+            shape (num_classes + 1, batch_size, num_anchors)
+        anchors: 2D tensor of shape (4, num_anchors)
+    """
+    # Dimshuffle: (b, c, h, w) -> (b, h, w, c)
+    preprocessed_inputs = tf.transpose(preprocessed_inputs, perm=[0, 2, 3, 1])
+    if use_bfloat16:
+      with tf.contrib.tpu.bfloat16_scope():
+        prediction_dict = detection_model.predict(preprocessed_inputs,
+                                                  true_image_shapes)
+    else:
+      prediction_dict = detection_model.predict(preprocessed_inputs,
+                                                true_image_shapes)
+
+    # Dimshuffle: (batch, anchors, depth) -> (depth, batch, anchors)
+    return [
+        tf.transpose(prediction_dict[BOX_ENCODINGS], perm=[2, 0, 1]),
+        tf.transpose(
+            prediction_dict[CLASS_PREDICTIONS_WITH_BACKGROUND], perm=[2, 0, 1]),
+        tf.transpose(prediction_dict[ANCHORS], perm=[1, 0]),
+    ]
+
+  @function.Defun(capture_resource_var_by_value=False)
+  def predict_tpu():
+    return tf.contrib.tpu.rewrite(predict_tpu_subgraph,
+                                  [preprocessed_inputs, true_image_shapes])
+
+  prediction_outputs = tpu_functional.TPUPartitionedCall(
+      args=predict_tpu.captured_inputs,
+      device_ordinal=tpu_ops.tpu_ordinal_selector(),
+      Tout=[o.type for o in predict_tpu.definition.signature.output_arg],
+      f=predict_tpu)
+
+  (preprocessed_inputs, box_encodings, class_predictions_with_background,
+   anchors) = recover_shape(preprocessed_inputs, prediction_outputs,
+                            shapes_info)
+
+  output_tensors = {
+      'preprocessed_inputs': preprocessed_inputs,
+      BOX_ENCODINGS: box_encodings,
+      CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_with_background,
+      ANCHORS: anchors,
+  }
+
+  if use_bfloat16:
+    output_tensors = utils.bfloat16_to_float32_nested(output_tensors)
+
+  postprocessed_tensors = detection_model.postprocess(output_tensors,
+                                                      true_image_shapes)
+  result_tensor_dict = exporter.add_output_tensor_nodes(postprocessed_tensors,
+                                                        'inference_op')
+
+  return placeholder_tensor, result_tensor_dict
diff --git a/research/object_detection/tpu_exporters/testdata/__init__.py b/research/object_detection/tpu_exporters/testdata/__init__.py
new file mode 100644
index 00000000..8b137891
--- /dev/null
+++ b/research/object_detection/tpu_exporters/testdata/__init__.py
@@ -0,0 +1 @@
+
diff --git a/research/object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config b/research/object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config
new file mode 100644
index 00000000..6fe8deba
--- /dev/null
+++ b/research/object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config
@@ -0,0 +1,80 @@
+# Faster R-CNN with Resnet-101 (v1), Atrous version
+# Trained on COCO, initialized from Imagenet classification checkpoint
+
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 600
+        max_dimension: 1024
+      }
+    }
+    feature_extractor {
+      type: 'faster_rcnn_resnet101'
+      first_stage_features_stride: 8
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 8
+        width_stride: 8
+      }
+    }
+    first_stage_atrous_rate: 2
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 14
+    maxpool_kernel_size: 2
+    maxpool_stride: 2
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 300
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+  }
+}
diff --git a/research/object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config b/research/object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config
new file mode 100644
index 00000000..107f5844
--- /dev/null
+++ b/research/object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config
@@ -0,0 +1,130 @@
+model {
+  ssd {
+    num_classes: 2
+    image_resizer {
+      fixed_shape_resizer {
+        height: 1280
+        width: 1280
+      }
+    }
+    feature_extractor {
+      type: "ssd_resnet50_v1_fpn"
+      depth_multiplier: 1.0
+      min_depth: 16
+      conv_hyperparams {
+        regularizer {
+          l2_regularizer {
+            weight: 0.000399999989895
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            mean: 0.0
+            stddev: 0.0299999993294
+          }
+        }
+        activation: RELU_6
+        batch_norm {
+          decay: 0.996999979019
+          center: true
+          scale: true
+          epsilon: 0.0010000000475
+          train: true
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+      fpn {
+        min_level: 2
+      }
+    }
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    box_predictor {
+      weight_shared_convolutional_box_predictor {
+        conv_hyperparams {
+          regularizer {
+            l2_regularizer {
+              weight: 0.000399999989895
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              mean: 0.0
+              stddev: 0.00999999977648
+            }
+          }
+          activation: RELU_6
+          batch_norm {
+            decay: 0.996999979019
+            scale: true
+            epsilon: 0.0010000000475
+          }
+        }
+        depth: 256
+        num_layers_before_predictor: 4
+        kernel_size: 3
+        class_prediction_bias_init: -4.59999990463
+      }
+    }
+    anchor_generator {
+      multiscale_anchor_generator {
+        min_level: 2
+        max_level: 7
+        anchor_scale: 3.0
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        scales_per_octave: 2
+      }
+    }
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 9.99999993923e-09
+        iou_threshold: 0.600000023842
+        max_detections_per_class: 300
+        max_total_detections: 600
+  use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+    normalize_loss_by_num_matches: true
+    loss {
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_loss {
+        weighted_sigmoid_focal {
+          gamma: 2.0
+          alpha: 0.25
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    encode_background_as_zeros: true
+    normalize_loc_loss_by_codesize: true
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+  }
+}
diff --git a/research/object_detection/tpu_exporters/utils.py b/research/object_detection/tpu_exporters/utils.py
new file mode 100644
index 00000000..f9bf566d
--- /dev/null
+++ b/research/object_detection/tpu_exporters/utils.py
@@ -0,0 +1,50 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Utilities for TPU inference."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+def bfloat16_to_float32(tensor):
+  """Converts a tensor to tf.float32 only if it is tf.bfloat16."""
+  if tensor.dtype == tf.bfloat16:
+    return tf.cast(tensor, dtype=tf.float32)
+  else:
+    return tensor
+
+
+def bfloat16_to_float32_nested(bfloat16_tensor_dict):
+  """Converts bfloat16 tensors in a nested structure to float32.
+
+  Other tensors not of dtype bfloat16 will be left as is.
+
+  Args:
+    bfloat16_tensor_dict: A Python dict, values being Tensor or Python
+      list/tuple of Tensor.
+
+  Returns:
+    A Python dict with the same structure as `bfloat16_tensor_dict`,
+    with all bfloat16 tensors converted to float32.
+  """
+  float32_tensor_dict = {}
+  for k, v in bfloat16_tensor_dict.items():
+    if isinstance(v, tf.Tensor):
+      float32_tensor_dict[k] = bfloat16_to_float32(v)
+    elif isinstance(v, (list, tuple)):
+      float32_tensor_dict[k] = [bfloat16_to_float32(t) for t in v]
+  return float32_tensor_dict
diff --git a/research/object_detection/tpu_exporters/utils_test.py b/research/object_detection/tpu_exporters/utils_test.py
new file mode 100644
index 00000000..74877290
--- /dev/null
+++ b/research/object_detection/tpu_exporters/utils_test.py
@@ -0,0 +1,55 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Test for Utility functions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from object_detection.tpu_exporters import utils
+
+
+class UtilsTest(tf.test.TestCase):
+
+  def testBfloat16ToFloat32(self):
+    bfloat16_tensor = tf.random.uniform([2, 3], dtype=tf.bfloat16)
+    float32_tensor = utils.bfloat16_to_float32(bfloat16_tensor)
+    self.assertEqual(float32_tensor.dtype, tf.float32)
+
+  def testOtherDtypesNotConverted(self):
+    int32_tensor = tf.ones([2, 3], dtype=tf.int32)
+    converted_tensor = utils.bfloat16_to_float32(int32_tensor)
+    self.assertEqual(converted_tensor.dtype, tf.int32)
+
+  def testBfloat16ToFloat32Nested(self):
+    tensor_dict = {
+        'key1': tf.random.uniform([2, 3], dtype=tf.bfloat16),
+        'key2': [
+            tf.random.uniform([1, 2], dtype=tf.bfloat16) for _ in range(3)
+        ],
+        'key3': tf.ones([2, 3], dtype=tf.int32),
+    }
+    tensor_dict = utils.bfloat16_to_float32_nested(tensor_dict)
+
+    self.assertEqual(tensor_dict['key1'].dtype, tf.float32)
+    for t in tensor_dict['key2']:
+      self.assertEqual(t.dtype, tf.float32)
+    self.assertEqual(tensor_dict['key3'].dtype, tf.int32)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index a88782ab..82acd332 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -73,7 +73,9 @@ def get_spatial_image_size(image_resizer_config):
       return [image_resizer_config.keep_aspect_ratio_resizer.max_dimension] * 2
     else:
       return [-1, -1]
-  if image_resizer_config.HasField("identity_resizer"):
+  if image_resizer_config.HasField(
+      "identity_resizer") or image_resizer_config.HasField(
+          "conditional_shape_resizer"):
     return [-1, -1]
   raise ValueError("Unknown image resizer type.")
 
@@ -856,11 +858,6 @@ def _update_train_steps(configs, train_steps):
   configs["train_config"].num_steps = int(train_steps)
 
 
-def _update_eval_steps(configs, eval_steps):
-  """Updates `configs` to reflect new number of eval steps per evaluation."""
-  configs["eval_config"].num_examples = int(eval_steps)
-
-
 def _update_all_eval_input_configs(configs, field, value):
   """Updates the content of `field` with `value` for all eval input configs."""
   for eval_input_config in configs["eval_input_configs"]:
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index 92679e92..392467a1 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -612,6 +612,12 @@ class ConfigUtilTest(tf.test.TestCase):
     image_shape = config_util.get_spatial_image_size(image_resizer_config)
     self.assertAllEqual(image_shape, [-1, -1])
 
+  def testGetSpatialImageSizeFromConditionalShapeResizer(self):
+    image_resizer_config = image_resizer_pb2.ImageResizer()
+    image_resizer_config.conditional_shape_resizer.size_threshold = 100
+    image_shape = config_util.get_spatial_image_size(image_resizer_config)
+    self.assertAllEqual(image_shape, [-1, -1])
+
   def testEvalShuffle(self):
     """Tests that `eval_shuffle` keyword arguments are applied correctly."""
     original_shuffle = True
diff --git a/research/object_detection/utils/model_util.py b/research/object_detection/utils/model_util.py
new file mode 100644
index 00000000..16789c18
--- /dev/null
+++ b/research/object_detection/utils/model_util.py
@@ -0,0 +1,88 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Utility functions for manipulating Keras models."""
+
+import tensorflow as tf
+
+
+def extract_submodel(model, inputs, outputs, name=None):
+  """Extracts a section of a Keras model into a new model.
+
+  This method walks an existing model from the specified outputs back to the
+  specified inputs in order to construct a new model containing only a portion
+  of the old model, while sharing the layers and weights with the original
+  model.
+
+  WARNING: This method does not work for submodels containing layers that have
+  been used multiple times in the original model, or in other models beyond
+  the original model. (E.g. does not work for submodels that contain layers that
+  use shared weights). This also means that multiple overlapping submodels
+  cannot be extracted from the same model.
+
+  It also relies on recursion and will hit python's recursion limit for large
+  submodels.
+
+  Args:
+    model: The existing Keras model this method extracts a submodel from.
+    inputs: The layer inputs in the existing model that start the submodel
+    outputs: The layer outputs in the existing model that should be output by
+      the submodel
+    name: The name for the extracted model
+
+  Returns:
+    The extracted submodel specified by the given inputs and outputs
+  """
+  output_to_layer = {}
+  output_to_layer_input = {}
+  for layer in model.layers:
+    layer_output = layer.output
+    layer_inputs = layer.input
+    output_to_layer[layer_output] = layer
+    output_to_layer_input[layer_output] = layer_inputs
+
+  model_inputs_dict = {}
+  memoized_results = {}
+
+  # Relies on recursion, very low limit in python
+  def _recurse_in_model(tensor):
+    """Walk the existing model recursively to copy a submodel."""
+    if tensor in memoized_results:
+      return memoized_results[tensor]
+    if (tensor == inputs) or (isinstance(inputs, list) and tensor in inputs):
+      if tensor not in model_inputs_dict:
+        model_inputs_dict[tensor] = tf.keras.layers.Input(tensor=tensor)
+      out = model_inputs_dict[tensor]
+    else:
+      cur_inputs = output_to_layer_input[tensor]
+      cur_layer = output_to_layer[tensor]
+      if isinstance(cur_inputs, list):
+        out = cur_layer([_recurse_in_model(inp) for inp in cur_inputs])
+      else:
+        out = cur_layer(_recurse_in_model(cur_inputs))
+    memoized_results[tensor] = out
+    return out
+
+  if isinstance(outputs, list):
+    model_outputs = [_recurse_in_model(tensor) for tensor in outputs]
+  else:
+    model_outputs = _recurse_in_model(outputs)
+
+  if isinstance(inputs, list):
+    model_inputs = [model_inputs_dict[tensor] for tensor in inputs]
+  else:
+    model_inputs = model_inputs_dict[inputs]
+
+  return tf.keras.Model(inputs=model_inputs, outputs=model_outputs, name=name)
diff --git a/research/object_detection/utils/model_util_test.py b/research/object_detection/utils/model_util_test.py
new file mode 100644
index 00000000..16fcc1a8
--- /dev/null
+++ b/research/object_detection/utils/model_util_test.py
@@ -0,0 +1,54 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Test utility functions for manipulating Keras models."""
+
+import tensorflow as tf
+
+from object_detection.utils import model_util
+
+
+class ExtractSubmodelUtilTest(tf.test.TestCase):
+
+  def test_simple_model(self):
+    inputs = tf.keras.Input(shape=(256,))  # Returns a placeholder tensor
+
+    # A layer instance is callable on a tensor, and returns a tensor.
+    x = tf.keras.layers.Dense(128, activation='relu', name='a')(inputs)
+    x = tf.keras.layers.Dense(64, activation='relu', name='b')(x)
+    x = tf.keras.layers.Dense(32, activation='relu', name='c')(x)
+    x = tf.keras.layers.Dense(16, activation='relu', name='d')(x)
+    x = tf.keras.layers.Dense(8, activation='relu', name='e')(x)
+    predictions = tf.keras.layers.Dense(10, activation='softmax')(x)
+
+    model = tf.keras.Model(inputs=inputs, outputs=predictions)
+
+    new_in = model.get_layer(
+        name='b').input
+    new_out = model.get_layer(
+        name='d').output
+
+    new_model = model_util.extract_submodel(
+        model=model,
+        inputs=new_in,
+        outputs=new_out)
+
+    batch_size = 3
+    ones = tf.ones((batch_size, 128))
+    final_out = new_model(ones)
+    self.assertAllEqual(final_out.shape, (batch_size, 16))
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 32609afa..cb51411c 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -16,7 +16,6 @@
 """A module for helper tensorflow ops."""
 import collections
 import math
-import numpy as np
 import six
 
 import tensorflow as tf
@@ -53,17 +52,19 @@ def normalized_to_image_coordinates(normalized_boxes, image_shape,
   """Converts a batch of boxes from normal to image coordinates.
 
   Args:
-    normalized_boxes: a float32 tensor of shape [None, num_boxes, 4] in
-      normalized coordinates.
-    image_shape: a float32 tensor of shape [4] containing the image shape.
+    normalized_boxes: a tensor of shape [None, num_boxes, 4] in
+      normalized coordinates. The dtype of this tensor must support tf.mul.
+    image_shape: a tensor of shape [4] containing the image shape, with same
+      dtype as `normalized_boxes`.
     parallel_iterations: parallelism for the map_fn op.
 
   Returns:
-    absolute_boxes: a float32 tensor of shape [None, num_boxes, 4] containing
-      the boxes in image coordinates.
+    absolute_boxes: a tensor of shape [None, num_boxes, 4] containing
+      the boxes in image coordinates, with same
+      dtype as `normalized_boxes`.
   """
-  x_scale = tf.cast(image_shape[2], tf.float32)
-  y_scale = tf.cast(image_shape[1], tf.float32)
+  x_scale = tf.cast(image_shape[2], normalized_boxes.dtype)
+  y_scale = tf.cast(image_shape[1], normalized_boxes.dtype)
   def _to_absolute_coordinates(normalized_boxes):
     y_min, x_min, y_max, x_max = tf.split(
         value=normalized_boxes, num_or_size_splits=4, axis=1)
@@ -77,7 +78,7 @@ def normalized_to_image_coordinates(normalized_boxes, image_shape,
   absolute_boxes = shape_utils.static_or_dynamic_map_fn(
       _to_absolute_coordinates,
       elems=(normalized_boxes),
-      dtype=tf.float32,
+      dtype=normalized_boxes.dtype,
       parallel_iterations=parallel_iterations,
       back_prop=True)
   return absolute_boxes
@@ -881,27 +882,33 @@ def merge_boxes_with_multiple_labels(boxes,
     merged_box_indices = tf.unsorted_segment_min(
         tf.range(num_boxes), unique_indices, num_unique_boxes)
     merged_boxes = tf.gather(boxes, merged_box_indices)
+    unique_indices = tf.to_int64(unique_indices)
+    classes = tf.to_int64(classes)
 
     def map_box_encodings(i):
       """Produces box K-hot and score encodings for each class index."""
       box_mask = tf.equal(
-          unique_indices, i * tf.ones(num_boxes, dtype=tf.int32))
+          unique_indices, i * tf.ones(num_boxes, dtype=tf.int64))
       box_mask = tf.reshape(box_mask, [-1])
       box_indices = tf.boolean_mask(classes, box_mask)
       box_confidences = tf.boolean_mask(confidences, box_mask)
       box_class_encodings = tf.sparse_to_dense(
-          box_indices, [num_classes], 1, validate_indices=False)
+          box_indices, [num_classes], tf.constant(1, dtype=tf.int64),
+          validate_indices=False)
       box_confidence_encodings = tf.sparse_to_dense(
           box_indices, [num_classes], box_confidences, validate_indices=False)
       return box_class_encodings, box_confidence_encodings
 
+    # Important to avoid int32 here since there is no GPU kernel for int32.
+    # int64 and float32 are fine.
     class_encodings, confidence_encodings = tf.map_fn(
         map_box_encodings,
-        tf.range(num_unique_boxes),
+        tf.range(tf.to_int64(num_unique_boxes)),
         back_prop=False,
-        dtype=(tf.int32, tf.float32))
+        dtype=(tf.int64, tf.float32))
 
     merged_boxes = tf.reshape(merged_boxes, [-1, 4])
+    class_encodings = tf.to_int32(class_encodings)
     class_encodings = tf.reshape(class_encodings, [-1, num_classes])
     confidence_encodings = tf.reshape(confidence_encodings, [-1, num_classes])
     merged_box_indices = tf.reshape(merged_box_indices, [-1])
@@ -1003,8 +1010,8 @@ def matmul_crop_and_resize(image, boxes, crop_size, scope=None):
     2) Only XLA supported operations are used (e.g., matrix multiplication).
     3) There is no `box_indices` argument --- to run this op on multiple images,
       one must currently call this op independently on each image.
-    4) All shapes and the `crop_size` parameter are assumed to be statically
-      defined.  Moreover, the number of boxes must be strictly nonzero.
+    4) The `crop_size` parameter is assumed to be statically defined.
+      Moreover, the number of boxes must be strictly nonzero.
 
   Args:
     image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,
@@ -1029,41 +1036,20 @@ def matmul_crop_and_resize(image, boxes, crop_size, scope=None):
 
   Returns:
     A 5-D tensor of shape `[batch, num_boxes, crop_height, crop_width, depth]`
-
-  Raises:
-    ValueError: if image tensor does not have shape
-      `[batch, image_height, image_width, depth]` and all dimensions statically
-      defined.
-    ValueError: if boxes tensor does not have shape `[batch, num_boxes, 4]`
-      where num_boxes > 0.
-    ValueError: if crop_size is not a list of two positive integers
   """
-  img_shape = image.shape.as_list()
-  boxes_shape = boxes.shape.as_list()
-  _, img_height, img_width, _ = img_shape
-  if not isinstance(crop_size, list) or len(crop_size) != 2:
-    raise ValueError('`crop_size` must be a list of length 2')
-  dimensions = img_shape + crop_size + boxes_shape
-  if not all([isinstance(dim, int) for dim in dimensions]):
-    raise ValueError('all input shapes must be statically defined')
-  if len(boxes_shape) != 3 or boxes_shape[2] != 4:
-    raise ValueError('`boxes` should have shape `[batch, num_boxes, 4]`')
-  if len(img_shape) != 4:
-    raise ValueError('image should have shape '
-                     '`[batch, image_height, image_width, depth]`')
-  num_crops = boxes_shape[0]
-  if not num_crops > 0:
-    raise ValueError('number of boxes must be > 0')
-  if not (crop_size[0] > 0 and crop_size[1] > 0):
-    raise ValueError('`crop_size` must be a list of two positive integers.')
+  img_shape = tf.shape(image)
+  img_height = img_shape[1]
+  img_width = img_shape[2]
 
   def _lin_space_weights(num, img_size):
     if num > 1:
-      start_weights = tf.linspace(img_size - 1.0, 0.0, num)
-      stop_weights = img_size - 1 - start_weights
+      start_weights = tf.linspace(tf.to_float(img_size) - 1.0, 0.0, num)
+      stop_weights = tf.to_float(img_size) - 1.0 - start_weights
     else:
-      start_weights = tf.constant(num * [.5 * (img_size - 1)], dtype=tf.float32)
-      stop_weights = tf.constant(num * [.5 * (img_size - 1)], dtype=tf.float32)
+      start_weights = tf.ones([num], dtype=tf.float32) * \
+          .5 * (tf.to_float(img_size) - 1.0)
+      stop_weights = tf.ones([num], dtype=tf.float32) * \
+          .5 * (tf.to_float(img_size) - 1.0)
     return (start_weights, stop_weights)
 
   with tf.name_scope(scope, 'MatMulCropAndResize'):
@@ -1076,19 +1062,19 @@ def matmul_crop_and_resize(image, boxes, crop_size, scope=None):
     [y1, x1, y2, x2] = tf.unstack(boxes, axis=2)
 
     # Pixel centers of input image and grid points along height and width
-    image_idx_h = tf.constant(
-        np.reshape(np.arange(img_height), (1, 1, 1, img_height)),
+    image_idx_h = tf.cast(
+        tf.reshape(tf.range(img_height), (1, 1, 1, img_height)),
         dtype=boxes.dtype)
-    image_idx_w = tf.constant(
-        np.reshape(np.arange(img_width), (1, 1, 1, img_width)),
+    image_idx_w = tf.cast(
+        tf.reshape(tf.range(img_width), (1, 1, 1, img_width)),
         dtype=boxes.dtype)
     grid_pos_h = tf.expand_dims(
-        tf.einsum('ab,c->abc', y1, y1_weights) + tf.einsum(
-            'ab,c->abc', y2, y2_weights),
+        tf.einsum('ab,c->abc', y1, y1_weights) +
+        tf.einsum('ab,c->abc', y2, y2_weights),
         axis=3)
     grid_pos_w = tf.expand_dims(
-        tf.einsum('ab,c->abc', x1, x1_weights) + tf.einsum(
-            'ab,c->abc', x2, x2_weights),
+        tf.einsum('ab,c->abc', x1, x1_weights) +
+        tf.einsum('ab,c->abc', x2, x2_weights),
         axis=3)
 
     # Create kernel matrices of pairwise kernel evaluations between pixel
@@ -1096,7 +1082,8 @@ def matmul_crop_and_resize(image, boxes, crop_size, scope=None):
     kernel_h = tf.nn.relu(1 - tf.abs(image_idx_h - grid_pos_h))
     kernel_w = tf.nn.relu(1 - tf.abs(image_idx_w - grid_pos_w))
 
-    # Compute matrix multiplication between the spatial dimensions of the image
+    # Compute matrix multiplication between
+    # the spatial dimensions of the image
     # and height-wise kernel using einsum.
     intermediate_image = tf.einsum('abci,aiop->abcop', kernel_h, image)
     # Compute matrix multiplication between the spatial dimensions of the
@@ -1124,6 +1111,58 @@ def native_crop_and_resize(image, boxes, crop_size, scope=None):
     return tf.reshape(cropped_regions, final_shape)
 
 
+def bfloat16_to_float32_nested(tensor_nested):
+  """Convert float32 tensors in a nested structure to bfloat16.
+
+  Args:
+    tensor_nested: A Python dict, values being Tensor or Python list/tuple of
+      Tensor.
+
+  Returns:
+    A Python dict with the same structure as `tensor_dict`,
+    with all bfloat16 tensors converted to float32.
+  """
+  if isinstance(tensor_nested, tf.Tensor):
+    if tensor_nested.dtype == tf.bfloat16:
+      return tf.cast(tensor_nested, dtype=tf.float32)
+    else:
+      return tensor_nested
+  elif isinstance(tensor_nested, (list, tuple)):
+    out_tensor_dict = [bfloat16_to_float32_nested(t) for t in tensor_nested]
+  elif isinstance(tensor_nested, dict):
+    out_tensor_dict = {
+        k: bfloat16_to_float32_nested(v) for k, v in tensor_nested.items()
+    }
+  return out_tensor_dict
+
+
+def gather_with_padding_values(input_tensor, indices, padding_value):
+  """Gathers elements from tensor and pads `padding_value` for ignore indices.
+
+  Gathers elements from `input_tensor` based on `indices`. If there are ignore
+  indices (which are "-1"s) in `indices`, `padding_value` will be gathered for
+  those positions.
+
+  Args:
+    input_tensor: A N-D tensor of shape [M, d_1, d_2 .. d_(N-1)] to gather
+      values from.
+    indices: A 1-D tensor in which each element is either an index in the
+      first dimension of input_tensor or -1.
+    padding_value: A (N-1)-D tensor of shape [d_1, d_2 .. d_(N-1)] which will be
+      used as gathered value for each ignore index in `indices`.
+
+  Returns:
+    gathered_tensor: A tensor of shape [L, d_1, d_2 .. d_(N-1)] containing
+      values gathered from input_tensor. The first dimension L is equal to the
+      length of `indices`.
+  """
+  padding_value = tf.expand_dims(padding_value, axis=0)
+  input_tensor = tf.concat([padding_value, input_tensor], axis=0)
+  gather_indices = indices + 1
+  gathered_tensor = tf.gather(input_tensor, gather_indices)
+  return gathered_tensor
+
+
 
 
 
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index 67b5a37d..c4f82e70 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -1223,6 +1223,35 @@ class MergeBoxesWithMultipleLabelsTest(tf.test.TestCase):
       self.assertAllEqual(np_merged_confidences.shape, [0, 5])
       self.assertAllEqual(np_merged_box_indices.shape, [0])
 
+  def testMergeBoxesWithMultipleLabelsUsesInt64(self):
+    boxes = tf.constant(
+        [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],
+         [0.25, 0.25, 0.75, 0.75]],
+        dtype=tf.float32)
+    class_indices = tf.constant([0, 4, 2], dtype=tf.int32)
+    class_confidences = tf.constant([0.8, 0.2, 0.1], dtype=tf.float32)
+    num_classes = 5
+    ops.merge_boxes_with_multiple_labels(
+        boxes, class_indices, class_confidences, num_classes)
+
+    graph = tf.get_default_graph()
+
+    def assert_dtype_is_int64(op_name):
+      op = graph.get_operation_by_name(op_name)
+      self.assertEqual(op.get_attr('dtype'), tf.int64)
+
+    def assert_t_is_int64(op_name):
+      op = graph.get_operation_by_name(op_name)
+      self.assertEqual(op.get_attr('T'), tf.int64)
+
+    assert_dtype_is_int64('map/TensorArray')
+    assert_dtype_is_int64('map/TensorArray_1')
+    assert_dtype_is_int64('map/while/TensorArrayReadV3')
+    assert_t_is_int64('map/while/TensorArrayWrite/TensorArrayWriteV3')
+    assert_t_is_int64(
+        'map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3')
+    assert_dtype_is_int64('map/TensorArrayStack/TensorArrayGatherV3')
+
 
 class NearestNeighborUpsamplingTest(test_case.TestCase):
 
@@ -1470,6 +1499,56 @@ class OpsTestCropAndResize(test_case.TestCase):
     self.assertAllClose(crop_output, expected_output)
 
 
+class TestBfloat16ToFloat32(test_case.TestCase):
+
+  def test_convert_list(self):
+    var_list = [
+        tf.constant([1.], dtype=tf.bfloat16),
+        tf.constant([2], dtype=tf.int32)
+    ]
+    casted_var_list = ops.bfloat16_to_float32_nested(var_list)
+    self.assertEqual(casted_var_list[0].dtype, tf.float32)
+    self.assertEqual(casted_var_list[1].dtype, tf.int32)
+
+  def test_convert_tensor_dict(self):
+    tensor_dict = {
+        'key1': tf.constant([1.], dtype=tf.bfloat16),
+        'key2': [
+            tf.constant([0.5], dtype=tf.bfloat16),
+            tf.constant([7], dtype=tf.int32),
+        ],
+        'key3': tf.constant([2], dtype=tf.uint8),
+    }
+    tensor_dict = ops.bfloat16_to_float32_nested(tensor_dict)
+
+    self.assertEqual(tensor_dict['key1'].dtype, tf.float32)
+    self.assertEqual(tensor_dict['key2'][0].dtype, tf.float32)
+    self.assertEqual(tensor_dict['key2'][1].dtype, tf.int32)
+    self.assertEqual(tensor_dict['key3'].dtype, tf.uint8)
+
+
+class TestGatherWithPaddingValues(test_case.TestCase):
+
+  def test_gather_with_padding_values(self):
+    indices = tf.constant([1, -1, 0, -1])
+    input_tensor = tf.constant([[0, 0, 0.1, 0.1], [0, 0, 0.2, 0.2]],
+                               dtype=tf.float32)
+    expected_gathered_tensor = [
+        [0, 0, 0.2, 0.2],
+        [0, 0, 0, 0],
+        [0, 0, 0.1, 0.1],
+        [0, 0, 0, 0],
+    ]
+    gathered_tensor = ops.gather_with_padding_values(
+        input_tensor,
+        indices=indices,
+        padding_value=tf.zeros_like(input_tensor[0]))
+    self.assertEqual(gathered_tensor.dtype, tf.float32)
+    with self.test_session():
+      gathered_tensor_np = gathered_tensor.eval()
+    self.assertAllClose(expected_gathered_tensor, gathered_tensor_np)
+
+
 
 
 
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index 33226a1c..d8c2b2ce 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -21,7 +21,6 @@ The functions do not return a value, instead they modify the image itself.
 """
 import abc
 import collections
-import functools
 # Set headless-friendly backend.
 import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
 import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top
@@ -65,6 +64,34 @@ STANDARD_COLORS = [
 ]
 
 
+def _get_multiplier_for_color_randomness():
+  """Returns a multiplier to get semi-random colors from successive indices.
+
+  This function computes a prime number, p, in the range [2, 17] that:
+  - is closest to len(STANDARD_COLORS) / 10
+  - does not divide len(STANDARD_COLORS)
+
+  If no prime numbers in that range satisfy the constraints, p is returned as 1.
+
+  Once p is established, it can be used as a multiplier to select
+  non-consecutive colors from STANDARD_COLORS:
+  colors = [(p * i) % len(STANDARD_COLORS) for i in range(20)]
+  """
+  num_colors = len(STANDARD_COLORS)
+  prime_candidates = [5, 7, 11, 13, 17]
+
+  # Remove all prime candidates that divide the number of colors.
+  prime_candidates = [p for p in prime_candidates if num_colors % p]
+  if not prime_candidates:
+    return 1
+
+  # Return the closest prime number to num_colors / 10.
+  abs_distance = [np.abs(num_colors / 10. - p) for p in prime_candidates]
+  num_candidates = len(abs_distance)
+  inds = [i for _, i in sorted(zip(abs_distance, range(num_candidates)))]
+  return prime_candidates[inds[0]]
+
+
 def save_image_array_as_png(image, output_path):
   """Saves an image (represented as a numpy array) to PNG.
 
@@ -266,46 +293,98 @@ def draw_bounding_boxes_on_image(image,
                                boxes[i, 3], color, thickness, display_str_list)
 
 
-def _visualize_boxes(image, boxes, classes, scores, category_index, **kwargs):
-  return visualize_boxes_and_labels_on_image_array(
-      image, boxes, classes, scores, category_index=category_index, **kwargs)
+def create_visualization_fn(category_index, include_masks=False,
+                            include_keypoints=False, include_track_ids=False,
+                            **kwargs):
+  """Constructs a visualization function that can be wrapped in a py_func.
+
+  py_funcs only accept positional arguments. This function returns a suitable
+  function with the correct positional argument mapping. The positional
+  arguments in order are:
+  0: image
+  1: boxes
+  2: classes
+  3: scores
+  [4-6]: masks (optional)
+  [4-6]: keypoints (optional)
+  [4-6]: track_ids (optional)
+
+  -- Example 1 --
+  vis_only_masks_fn = create_visualization_fn(category_index,
+    include_masks=True, include_keypoints=False, include_track_ids=False,
+    **kwargs)
+  image = tf.py_func(vis_only_masks_fn,
+                     inp=[image, boxes, classes, scores, masks],
+                     Tout=tf.uint8)
+
+  -- Example 2 --
+  vis_masks_and_track_ids_fn = create_visualization_fn(category_index,
+    include_masks=True, include_keypoints=False, include_track_ids=True,
+    **kwargs)
+  image = tf.py_func(vis_masks_and_track_ids_fn,
+                     inp=[image, boxes, classes, scores, masks, track_ids],
+                     Tout=tf.uint8)
 
+  Args:
+    category_index: a dict that maps integer ids to category dicts. e.g.
+      {1: {1: 'dog'}, 2: {2: 'cat'}, ...}
+    include_masks: Whether masks should be expected as a positional argument in
+      the returned function.
+    include_keypoints: Whether keypoints should be expected as a positional
+      argument in the returned function.
+    include_track_ids: Whether track ids should be expected as a positional
+      argument in the returned function.
+    **kwargs: Additional kwargs that will be passed to
+      visualize_boxes_and_labels_on_image_array.
 
-def _visualize_boxes_and_masks(image, boxes, classes, scores, masks,
-                               category_index, **kwargs):
-  return visualize_boxes_and_labels_on_image_array(
-      image,
-      boxes,
-      classes,
-      scores,
-      category_index=category_index,
-      instance_masks=masks,
-      **kwargs)
-
+  Returns:
+    Returns a function that only takes tensors as positional arguments.
+  """
 
-def _visualize_boxes_and_keypoints(image, boxes, classes, scores, keypoints,
-                                   category_index, **kwargs):
-  return visualize_boxes_and_labels_on_image_array(
-      image,
-      boxes,
-      classes,
-      scores,
-      category_index=category_index,
-      keypoints=keypoints,
-      **kwargs)
+  def visualization_py_func_fn(*args):
+    """Visualization function that can be wrapped in a tf.py_func.
 
+    Args:
+      *args: First 4 positional arguments must be:
+        image - uint8 numpy array with shape (img_height, img_width, 3).
+        boxes - a numpy array of shape [N, 4].
+        classes - a numpy array of shape [N].
+        scores - a numpy array of shape [N] or None.
+        -- Optional positional arguments --
+        instance_masks - a numpy array of shape [N, image_height, image_width].
+        keypoints - a numpy array of shape [N, num_keypoints, 2].
+        track_ids - a numpy array of shape [N] with unique track ids.
 
-def _visualize_boxes_and_masks_and_keypoints(
-    image, boxes, classes, scores, masks, keypoints, category_index, **kwargs):
-  return visualize_boxes_and_labels_on_image_array(
-      image,
-      boxes,
-      classes,
-      scores,
-      category_index=category_index,
-      instance_masks=masks,
-      keypoints=keypoints,
-      **kwargs)
+    Returns:
+      uint8 numpy array with shape (img_height, img_width, 3) with overlaid
+      boxes.
+    """
+    image = args[0]
+    boxes = args[1]
+    classes = args[2]
+    scores = args[3]
+    masks = keypoints = track_ids = None
+    pos_arg_ptr = 4  # Positional argument for first optional tensor (masks).
+    if include_masks:
+      masks = args[pos_arg_ptr]
+      pos_arg_ptr += 1
+    if include_keypoints:
+      keypoints = args[pos_arg_ptr]
+      pos_arg_ptr += 1
+    if include_track_ids:
+      track_ids = args[pos_arg_ptr]
+
+    return visualize_boxes_and_labels_on_image_array(
+        image,
+        boxes,
+        classes,
+        scores,
+        category_index=category_index,
+        instance_masks=masks,
+        keypoints=keypoints,
+        track_ids=track_ids,
+        **kwargs)
+  return visualization_py_func_fn
 
 
 def _resize_original_image(image, image_shape):
@@ -327,6 +406,7 @@ def draw_bounding_boxes_on_image_tensors(images,
                                          true_image_shape=None,
                                          instance_masks=None,
                                          keypoints=None,
+                                         track_ids=None,
                                          max_boxes_to_draw=20,
                                          min_score_thresh=0.2,
                                          use_normalized_coordinates=True):
@@ -350,6 +430,9 @@ def draw_bounding_boxes_on_image_tensors(images,
       instance masks.
     keypoints: A 4D float32 tensor of shape [N, max_detection, num_keypoints, 2]
       with keypoints.
+    track_ids: [N, max_detections] int32 tensor of unique tracks ids (i.e.
+      instance ids for each object). If provided, the color-coding of boxes is
+      dictated by these ids, and not classes.
     max_boxes_to_draw: Maximum number of boxes to draw on an image. Default 20.
     min_score_thresh: Minimum score threshold for visualization. Default 0.2.
     use_normalized_coordinates: Whether to assume boxes and kepoints are in
@@ -380,40 +463,20 @@ def draw_bounding_boxes_on_image_tensors(images,
   else:
     original_shapes = original_image_spatial_shape
 
-  if instance_masks is not None and keypoints is None:
-    visualize_boxes_fn = functools.partial(
-        _visualize_boxes_and_masks,
-        category_index=category_index,
-        **visualization_keyword_args)
-    elems = [
-        true_shapes, original_shapes, images, boxes, classes, scores,
-        instance_masks
-    ]
-  elif instance_masks is None and keypoints is not None:
-    visualize_boxes_fn = functools.partial(
-        _visualize_boxes_and_keypoints,
-        category_index=category_index,
-        **visualization_keyword_args)
-    elems = [
-        true_shapes, original_shapes, images, boxes, classes, scores, keypoints
-    ]
-  elif instance_masks is not None and keypoints is not None:
-    visualize_boxes_fn = functools.partial(
-        _visualize_boxes_and_masks_and_keypoints,
-        category_index=category_index,
-        **visualization_keyword_args)
-    elems = [
-        true_shapes, original_shapes, images, boxes, classes, scores,
-        instance_masks, keypoints
-    ]
-  else:
-    visualize_boxes_fn = functools.partial(
-        _visualize_boxes,
-        category_index=category_index,
-        **visualization_keyword_args)
-    elems = [
-        true_shapes, original_shapes, images, boxes, classes, scores
-    ]
+  visualize_boxes_fn = create_visualization_fn(
+      category_index,
+      include_masks=instance_masks is not None,
+      include_keypoints=keypoints is not None,
+      include_track_ids=track_ids is not None,
+      **visualization_keyword_args)
+
+  elems = [true_shapes, original_shapes, images, boxes, classes, scores]
+  if instance_masks is not None:
+    elems.append(instance_masks)
+  if keypoints is not None:
+    elems.append(keypoints)
+  if track_ids is not None:
+    elems.append(track_ids)
 
   def draw_boxes(image_and_detections):
     """Draws boxes on image."""
@@ -627,6 +690,7 @@ def visualize_boxes_and_labels_on_image_array(
     instance_masks=None,
     instance_boundaries=None,
     keypoints=None,
+    track_ids=None,
     use_normalized_coordinates=False,
     max_boxes_to_draw=20,
     min_score_thresh=.5,
@@ -634,7 +698,8 @@ def visualize_boxes_and_labels_on_image_array(
     line_thickness=4,
     groundtruth_box_visualization_color='black',
     skip_scores=False,
-    skip_labels=False):
+    skip_labels=False,
+    skip_track_ids=False):
   """Overlay labeled boxes on an image with formatted scores and label names.
 
   This function groups boxes that correspond to the same location
@@ -658,6 +723,9 @@ def visualize_boxes_and_labels_on_image_array(
       with values ranging between 0 and 1, can be None.
     keypoints: a numpy array of shape [N, num_keypoints, 2], can
       be None
+    track_ids: a numpy array of shape [N] with unique track ids. If provided,
+      color-coding of boxes will be determined by these ids, and not the class
+      indices.
     use_normalized_coordinates: whether boxes is to be interpreted as
       normalized coordinates or not.
     max_boxes_to_draw: maximum number of boxes to visualize.  If None, draw
@@ -671,6 +739,7 @@ def visualize_boxes_and_labels_on_image_array(
       boxes
     skip_scores: whether to skip score when drawing a single detection
     skip_labels: whether to skip label when drawing a single detection
+    skip_track_ids: whether to skip track id when drawing a single detection
 
   Returns:
     uint8 numpy array with shape (img_height, img_width, 3) with overlaid boxes.
@@ -682,6 +751,7 @@ def visualize_boxes_and_labels_on_image_array(
   box_to_instance_masks_map = {}
   box_to_instance_boundaries_map = {}
   box_to_keypoints_map = collections.defaultdict(list)
+  box_to_track_ids_map = {}
   if not max_boxes_to_draw:
     max_boxes_to_draw = boxes.shape[0]
   for i in range(min(max_boxes_to_draw, boxes.shape[0])):
@@ -693,6 +763,8 @@ def visualize_boxes_and_labels_on_image_array(
         box_to_instance_boundaries_map[box] = instance_boundaries[i]
       if keypoints is not None:
         box_to_keypoints_map[box].extend(keypoints[i])
+      if track_ids is not None:
+        box_to_track_ids_map[box] = track_ids[i]
       if scores is None:
         box_to_color_map[box] = groundtruth_box_visualization_color
       else:
@@ -709,9 +781,18 @@ def visualize_boxes_and_labels_on_image_array(
             display_str = '{}%'.format(int(100*scores[i]))
           else:
             display_str = '{}: {}%'.format(display_str, int(100*scores[i]))
+        if not skip_track_ids and track_ids is not None:
+          if not display_str:
+            display_str = 'ID {}'.format(track_ids[i])
+          else:
+            display_str = '{}: ID {}'.format(display_str, track_ids[i])
         box_to_display_str_map[box].append(display_str)
         if agnostic_mode:
           box_to_color_map[box] = 'DarkOrange'
+        elif track_ids is not None:
+          prime_multipler = _get_multiplier_for_color_randomness()
+          box_to_color_map[box] = STANDARD_COLORS[
+              (prime_multipler * track_ids[i]) % len(STANDARD_COLORS)]
         else:
           box_to_color_map[box] = STANDARD_COLORS[
               classes[i] % len(STANDARD_COLORS)]
diff --git a/research/object_detection/utils/visualization_utils_test.py b/research/object_detection/utils/visualization_utils_test.py
index 4799cd44..7d7489ad 100644
--- a/research/object_detection/utils/visualization_utils_test.py
+++ b/research/object_detection/utils/visualization_utils_test.py
@@ -29,6 +29,30 @@ _TESTDATA_PATH = 'object_detection/test_images'
 
 class VisualizationUtilsTest(tf.test.TestCase):
 
+  def test_get_prime_multiplier_for_color_randomness(self):
+    # Show that default multipler is not 1 and does not divide the total number
+    # of standard colors.
+    multiplier = visualization_utils._get_multiplier_for_color_randomness()
+    self.assertNotEqual(
+        0, multiplier % len(visualization_utils.STANDARD_COLORS))
+    self.assertNotEqual(1, multiplier)
+
+    # Show that with 34 colors, the closest prime number to 34/10 that
+    # satisfies the constraints is 5.
+    visualization_utils.STANDARD_COLORS = [
+        'color_{}'.format(str(i)) for i in range(34)
+    ]
+    multiplier = visualization_utils._get_multiplier_for_color_randomness()
+    self.assertEqual(5, multiplier)
+
+    # Show that with 110 colors, the closest prime number to 110/10 that
+    # satisfies the constraints is 13 (since 11 equally divides 110).
+    visualization_utils.STANDARD_COLORS = [
+        'color_{}'.format(str(i)) for i in range(110)
+    ]
+    multiplier = visualization_utils._get_multiplier_for_color_randomness()
+    self.assertEqual(13, multiplier)
+
   def create_colorful_test_image(self):
     """This function creates an image that can be used to test vis functions.
 
@@ -158,6 +182,55 @@ class VisualizationUtilsTest(tf.test.TestCase):
           image_pil = Image.fromarray(images_with_boxes_np[i, ...])
           image_pil.save(output_file)
 
+  def test_draw_bounding_boxes_on_image_tensors_with_track_ids(self):
+    """Tests that bounding box utility produces reasonable results."""
+    category_index = {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}}
+
+    fname = os.path.join(_TESTDATA_PATH, 'image1.jpg')
+    image_np = np.array(Image.open(fname))
+    images_np = np.stack((image_np, image_np), axis=0)
+    original_image_shape = [[636, 512], [636, 512]]
+
+    with tf.Graph().as_default():
+      images_tensor = tf.constant(value=images_np, dtype=tf.uint8)
+      image_shape = tf.constant(original_image_shape, dtype=tf.int32)
+      boxes = tf.constant([[[0.4, 0.25, 0.75, 0.75],
+                            [0.5, 0.3, 0.7, 0.9],
+                            [0.7, 0.5, 0.8, 0.9]],
+                           [[0.41, 0.25, 0.75, 0.75],
+                            [0.51, 0.3, 0.7, 0.9],
+                            [0.75, 0.5, 0.8, 0.9]]])
+      classes = tf.constant([[1, 1, 2], [1, 1, 2]], dtype=tf.int64)
+      scores = tf.constant([[0.8, 0.5, 0.7], [0.6, 0.5, 0.8]])
+      track_ids = tf.constant([[3, 9, 7], [3, 9, 144]], dtype=tf.int32)
+      images_with_boxes = (
+          visualization_utils.draw_bounding_boxes_on_image_tensors(
+              images_tensor,
+              boxes,
+              classes,
+              scores,
+              category_index,
+              original_image_spatial_shape=image_shape,
+              true_image_shape=image_shape,
+              track_ids=track_ids,
+              min_score_thresh=0.2))
+
+      with self.test_session() as sess:
+        sess.run(tf.global_variables_initializer())
+
+        # Write output images for visualization.
+        images_with_boxes_np = sess.run(images_with_boxes)
+        self.assertEqual(images_np.shape[0], images_with_boxes_np.shape[0])
+        self.assertEqual(images_np.shape[3], images_with_boxes_np.shape[3])
+        self.assertEqual(
+            tuple(original_image_shape[0]), images_with_boxes_np.shape[1:3])
+        for i in range(images_with_boxes_np.shape[0]):
+          img_name = 'image_with_track_ids_' + str(i) + '.png'
+          output_file = os.path.join(self.get_temp_dir(), img_name)
+          logging.info('Writing output image %d to %s', i, output_file)
+          image_pil = Image.fromarray(images_with_boxes_np[i, ...])
+          image_pil.save(output_file)
+
   def test_draw_bounding_boxes_on_image_tensors_with_additional_channels(self):
     """Tests the case where input image tensor has more than 3 channels."""
     category_index = {1: {'id': 1, 'name': 'dog'}}
