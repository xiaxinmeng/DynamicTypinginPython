commit 0ba83cf0211401a8505505276c59418f15338d31
Author: pkulzc <lzc@google.com>
Date:   Thu Oct 17 15:55:28 2019 -0700

    Release MobileNet V3 models and SSDLite models with MobileNet V3 backbone. (#7678)
    
    * Merged commit includes the following changes:
    275131829  by Sergio Guadarrama:
    
        updates mobilenet/README.md to be github compatible adds V2+ reference to mobilenet_v1.md file and fixes invalid markdown
    
    --
    274908068  by Sergio Guadarrama:
    
        Opensource MobilenetV3 detection models.
    
    --
    274697808  by Sergio Guadarrama:
    
        Fixed cases where tf.TensorShape was constructed with float dimensions
    
        This is a prerequisite for making TensorShape and Dimension more strict
        about the types of their arguments.
    
    --
    273577462  by Sergio Guadarrama:
    
        Fixing `conv_defs['defaults']` override issue.
    
    --
    272801298  by Sergio Guadarrama:
    
        Adds links to trained models for Moblienet V3, adds a version of minimalistic mobilenet-v3 to the definitions.
    
    --
    268928503  by Sergio Guadarrama:
    
        Mobilenet v2 with group normalization.
    
    --
    263492735  by Sergio Guadarrama:
    
        Internal change
    
    260037126  by Sergio Guadarrama:
    
        Adds an option of using a custom depthwise operation in `expanded_conv`.
    
    --
    259997001  by Sergio Guadarrama:
    
        Explicitly mark Python binaries/tests with python_version = "PY2".
    
    --
    252697685  by Sergio Guadarrama:
    
        Internal change
    
    251918746  by Sergio Guadarrama:
    
        Internal change
    
    251909704  by Sergio Guadarrama:
    
        Mobilenet V3 backbone implementation.
    
    --
    247510236  by Sergio Guadarrama:
    
        Internal change
    
    246196802  by Sergio Guadarrama:
    
        Internal change
    
    246014539  by Sergio Guadarrama:
    
        Internal change
    
    245891435  by Sergio Guadarrama:
    
        Internal change
    
    245834925  by Sergio Guadarrama:
    
        n/a
    
    --
    
    PiperOrigin-RevId: 275131829
    
    * Merged commit includes the following changes:
    274959989  by Zhichao Lu:
    
        Update detection model zoo with MobilenetV3 SSD candidates.
    
    --
    274908068  by Zhichao Lu:
    
        Opensource MobilenetV3 detection models.
    
    --
    274695889  by richardmunoz:
    
        RandomPatchGaussian preprocessing step
    
        This step can be used during model training to randomly apply gaussian noise to a random image patch. Example addition to an Object Detection API pipeline config:
    
        train_config {
          ...
          data_augmentation_options {
            random_patch_gaussian {
              random_coef: 0.5
              min_patch_size: 1
              max_patch_size: 250
              min_gaussian_stddev: 0.0
              max_gaussian_stddev: 1.0
            }
          }
          ...
        }
    
    --
    274257872  by lzc:
    
        Internal change.
    
    --
    274114689  by Zhichao Lu:
    
        Pass native_resize flag to other FPN variants.
    
    --
    274112308  by lzc:
    
        Internal change.
    
    --
    274090763  by richardmunoz:
    
        Util function for getting a patch mask on an image for use with the Object Detection API
    
    --
    274069806  by Zhichao Lu:
    
        Adding functions which will help compute predictions and losses for CenterNet.
    
    --
    273860828  by lzc:
    
        Internal change.
    
    --
    273380069  by richardmunoz:
    
        RandomImageDownscaleToTargetPixels preprocessing step
    
        This step can be used during model training to randomly downscale an image to a random target number of pixels. If the image does not contain more than the target number of pixels, then downscaling is skipped. Example addition to an Object Detection API pipeline config:
    
        train_config {
          ...
          data_augmentation_options {
            random_downscale_to_target_pixels {
              random_coef: 0.5
              min_target_pixels: 300000
              max_target_pixels: 500000
            }
          }
          ...
        }
    
    --
    272987602  by Zhichao Lu:
    
        Avoid -inf when empty box list is passed.
    
    --
    272525836  by Zhichao Lu:
    
        Cleanup repeated resizing code in meta archs.
    
    --
    272458667  by richardmunoz:
    
        RandomJpegQuality preprocessing step
    
        This step can be used during model training to randomly encode the image into a jpeg with a random quality level. Example addition to an Object Detection API pipeline config:
    
        train_config {
          ...
          data_augmentation_options {
            random_jpeg_quality {
              random_coef: 0.5
              min_jpeg_quality: 80
              max_jpeg_quality: 100
            }
          }
          ...
        }
    
    --
    271412717  by Zhichao Lu:
    
        Enables TPU training with the V2 eager + tf.function Object Detection training loops.
    
    --
    270744153  by Zhichao Lu:
    
        Adding the offset and size target assigners for CenterNet.
    
    --
    269916081  by Zhichao Lu:
    
        Include basic installation in Object Detection API tutorial.
        Also:
         - Use TF2.0
         - Use saved_model
    
    --
    269376056  by Zhichao Lu:
    
        Fix to variable loading in RetinaNet w/ custom loops. (makes the code rely on the exact name scopes that are generated a little bit less)
    
    --
    269256251  by lzc:
    
        Add use_partitioned_nms field to config and update post_prossing_builder to honor that flag when building nms function.
    
    --
    268865295  by Zhichao Lu:
    
        Adding functionality for importing and merging back internal state of the metric.
    
    --
    268640984  by Zhichao Lu:
    
        Fix computation of gaussian sigma value to create CenterNet heatmap target.
    
    --
    267475576  by Zhichao Lu:
    
        Fix for exporter trying to export non-existent exponential moving averages.
    
    --
    267286768  by Zhichao Lu:
    
        Update mixed-precision policy.
    
    --
    266166879  by Zhichao Lu:
    
        Internal change
    
    265860884  by Zhichao Lu:
    
        Apply floor function to center coordinates when creating heatmap for CenterNet target.
    
    --
    265702749  by Zhichao Lu:
    
        Internal change
    
    --
    264241949  by ronnyvotel:
    
        Updating Faster R-CNN 'final_anchors' to be in normalized coordinates.
    
    --
    264175192  by lzc:
    
        Update model_fn to only read hparams if it is not None.
    
    --
    264159328  by Zhichao Lu:
    
        Modify nearest neighbor upsampling to eliminate a multiply operation. For quantized models, the multiply operation gets unnecessarily quantized and reduces accuracy (simple stacking would work in place of the broadcast op which doesn't require quantization). Also removes an unnecessary reshape op.
    
    --
    263668306  by Zhichao Lu:
    
        Add the option to use dynamic map_fn for batch NMS
    
    --
    263031163  by Zhichao Lu:
    
        Mark outside compilation for NMS as optional.
    
    --
    263024916  by Zhichao Lu:
    
        Add an ExperimentalModel meta arch for experimenting with new model types.
    
    --
    262655894  by Zhichao Lu:
    
        Add the center heatmap target assigner for CenterNet
    
    --
    262431036  by Zhichao Lu:
    
        Adding add_eval_dict to allow for evaluation on model_v2
    
    --
    262035351  by ronnyvotel:
    
        Removing any non-Tensor predictions from the third stage of Mask R-CNN.
    
    --
    261953416  by Zhichao Lu:
    
        Internal change.
    
    --
    261834966  by Zhichao Lu:
    
        Fix the NMS OOM issue on TPU by forcing NMS to run outside of TPU.
    
    --
    261775941  by Zhichao Lu:
    
        Make Keras InputLayer compatible with both TF 1.x and TF 2.0.
    
    --
    261775633  by Zhichao Lu:
    
        Visualize additional channels with ground-truth bounding boxes.
    
    --
    261768117  by lzc:
    
        Internal change.
    
    --
    261766773  by ronnyvotel:
    
        Exposing `return_raw_detections_during_predict` in Faster R-CNN Proto.
    
    --
    260975089  by ronnyvotel:
    
        Moving calculation of batched prediction tensor names after all tensors in prediction dictionary are created.
    
    --
    259816913  by ronnyvotel:
    
        Adding raw detection boxes and feature map indices to SSD
    
    --
    259791955  by Zhichao Lu:
    
        Added a flag to control the use partitioned_non_max_suppression.
    
    --
    259580475  by Zhichao Lu:
    
        Tweak quantization-aware training re-writer to support NasFpn model architecture.
    
    --
    259579943  by rathodv:
    
        Add a meta target assigner proto and builders in OD API.
    
    --
    259577741  by Zhichao Lu:
    
        Internal change.
    
    --
    259366315  by lzc:
    
        Internal change.
    
    --
    259344310  by ronnyvotel:
    
        Updating faster rcnn so that raw_detection_boxes from predict() are in normalized coordinates.
    
    --
    259338670  by Zhichao Lu:
    
        Add support for use_native_resize_op to more feature extractors. Use dynamic shapes when static shapes are not available.
    
    --
    259083543  by ronnyvotel:
    
        Updating/fixing documentation.
    
    --
    259078937  by rathodv:
    
        Add prediction fields for tensors returned from detection_model.predict.
    
    --
    259044601  by Zhichao Lu:
    
        Add protocol buffer and builders for temperature scaling calibration.
    
    --
    259036770  by lzc:
    
        Internal changes.
    
    --
    259006223  by ronnyvotel:
    
        Adding detection anchor indices to Faster R-CNN Config. This is useful when one wishes to associate final detections and the anchors (or pre-nms boxes) from which they originated.
    
    --
    258872501  by Zhichao Lu:
    
        Run the training pipeline of ssd + resnet_v1_50 + fpn with a checkpoint.
    
    --
    258840686  by ronnyvotel:
    
        Adding standard outputs to DetectionModel.predict(). This CL only updates Faster R-CNN. Other meta architectures will be updated in future CLs.
    
    --
    258672969  by lzc:
    
        Internal change.
    
    --
    258649494  by lzc:
    
        Internal changes.
    
    --
    258630321  by ronnyvotel:
    
        Fixing documentation in shape_utils.flatten_dimensions().
    
    --
    258468145  by Zhichao Lu:
    
        Add additional output tensors parameter to Postprocess op.
    
    --
    258099219  by Zhichao Lu:
    
        Internal changes
    
    --
    
    PiperOrigin-RevId: 274959989

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index c99c21ca..37a5b3ef 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -101,6 +101,21 @@ reporting an issue.
 
 ## Release information
 
+### Oct 15th, 2019
+We have released two MobileNet V3 SSDLite models (presented in
+[Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)).
+
+* SSDLite with MobileNet-V3-Large backbone, which is 27% faster than Mobilenet
+V2 SSDLite (119ms vs 162ms) on a Google Pixel phone CPU at the same mAP.
+* SSDLite with MobileNet-V3-Small backbone, which is 37% faster than MnasNet
+SSDLite reduced with depth-multiplier (43ms vs 68ms) at the same mAP.
+
+Along with the model definition, we are also releasing model checkpoints
+trained on the COCO dataset.
+
+<b>Thanks to contributors</b>: Bo Chen, Zhichao Lu, Vivek Rathod, Jonathan Huang
+
+
 ### July 1st, 2019
 
 We have released an updated set of utils and an updated
@@ -265,8 +280,7 @@ release includes:
   distributed training and evaluation pipelines via
   [Google Cloud](g3doc/running_on_cloud.md).
 
-
-<b>Thanks to contributors</b>: Jonathan Huang, Vivek Rathod, Derek Chow,
-Chen Sun, Menglong Zhu, Matthew Tang, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Jasper Uijlings,
-Viacheslav Kovalevskyi, Kevin Murphy
-
+<b>Thanks to contributors</b>: Jonathan Huang, Vivek Rathod, Derek Chow, Chen
+Sun, Menglong Zhu, Matthew Tang, Anoop Korattikara, Alireza Fathi, Ian Fischer,
+Zbigniew Wojna, Yang Song, Sergio Guadarrama, Jasper Uijlings, Viacheslav
+Kovalevskyi, Kevin Murphy
diff --git a/research/object_detection/builders/box_predictor_builder.py b/research/object_detection/builders/box_predictor_builder.py
index abb3f2f8..439efff4 100644
--- a/research/object_detection/builders/box_predictor_builder.py
+++ b/research/object_detection/builders/box_predictor_builder.py
@@ -245,7 +245,8 @@ def build_weight_shared_convolutional_box_predictor(
     apply_batch_norm=True,
     use_depthwise=False,
     score_converter_fn=tf.identity,
-    box_encodings_clip_range=None):
+    box_encodings_clip_range=None,
+    keyword_args=None):
   """Builds and returns a WeightSharedConvolutionalBoxPredictor class.
 
   Args:
@@ -274,6 +275,7 @@ def build_weight_shared_convolutional_box_predictor(
     score_converter_fn: Callable score converter to perform elementwise op on
       class scores.
     box_encodings_clip_range: Min and max values for clipping the box_encodings.
+    keyword_args: A dictionary with additional args.
 
   Returns:
     A WeightSharedConvolutionalBoxPredictor class.
@@ -329,7 +331,8 @@ def build_weight_shared_convolutional_keras_box_predictor(
     use_depthwise=False,
     score_converter_fn=tf.identity,
     box_encodings_clip_range=None,
-    name='WeightSharedConvolutionalBoxPredictor'):
+    name='WeightSharedConvolutionalBoxPredictor',
+    keyword_args=None):
   """Builds the Keras WeightSharedConvolutionalBoxPredictor from the arguments.
 
   Args:
@@ -371,6 +374,7 @@ def build_weight_shared_convolutional_keras_box_predictor(
     box_encodings_clip_range: Min and max values for clipping the box_encodings.
     name: A string name scope to assign to the box predictor. If `None`, Keras
       will auto-generate one from the class name.
+    keyword_args: A dictionary with additional args.
 
   Returns:
     A Keras WeightSharedConvolutionalBoxPredictor class.
@@ -728,6 +732,8 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes,
       box_encodings_clip_range = BoxEncodingsClipRange(
           min=config_box_predictor.box_encodings_clip_range.min,
           max=config_box_predictor.box_encodings_clip_range.max)
+    keyword_args = None
+
     return build_weight_shared_convolutional_box_predictor(
         is_training=is_training,
         num_classes=num_classes,
@@ -746,7 +752,8 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes,
         apply_batch_norm=apply_batch_norm,
         use_depthwise=config_box_predictor.use_depthwise,
         score_converter_fn=score_converter_fn,
-        box_encodings_clip_range=box_encodings_clip_range)
+        box_encodings_clip_range=box_encodings_clip_range,
+        keyword_args=keyword_args)
 
 
   if box_predictor_oneof == 'mask_rcnn_box_predictor':
@@ -891,6 +898,7 @@ def build_keras(hyperparams_fn, freeze_batchnorm, inplace_batchnorm_update,
       box_encodings_clip_range = BoxEncodingsClipRange(
           min=config_box_predictor.box_encodings_clip_range.min,
           max=config_box_predictor.box_encodings_clip_range.max)
+    keyword_args = None
 
     return build_weight_shared_convolutional_keras_box_predictor(
         is_training=is_training,
@@ -913,7 +921,8 @@ def build_keras(hyperparams_fn, freeze_batchnorm, inplace_batchnorm_update,
         apply_batch_norm=apply_batch_norm,
         use_depthwise=config_box_predictor.use_depthwise,
         score_converter_fn=score_converter_fn,
-        box_encodings_clip_range=box_encodings_clip_range)
+        box_encodings_clip_range=box_encodings_clip_range,
+        keyword_args=keyword_args)
 
   if box_predictor_oneof == 'mask_rcnn_box_predictor':
     config_box_predictor = box_predictor_config.mask_rcnn_box_predictor
diff --git a/research/object_detection/builders/box_predictor_builder_test.py b/research/object_detection/builders/box_predictor_builder_test.py
index 2f211e2c..2494bc30 100644
--- a/research/object_detection/builders/box_predictor_builder_test.py
+++ b/research/object_detection/builders/box_predictor_builder_test.py
@@ -355,6 +355,7 @@ class WeightSharedConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
 
 
 
+
 class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
 
   def test_box_predictor_builder_calls_fc_argscope_fn(self):
diff --git a/research/object_detection/builders/calibration_builder.py b/research/object_detection/builders/calibration_builder.py
index b5c865a5..a99d38b9 100644
--- a/research/object_detection/builders/calibration_builder.py
+++ b/research/object_detection/builders/calibration_builder.py
@@ -213,6 +213,35 @@ def build(calibration_config):
           name='calibrate_scores')
       return calibrated_class_predictions_with_background
 
+  elif (calibration_config.WhichOneof('calibrator') ==
+        'temperature_scaling_calibration'):
+
+    def calibration_fn(class_predictions_with_background):
+      """Calibrate predictions via temperature scaling.
+
+      Predictions logits scores are scaled by the temperature scaler. Note that
+      the 0-indexed background class is also transformed.
+
+      Args:
+        class_predictions_with_background: tf.float32 tensor of shape
+          [batch_size, num_anchors, num_classes + 1] containing logits scores.
+          This is usually produced before a sigmoid or softmax layer.
+
+      Returns:
+        tf.float32 tensor of the same shape as the input.
+
+      Raises:
+        ValueError: If temperature scaler is of incorrect value.
+      """
+      scaler = calibration_config.temperature_scaling_calibration.scaler
+      if scaler <= 0:
+        raise ValueError('The scaler in temperature scaling must be positive.')
+      calibrated_class_predictions_with_background = tf.math.divide(
+          class_predictions_with_background,
+          scaler,
+          name='calibrate_score')
+      return calibrated_class_predictions_with_background
+
   # TODO(zbeaver): Add sigmoid calibration.
   else:
     raise ValueError('No calibration builder defined for "Oneof" in '
diff --git a/research/object_detection/builders/calibration_builder_test.py b/research/object_detection/builders/calibration_builder_test.py
index 98cf4f14..05971e69 100644
--- a/research/object_detection/builders/calibration_builder_test.py
+++ b/research/object_detection/builders/calibration_builder_test.py
@@ -169,6 +169,35 @@ class CalibrationBuilderTest(tf.test.TestCase):
     self.assertAllClose(calibrated_scores_np, [[[0.5, 0.6], [0.5, 0.3]],
                                                [[0.5, 0.7], [0.5, 0.96]]])
 
+  def test_temperature_scaling(self):
+    """Tests that calibration produces correct temperature scaling values."""
+    calibration_config = calibration_pb2.CalibrationConfig()
+    calibration_config.temperature_scaling_calibration.scaler = 2.0
+
+    od_graph = tf.Graph()
+    with self.test_session(graph=od_graph) as sess:
+      calibration_fn = calibration_builder.build(calibration_config)
+      # batch_size = 2, num_classes = 2, num_anchors = 2.
+      class_predictions_with_background = tf.constant(
+          [[[0.1, 0.2, 0.3], [0.4, 0.5, 0.0]],
+           [[0.6, 0.7, 0.8], [0.9, 1.0, 1.0]]],
+          dtype=tf.float32)
+      calibrated_scores = calibration_fn(class_predictions_with_background)
+      calibrated_scores_np = sess.run(calibrated_scores)
+    self.assertAllClose(calibrated_scores_np,
+                        [[[0.05, 0.1, 0.15], [0.2, 0.25, 0.0]],
+                         [[0.3, 0.35, 0.4], [0.45, 0.5, 0.5]]])
+
+  def test_temperature_scaling_incorrect_value_error(self):
+    calibration_config = calibration_pb2.CalibrationConfig()
+    calibration_config.temperature_scaling_calibration.scaler = 0
+
+    calibration_fn = calibration_builder.build(calibration_config)
+    class_predictions_with_background = tf.constant(
+        [[[0.1, 0.2, 0.3]]], dtype=tf.float32)
+    with self.assertRaises(ValueError):
+      calibration_fn(class_predictions_with_background)
+
   def test_skips_class_when_calibration_parameters_not_present(self):
     """Tests that graph fails when parameters not present for all classes."""
     # Only adding calibration parameters for class id = 0, even though class id
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index de4ff96d..158abb19 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -49,8 +49,8 @@ def read_dataset(file_read_func, input_files, config):
   """Reads a dataset, and handles repetition and shuffling.
 
   Args:
-    file_read_func: Function to use in tf.data.experimental.parallel_interleave,
-      to read every individual file into a tf.data.Dataset.
+    file_read_func: Function to use in tf.contrib.data.parallel_interleave, to
+      read every individual file into a tf.data.Dataset.
     input_files: A list of file paths to read.
     config: A input_reader_builder.InputReader object.
 
@@ -79,7 +79,7 @@ def read_dataset(file_read_func, input_files, config):
                        'still slightly shuffled since `num_readers` > 1.')
   filename_dataset = filename_dataset.repeat(config.num_epochs or None)
   records_dataset = filename_dataset.apply(
-      tf.data.experimental.parallel_interleave(
+      tf.contrib.data.parallel_interleave(
           file_read_func,
           cycle_length=num_readers,
           block_length=config.read_block_length,
@@ -154,7 +154,8 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
       data_map_fn = dataset.map
     dataset = data_map_fn(process_fn, num_parallel_calls=num_parallel_calls)
     if batch_size:
-      dataset = dataset.batch(batch_size, drop_remainder=True)
+      dataset = dataset.apply(
+          tf.contrib.data.batch_and_drop_remainder(batch_size))
     dataset = dataset.prefetch(input_reader_config.num_prefetch_batches)
     return dataset
 
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index aadac80b..eb6f1ead 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -39,6 +39,7 @@ from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_n
 from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
 from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
 from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
+from object_detection.models import ssd_resnet_v1_fpn_keras_feature_extractor as ssd_resnet_v1_fpn_keras
 from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn
 from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
 from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
@@ -52,6 +53,8 @@ from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobile
 from object_detection.models.ssd_mobilenet_v2_fpn_feature_extractor import SSDMobileNetV2FpnFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_fpn_keras_feature_extractor import SSDMobileNetV2FpnKerasFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_keras_feature_extractor import SSDMobileNetV2KerasFeatureExtractor
+from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3LargeFeatureExtractor
+from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3SmallFeatureExtractor
 from object_detection.models.ssd_pnasnet_feature_extractor import SSDPNASNetFeatureExtractor
 from object_detection.predictors import rfcn_box_predictor
 from object_detection.predictors import rfcn_keras_box_predictor
@@ -68,6 +71,8 @@ SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
     'ssd_mobilenet_v1_ppn': SSDMobileNetV1PpnFeatureExtractor,
     'ssd_mobilenet_v2': SSDMobileNetV2FeatureExtractor,
     'ssd_mobilenet_v2_fpn': SSDMobileNetV2FpnFeatureExtractor,
+    'ssd_mobilenet_v3_large': SSDMobileNetV3LargeFeatureExtractor,
+    'ssd_mobilenet_v3_small': SSDMobileNetV3SmallFeatureExtractor,
     'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
     'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
     'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
@@ -85,6 +90,12 @@ SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
     'ssd_mobilenet_v1_fpn_keras': SSDMobileNetV1FpnKerasFeatureExtractor,
     'ssd_mobilenet_v2_keras': SSDMobileNetV2KerasFeatureExtractor,
     'ssd_mobilenet_v2_fpn_keras': SSDMobileNetV2FpnKerasFeatureExtractor,
+    'ssd_resnet50_v1_fpn_keras':
+        ssd_resnet_v1_fpn_keras.SSDResNet50V1FpnKerasFeatureExtractor,
+    'ssd_resnet101_v1_fpn_keras':
+        ssd_resnet_v1_fpn_keras.SSDResNet101V1FpnKerasFeatureExtractor,
+    'ssd_resnet152_v1_fpn_keras':
+        ssd_resnet_v1_fpn_keras.SSDResNet152V1FpnKerasFeatureExtractor,
 }
 
 # A map of names to Faster R-CNN feature extractors.
@@ -111,31 +122,6 @@ FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
 }
 
 
-def build(model_config, is_training, add_summaries=True):
-  """Builds a DetectionModel based on the model config.
-
-  Args:
-    model_config: A model.proto object containing the config for the desired
-      DetectionModel.
-    is_training: True if this model is being built for training purposes.
-    add_summaries: Whether to add tensorflow summaries in the model graph.
-  Returns:
-    DetectionModel based on the config.
-
-  Raises:
-    ValueError: On invalid meta architecture or model.
-  """
-  if not isinstance(model_config, model_pb2.DetectionModel):
-    raise ValueError('model_config not of type model_pb2.DetectionModel.')
-  meta_architecture = model_config.WhichOneof('model')
-  if meta_architecture == 'ssd':
-    return _build_ssd_model(model_config.ssd, is_training, add_summaries)
-  if meta_architecture == 'faster_rcnn':
-    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,
-                                    add_summaries)
-  raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))
-
-
 def _build_ssd_feature_extractor(feature_extractor_config,
                                  is_training,
                                  freeze_batchnorm,
@@ -231,6 +217,7 @@ def _build_ssd_feature_extractor(feature_extractor_config,
             feature_extractor_config.fpn.additional_layer_depth,
     })
 
+
   return feature_extractor_class(**kwargs)
 
 
@@ -330,6 +317,8 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
       use_confidences_as_targets=ssd_config.use_confidences_as_targets,
       implicit_example_weight=ssd_config.implicit_example_weight,
       equalization_loss_config=equalization_loss_config,
+      return_raw_detections_during_predict=(
+          ssd_config.return_raw_detections_during_predict),
       **kwargs)
 
 
@@ -485,6 +474,7 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       max_size_per_class=frcnn_config.first_stage_max_proposals,
       max_total_size=frcnn_config.first_stage_max_proposals,
       use_static_shapes=use_static_shapes,
+      use_partitioned_nms=frcnn_config.use_partitioned_nms_in_first_stage,
       use_combined_nms=frcnn_config.use_combined_nms_in_first_stage)
   first_stage_loc_loss_weight = (
       frcnn_config.first_stage_localization_loss_weight)
@@ -580,7 +570,9 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       'crop_and_resize_fn': crop_and_resize_fn,
       'clip_anchors_to_image': clip_anchors_to_image,
       'use_static_shapes': use_static_shapes,
-      'resize_masks': frcnn_config.resize_masks
+      'resize_masks': frcnn_config.resize_masks,
+      'return_raw_detections_during_predict': (
+          frcnn_config.return_raw_detections_during_predict)
   }
 
   if (isinstance(second_stage_box_predictor,
@@ -599,3 +591,44 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
         second_stage_mask_prediction_loss_weight=(
             second_stage_mask_prediction_loss_weight),
         **common_kwargs)
+
+EXPERIMENTAL_META_ARCH_BUILDER_MAP = {
+}
+
+
+def _build_experimental_model(config, is_training, add_summaries=True):
+  return EXPERIMENTAL_META_ARCH_BUILDER_MAP[config.name](
+      is_training, add_summaries)
+
+META_ARCHITECURE_BUILDER_MAP = {
+    'ssd': _build_ssd_model,
+    'faster_rcnn': _build_faster_rcnn_model,
+    'experimental_model': _build_experimental_model
+}
+
+
+def build(model_config, is_training, add_summaries=True):
+  """Builds a DetectionModel based on the model config.
+
+  Args:
+    model_config: A model.proto object containing the config for the desired
+      DetectionModel.
+    is_training: True if this model is being built for training purposes.
+    add_summaries: Whether to add tensorflow summaries in the model graph.
+  Returns:
+    DetectionModel based on the config.
+
+  Raises:
+    ValueError: On invalid meta architecture or model.
+  """
+  if not isinstance(model_config, model_pb2.DetectionModel):
+    raise ValueError('model_config not of type model_pb2.DetectionModel.')
+
+  meta_architecture = model_config.WhichOneof('model')
+
+  if meta_architecture not in META_ARCHITECURE_BUILDER_MAP:
+    raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))
+  else:
+    build_func = META_ARCHITECURE_BUILDER_MAP[meta_architecture]
+    return build_func(getattr(model_config, meta_architecture), is_training,
+                      add_summaries)
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
index 1cf5d8d6..ed3bf504 100644
--- a/research/object_detection/builders/model_builder_test.py
+++ b/research/object_detection/builders/model_builder_test.py
@@ -327,6 +327,20 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
                                  'inplace batchnorm updates not supported'):
       model_builder.build(model_proto, is_training=True)
 
+  def test_create_experimental_model(self):
+
+    model_text_proto = """
+      experimental_model {
+        name: 'model42'
+      }"""
+
+    build_func = lambda *args: 42
+    model_builder.EXPERIMENTAL_META_ARCH_BUILDER_MAP['model42'] = build_func
+    model_proto = model_pb2.DetectionModel()
+    text_format.Merge(model_text_proto, model_proto)
+
+    self.assertEqual(model_builder.build(model_proto, is_training=True), 42)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/post_processing_builder.py b/research/object_detection/builders/post_processing_builder.py
index 7e25e53f..ba88a3b6 100644
--- a/research/object_detection/builders/post_processing_builder.py
+++ b/research/object_detection/builders/post_processing_builder.py
@@ -89,8 +89,7 @@ def _build_non_max_suppressor(nms_config):
   if nms_config.soft_nms_sigma < 0.0:
     raise ValueError('soft_nms_sigma should be non-negative.')
   if nms_config.use_combined_nms and nms_config.use_class_agnostic_nms:
-      raise ValueError('combined_nms does not support class_agnostic_nms')
-
+    raise ValueError('combined_nms does not support class_agnostic_nms.')
   non_max_suppressor_fn = functools.partial(
       post_processing.batch_multiclass_non_max_suppression,
       score_thresh=nms_config.score_threshold,
@@ -101,6 +100,7 @@ def _build_non_max_suppressor(nms_config):
       use_class_agnostic_nms=nms_config.use_class_agnostic_nms,
       max_classes_per_detection=nms_config.max_classes_per_detection,
       soft_nms_sigma=nms_config.soft_nms_sigma,
+      use_partitioned_nms=nms_config.use_partitioned_nms,
       use_combined_nms=nms_config.use_combined_nms)
   return non_max_suppressor_fn
 
@@ -143,8 +143,12 @@ def _build_score_converter(score_converter_config, logit_scale):
 def _build_calibrated_score_converter(score_converter_fn, calibration_config):
   """Wraps a score_converter_fn, adding a calibration step.
 
-  Builds a score converter function witha calibration transformation according
-  to calibration_builder.py. Calibration applies positive monotonic
+  Builds a score converter function with a calibration transformation according
+  to calibration_builder.py. The score conversion function may be applied before
+  or after the calibration transformation, depending on the calibration method.
+  If the method is temperature scaling, the score conversion is
+  after the calibration transformation. Otherwise, the score conversion is
+  before the calibration transformation. Calibration applies positive monotonic
   transformations to inputs (i.e. score ordering is strictly preserved or
   adjacent scores are mapped to the same score). When calibration is
   class-agnostic, the highest-scoring class remains unchanged, unless two
@@ -162,8 +166,14 @@ def _build_calibrated_score_converter(score_converter_fn, calibration_config):
   """
   calibration_fn = calibration_builder.build(calibration_config)
   def calibrated_score_converter_fn(logits):
-    converted_logits = score_converter_fn(logits)
-    return calibration_fn(converted_logits)
+    if (calibration_config.WhichOneof('calibrator') ==
+        'temperature_scaling_calibration'):
+      calibrated_logits = calibration_fn(logits)
+      return score_converter_fn(calibrated_logits)
+    else:
+      converted_logits = score_converter_fn(logits)
+      return calibration_fn(converted_logits)
+
   calibrated_score_converter_fn.__name__ = (
       'calibrate_with_%s' % calibration_config.WhichOneof('calibrator'))
   return calibrated_score_converter_fn
diff --git a/research/object_detection/builders/post_processing_builder_test.py b/research/object_detection/builders/post_processing_builder_test.py
index aa1930c4..5514a517 100644
--- a/research/object_detection/builders/post_processing_builder_test.py
+++ b/research/object_detection/builders/post_processing_builder_test.py
@@ -160,6 +160,26 @@ class PostProcessingBuilderTest(tf.test.TestCase):
       expected_calibrated_scores = sess.run(tf.constant([0.5, 0.5], tf.float32))
       self.assertAllClose(calibrated_scores, expected_calibrated_scores)
 
+  def test_build_temperature_scaling_calibrator(self):
+    post_processing_text_proto = """
+      score_converter: SOFTMAX
+      calibration_config {
+        temperature_scaling_calibration {
+          scaler: 2.0
+          }}"""
+    post_processing_config = post_processing_pb2.PostProcessing()
+    text_format.Merge(post_processing_text_proto, post_processing_config)
+    _, calibrated_score_conversion_fn = post_processing_builder.build(
+        post_processing_config)
+    self.assertEqual(calibrated_score_conversion_fn.__name__,
+                     'calibrate_with_temperature_scaling_calibration')
+
+    input_scores = tf.constant([1, 1], tf.float32)
+    outputs = calibrated_score_conversion_fn(input_scores)
+    with self.test_session() as sess:
+      calibrated_scores = sess.run(outputs)
+      expected_calibrated_scores = sess.run(tf.constant([0.5, 0.5], tf.float32))
+      self.assertAllClose(calibrated_scores, expected_calibrated_scores)
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/preprocessor_builder.py b/research/object_detection/builders/preprocessor_builder.py
index 8f196ad1..de71905c 100644
--- a/research/object_detection/builders/preprocessor_builder.py
+++ b/research/object_detection/builders/preprocessor_builder.py
@@ -95,6 +95,12 @@ PREPROCESSING_FUNCTION_MAP = {
         preprocessor.random_crop_to_aspect_ratio,
     'random_black_patches':
         preprocessor.random_black_patches,
+    'random_jpeg_quality':
+        preprocessor.random_jpeg_quality,
+    'random_downscale_to_target_pixels':
+        preprocessor.random_downscale_to_target_pixels,
+    'random_patch_gaussian':
+        preprocessor.random_patch_gaussian,
     'rgb_to_gray':
         preprocessor.rgb_to_gray,
     'scale_boxes_to_pixel_coordinates': (
diff --git a/research/object_detection/builders/preprocessor_builder_test.py b/research/object_detection/builders/preprocessor_builder_test.py
index 529561df..8e3343ac 100644
--- a/research/object_detection/builders/preprocessor_builder_test.py
+++ b/research/object_detection/builders/preprocessor_builder_test.py
@@ -363,6 +363,62 @@ class PreprocessorBuilderTest(tf.test.TestCase):
                                         'probability': 0.95,
                                         'size_to_image_ratio': 0.12})
 
+  def test_build_random_jpeg_quality(self):
+    preprocessor_text_proto = """
+    random_jpeg_quality {
+      random_coef: 0.5
+      min_jpeg_quality: 40
+      max_jpeg_quality: 90
+    }
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Parse(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.random_jpeg_quality)
+    self.assert_dictionary_close(args, {'random_coef': 0.5,
+                                        'min_jpeg_quality': 40,
+                                        'max_jpeg_quality': 90})
+
+  def test_build_random_downscale_to_target_pixels(self):
+    preprocessor_text_proto = """
+    random_downscale_to_target_pixels {
+      random_coef: 0.5
+      min_target_pixels: 200
+      max_target_pixels: 900
+    }
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Parse(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.random_downscale_to_target_pixels)
+    self.assert_dictionary_close(args, {
+        'random_coef': 0.5,
+        'min_target_pixels': 200,
+        'max_target_pixels': 900
+    })
+
+  def test_build_random_patch_gaussian(self):
+    preprocessor_text_proto = """
+    random_patch_gaussian {
+      random_coef: 0.5
+      min_patch_size: 10
+      max_patch_size: 300
+      min_gaussian_stddev: 0.2
+      max_gaussian_stddev: 1.5
+    }
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Parse(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.random_patch_gaussian)
+    self.assert_dictionary_close(args, {
+        'random_coef': 0.5,
+        'min_patch_size': 10,
+        'max_patch_size': 300,
+        'min_gaussian_stddev': 0.2,
+        'max_gaussian_stddev': 1.5
+    })
+
   def test_auto_augment_image(self):
     preprocessor_text_proto = """
     autoaugment_image {
diff --git a/research/object_detection/builders/target_assigner_builder.py b/research/object_detection/builders/target_assigner_builder.py
new file mode 100644
index 00000000..f6434f65
--- /dev/null
+++ b/research/object_detection/builders/target_assigner_builder.py
@@ -0,0 +1,40 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""A function to build an object detection box coder from configuration."""
+from object_detection.builders import box_coder_builder
+from object_detection.builders import matcher_builder
+from object_detection.builders import region_similarity_calculator_builder
+from object_detection.core import target_assigner
+
+
+def build(target_assigner_config):
+  """Builds a TargetAssigner object based on the config.
+
+  Args:
+    target_assigner_config: A target_assigner proto message containing config
+      for the desired target assigner.
+
+  Returns:
+    TargetAssigner object based on the config.
+  """
+  matcher_instance = matcher_builder.build(target_assigner_config.matcher)
+  similarity_calc_instance = region_similarity_calculator_builder.build(
+      target_assigner_config.similarity_calculator)
+  box_coder = box_coder_builder.build(target_assigner_config.box_coder)
+  return target_assigner.TargetAssigner(
+      matcher=matcher_instance,
+      similarity_calc=similarity_calc_instance,
+      box_coder_instance=box_coder)
diff --git a/research/object_detection/builders/target_assigner_builder_test.py b/research/object_detection/builders/target_assigner_builder_test.py
new file mode 100644
index 00000000..9ca71b1d
--- /dev/null
+++ b/research/object_detection/builders/target_assigner_builder_test.py
@@ -0,0 +1,50 @@
+"""Tests for google3.third_party.tensorflow_models.object_detection.builders.target_assigner_builder."""
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import tensorflow as tf
+
+from google.protobuf import text_format
+
+
+from object_detection.builders import target_assigner_builder
+from object_detection.core import target_assigner
+from object_detection.protos import target_assigner_pb2
+
+
+class TargetAssignerBuilderTest(tf.test.TestCase):
+
+  def test_build_a_target_assigner(self):
+    target_assigner_text_proto = """
+      matcher {
+        argmax_matcher {matched_threshold: 0.5}
+      }
+      similarity_calculator {
+        iou_similarity {}
+      }
+      box_coder {
+        faster_rcnn_box_coder {}
+      }
+    """
+    target_assigner_proto = target_assigner_pb2.TargetAssigner()
+    text_format.Merge(target_assigner_text_proto, target_assigner_proto)
+    target_assigner_instance = target_assigner_builder.build(
+        target_assigner_proto)
+    self.assertIsInstance(target_assigner_instance,
+                          target_assigner.TargetAssigner)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/core/anchor_generator.py b/research/object_detection/core/anchor_generator.py
index 957cd753..070b1d68 100644
--- a/research/object_detection/core/anchor_generator.py
+++ b/research/object_detection/core/anchor_generator.py
@@ -130,6 +130,23 @@ class AnchorGenerator(six.with_metaclass(ABCMeta, object)):
     """
     pass
 
+  def anchor_index_to_feature_map_index(self, boxlist_list):
+    """Returns a 1-D array of feature map indices for each anchor.
+
+    Args:
+      boxlist_list: a list of Boxlist, each holding a collection of N anchor
+        boxes. This list is produced in self.generate().
+
+    Returns:
+      A [num_anchors] integer array, where each element indicates which feature
+      map index the anchor belongs to.
+    """
+    feature_map_indices_list = []
+    for i, boxes in enumerate(boxlist_list):
+      feature_map_indices_list.append(
+          i * tf.ones([boxes.num_boxes()], dtype=tf.int32))
+    return tf.concat(feature_map_indices_list, axis=0)
+
   def _assert_correct_number_of_anchors(self, anchors_list,
                                         feature_map_shape_list):
     """Assert that correct number of anchors was generated.
diff --git a/research/object_detection/core/batch_multiclass_nms_test.py b/research/object_detection/core/batch_multiclass_nms_test.py
index 878d8675..cd0c56bf 100644
--- a/research/object_detection/core/batch_multiclass_nms_test.py
+++ b/research/object_detection/core/batch_multiclass_nms_test.py
@@ -87,7 +87,8 @@ class BatchMulticlassNonMaxSuppressionTest(test_case.TestCase,
       iou = sess.run(iou)
       self.assertAllClose(iou, expected_iou)
 
-  def test_batch_multiclass_nms_with_batch_size_2(self):
+  @parameterized.parameters(False, True)
+  def test_batch_multiclass_nms_with_batch_size_2(self, use_dynamic_map_fn):
     boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],
                           [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
                           [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],
@@ -122,7 +123,8 @@ class BatchMulticlassNonMaxSuppressionTest(test_case.TestCase,
      nmsed_additional_fields, num_detections
     ) = post_processing.batch_multiclass_non_max_suppression(
         boxes, scores, score_thresh, iou_thresh,
-        max_size_per_class=max_output_size, max_total_size=max_output_size)
+        max_size_per_class=max_output_size, max_total_size=max_output_size,
+        use_dynamic_map_fn=use_dynamic_map_fn)
 
     self.assertIsNone(nmsed_masks)
     self.assertIsNone(nmsed_additional_fields)
@@ -713,5 +715,7 @@ class BatchMulticlassNonMaxSuppressionTest(test_case.TestCase,
       self.assertAllClose(nmsed_classes, exp_nms_classes)
       self.assertListEqual(num_detections.tolist(), [3, 3])
 
+  # TODO(bhattad): Remove conditional after CMLE moves to TF 1.9
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/box_list.py b/research/object_detection/core/box_list.py
index 6d3ddb0f..cda75755 100644
--- a/research/object_detection/core/box_list.py
+++ b/research/object_detection/core/box_list.py
@@ -53,7 +53,8 @@ class BoxList(object):
           float32 format.
     """
     if len(boxes.get_shape()) != 2 or boxes.get_shape()[-1] != 4:
-      raise ValueError('Invalid dimensions for box data.')
+      raise ValueError('Invalid dimensions for box data: {}'.format(
+          boxes.shape))
     if boxes.dtype != tf.float32:
       raise ValueError('Invalid tensor type: should be tf.float32')
     self.data = {'boxes': boxes}
diff --git a/research/object_detection/core/post_processing.py b/research/object_detection/core/post_processing.py
index 95407687..90f1e06d 100644
--- a/research/object_detection/core/post_processing.py
+++ b/research/object_detection/core/post_processing.py
@@ -393,6 +393,7 @@ def multiclass_non_max_suppression(boxes,
                                    masks=None,
                                    boundaries=None,
                                    pad_to_max_output_size=False,
+                                   use_partitioned_nms=False,
                                    additional_fields=None,
                                    soft_nms_sigma=0.0,
                                    scope=None):
@@ -438,6 +439,8 @@ def multiclass_non_max_suppression(boxes,
       depending on whether a separate boundary is predicted per class.
     pad_to_max_output_size: If true, the output nmsed boxes are padded to be of
       length `max_size_per_class`. Defaults to false.
+    use_partitioned_nms: If true, use partitioned version of
+      non_max_suppression.
     additional_fields: (optional) If not None, a dictionary that maps keys to
       tensors whose first dimensions are all of size `k`. After non-maximum
       suppression, all tensors corresponding to the selected boxes will be
@@ -506,15 +509,26 @@ def multiclass_non_max_suppression(boxes,
       selected_scores = None
       if pad_to_max_output_size:
         max_selection_size = max_size_per_class
-        (selected_indices, num_valid_nms_boxes,
-         boxlist_and_class_scores.data['boxes'],
-         boxlist_and_class_scores.data['scores'],
-         _) = partitioned_non_max_suppression_padded(
-             boxlist_and_class_scores.get(),
-             boxlist_and_class_scores.get_field(fields.BoxListFields.scores),
-             max_selection_size,
-             iou_threshold=iou_thresh,
-             score_threshold=score_thresh)
+        if use_partitioned_nms:
+          (selected_indices, num_valid_nms_boxes,
+           boxlist_and_class_scores.data['boxes'],
+           boxlist_and_class_scores.data['scores'],
+           _) = partitioned_non_max_suppression_padded(
+               boxlist_and_class_scores.get(),
+               boxlist_and_class_scores.get_field(fields.BoxListFields.scores),
+               max_selection_size,
+               iou_threshold=iou_thresh,
+               score_threshold=score_thresh)
+        else:
+          selected_indices, num_valid_nms_boxes = (
+              tf.image.non_max_suppression_padded(
+                  boxlist_and_class_scores.get(),
+                  boxlist_and_class_scores.get_field(
+                      fields.BoxListFields.scores),
+                  max_selection_size,
+                  iou_threshold=iou_thresh,
+                  score_threshold=score_thresh,
+                  pad_to_max_output_size=True))
         nms_result = box_list_ops.gather(boxlist_and_class_scores,
                                          selected_indices)
         selected_scores = nms_result.get_field(fields.BoxListFields.scores)
@@ -606,6 +620,7 @@ def class_agnostic_non_max_suppression(boxes,
                                        masks=None,
                                        boundaries=None,
                                        pad_to_max_output_size=False,
+                                       use_partitioned_nms=False,
                                        additional_fields=None,
                                        soft_nms_sigma=0.0,
                                        scope=None):
@@ -653,6 +668,8 @@ def class_agnostic_non_max_suppression(boxes,
       depending on whether a separate boundary is predicted per class.
     pad_to_max_output_size: If true, the output nmsed boxes are padded to be of
       length `max_size_per_class`. Defaults to false.
+    use_partitioned_nms: If true, use partitioned version of
+      non_max_suppression.
     additional_fields: (optional) If not None, a dictionary that maps keys to
       tensors whose first dimensions are all of size `k`. After non-maximum
       suppression, all tensors corresponding to the selected boxes will be added
@@ -719,19 +736,30 @@ def class_agnostic_non_max_suppression(boxes,
     selected_scores = None
     if pad_to_max_output_size:
       max_selection_size = max_total_size
-      (selected_indices, num_valid_nms_boxes,
-       boxlist_and_class_scores.data['boxes'],
-       boxlist_and_class_scores.data['scores'],
-       argsort_ids) = partitioned_non_max_suppression_padded(
-           boxlist_and_class_scores.get(),
-           boxlist_and_class_scores.get_field(fields.BoxListFields.scores),
-           max_selection_size,
-           iou_threshold=iou_thresh,
-           score_threshold=score_thresh)
+      if use_partitioned_nms:
+        (selected_indices, num_valid_nms_boxes,
+         boxlist_and_class_scores.data['boxes'],
+         boxlist_and_class_scores.data['scores'],
+         argsort_ids) = partitioned_non_max_suppression_padded(
+             boxlist_and_class_scores.get(),
+             boxlist_and_class_scores.get_field(fields.BoxListFields.scores),
+             max_selection_size,
+             iou_threshold=iou_thresh,
+             score_threshold=score_thresh)
+        classes_with_max_scores = tf.gather(classes_with_max_scores,
+                                            argsort_ids)
+      else:
+        selected_indices, num_valid_nms_boxes = (
+            tf.image.non_max_suppression_padded(
+                boxlist_and_class_scores.get(),
+                boxlist_and_class_scores.get_field(fields.BoxListFields.scores),
+                max_selection_size,
+                iou_threshold=iou_thresh,
+                score_threshold=score_thresh,
+                pad_to_max_output_size=True))
       nms_result = box_list_ops.gather(boxlist_and_class_scores,
                                        selected_indices)
       selected_scores = nms_result.get_field(fields.BoxListFields.scores)
-      classes_with_max_scores = tf.gather(classes_with_max_scores, argsort_ids)
     else:
       max_selection_size = tf.minimum(max_total_size,
                                       boxlist_and_class_scores.num_boxes())
@@ -779,6 +807,7 @@ def class_agnostic_non_max_suppression(boxes,
                  selected_scores, -1*tf.ones(max_selection_size)))
 
     selected_classes = tf.gather(classes_with_max_scores, selected_indices)
+    selected_classes = tf.cast(selected_classes, tf.float32)
     nms_result.add_field(fields.BoxListFields.classes, selected_classes)
     selected_boxes = nms_result
     sorted_boxes = box_list_ops.sort_by_field(selected_boxes,
@@ -818,9 +847,11 @@ def batch_multiclass_non_max_suppression(boxes,
                                          soft_nms_sigma=0.0,
                                          scope=None,
                                          use_static_shapes=False,
+                                         use_partitioned_nms=False,
                                          parallel_iterations=32,
                                          use_class_agnostic_nms=False,
                                          max_classes_per_detection=1,
+                                         use_dynamic_map_fn=False,
                                          use_combined_nms=False):
   """Multi-class version of non maximum suppression that operates on a batch.
 
@@ -867,15 +898,18 @@ def batch_multiclass_non_max_suppression(boxes,
       False.
     scope: tf scope name.
     use_static_shapes: If true, the output nmsed boxes are padded to be of
-      length `minimum(max_total_size, max_size_per_class*num_classes)`.
-      If false, they are padded to be of length `max_total_size`.
+      length `max_size_per_class` and it doesn't clip boxes to max_total_size.
       Defaults to false.
+    use_partitioned_nms: If true, use partitioned version of
+      non_max_suppression.
     parallel_iterations: (optional) number of batch items to process in
       parallel.
     use_class_agnostic_nms: If true, this uses class-agnostic non max
       suppression
     max_classes_per_detection: Maximum number of retained classes per detection
       box in class-agnostic NMS.
+    use_dynamic_map_fn: If true, images in the batch will be processed within a
+      dynamic loop. Otherwise, a static loop will be used if possible.
     use_combined_nms: If true, it uses tf.image.combined_non_max_suppression (
       multi-class version of NMS that operates on a batch).
       It greedily selects a subset of detection bounding boxes, pruning away
@@ -1108,6 +1142,7 @@ def batch_multiclass_non_max_suppression(boxes,
             change_coordinate_frame=change_coordinate_frame,
             masks=per_image_masks,
             pad_to_max_output_size=use_static_shapes,
+            use_partitioned_nms=use_partitioned_nms,
             additional_fields=per_image_additional_fields,
             soft_nms_sigma=soft_nms_sigma)
       else:
@@ -1122,6 +1157,7 @@ def batch_multiclass_non_max_suppression(boxes,
             change_coordinate_frame=change_coordinate_frame,
             masks=per_image_masks,
             pad_to_max_output_size=use_static_shapes,
+            use_partitioned_nms=use_partitioned_nms,
             additional_fields=per_image_additional_fields,
             soft_nms_sigma=soft_nms_sigma)
 
@@ -1147,7 +1183,12 @@ def batch_multiclass_non_max_suppression(boxes,
       num_additional_fields = len(ordered_additional_fields)
     num_nmsed_outputs = 4 + num_additional_fields
 
-    batch_outputs = shape_utils.static_or_dynamic_map_fn(
+    if use_dynamic_map_fn:
+      map_fn = tf.map_fn
+    else:
+      map_fn = shape_utils.static_or_dynamic_map_fn
+
+    batch_outputs = map_fn(
         _single_image_nms_fn,
         elems=([boxes, scores, masks, clip_window] +
                list(ordered_additional_fields.values()) + [num_valid_boxes]),
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index c89a228c..1c74a585 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -70,19 +70,20 @@ from __future__ import print_function
 import functools
 import inspect
 import sys
+
 import six
 from six.moves import range
 from six.moves import zip
 import tensorflow as tf
 
 from tensorflow.python.ops import control_flow_ops
-
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import keypoint_ops
 from object_detection.core import preprocessor_cache
 from object_detection.core import standard_fields as fields
 from object_detection.utils import autoaugment_utils
+from object_detection.utils import patch_ops
 from object_detection.utils import shape_utils
 
 
@@ -2284,6 +2285,282 @@ def random_black_patches(image,
     return image
 
 
+def random_jpeg_quality(image,
+                        min_jpeg_quality=0,
+                        max_jpeg_quality=100,
+                        random_coef=0.0,
+                        seed=None,
+                        preprocess_vars_cache=None):
+  """Randomly encode the image to a random JPEG quality level.
+
+  Args:
+    image: rank 3 float32 tensor with shape [height, width, channels] and
+      values in the range [0, 255].
+    min_jpeg_quality: An int for the lower bound for selecting a random jpeg
+      quality level.
+    max_jpeg_quality: An int for the upper bound for selecting a random jpeg
+      quality level.
+    random_coef: a random coefficient that defines the chance of getting the
+      original image. If random_coef is 0, we will always get the encoded image,
+      and if it is 1.0, we will always get the original image.
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+      performed augmentations. Updated in-place. If this function is called
+      multiple times with the same non-null cache, it will perform
+      deterministically.
+
+  Returns:
+    image: image which is the same shape as input image.
+  """
+  def _adjust_jpeg_quality():
+    """Encodes the image as jpeg with a random quality and then decodes."""
+    generator_func = functools.partial(
+        tf.random_uniform, [],
+        minval=min_jpeg_quality,
+        maxval=max_jpeg_quality,
+        dtype=tf.int32,
+        seed=seed)
+    quality = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.JPEG_QUALITY,
+        preprocess_vars_cache, key='quality')
+
+    # Need to convert to uint8 before calling adjust_jpeg_quality since it
+    # assumes that float features are in the range [0, 1], where herein the
+    # range is [0, 255].
+    image_uint8 = tf.cast(image, tf.uint8)
+    adjusted_image = tf.image.adjust_jpeg_quality(image_uint8, quality)
+    return tf.cast(adjusted_image, tf.float32)
+
+  with tf.name_scope('RandomJpegQuality', values=[image]):
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    do_encoding_random = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.JPEG_QUALITY,
+        preprocess_vars_cache)
+    do_encoding_random = tf.greater_equal(do_encoding_random, random_coef)
+    image = tf.cond(do_encoding_random, _adjust_jpeg_quality,
+                    lambda: tf.cast(image, tf.float32))
+
+  return image
+
+
+def random_downscale_to_target_pixels(image,
+                                      masks=None,
+                                      min_target_pixels=300000,
+                                      max_target_pixels=800000,
+                                      random_coef=0.0,
+                                      seed=None,
+                                      preprocess_vars_cache=None):
+  """Randomly downscales the image to a target number of pixels.
+
+  If the image contains less than the chosen target number of pixels, it will
+  not be downscaled.
+
+  Args:
+    image: Rank 3 float32 tensor with shape [height, width, channels] and
+      values in the range [0, 255].
+    masks: (optional) Rank 3 float32 tensor with shape
+      [num_instances, height, width] containing instance masks. The masks are of
+      the same height, width as the input `image`.
+    min_target_pixels: Integer. An inclusive lower bound for for the target
+      number of pixels.
+    max_target_pixels: Integer. An exclusive upper bound for for the target
+      number of pixels.
+    random_coef: Float. Random coefficient that defines the chance of getting
+      the original image. If random_coef is 0, we will always apply downscaling,
+      and if it is 1.0, we will always get the original image.
+    seed: (optional) Integer. Random seed.
+    preprocess_vars_cache: (optional) PreprocessorCache object that records
+      previously performed augmentations. Updated in-place. If this function is
+      called multiple times with the same non-null cache, it will perform
+      deterministically.
+
+  Returns:
+    Tuple with elements:
+      image: Resized image which is the same rank as input image.
+      masks: If masks is not None, resized masks which are the same rank as
+        the input masks.
+
+  Raises:
+    ValueError: If min_target_pixels or max_target_pixels are not positive.
+  """
+  if min_target_pixels <= 0:
+    raise ValueError('Minimum target pixels must be positive')
+  if max_target_pixels <= 0:
+    raise ValueError('Maximum target pixels must be positive')
+
+  def _resize_image_to_target(target_height, target_width):
+    # pylint: disable=unbalanced-tuple-unpacking
+    new_image, _ = resize_image(image, None, target_height, target_width)
+    return (new_image,)
+
+  def _resize_image_and_masks_to_target(target_height, target_width):
+    # pylint: disable=unbalanced-tuple-unpacking
+    new_image, new_masks, _ = resize_image(image, masks, target_height,
+                                           target_width)
+    return new_image, new_masks
+
+  with tf.name_scope('RandomDownscaleToTargetPixels', values=[image]):
+    generator_fn = functools.partial(tf.random_uniform, [], seed=seed)
+    do_downscale_random = _get_or_create_preprocess_rand_vars(
+        generator_fn,
+        preprocessor_cache.PreprocessorCache.DOWNSCALE_TO_TARGET_PIXELS,
+        preprocess_vars_cache)
+    do_downscale_random = tf.greater_equal(do_downscale_random, random_coef)
+
+    generator_fn = functools.partial(
+        tf.random_uniform, [],
+        minval=min_target_pixels,
+        maxval=max_target_pixels,
+        dtype=tf.int32,
+        seed=seed)
+    target_pixels = _get_or_create_preprocess_rand_vars(
+        generator_fn,
+        preprocessor_cache.PreprocessorCache.DOWNSCALE_TO_TARGET_PIXELS,
+        preprocess_vars_cache,
+        key='target_pixels')
+
+    image_shape = tf.shape(image)
+    image_height = image_shape[0]
+    image_width = image_shape[1]
+    image_pixels = image_height * image_width
+    scale_factor = tf.sqrt(
+        tf.cast(target_pixels, dtype=tf.float32) /
+        tf.cast(image_pixels, dtype=tf.float32))
+    target_height = tf.cast(
+        scale_factor * tf.cast(image_height, dtype=tf.float32), dtype=tf.int32)
+    target_width = tf.cast(
+        scale_factor * tf.cast(image_width, dtype=tf.float32), dtype=tf.int32)
+    image_larger_than_target = tf.greater(image_pixels, target_pixels)
+
+    should_apply_resize = tf.logical_and(do_downscale_random,
+                                         image_larger_than_target)
+    if masks is not None:
+      resize_fn = functools.partial(_resize_image_and_masks_to_target,
+                                    target_height, target_width)
+      return tf.cond(should_apply_resize, resize_fn,
+                     lambda: (tf.cast(image, dtype=tf.float32), masks))
+    else:
+      resize_fn = lambda: _resize_image_to_target(target_height, target_width)
+      return tf.cond(should_apply_resize, resize_fn,
+                     lambda: (tf.cast(image, dtype=tf.float32),))
+
+
+def random_patch_gaussian(image,
+                          min_patch_size=1,
+                          max_patch_size=250,
+                          min_gaussian_stddev=0.0,
+                          max_gaussian_stddev=1.0,
+                          random_coef=0.0,
+                          seed=None,
+                          preprocess_vars_cache=None):
+  """Randomly applies gaussian noise to a random patch on the image.
+
+  The gaussian noise is applied to the image with values scaled to the range
+  [0.0, 1.0]. The result of applying gaussian noise to the scaled image is
+  clipped to be within the range [0.0, 1.0], equivalent to the range
+  [0.0, 255.0] after rescaling the image back.
+
+  See "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian
+  Augmentation " by Lopes et al., 2019, for further details.
+  https://arxiv.org/abs/1906.02611
+
+  Args:
+    image: Rank 3 float32 tensor with shape [height, width, channels] and
+      values in the range [0.0, 255.0].
+    min_patch_size: Integer. An inclusive lower bound for the patch size.
+    max_patch_size:  Integer. An exclusive upper bound for the patch size.
+    min_gaussian_stddev: Float. An inclusive lower bound for the standard
+      deviation of the gaussian noise.
+    max_gaussian_stddev: Float. An exclusive upper bound for the standard
+      deviation of the gaussian noise.
+    random_coef: Float. Random coefficient that defines the chance of getting
+      the original image. If random_coef is 0.0, we will always apply
+      downscaling, and if it is 1.0, we will always get the original image.
+    seed: (optional) Integer. Random seed.
+    preprocess_vars_cache: (optional) PreprocessorCache object that records
+      previously performed augmentations. Updated in-place. If this function is
+      called multiple times with the same non-null cache, it will perform
+      deterministically.
+
+  Returns:
+    Rank 3 float32 tensor with same shape as the input image and with gaussian
+    noise applied within a random patch.
+
+  Raises:
+    ValueError: If min_patch_size is < 1.
+  """
+  if min_patch_size < 1:
+    raise ValueError('Minimum patch size must be >= 1.')
+
+  get_or_create_rand_vars_fn = functools.partial(
+      _get_or_create_preprocess_rand_vars,
+      function_id=preprocessor_cache.PreprocessorCache.PATCH_GAUSSIAN,
+      preprocess_vars_cache=preprocess_vars_cache)
+
+  def _apply_patch_gaussian(image):
+    """Applies a patch gaussian with random size, location, and stddev."""
+    patch_size = get_or_create_rand_vars_fn(
+        functools.partial(
+            tf.random_uniform, [],
+            minval=min_patch_size,
+            maxval=max_patch_size,
+            dtype=tf.int32,
+            seed=seed),
+        key='patch_size')
+    gaussian_stddev = get_or_create_rand_vars_fn(
+        functools.partial(
+            tf.random_uniform, [],
+            minval=min_gaussian_stddev,
+            maxval=max_gaussian_stddev,
+            dtype=tf.float32,
+            seed=seed),
+        key='gaussian_stddev')
+
+    image_shape = tf.shape(image)
+    y = get_or_create_rand_vars_fn(
+        functools.partial(
+            tf.random_uniform, [],
+            minval=0,
+            maxval=image_shape[0],
+            dtype=tf.int32,
+            seed=seed),
+        key='y')
+    x = get_or_create_rand_vars_fn(
+        functools.partial(
+            tf.random_uniform, [],
+            minval=0,
+            maxval=image_shape[1],
+            dtype=tf.int32,
+            seed=seed),
+        key='x')
+    gaussian = get_or_create_rand_vars_fn(
+        functools.partial(
+            tf.random.normal,
+            image_shape,
+            stddev=gaussian_stddev,
+            dtype=tf.float32,
+            seed=seed),
+        key='gaussian')
+
+    scaled_image = image / 255.0
+    image_plus_gaussian = tf.clip_by_value(scaled_image + gaussian, 0.0, 1.0)
+    patch_mask = patch_ops.get_patch_mask(y, x, patch_size, image_shape)
+    patch_mask = tf.expand_dims(patch_mask, -1)
+    patch_mask = tf.tile(patch_mask, [1, 1, image_shape[2]])
+    patched_image = tf.where(patch_mask, image_plus_gaussian, scaled_image)
+    return patched_image * 255.0
+
+  with tf.name_scope('RandomPatchGaussian', values=[image]):
+    image = tf.cast(image, tf.float32)
+    patch_gaussian_random = get_or_create_rand_vars_fn(
+        functools.partial(tf.random_uniform, [], seed=seed))
+    do_patch_gaussian = tf.greater_equal(patch_gaussian_random, random_coef)
+    image = tf.cond(do_patch_gaussian,
+                    lambda: _apply_patch_gaussian(image),
+                    lambda: image)
+  return image
+
+
 # TODO(barretzoph): Put in AutoAugment Paper link when paper is live.
 def autoaugment_image(image, boxes, policy_name='v0'):
   """Apply an autoaugment policy to the image and boxes.
@@ -3538,6 +3815,12 @@ def get_default_func_arg_map(include_label_weights=True,
           groundtruth_keypoints,
       ),
       random_black_patches: (fields.InputDataFields.image,),
+      random_jpeg_quality: (fields.InputDataFields.image,),
+      random_downscale_to_target_pixels: (
+          fields.InputDataFields.image,
+          groundtruth_instance_masks,
+      ),
+      random_patch_gaussian: (fields.InputDataFields.image,),
       autoaugment_image: (fields.InputDataFields.image,
                           fields.InputDataFields.groundtruth_boxes,),
       retain_boxes_above_threshold: (
diff --git a/research/object_detection/core/preprocessor_cache.py b/research/object_detection/core/preprocessor_cache.py
index 13471fe4..706d44cd 100644
--- a/research/object_detection/core/preprocessor_cache.py
+++ b/research/object_detection/core/preprocessor_cache.py
@@ -54,15 +54,19 @@ class PreprocessorCache(object):
   SELF_CONCAT_IMAGE = 'self_concat_image'
   SSD_CROP_SELECTOR_ID = 'ssd_crop_selector_id'
   SSD_CROP_PAD_SELECTOR_ID = 'ssd_crop_pad_selector_id'
+  JPEG_QUALITY = 'jpeg_quality'
+  DOWNSCALE_TO_TARGET_PIXELS = 'downscale_to_target_pixels'
+  PATCH_GAUSSIAN = 'patch_gaussian'
 
-  # 23 permitted function ids
+  # 27 permitted function ids
   _VALID_FNS = [ROTATION90, HORIZONTAL_FLIP, VERTICAL_FLIP, PIXEL_VALUE_SCALE,
                 IMAGE_SCALE, RGB_TO_GRAY, ADJUST_BRIGHTNESS, ADJUST_CONTRAST,
                 ADJUST_HUE, ADJUST_SATURATION, DISTORT_COLOR, STRICT_CROP_IMAGE,
                 CROP_IMAGE, PAD_IMAGE, CROP_TO_ASPECT_RATIO, RESIZE_METHOD,
                 PAD_TO_ASPECT_RATIO, BLACK_PATCHES, ADD_BLACK_PATCH, SELECTOR,
                 SELECTOR_TUPLES, SELF_CONCAT_IMAGE, SSD_CROP_SELECTOR_ID,
-                SSD_CROP_PAD_SELECTOR_ID]
+                SSD_CROP_PAD_SELECTOR_ID, JPEG_QUALITY,
+                DOWNSCALE_TO_TARGET_PIXELS, PATCH_GAUSSIAN]
 
   def __init__(self):
     self._history = defaultdict(dict)
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index d2efb539..39167742 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -19,9 +19,9 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl.testing import parameterized
 import numpy as np
 import six
-
 from six.moves import range
 from six.moves import zip
 import tensorflow as tf
@@ -36,7 +36,7 @@ else:
   from unittest import mock  # pylint: disable=g-import-not-at-top
 
 
-class PreprocessorTest(tf.test.TestCase):
+class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
 
   def createColorfulTestImage(self):
     ch255 = tf.fill([1, 100, 200, 1], tf.constant(255, dtype=tf.uint8))
@@ -2478,6 +2478,233 @@ class PreprocessorTest(tf.test.TestCase):
           [images_shape, blacked_images_shape])
       self.assertAllEqual(images_shape_, blacked_images_shape_)
 
+  def testRandomJpegQuality(self):
+    preprocessing_options = [(preprocessor.random_jpeg_quality, {
+        'min_jpeg_quality': 0,
+        'max_jpeg_quality': 100
+    })]
+    images = self.createTestImages()
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    encoded_images = processed_tensor_dict[fields.InputDataFields.image]
+    images_shape = tf.shape(images)
+    encoded_images_shape = tf.shape(encoded_images)
+
+    with self.test_session() as sess:
+      images_shape_out, encoded_images_shape_out = sess.run(
+          [images_shape, encoded_images_shape])
+      self.assertAllEqual(images_shape_out, encoded_images_shape_out)
+
+  def testRandomJpegQualityKeepsStaticChannelShape(self):
+    # Set at least three weeks past the forward compatibility horizon for
+    # tf 1.14 of 2019/11/01.
+    # https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/compat/compat.py#L30
+    if not tf.compat.forward_compatible(year=2019, month=12, day=1):
+      self.skipTest('Skipping test for future functionality.')
+
+    preprocessing_options = [(preprocessor.random_jpeg_quality, {
+        'min_jpeg_quality': 0,
+        'max_jpeg_quality': 100
+    })]
+    images = self.createTestImages()
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    encoded_images = processed_tensor_dict[fields.InputDataFields.image]
+    images_static_channels = images.shape[-1]
+    encoded_images_static_channels = encoded_images.shape[-1]
+    self.assertEqual(images_static_channels, encoded_images_static_channels)
+
+  def testRandomJpegQualityWithCache(self):
+    preprocessing_options = [(preprocessor.random_jpeg_quality, {
+        'min_jpeg_quality': 0,
+        'max_jpeg_quality': 100
+    })]
+    self._testPreprocessorCache(preprocessing_options)
+
+  def testRandomJpegQualityWithRandomCoefOne(self):
+    preprocessing_options = [(preprocessor.random_jpeg_quality, {
+        'random_coef': 1.0
+    })]
+    images = self.createTestImages()
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    encoded_images = processed_tensor_dict[fields.InputDataFields.image]
+    images_shape = tf.shape(images)
+    encoded_images_shape = tf.shape(encoded_images)
+
+    with self.test_session() as sess:
+      (images_out, encoded_images_out, images_shape_out,
+       encoded_images_shape_out) = sess.run(
+           [images, encoded_images, images_shape, encoded_images_shape])
+      self.assertAllEqual(images_shape_out, encoded_images_shape_out)
+      self.assertAllEqual(images_out, encoded_images_out)
+
+  def testRandomDownscaleToTargetPixels(self):
+    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
+        'min_target_pixels': 100,
+        'max_target_pixels': 101
+    })]
+    images = tf.random_uniform([1, 25, 100, 3])
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+    downscaled_shape = tf.shape(downscaled_images)
+    expected_shape = [1, 5, 20, 3]
+    with self.test_session() as sess:
+      downscaled_shape_out = sess.run(downscaled_shape)
+      self.assertAllEqual(downscaled_shape_out, expected_shape)
+
+  def testRandomDownscaleToTargetPixelsWithMasks(self):
+    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
+        'min_target_pixels': 100,
+        'max_target_pixels': 101
+    })]
+    images = tf.random_uniform([1, 25, 100, 3])
+    masks = tf.random_uniform([10, 25, 100])
+    tensor_dict = {
+        fields.InputDataFields.image: images,
+        fields.InputDataFields.groundtruth_instance_masks: masks
+    }
+    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+        include_instance_masks=True)
+    processed_tensor_dict = preprocessor.preprocess(
+        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+    downscaled_masks = processed_tensor_dict[
+        fields.InputDataFields.groundtruth_instance_masks]
+    downscaled_images_shape = tf.shape(downscaled_images)
+    downscaled_masks_shape = tf.shape(downscaled_masks)
+    expected_images_shape = [1, 5, 20, 3]
+    expected_masks_shape = [10, 5, 20]
+    with self.test_session() as sess:
+      downscaled_images_shape_out, downscaled_masks_shape_out = sess.run(
+          [downscaled_images_shape, downscaled_masks_shape])
+      self.assertAllEqual(downscaled_images_shape_out, expected_images_shape)
+      self.assertAllEqual(downscaled_masks_shape_out, expected_masks_shape)
+
+  @parameterized.parameters(
+      {'test_masks': False},
+      {'test_masks': True}
+  )
+  def testRandomDownscaleToTargetPixelsWithCache(self, test_masks):
+    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
+        'min_target_pixels': 100,
+        'max_target_pixels': 999
+    })]
+    self._testPreprocessorCache(preprocessing_options, test_masks=test_masks)
+
+  def testRandomDownscaleToTargetPixelsWithRandomCoefOne(self):
+    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
+        'random_coef': 1.0,
+        'min_target_pixels': 10,
+        'max_target_pixels': 20,
+    })]
+    images = tf.random_uniform([1, 25, 100, 3])
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+    images_shape = tf.shape(images)
+    downscaled_images_shape = tf.shape(downscaled_images)
+
+    with self.test_session() as sess:
+      (images_out, downscaled_images_out, images_shape_out,
+       downscaled_images_shape_out) = sess.run(
+           [images, downscaled_images, images_shape, downscaled_images_shape])
+      self.assertAllEqual(images_shape_out, downscaled_images_shape_out)
+      self.assertAllEqual(images_out, downscaled_images_out)
+
+  def testRandomDownscaleToTargetPixelsIgnoresSmallImages(self):
+    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
+        'min_target_pixels': 1000,
+        'max_target_pixels': 1001
+    })]
+    images = tf.random_uniform([1, 10, 10, 3])
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+    images_shape = tf.shape(images)
+    downscaled_images_shape = tf.shape(downscaled_images)
+    with self.test_session() as sess:
+      (images_out, downscaled_images_out, images_shape_out,
+       downscaled_images_shape_out) = sess.run(
+           [images, downscaled_images, images_shape, downscaled_images_shape])
+      self.assertAllEqual(images_shape_out, downscaled_images_shape_out)
+      self.assertAllEqual(images_out, downscaled_images_out)
+
+  def testRandomPatchGaussianShape(self):
+    preprocessing_options = [(preprocessor.random_patch_gaussian, {
+        'min_patch_size': 1,
+        'max_patch_size': 200,
+        'min_gaussian_stddev': 0.0,
+        'max_gaussian_stddev': 2.0
+    })]
+    images = self.createTestImages()
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    patched_images = processed_tensor_dict[fields.InputDataFields.image]
+    images_shape = tf.shape(images)
+    patched_images_shape = tf.shape(patched_images)
+    self.assertAllEqual(images_shape, patched_images_shape)
+
+  def testRandomPatchGaussianClippedToLowerBound(self):
+    preprocessing_options = [(preprocessor.random_patch_gaussian, {
+        'min_patch_size': 20,
+        'max_patch_size': 40,
+        'min_gaussian_stddev': 50,
+        'max_gaussian_stddev': 100
+    })]
+    images = tf.zeros([1, 5, 4, 3])
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    patched_images = processed_tensor_dict[fields.InputDataFields.image]
+    self.assertAllGreaterEqual(patched_images, 0.0)
+
+  def testRandomPatchGaussianClippedToUpperBound(self):
+    preprocessing_options = [(preprocessor.random_patch_gaussian, {
+        'min_patch_size': 20,
+        'max_patch_size': 40,
+        'min_gaussian_stddev': 50,
+        'max_gaussian_stddev': 100
+    })]
+    images = tf.constant(255.0, shape=[1, 5, 4, 3])
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    patched_images = processed_tensor_dict[fields.InputDataFields.image]
+    self.assertAllLessEqual(patched_images, 255.0)
+
+  def testRandomPatchGaussianWithCache(self):
+    preprocessing_options = [(preprocessor.random_patch_gaussian, {
+        'min_patch_size': 1,
+        'max_patch_size': 200,
+        'min_gaussian_stddev': 0.0,
+        'max_gaussian_stddev': 2.0
+    })]
+    self._testPreprocessorCache(preprocessing_options)
+
+  def testRandomPatchGaussianWithRandomCoefOne(self):
+    preprocessing_options = [(preprocessor.random_patch_gaussian, {
+        'random_coef': 1.0
+    })]
+    images = self.createTestImages()
+    tensor_dict = {fields.InputDataFields.image: images}
+    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+    patched_images = processed_tensor_dict[fields.InputDataFields.image]
+    images_shape = tf.shape(images)
+    patched_images_shape = tf.shape(patched_images)
+
+    self.assertAllEqual(images_shape, patched_images_shape)
+    self.assertAllEqual(images, patched_images)
+
   def testAutoAugmentImage(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.autoaugment_image, {
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index 98d5d528..628902eb 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -168,6 +168,22 @@ class BoxListFields(object):
   is_crowd = 'is_crowd'
 
 
+class PredictionFields(object):
+  """Naming conventions for standardized prediction outputs.
+
+  Attributes:
+    feature_maps: List of feature maps for prediction.
+    anchors: Generated anchors.
+    raw_detection_boxes: Decoded detection boxes without NMS.
+    raw_detection_feature_map_indices: Feature map indices from which each raw
+      detection box was produced.
+  """
+  feature_maps = 'feature_maps'
+  anchors = 'anchors'
+  raw_detection_boxes = 'raw_detection_boxes'
+  raw_detection_feature_map_indices = 'raw_detection_feature_map_indices'
+
+
 class TfExampleFields(object):
   """TF-example proto feature names for object detection.
 
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 21897c08..3e3ba1ae 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -41,8 +41,9 @@ import tensorflow as tf
 
 from object_detection.box_coders import faster_rcnn_box_coder
 from object_detection.box_coders import mean_stddev_box_coder
-from object_detection.core import box_coder as bcoder
+from object_detection.core import box_coder
 from object_detection.core import box_list
+from object_detection.core import box_list_ops
 from object_detection.core import matcher as mat
 from object_detection.core import region_similarity_calculator as sim_calc
 from object_detection.core import standard_fields as fields
@@ -57,7 +58,7 @@ class TargetAssigner(object):
   def __init__(self,
                similarity_calc,
                matcher,
-               box_coder,
+               box_coder_instance,
                negative_class_weight=1.0):
     """Construct Object Detection Target Assigner.
 
@@ -65,8 +66,8 @@ class TargetAssigner(object):
       similarity_calc: a RegionSimilarityCalculator
       matcher: an object_detection.core.Matcher used to match groundtruth to
         anchors.
-      box_coder: an object_detection.core.BoxCoder used to encode matching
-        groundtruth boxes with respect to anchors.
+      box_coder_instance: an object_detection.core.BoxCoder used to encode
+        matching groundtruth boxes with respect to anchors.
       negative_class_weight: classification weight to be associated to negative
         anchors (default: 1.0). The weight must be in [0., 1.].
 
@@ -78,11 +79,11 @@ class TargetAssigner(object):
       raise ValueError('similarity_calc must be a RegionSimilarityCalculator')
     if not isinstance(matcher, mat.Matcher):
       raise ValueError('matcher must be a Matcher')
-    if not isinstance(box_coder, bcoder.BoxCoder):
+    if not isinstance(box_coder_instance, box_coder.BoxCoder):
       raise ValueError('box_coder must be a BoxCoder')
     self._similarity_calc = similarity_calc
     self._matcher = matcher
-    self._box_coder = box_coder
+    self._box_coder = box_coder_instance
     self._negative_class_weight = negative_class_weight
 
   @property
@@ -391,7 +392,7 @@ def create_target_assigner(reference, stage=None,
   if reference == 'Multibox' and stage == 'proposal':
     similarity_calc = sim_calc.NegSqDistSimilarity()
     matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
+    box_coder_instance = mean_stddev_box_coder.MeanStddevBoxCoder()
 
   elif reference == 'FasterRCNN' and stage == 'proposal':
     similarity_calc = sim_calc.IouSimilarity()
@@ -399,7 +400,7 @@ def create_target_assigner(reference, stage=None,
                                            unmatched_threshold=0.3,
                                            force_match_for_each_row=True,
                                            use_matmul_gather=use_matmul_gather)
-    box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
+    box_coder_instance = faster_rcnn_box_coder.FasterRcnnBoxCoder(
         scale_factors=[10.0, 10.0, 5.0, 5.0])
 
   elif reference == 'FasterRCNN' and stage == 'detection':
@@ -408,7 +409,7 @@ def create_target_assigner(reference, stage=None,
     matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
                                            negatives_lower_than_unmatched=True,
                                            use_matmul_gather=use_matmul_gather)
-    box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
+    box_coder_instance = faster_rcnn_box_coder.FasterRcnnBoxCoder(
         scale_factors=[10.0, 10.0, 5.0, 5.0])
 
   elif reference == 'FastRCNN':
@@ -418,12 +419,12 @@ def create_target_assigner(reference, stage=None,
                                            force_match_for_each_row=False,
                                            negatives_lower_than_unmatched=False,
                                            use_matmul_gather=use_matmul_gather)
-    box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
+    box_coder_instance = faster_rcnn_box_coder.FasterRcnnBoxCoder()
 
   else:
     raise ValueError('No valid combination of reference and stage.')
 
-  return TargetAssigner(similarity_calc, matcher, box_coder,
+  return TargetAssigner(similarity_calc, matcher, box_coder_instance,
                         negative_class_weight=negative_class_weight)
 
 
@@ -702,3 +703,5 @@ def batch_assign_confidences(target_assigner,
   batch_match = tf.stack(match_list)
   return (batch_cls_targets, batch_cls_weights, batch_reg_targets,
           batch_reg_weights, batch_match)
+
+
diff --git a/research/object_detection/export_tflite_ssd_graph_lib.py b/research/object_detection/export_tflite_ssd_graph_lib.py
index bb1b0f85..cf276fd4 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib.py
@@ -67,7 +67,8 @@ def append_postprocessing_op(frozen_graph_def,
                              num_classes,
                              scale_values,
                              detections_per_class=100,
-                             use_regular_nms=False):
+                             use_regular_nms=False,
+                             additional_output_tensors=()):
   """Appends postprocessing custom op.
 
   Args:
@@ -82,11 +83,13 @@ def append_postprocessing_op(frozen_graph_def,
     num_classes: number of classes in SSD detector
     scale_values: scale values is a dict with following key-value pairs
       {y_scale: 10, x_scale: 10, h_scale: 5, w_scale: 5} that are used in decode
-      centersize boxes
+        centersize boxes
     detections_per_class: In regular NonMaxSuppression, number of anchors used
-    for NonMaxSuppression per class
-    use_regular_nms: Flag to set postprocessing op to use Regular NMS instead
-      of Fast NMS.
+      for NonMaxSuppression per class
+    use_regular_nms: Flag to set postprocessing op to use Regular NMS instead of
+      Fast NMS.
+    additional_output_tensors: Array of additional tensor names to output.
+      Tensors are appended after postprocessing output.
 
   Returns:
     transformed_graph_def: Frozen GraphDef with postprocessing custom op
@@ -140,7 +143,8 @@ def append_postprocessing_op(frozen_graph_def,
       ['raw_outputs/box_encodings', 'raw_outputs/class_predictions', 'anchors'])
   # Transform the graph to append new postprocessing op
   input_names = []
-  output_names = ['TFLite_Detection_PostProcess']
+  output_names = ['TFLite_Detection_PostProcess'
+                 ] + list(additional_output_tensors)
   transforms = ['strip_unused_nodes']
   transformed_graph_def = TransformGraph(frozen_graph_def, input_names,
                                          output_names, transforms)
@@ -156,7 +160,8 @@ def export_tflite_graph(pipeline_config,
                         detections_per_class=100,
                         use_regular_nms=False,
                         binary_graph_name='tflite_graph.pb',
-                        txt_graph_name='tflite_graph.pbtxt'):
+                        txt_graph_name='tflite_graph.pbtxt',
+                        additional_output_tensors=()):
   """Exports a tflite compatible graph and anchors for ssd detection model.
 
   Anchors are written to a tensor and tflite compatible graph
@@ -173,11 +178,13 @@ def export_tflite_graph(pipeline_config,
     max_detections: Maximum number of detections (boxes) to show
     max_classes_per_detection: Number of classes to display per detection
     detections_per_class: In regular NonMaxSuppression, number of anchors used
-    for NonMaxSuppression per class
-    use_regular_nms: Flag to set postprocessing op to use Regular NMS instead
-      of Fast NMS.
+      for NonMaxSuppression per class
+    use_regular_nms: Flag to set postprocessing op to use Regular NMS instead of
+      Fast NMS.
     binary_graph_name: Name of the exported graph file in binary format.
     txt_graph_name: Name of the exported graph file in text format.
+    additional_output_tensors: Array of additional tensor names to output.
+      Additional tensors are appended to the end of output tensor list.
 
   Raises:
     ValueError: if the pipeline config contains models other than ssd or uses an
@@ -191,12 +198,12 @@ def export_tflite_graph(pipeline_config,
 
   num_classes = pipeline_config.model.ssd.num_classes
   nms_score_threshold = {
-      pipeline_config.model.ssd.post_processing.batch_non_max_suppression.
-      score_threshold
+      pipeline_config.model.ssd.post_processing.batch_non_max_suppression
+      .score_threshold
   }
   nms_iou_threshold = {
-      pipeline_config.model.ssd.post_processing.batch_non_max_suppression.
-      iou_threshold
+      pipeline_config.model.ssd.post_processing.batch_non_max_suppression
+      .iou_threshold
   }
   scale_values = {}
   scale_values['y_scale'] = {
@@ -291,7 +298,7 @@ def export_tflite_graph(pipeline_config,
       output_node_names=','.join([
           'raw_outputs/box_encodings', 'raw_outputs/class_predictions',
           'anchors'
-      ]),
+      ] + list(additional_output_tensors)),
       restore_op_name='save/restore_all',
       filename_tensor_name='save/Const:0',
       clear_devices=True,
@@ -301,9 +308,16 @@ def export_tflite_graph(pipeline_config,
   # Add new operation to do post processing in a custom op (TF Lite only)
   if add_postprocessing_op:
     transformed_graph_def = append_postprocessing_op(
-        frozen_graph_def, max_detections, max_classes_per_detection,
-        nms_score_threshold, nms_iou_threshold, num_classes, scale_values,
-        detections_per_class, use_regular_nms)
+        frozen_graph_def,
+        max_detections,
+        max_classes_per_detection,
+        nms_score_threshold,
+        nms_iou_threshold,
+        num_classes,
+        scale_values,
+        detections_per_class,
+        use_regular_nms,
+        additional_output_tensors=additional_output_tensors)
   else:
     # Return frozen without adding post-processing custom op
     transformed_graph_def = frozen_graph_def
diff --git a/research/object_detection/export_tflite_ssd_graph_lib_test.py b/research/object_detection/export_tflite_ssd_graph_lib_test.py
index cb93a87d..b469d595 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib_test.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib_test.py
@@ -12,7 +12,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 """Tests for object_detection.export_tflite_ssd_graph."""
 from __future__ import absolute_import
 from __future__ import division
@@ -31,7 +30,6 @@ from object_detection.protos import graph_rewriter_pb2
 from object_detection.protos import pipeline_pb2
 from object_detection.protos import post_processing_pb2
 
-
 if six.PY2:
   import mock  # pylint: disable=g-import-not-at-top
 else:
@@ -130,7 +128,10 @@ class ExportTfliteGraphTest(tf.test.TestCase):
             feed_dict={input_tensor: np.random.rand(1, 10, 10, num_channels)})
     return box_encodings_np, class_predictions_np
 
-  def _export_graph(self, pipeline_config, num_channels=3):
+  def _export_graph(self,
+                    pipeline_config,
+                    num_channels=3,
+                    additional_output_tensors=()):
     """Exports a tflite graph."""
     output_dir = self.get_temp_dir()
     trained_checkpoint_prefix = os.path.join(output_dir, 'model.ckpt')
@@ -147,18 +148,22 @@ class ExportTfliteGraphTest(tf.test.TestCase):
       mock_builder.return_value = FakeModel()
 
       with tf.Graph().as_default():
+        tf.identity(
+            tf.constant([[1, 2], [3, 4]], tf.uint8), name='UnattachedTensor')
         export_tflite_ssd_graph_lib.export_tflite_graph(
             pipeline_config=pipeline_config,
             trained_checkpoint_prefix=trained_checkpoint_prefix,
             output_dir=output_dir,
             add_postprocessing_op=False,
             max_detections=10,
-            max_classes_per_detection=1)
+            max_classes_per_detection=1,
+            additional_output_tensors=additional_output_tensors)
     return tflite_graph_file
 
   def _export_graph_with_postprocessing_op(self,
                                            pipeline_config,
-                                           num_channels=3):
+                                           num_channels=3,
+                                           additional_output_tensors=()):
     """Exports a tflite graph with custom postprocessing op."""
     output_dir = self.get_temp_dir()
     trained_checkpoint_prefix = os.path.join(output_dir, 'model.ckpt')
@@ -175,13 +180,16 @@ class ExportTfliteGraphTest(tf.test.TestCase):
       mock_builder.return_value = FakeModel()
 
       with tf.Graph().as_default():
+        tf.identity(
+            tf.constant([[1, 2], [3, 4]], tf.uint8), name='UnattachedTensor')
         export_tflite_ssd_graph_lib.export_tflite_graph(
             pipeline_config=pipeline_config,
             trained_checkpoint_prefix=trained_checkpoint_prefix,
             output_dir=output_dir,
             add_postprocessing_op=True,
             max_detections=10,
-            max_classes_per_detection=1)
+            max_classes_per_detection=1,
+            additional_output_tensors=additional_output_tensors)
     return tflite_graph_file
 
   def test_export_tflite_graph_with_moving_averages(self):
@@ -325,7 +333,8 @@ class ExportTfliteGraphTest(tf.test.TestCase):
       with tf.gfile.Open(tflite_graph_file) as f:
         graph_def.ParseFromString(f.read())
       all_op_names = [node.name for node in graph_def.node]
-      self.assertTrue('TFLite_Detection_PostProcess' in all_op_names)
+      self.assertIn('TFLite_Detection_PostProcess', all_op_names)
+      self.assertNotIn('UnattachedTensor', all_op_names)
       for node in graph_def.node:
         if node.name == 'TFLite_Detection_PostProcess':
           self.assertTrue(node.attr['_output_quantized'].b is True)
@@ -342,6 +351,42 @@ class ExportTfliteGraphTest(tf.test.TestCase):
                   for t in node.attr['_output_types'].list.type
               ]))
 
+  def test_export_tflite_graph_with_additional_tensors(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = False
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    tflite_graph_file = self._export_graph(
+        pipeline_config, additional_output_tensors=['UnattachedTensor'])
+    self.assertTrue(os.path.exists(tflite_graph_file))
+    graph = tf.Graph()
+    with graph.as_default():
+      graph_def = tf.GraphDef()
+      with tf.gfile.Open(tflite_graph_file) as f:
+        graph_def.ParseFromString(f.read())
+      all_op_names = [node.name for node in graph_def.node]
+      self.assertIn('UnattachedTensor', all_op_names)
+
+  def test_export_tflite_graph_with_postprocess_op_and_additional_tensors(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = False
+    pipeline_config.model.ssd.post_processing.score_converter = (
+        post_processing_pb2.PostProcessing.SIGMOID)
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    pipeline_config.model.ssd.num_classes = 2
+    tflite_graph_file = self._export_graph_with_postprocessing_op(
+        pipeline_config, additional_output_tensors=['UnattachedTensor'])
+    self.assertTrue(os.path.exists(tflite_graph_file))
+    graph = tf.Graph()
+    with graph.as_default():
+      graph_def = tf.GraphDef()
+      with tf.gfile.Open(tflite_graph_file) as f:
+        graph_def.ParseFromString(f.read())
+      all_op_names = [node.name for node in graph_def.node]
+      self.assertIn('TFLite_Detection_PostProcess', all_op_names)
+      self.assertIn('UnattachedTensor', all_op_names)
+
   @mock.patch.object(exporter, 'rewrite_nn_resize_op')
   def test_export_with_nn_resize_op_not_called_without_fpn(self, mock_get):
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index 97053a49..b9937192 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -40,50 +40,54 @@ def rewrite_nn_resize_op(is_quantized=False):
   Args:
     is_quantized: True if the default graph is quantized.
   """
-  input_pattern = graph_matcher.OpTypePattern(
-      'FakeQuantWithMinMaxVars' if is_quantized else '*')
-  reshape_1_pattern = graph_matcher.OpTypePattern(
-      'Reshape', inputs=[input_pattern, 'Const'], ordered_inputs=False)
-  mul_pattern = graph_matcher.OpTypePattern(
-      'Mul', inputs=[reshape_1_pattern, 'Const'], ordered_inputs=False)
-  # The quantization script may or may not insert a fake quant op after the
-  # Mul. In either case, these min/max vars are not needed once replaced with
-  # the TF version of NN resize.
-  fake_quant_pattern = graph_matcher.OpTypePattern(
-      'FakeQuantWithMinMaxVars',
-      inputs=[mul_pattern, 'Identity', 'Identity'],
-      ordered_inputs=False)
-  reshape_2_pattern = graph_matcher.OpTypePattern(
-      'Reshape',
-      inputs=[graph_matcher.OneofPattern([fake_quant_pattern, mul_pattern]),
-              'Const'],
-      ordered_inputs=False)
-  add_type_name = 'Add'
-  if tf.compat.forward_compatible(2019, 6, 26):
-    add_type_name = 'AddV2'
-  add_pattern = graph_matcher.OpTypePattern(
-      add_type_name, inputs=[reshape_2_pattern, '*'], ordered_inputs=False)
-
-  matcher = graph_matcher.GraphMatcher(add_pattern)
-  for match in matcher.match_graph(tf.get_default_graph()):
-    projection_op = match.get_op(input_pattern)
-    reshape_2_op = match.get_op(reshape_2_pattern)
-    add_op = match.get_op(add_pattern)
-    nn_resize = tf.image.resize_nearest_neighbor(
-        projection_op.outputs[0],
-        add_op.outputs[0].shape.dims[1:3],
-        align_corners=False,
-        name=os.path.split(reshape_2_op.name)[0] + '/resize_nearest_neighbor')
-
-    for index, op_input in enumerate(add_op.inputs):
-      if op_input == reshape_2_op.outputs[0]:
-        add_op._update_input(index, nn_resize)  # pylint: disable=protected-access
-        break
+  def remove_nn():
+    """Remove nearest neighbor upsampling structure and replace with TF op."""
+    input_pattern = graph_matcher.OpTypePattern(
+        'FakeQuantWithMinMaxVars' if is_quantized else '*')
+    stack_1_pattern = graph_matcher.OpTypePattern(
+        'Pack', inputs=[input_pattern, input_pattern], ordered_inputs=False)
+    stack_2_pattern = graph_matcher.OpTypePattern(
+        'Pack', inputs=[stack_1_pattern, stack_1_pattern], ordered_inputs=False)
+    reshape_pattern = graph_matcher.OpTypePattern(
+        'Reshape', inputs=[stack_2_pattern, 'Const'], ordered_inputs=False)
+    consumer_pattern = graph_matcher.OpTypePattern(
+        'Add|AddV2|Max|Mul', inputs=[reshape_pattern, '*'],
+        ordered_inputs=False)
+
+    match_counter = 0
+    matcher = graph_matcher.GraphMatcher(consumer_pattern)
+    for match in matcher.match_graph(tf.get_default_graph()):
+      match_counter += 1
+      projection_op = match.get_op(input_pattern)
+      reshape_op = match.get_op(reshape_pattern)
+      consumer_op = match.get_op(consumer_pattern)
+      nn_resize = tf.image.resize_nearest_neighbor(
+          projection_op.outputs[0],
+          reshape_op.outputs[0].shape.dims[1:3],
+          align_corners=False,
+          name=os.path.split(reshape_op.name)[0] + '/resize_nearest_neighbor')
+
+      for index, op_input in enumerate(consumer_op.inputs):
+        if op_input == reshape_op.outputs[0]:
+          consumer_op._update_input(index, nn_resize)  # pylint: disable=protected-access
+          break
+
+    tf.logging.info('Found and fixed {} matches'.format(match_counter))
+    return match_counter
+
+  # Applying twice because both inputs to Add could be NN pattern
+  total_removals = 0
+  while remove_nn():
+    total_removals += 1
+    # This number is chosen based on the nas-fpn architecture.
+    if total_removals > 4:
+      raise ValueError('Graph removal encountered a infinite loop.')
 
 
 def replace_variable_values_with_moving_averages(graph,
                                                  current_checkpoint_file,
-                                                 new_checkpoint_file):
+                                                 new_checkpoint_file,
+                                                 no_ema_collection=None):
   """Replaces variable values in the checkpoint with their moving averages.
 
   If the current checkpoint has shadow variables maintaining moving averages of
@@ -95,10 +99,14 @@ def replace_variable_values_with_moving_averages(graph,
     current_checkpoint_file: a checkpoint containing both original variables and
       their moving averages.
     new_checkpoint_file: file path to write a new checkpoint.
+    no_ema_collection: A list of namescope substrings to match the variables
+      to eliminate EMA.
   """
   with graph.as_default():
     variable_averages = tf.train.ExponentialMovingAverage(0.0)
     ema_variables_to_restore = variable_averages.variables_to_restore()
+    ema_variables_to_restore = config_util.remove_unecessary_ema(
+        ema_variables_to_restore, no_ema_collection)
     with tf.Session() as sess:
       read_saver = tf.train.Saver(ema_variables_to_restore)
       read_saver.restore(sess, current_checkpoint_file)
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index 0042d7f2..1f37dcfd 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -21,6 +21,7 @@ import tensorflow as tf
 from google.protobuf import text_format
 from tensorflow.python.framework import dtypes
 from tensorflow.python.ops import array_ops
+from tensorflow.python.tools import strip_unused_lib
 from object_detection import exporter
 from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
@@ -1056,6 +1057,42 @@ class ExportInferenceGraphTest(tf.test.TestCase):
 
     self.assertTrue(resize_op_found)
 
+  def test_rewrite_nn_resize_op_multiple_path(self):
+    g = tf.Graph()
+    with g.as_default():
+      with tf.name_scope('nearest_upsampling'):
+        x = array_ops.placeholder(dtypes.float32, shape=(8, 10, 10, 8))
+        x_stack = tf.stack([tf.stack([x] * 2, axis=3)] * 2, axis=2)
+        x_reshape = tf.reshape(x_stack, [8, 20, 20, 8])
+
+      with tf.name_scope('nearest_upsampling'):
+        x_2 = array_ops.placeholder(dtypes.float32, shape=(8, 10, 10, 8))
+        x_stack_2 = tf.stack([tf.stack([x_2] * 2, axis=3)] * 2, axis=2)
+        x_reshape_2 = tf.reshape(x_stack_2, [8, 20, 20, 8])
+
+      t = x_reshape + x_reshape_2
+
+      exporter.rewrite_nn_resize_op()
+
+    graph_def = g.as_graph_def()
+    graph_def = strip_unused_lib.strip_unused(
+        graph_def,
+        input_node_names=[
+            'nearest_upsampling/Placeholder', 'nearest_upsampling_1/Placeholder'
+        ],
+        output_node_names=['add'],
+        placeholder_type_enum=dtypes.float32.as_datatype_enum)
+
+    counter_resize_op = 0
+    t_input_ops = [op.name for op in t.op.inputs]
+    for node in graph_def.node:
+      # Make sure Stacks are replaced.
+      self.assertNotEqual(node.op, 'Pack')
+      if node.op == 'ResizeNearestNeighbor':
+        counter_resize_op += 1
+        self.assertIn(node.name + ':0', t_input_ops)
+    self.assertEqual(counter_resize_op, 2)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/g3doc/challenge_evaluation.md b/research/object_detection/g3doc/challenge_evaluation.md
index a126715e..d8ea2101 100644
--- a/research/object_detection/g3doc/challenge_evaluation.md
+++ b/research/object_detection/g3doc/challenge_evaluation.md
@@ -66,6 +66,9 @@ python models/research/object_detection/metrics/oid_challenge_evaluation.py \
     --output_metrics=${OUTPUT_METRICS} \
 ```
 
+Note that predictions file must contain the following keys:
+ImageID,LabelName,Score,XMin,XMax,YMin,YMax
+
 For the Object Detection Track, the participants will be ranked on:
 
 -   "OpenImagesDetectionChallenge_Precision/mAP@0.5IOU"
@@ -94,10 +97,11 @@ evaluation metric implementation is available in the class
     masks.
     Those should be transformed into a single CSV file in the format:
 
-    ImageID,LabelName,ImageWidth,ImageHeight,XMin,YMin,XMax,YMax,GroupOf,Mask
-    where Mask is MS COCO RLE encoding of a binary mask stored in .png file.
-
-    NOTE: the util to make the transformation will be released soon.
+    ImageID,LabelName,ImageWidth,ImageHeight,XMin,YMin,XMax,YMax,IsGroupOf,Mask
+    where Mask is MS COCO RLE encoding, compressed with zip, and re-coded with
+    base64 encoding of a binary mask stored in .png file. See an example
+    implementation of the encoding function
+    [here](https://gist.github.com/pculliton/209398a2a52867580c6103e25e55d93c).
 
 1.  Run the following command to create hierarchical expansion of the instance
     segmentation, bounding boxes and image-level label annotations: {value=4}
@@ -142,6 +146,11 @@ python models/research/object_detection/metrics/oid_challenge_evaluation.py \
     --output_metrics=${OUTPUT_METRICS} \
 ```
 
+Note that predictions file must contain the following keys:
+ImageID,ImageWidth,ImageHeight,LabelName,Score,Mask
+
+Mask must be encoded the same way as groundtruth masks.
+
 For the Instance Segmentation Track, the participants will be ranked on:
 
 -   "OpenImagesInstanceSegmentationChallenge_Precision/mAP@0.5IOU"
@@ -196,6 +205,9 @@ python object_detection/metrics/oid_vrd_challenge_evaluation.py \
     --output_metrics=${OUTPUT_METRICS}
 ```
 
+Note that predictions file must contain the following keys:
+ImageID,LabelName1,LabelName2,RelationshipLabel,Score,XMin1,XMax1,YMin1,YMax1,XMin2,XMax2,YMin2,YMax2
+
 The participants of the challenge will be evaluated by weighted average of the following three metrics:
 
 - "VRDMetric_Relationships_mAP@0.5IOU"
diff --git a/research/object_detection/g3doc/detection_model_zoo.md b/research/object_detection/g3doc/detection_model_zoo.md
index bb5c83c1..6d3f7bca 100644
--- a/research/object_detection/g3doc/detection_model_zoo.md
+++ b/research/object_detection/g3doc/detection_model_zoo.md
@@ -35,17 +35,20 @@ tar -xzvf ssd_mobilenet_v1_coco.tar.gz
 
 Inside the un-tar'ed directory, you will find:
 
-* a graph proto (`graph.pbtxt`)
-* a checkpoint
-  (`model.ckpt.data-00000-of-00001`, `model.ckpt.index`, `model.ckpt.meta`)
-* a frozen graph proto with weights baked into the graph as constants
-  (`frozen_inference_graph.pb`) to be used for out of the box inference
-    (try this out in the Jupyter notebook!)
-* a config file (`pipeline.config`) which was used to generate the graph.  These
-  directly correspond to a config file in the
-  [samples/configs](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)) directory but often with a modified score threshold.  In the case
-  of the heavier Faster R-CNN models, we also provide a version of the model
-  that uses a highly reduced number of proposals for speed.
+*   a graph proto (`graph.pbtxt`)
+*   a checkpoint (`model.ckpt.data-00000-of-00001`, `model.ckpt.index`,
+    `model.ckpt.meta`)
+*   a frozen graph proto with weights baked into the graph as constants
+    (`frozen_inference_graph.pb`) to be used for out of the box inference (try
+    this out in the Jupyter notebook!)
+*   a config file (`pipeline.config`) which was used to generate the graph.
+    These directly correspond to a config file in the
+    [samples/configs](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs))
+    directory but often with a modified score threshold. In the case of the
+    heavier Faster R-CNN models, we also provide a version of the model that
+    uses a highly reduced number of proposals for speed.
+*   Mobile model only: a TfLite file (`model.tflite`) that can be deployed on
+    mobile devices.
 
 Some remarks on frozen inference graphs:
 
@@ -100,6 +103,13 @@ Note: The asterisk () at the end of model name indicates that this model supp
 
 Note: If you download the tar.gz file of quantized models and un-tar, you will get different set of files - a checkpoint, a config file and tflite frozen graphs (txt/binary).
 
+### Mobile models
+
+Model name                                                                                                                          | Pixel 1 Latency (ms) | COCO mAP | Outputs
+----------------------------------------------------------------------------------------------------------------------------------- | :------------------: | :------: | :-----:
+[ssd_mobilenet_v3_large_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2019_08_14.tar.gz) | 119                  | 22.3     | Boxes
+[ssd_mobilenet_v3_small_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2019_08_14.tar.gz) | 43                   | 15.6     | Boxes
+
 ## Kitti-trained models
 
 Model name                                                                                                                                                        | Speed (ms) | Pascal mAP@0.5 | Outputs
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 190bd184..aad977f3 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -71,7 +71,8 @@ def transform_input_data(tensor_dict,
                          merge_multiple_boxes=False,
                          retain_original_image=False,
                          use_multiclass_scores=False,
-                         use_bfloat16=False):
+                         use_bfloat16=False,
+                         retain_original_image_additional_channels=False):
   """A single function that is responsible for all input data transformations.
 
   Data transformation functions are applied in the following order.
@@ -110,6 +111,8 @@ def transform_input_data(tensor_dict,
       this is True and multiclass_scores is empty, one-hot encoding of
       `groundtruth_classes` is used as a fallback.
     use_bfloat16: (optional) a bool, whether to use bfloat16 in training.
+    retain_original_image_additional_channels: (optional) Whether to retain
+      original image additional channels in the output dictionary.
 
   Returns:
     A dictionary keyed by fields.InputDataFields containing the tensors obtained
@@ -139,6 +142,10 @@ def transform_input_data(tensor_dict,
     channels = out_tensor_dict[fields.InputDataFields.image_additional_channels]
     out_tensor_dict[fields.InputDataFields.image] = tf.concat(
         [out_tensor_dict[fields.InputDataFields.image], channels], axis=2)
+    if retain_original_image_additional_channels:
+      out_tensor_dict[
+          fields.InputDataFields.image_additional_channels] = tf.cast(
+              image_resizer_fn(channels, None)[0], tf.uint8)
 
   # Apply data augmentation ops.
   if data_augmentation_fn is not None:
@@ -445,6 +452,9 @@ def _get_features_dict(input_dict):
   if fields.InputDataFields.original_image in input_dict:
     features[fields.InputDataFields.original_image] = input_dict[
         fields.InputDataFields.original_image]
+  if fields.InputDataFields.image_additional_channels in input_dict:
+    features[fields.InputDataFields.image_additional_channels] = input_dict[
+        fields.InputDataFields.image_additional_channels]
   return features
 
 
@@ -663,7 +673,9 @@ def eval_input(eval_config, eval_input_config, model_config,
         image_resizer_fn=image_resizer_fn,
         num_classes=num_classes,
         data_augmentation_fn=None,
-        retain_original_image=eval_config.retain_original_images)
+        retain_original_image=eval_config.retain_original_images,
+        retain_original_image_additional_channels=
+        eval_config.retain_original_image_additional_channels)
     tensor_dict = pad_input_data_to_static_shapes(
         tensor_dict=transform_data_fn(tensor_dict),
         max_num_boxes=eval_input_config.max_number_of_boxes,
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index a487211f..cf3b5ff1 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -301,6 +301,70 @@ class InputsTest(test_case.TestCase, parameterized.TestCase):
     self.assertEqual(
         tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)
 
+  def test_ssd_inceptionV2_eval_input_with_additional_channels(
+      self, eval_batch_size=1):
+    """Tests the eval input function for SSDInceptionV2 with additional channels.
+
+    Args:
+      eval_batch_size: Batch size for eval set.
+    """
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    model_config = configs['model']
+    model_config.ssd.num_classes = 37
+    configs['eval_input_configs'][0].num_additional_channels = 1
+    eval_config = configs['eval_config']
+    eval_config.batch_size = eval_batch_size
+    eval_config.retain_original_image_additional_channels = True
+    eval_input_fn = inputs.create_eval_input_fn(
+        eval_config, configs['eval_input_configs'][0], model_config)
+    features, labels = _make_initializable_iterator(eval_input_fn()).get_next()
+    self.assertAllEqual([eval_batch_size, 300, 300, 4],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 300, 300, 3],
+        features[fields.InputDataFields.original_image].shape.as_list())
+    self.assertEqual(tf.uint8,
+                     features[fields.InputDataFields.original_image].dtype)
+    self.assertAllEqual([eval_batch_size, 300, 300, 1], features[
+        fields.InputDataFields.image_additional_channels].shape.as_list())
+    self.assertEqual(
+        tf.uint8,
+        features[fields.InputDataFields.image_additional_channels].dtype)
+    self.assertAllEqual([eval_batch_size],
+                        features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100, model_config.ssd.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100],
+        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_weights].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100],
+        labels[fields.InputDataFields.groundtruth_area].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_area].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100],
+        labels[fields.InputDataFields.groundtruth_is_crowd].shape.as_list())
+    self.assertEqual(tf.bool,
+                     labels[fields.InputDataFields.groundtruth_is_crowd].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100],
+        labels[fields.InputDataFields.groundtruth_difficult].shape.as_list())
+    self.assertEqual(tf.int32,
+                     labels[fields.InputDataFields.groundtruth_difficult].dtype)
+
   def test_predict_input(self):
     """Tests the predict input function."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index fa758b50..dffe8f10 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -326,7 +326,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                clip_anchors_to_image=False,
                use_static_shapes=False,
                resize_masks=True,
-               freeze_batchnorm=False):
+               freeze_batchnorm=False,
+               return_raw_detections_during_predict=False):
     """FasterRCNNMetaArch Constructor.
 
     Args:
@@ -455,7 +456,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
         stage box predictor during training or not. When training with a small
         batch size (e.g. 1), it is desirable to freeze batch norm update and
         use pretrained batch norm params.
-
+      return_raw_detections_during_predict: Whether to return raw detection
+        boxes in the predict() method. These are decoded boxes that have not
+        been through postprocessing (i.e. NMS). Default False.
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at
         training time.
@@ -623,6 +626,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
     if self._number_of_stages <= 0 or self._number_of_stages > 3:
       raise ValueError('Number of stages should be a value in {1, 2, 3}.')
     self._batched_prediction_tensor_names = []
+    self._return_raw_detections_during_predict = (
+        return_raw_detections_during_predict)
 
   @property
   def first_stage_feature_extractor_scope(self):
@@ -694,16 +699,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Raises:
       ValueError: if inputs tensor does not have type tf.float32
     """
-    if inputs.dtype is not tf.float32:
-      raise ValueError('`preprocess` expects a tf.float32 tensor')
+
     with tf.name_scope('Preprocessor'):
-      outputs = shape_utils.static_or_dynamic_map_fn(
-          self._image_resizer_fn,
-          elems=inputs,
-          dtype=[tf.float32, tf.int32],
-          parallel_iterations=self._parallel_iterations)
-      resized_inputs = outputs[0]
-      true_image_shapes = outputs[1]
+      (resized_inputs,
+       true_image_shapes) = shape_utils.resize_images_and_return_shapes(
+           inputs, self._image_resizer_fn)
+
       return (self._feature_extractor.preprocess(resized_inputs),
               true_image_shapes)
 
@@ -790,31 +791,42 @@ class FasterRCNNMetaArch(model.DetectionModel):
           for the first stage RPN (in absolute coordinates).  Note that
           `num_anchors` can differ depending on whether the model is created in
           training or inference mode.
+        7) feature_maps: A single element list containing a 4-D float32 tensor
+          with shape batch_size, height, width, depth] representing the RPN
+          features to crop.
 
         (and if number_of_stages > 1):
-        7) refined_box_encodings: a 3-D tensor with shape
+        8) refined_box_encodings: a 3-D tensor with shape
           [total_num_proposals, num_classes, self._box_coder.code_size]
           representing predicted (final) refined box encodings, where
           total_num_proposals=batch_size*self._max_num_proposals. If using
           a shared box across classes the shape will instead be
           [total_num_proposals, 1, self._box_coder.code_size].
-        8) class_predictions_with_background: a 3-D tensor with shape
+        9) class_predictions_with_background: a 3-D tensor with shape
           [total_num_proposals, num_classes + 1] containing class
           predictions (logits) for each of the anchors, where
           total_num_proposals=batch_size*self._max_num_proposals.
           Note that this tensor *includes* background class predictions
           (at class index 0).
-        9) num_proposals: An int32 tensor of shape [batch_size] representing the
-          number of proposals generated by the RPN.  `num_proposals` allows us
-          to keep track of which entries are to be treated as zero paddings and
-          which are not since we always pad the number of proposals to be
+        10) num_proposals: An int32 tensor of shape [batch_size] representing
+          the number of proposals generated by the RPN.  `num_proposals` allows
+          us to keep track of which entries are to be treated as zero paddings
+          and which are not since we always pad the number of proposals to be
           `self.max_num_proposals` for each image.
-        10) proposal_boxes: A float32 tensor of shape
+        11) proposal_boxes: A float32 tensor of shape
           [batch_size, self.max_num_proposals, 4] representing
           decoded proposal bounding boxes in absolute coordinates.
-        11) mask_predictions: (optional) a 4-D tensor with shape
+        12) mask_predictions: (optional) a 4-D tensor with shape
           [total_num_padded_proposals, num_classes, mask_height, mask_width]
           containing instance mask predictions.
+        13) raw_detection_boxes: (optional) a
+          [batch_size, self.max_num_proposals, num_classes, 4] float32 tensor
+          with detections prior to NMS in normalized coordinates.
+        14) raw_detection_feature_map_indices: (optional) a
+          [batch_size, self.max_num_proposals, num_classes] int32 tensor with
+          indices indicating which feature map each raw detection box was
+          produced from. The indices correspond to the elements in the
+          'feature_maps' field.
 
     Raises:
       ValueError: If `predict` is called before `preprocess`.
@@ -868,6 +880,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
           for the first stage RPN (in absolute coordinates).  Note that
           `num_anchors` can differ depending on whether the model is created in
           training or inference mode.
+        7) feature_maps: A single element list containing a 4-D float32 tensor
+          with shape batch_size, height, width, depth] representing the RPN
+          features to crop.
     """
     (rpn_box_predictor_features, rpn_features_to_crop, anchors_boxlist,
      image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
@@ -907,6 +922,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
                     dtype=tf.float32),
         'anchors':
             anchors_boxlist.data['boxes'],
+        fields.PredictionFields.feature_maps: [rpn_features_to_crop]
     }
     return prediction_dict
 
@@ -985,18 +1001,25 @@ class FasterRCNNMetaArch(model.DetectionModel):
           of the image.
         6) box_classifier_features: a 4-D float32/bfloat16 tensor
           representing the features for each proposal.
+        If self._return_raw_detections_during_predict is True, the dictionary
+        will also contain:
+        7) raw_detection_boxes: a 4-D float32 tensor with shape
+          [batch_size, self.max_num_proposals, num_classes, 4] in normalized
+          coordinates.
+        8) raw_detection_feature_map_indices: a 3-D int32 tensor with shape
+          [batch_size, self.max_num_proposals, num_classes].
     """
     proposal_boxes_normalized, num_proposals = self._proposal_postprocess(
         rpn_box_encodings, rpn_objectness_predictions_with_background, anchors,
         image_shape, true_image_shapes)
     prediction_dict = self._box_prediction(rpn_features_to_crop,
                                            proposal_boxes_normalized,
-                                           image_shape)
+                                           image_shape, true_image_shapes)
     prediction_dict['num_proposals'] = num_proposals
     return prediction_dict
 
   def _box_prediction(self, rpn_features_to_crop, proposal_boxes_normalized,
-                      image_shape):
+                      image_shape, true_image_shapes):
     """Predicts the output tensors from second stage of Faster R-CNN.
 
     Args:
@@ -1008,6 +1031,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
         proposal boxes for all images in the batch.  These boxes are represented
         as normalized coordinates.
       image_shape: A 1D int32 tensors of size [4] containing the image shape.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
@@ -1034,6 +1061,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
           of the image.
         5) box_classifier_features: a 4-D float32/bfloat16 tensor
           representing the features for each proposal.
+        If self._return_raw_detections_during_predict is True, the dictionary
+        will also contain:
+        6) raw_detection_boxes: a 4-D float32 tensor with shape
+          [batch_size, self.max_num_proposals, num_classes, 4] in normalized
+          coordinates.
+        7) raw_detection_feature_map_indices: a 3-D int32 tensor with shape
+          [batch_size, self.max_num_proposals, num_classes].
+        8) final_anchors: a 3-D float tensor of shape [batch_size,
+          self.max_num_proposals, 4] containing the reference anchors for raw
+          detection boxes in normalized coordinates.
     """
     flattened_proposal_feature_maps = (
         self._compute_second_stage_input_feature_maps(
@@ -1071,10 +1108,54 @@ class FasterRCNNMetaArch(model.DetectionModel):
         'proposal_boxes': absolute_proposal_boxes,
         'box_classifier_features': box_classifier_features,
         'proposal_boxes_normalized': proposal_boxes_normalized,
+        'final_anchors': proposal_boxes_normalized
     }
 
+    if self._return_raw_detections_during_predict:
+      prediction_dict.update(self._raw_detections_and_feature_map_inds(
+          refined_box_encodings, absolute_proposal_boxes, true_image_shapes))
+
     return prediction_dict
 
+  def _raw_detections_and_feature_map_inds(
+      self, refined_box_encodings, absolute_proposal_boxes, true_image_shapes):
+    """Returns raw detections and feat map inds from where they originated.
+
+    Args:
+      refined_box_encodings: [total_num_proposals, num_classes,
+        self._box_coder.code_size] float32 tensor.
+      absolute_proposal_boxes: [batch_size, self.max_num_proposals, 4] float32
+        tensor representing decoded proposal bounding boxes in absolute
+        coordinates.
+      true_image_shapes: [batch, 3] int32 tensor where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
+
+    Returns:
+      A dictionary with raw detection boxes, and the feature map indices from
+      which they originated.
+    """
+    box_encodings_batch = tf.reshape(
+        refined_box_encodings,
+        [-1, self.max_num_proposals, refined_box_encodings.shape[1],
+         self._box_coder.code_size])
+    raw_detection_boxes_absolute = self._batch_decode_boxes(
+        box_encodings_batch, absolute_proposal_boxes)
+
+    raw_detection_boxes_normalized = shape_utils.static_or_dynamic_map_fn(
+        self._normalize_and_clip_boxes,
+        elems=[raw_detection_boxes_absolute, true_image_shapes],
+        dtype=tf.float32)
+    detection_feature_map_indices = tf.zeros_like(
+        raw_detection_boxes_normalized[:, :, :, 0], dtype=tf.int32)
+    return {
+        fields.PredictionFields.raw_detection_boxes:
+            raw_detection_boxes_normalized,
+        fields.PredictionFields.raw_detection_feature_map_indices:
+            detection_feature_map_indices
+    }
+
   def _extract_box_classifier_features(self, flattened_feature_maps):
     if self._feature_extractor_for_box_classifier_features == (
         _UNINITIALIZED_FEATURE_EXTRACTOR):
@@ -1416,11 +1497,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
         detection_boxes: [batch, max_detection, 4]
         detection_scores: [batch, max_detections]
         detection_multiclass_scores: [batch, max_detections, 2]
+        detection_anchor_indices: [batch, max_detections]
         detection_classes: [batch, max_detections]
           (this entry is only created if rpn_mode=False)
         num_detections: [batch]
-        raw_detection_boxes: [batch, max_detections, 4]
-        raw_detection_scores: [batch, max_detections, num_classes + 1]
+        raw_detection_boxes: [batch, total_detections, 4]
+        raw_detection_scores: [batch, total_detections, num_classes + 1]
 
     Raises:
       ValueError: If `predict` is called before `preprocess`.
@@ -1473,6 +1555,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
     if self._number_of_stages == 3:
       # Post processing is already performed in 3rd stage. We need to transfer
       # postprocessed tensors from `prediction_dict` to `detections_dict`.
+      # Remove any items from the prediction dictionary if they are not pure
+      # Tensors.
+      non_tensor_predictions = [
+          k for k, v in prediction_dict.items() if not isinstance(v, tf.Tensor)]
+      for k in non_tensor_predictions:
+        tf.logging.info('Removing {0} from prediction_dict'.format(k))
+        prediction_dict.pop(k)
       return prediction_dict
 
   def _add_detection_features_output_node(self, detection_boxes,
@@ -1621,8 +1710,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
         normalize_boxes,
         elems=[raw_proposal_boxes, image_shapes],
         dtype=tf.float32)
-    proposal_multiclass_scores = nmsed_additional_fields.get(
-        'multiclass_scores') if nmsed_additional_fields else None,
+    proposal_multiclass_scores = (
+        nmsed_additional_fields.get('multiclass_scores')
+        if nmsed_additional_fields else None)
     return (normalized_proposal_boxes, proposal_scores,
             proposal_multiclass_scores, num_proposals,
             raw_normalized_proposal_boxes, rpn_objectness_softmax)
@@ -1899,9 +1989,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
       A dictionary containing:
         `detection_boxes`: [batch, max_detection, 4] in normalized co-ordinates.
         `detection_scores`: [batch, max_detections]
-         detection_multiclass_scores: [batch, max_detections,
+         `detection_multiclass_scores`: [batch, max_detections,
           num_classes_with_background] tensor with class score distribution for
           post-processed detection boxes including background class if any.
+        `detection_anchor_indices`: [batch, max_detections] with anchor
+          indices.
         `detection_classes`: [batch, max_detections]
         `num_detections`: [batch]
         `detection_masks`:
@@ -1909,10 +2001,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
           that a pixel-wise sigmoid score converter is applied to the detection
           masks.
         `raw_detection_boxes`: [batch, total_detections, 4] tensor with decoded
-          detection boxes before Non-Max Suppression.
+          detection boxes in normalized coordinates, before Non-Max Suppression.
+          The value total_detections is the number of second stage anchors
+          (i.e. the total number of boxes before NMS).
         `raw_detection_scores`: [batch, total_detections,
           num_classes_with_background] tensor of multi-class scores for
-          raw detection boxes.
+          raw detection boxes. The value total_detections is the number of
+          second stage anchors (i.e. the total number of boxes before NMS).
     """
     refined_box_encodings_batch = tf.reshape(
         refined_box_encodings,
@@ -1943,8 +2038,14 @@ class FasterRCNNMetaArch(model.DetectionModel):
           mask_predictions, [-1, self.max_num_proposals,
                              self.num_classes, mask_height, mask_width])
 
+    batch_size = shape_utils.combined_static_and_dynamic_shape(
+        refined_box_encodings_batch)[0]
+    batch_anchor_indices = tf.tile(
+        tf.expand_dims(tf.range(self.max_num_proposals), 0),
+        multiples=[batch_size, 1])
     additional_fields = {
-        'multiclass_scores': class_predictions_with_background_batch_normalized
+        'multiclass_scores': class_predictions_with_background_batch_normalized,
+        'anchor_indices': tf.cast(batch_anchor_indices, tf.float32)
     }
     (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
      nmsed_additional_fields, num_detections) = self._second_stage_nms_fn(
@@ -1965,25 +2066,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
     else:
       raw_detection_boxes = tf.squeeze(refined_decoded_boxes_batch, axis=2)
 
-    def normalize_and_clip_boxes(args):
-      """Normalize and clip boxes."""
-      boxes_per_image = args[0]
-      image_shape = args[1]
-      normalized_boxes_per_image = box_list_ops.to_normalized_coordinates(
-          box_list.BoxList(boxes_per_image),
-          image_shape[0],
-          image_shape[1],
-          check_range=False).get()
-
-      normalized_boxes_per_image = box_list_ops.clip_to_window(
-          box_list.BoxList(normalized_boxes_per_image),
-          tf.constant([0.0, 0.0, 1.0, 1.0], tf.float32),
-          filter_nonoverlapping=False).get()
-
-      return normalized_boxes_per_image
-
     raw_normalized_detection_boxes = shape_utils.static_or_dynamic_map_fn(
-        normalize_and_clip_boxes,
+        self._normalize_and_clip_boxes,
         elems=[raw_detection_boxes, image_shapes],
         dtype=tf.float32)
 
@@ -1996,6 +2080,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
             nmsed_classes,
         fields.DetectionResultFields.detection_multiclass_scores:
             nmsed_additional_fields['multiclass_scores'],
+        fields.DetectionResultFields.detection_anchor_indices:
+            tf.cast(nmsed_additional_fields['anchor_indices'], tf.int32),
         fields.DetectionResultFields.num_detections:
             tf.cast(num_detections, dtype=tf.float32),
         fields.DetectionResultFields.raw_detection_boxes:
@@ -2041,6 +2127,35 @@ class FasterRCNNMetaArch(model.DetectionModel):
                       tf.stack([combined_shape[0], combined_shape[1],
                                 num_classes, 4]))
 
+  def _normalize_and_clip_boxes(self, boxes_and_image_shape):
+    """Normalize and clip boxes."""
+    boxes_per_image = boxes_and_image_shape[0]
+    image_shape = boxes_and_image_shape[1]
+
+    boxes_contains_classes_dim = boxes_per_image.shape.ndims == 3
+    if boxes_contains_classes_dim:
+      boxes_per_image = shape_utils.flatten_first_n_dimensions(
+          boxes_per_image, 2)
+    normalized_boxes_per_image = box_list_ops.to_normalized_coordinates(
+        box_list.BoxList(boxes_per_image),
+        image_shape[0],
+        image_shape[1],
+        check_range=False).get()
+
+    normalized_boxes_per_image = box_list_ops.clip_to_window(
+        box_list.BoxList(normalized_boxes_per_image),
+        tf.constant([0.0, 0.0, 1.0, 1.0], tf.float32),
+        filter_nonoverlapping=False).get()
+
+    if boxes_contains_classes_dim:
+      max_num_proposals, num_classes, _ = (
+          shape_utils.combined_static_and_dynamic_shape(
+              boxes_and_image_shape[0]))
+      normalized_boxes_per_image = shape_utils.expand_first_dimension(
+          normalized_boxes_per_image, [max_num_proposals, num_classes])
+
+    return normalized_boxes_per_image
+
   def loss(self, prediction_dict, true_image_shapes, scope=None):
     """Compute scalar loss tensors given prediction tensors.
 
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index 33cccfca..fef51502 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -244,7 +244,8 @@ class FasterRCNNMetaArchTest(
                                                 max_num_proposals,
                                                 initial_crop_size,
                                                 maxpool_stride,
-                                                3)
+                                                3),
+        'feature_maps': [(2, image_size, image_size, 512)]
     }
 
     for input_shape in input_shapes:
@@ -274,9 +275,12 @@ class FasterRCNNMetaArchTest(
                   'detection_boxes', 'detection_scores',
                   'detection_multiclass_scores', 'detection_classes',
                   'detection_masks', 'num_detections', 'mask_predictions',
-                  'raw_detection_boxes', 'raw_detection_scores'
+                  'raw_detection_boxes', 'raw_detection_scores',
+                  'detection_anchor_indices', 'final_anchors',
               ])))
       for key in expected_shapes:
+        if isinstance(tensor_dict_out[key], list):
+          continue
         self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
       self.assertAllEqual(tensor_dict_out['detection_boxes'].shape, [2, 5, 4])
       self.assertAllEqual(tensor_dict_out['detection_masks'].shape,
@@ -288,6 +292,101 @@ class FasterRCNNMetaArchTest(
       self.assertAllEqual(tensor_dict_out['mask_predictions'].shape,
                           [10, num_classes, 14, 14])
 
+  @parameterized.parameters(
+      {'use_keras': True},
+      {'use_keras': False},
+  )
+  def test_raw_detection_boxes_and_anchor_indices_correct(self, use_keras):
+    batch_size = 2
+    image_size = 10
+    max_num_proposals = 8
+    initial_crop_size = 3
+    maxpool_stride = 1
+
+    input_shapes = [(batch_size, image_size, image_size, 3),
+                    (None, image_size, image_size, 3),
+                    (batch_size, None, None, 3),
+                    (None, None, None, 3)]
+    expected_num_anchors = image_size * image_size * 3 * 3
+    expected_shapes = {
+        'rpn_box_predictor_features':
+        (batch_size, image_size, image_size, 512),
+        'rpn_features_to_crop': (batch_size, image_size, image_size, 3),
+        'image_shape': (4,),
+        'rpn_box_encodings': (batch_size, expected_num_anchors, 4),
+        'rpn_objectness_predictions_with_background':
+        (batch_size, expected_num_anchors, 2),
+        'anchors': (expected_num_anchors, 4),
+        'refined_box_encodings': (batch_size * max_num_proposals, 1, 4),
+        'class_predictions_with_background':
+            (batch_size * max_num_proposals, 2 + 1),
+        'num_proposals': (batch_size,),
+        'proposal_boxes': (batch_size, max_num_proposals, 4),
+        'proposal_boxes_normalized': (batch_size, max_num_proposals, 4),
+        'box_classifier_features':
+        self._get_box_classifier_features_shape(image_size,
+                                                batch_size,
+                                                max_num_proposals,
+                                                initial_crop_size,
+                                                maxpool_stride,
+                                                3),
+        'feature_maps': [(batch_size, image_size, image_size, 3)],
+        'raw_detection_feature_map_indices': (batch_size, max_num_proposals, 1),
+        'raw_detection_boxes': (batch_size, max_num_proposals, 1, 4),
+        'final_anchors': (batch_size, max_num_proposals, 4)
+    }
+
+    for input_shape in input_shapes:
+      test_graph = tf.Graph()
+      with test_graph.as_default():
+        model = self._build_model(
+            is_training=False,
+            use_keras=use_keras,
+            number_of_stages=2,
+            second_stage_batch_size=2,
+            share_box_across_classes=True,
+            return_raw_detections_during_predict=True)
+        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
+        _, true_image_shapes = model.preprocess(preprocessed_inputs)
+        predict_tensor_dict = model.predict(preprocessed_inputs,
+                                            true_image_shapes)
+        postprocess_tensor_dict = model.postprocess(predict_tensor_dict,
+                                                    true_image_shapes)
+        init_op = tf.global_variables_initializer()
+      with self.test_session(graph=test_graph) as sess:
+        sess.run(init_op)
+        [predict_dict_out, postprocess_dict_out] = sess.run(
+            [predict_tensor_dict, postprocess_tensor_dict], feed_dict={
+                preprocessed_inputs:
+                    np.zeros((batch_size, image_size, image_size, 3))})
+      self.assertEqual(
+          set(predict_dict_out.keys()),
+          set(expected_shapes.keys()))
+      for key in expected_shapes:
+        if isinstance(predict_dict_out[key], list):
+          continue
+        self.assertAllEqual(predict_dict_out[key].shape, expected_shapes[key])
+      # Verify that the raw detections from predict and postprocess are the
+      # same.
+      self.assertAllClose(
+          np.squeeze(predict_dict_out['raw_detection_boxes']),
+          postprocess_dict_out['raw_detection_boxes'])
+      # Verify that the raw detection boxes at detection anchor indices are the
+      # same as the postprocessed detections.
+      for i in range(batch_size):
+        num_detections_per_image = int(
+            postprocess_dict_out['num_detections'][i])
+        detection_boxes_per_image = postprocess_dict_out[
+            'detection_boxes'][i][:num_detections_per_image]
+        detection_anchor_indices_per_image = postprocess_dict_out[
+            'detection_anchor_indices'][i][:num_detections_per_image]
+        raw_detections_per_image = np.squeeze(predict_dict_out[
+            'raw_detection_boxes'][i])
+        raw_detections_at_anchor_indices = raw_detections_per_image[
+            detection_anchor_indices_per_image]
+        self.assertAllClose(detection_boxes_per_image,
+                            raw_detections_at_anchor_indices)
+
   @parameterized.parameters(
       {'masks_are_class_agnostic': False, 'use_keras': True},
       {'masks_are_class_agnostic': True, 'use_keras': True},
@@ -345,7 +444,8 @@ class FasterRCNNMetaArchTest(
               self._get_box_classifier_features_shape(
                   image_size, batch_size, max_num_proposals, initial_crop_size,
                   maxpool_stride, 3),
-          'mask_predictions': (2 * max_num_proposals, mask_shape_1, 14, 14)
+          'mask_predictions': (2 * max_num_proposals, mask_shape_1, 14, 14),
+          'feature_maps': [(2, image_size, image_size, 512)]
       }
 
       init_op = tf.global_variables_initializer()
@@ -359,8 +459,11 @@ class FasterRCNNMetaArchTest(
                     'rpn_box_encodings',
                     'rpn_objectness_predictions_with_background',
                     'anchors',
+                    'final_anchors',
                 ])))
         for key in expected_shapes:
+          if isinstance(tensor_dict_out[key], list):
+            continue
           self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
 
         anchors_shape_out = tensor_dict_out['anchors'].shape
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index c0d99b1b..ef8011a5 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -118,27 +118,30 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     text_format.Merge(hyperparams_text_proto, hyperparams)
     return hyperparams_builder.KerasLayerHyperparams(hyperparams)
 
-  def _get_second_stage_box_predictor_text_proto(self):
+  def _get_second_stage_box_predictor_text_proto(
+      self, share_box_across_classes=False):
+    share_box_field = 'true' if share_box_across_classes else 'false'
     box_predictor_text_proto = """
-      mask_rcnn_box_predictor {
-        fc_hyperparams {
+      mask_rcnn_box_predictor {{
+        fc_hyperparams {{
           op: FC
           activation: NONE
-          regularizer {
-            l2_regularizer {
+          regularizer {{
+            l2_regularizer {{
               weight: 0.0005
-            }
-          }
-          initializer {
-            variance_scaling_initializer {
+            }}
+          }}
+          initializer {{
+            variance_scaling_initializer {{
               factor: 1.0
               uniform: true
               mode: FAN_AVG
-            }
-          }
-        }
-      }
-    """
+            }}
+          }}
+        }}
+        share_box_across_classes: {share_box_across_classes}
+      }}
+    """.format(share_box_across_classes=share_box_field)
     return box_predictor_text_proto
 
   def _add_mask_to_second_stage_box_predictor_text_proto(
@@ -169,10 +172,11 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
 
   def _get_second_stage_box_predictor(self, num_classes, is_training,
                                       predict_masks, masks_are_class_agnostic,
+                                      share_box_across_classes=False,
                                       use_keras=False):
     box_predictor_proto = box_predictor_pb2.BoxPredictor()
-    text_format.Merge(self._get_second_stage_box_predictor_text_proto(),
-                      box_predictor_proto)
+    text_format.Merge(self._get_second_stage_box_predictor_text_proto(
+        share_box_across_classes), box_predictor_proto)
     if predict_masks:
       text_format.Merge(
           self._add_mask_to_second_stage_box_predictor_text_proto(
@@ -219,7 +223,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                    clip_anchors_to_image=False,
                    use_matmul_gather_in_matcher=False,
                    use_static_shapes=False,
-                   calibration_mapping_value=None):
+                   calibration_mapping_value=None,
+                   share_box_across_classes=False,
+                   return_raw_detections_during_predict=False):
 
     def image_resizer_fn(image, masks=None):
       """Fake image resizer function."""
@@ -404,6 +410,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         'clip_anchors_to_image': clip_anchors_to_image,
         'use_static_shapes': use_static_shapes,
         'resize_masks': True,
+        'return_raw_detections_during_predict':
+            return_raw_detections_during_predict
     }
 
     return self._get_model(
@@ -412,7 +420,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
             is_training=is_training,
             use_keras=use_keras,
             predict_masks=predict_masks,
-            masks_are_class_agnostic=masks_are_class_agnostic), **common_kwargs)
+            masks_are_class_agnostic=masks_are_class_agnostic,
+            share_box_across_classes=share_box_across_classes), **common_kwargs)
 
   @parameterized.parameters(
       {'use_static_shapes': False, 'use_keras': True},
@@ -538,7 +547,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       expected_output_keys = set([
           'rpn_box_predictor_features', 'rpn_features_to_crop', 'image_shape',
           'rpn_box_encodings', 'rpn_objectness_predictions_with_background',
-          'anchors'])
+          'anchors', 'feature_maps'])
       # At training time, anchors that exceed image bounds are pruned.  Thus
       # the `expected_num_anchors` in the above inference mode test is now
       # a strict upper bound on the number of anchors.
@@ -612,7 +621,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                           expected_output_shapes['proposal_boxes_normalized'])
       self.assertAllEqual(results[11].shape,
                           expected_output_shapes['box_classifier_features'])
-
+      self.assertAllEqual(results[12].shape,
+                          expected_output_shapes['final_anchors'])
     batch_size = 2
     image_size = 10
     max_num_proposals = 8
@@ -648,7 +658,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
               prediction_dict['num_proposals'],
               prediction_dict['proposal_boxes'],
               prediction_dict['proposal_boxes_normalized'],
-              prediction_dict['box_classifier_features'])
+              prediction_dict['box_classifier_features'],
+              prediction_dict['final_anchors'])
 
     expected_num_anchors = image_size * image_size * 3 * 3
     expected_shapes = {
@@ -671,7 +682,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                                                 max_num_proposals,
                                                 initial_crop_size,
                                                 maxpool_stride,
-                                                3)
+                                                3),
+        'feature_maps': [(2, image_size, image_size, 512)],
+        'final_anchors': (2, max_num_proposals, 4)
     }
 
     if use_static_shapes:
@@ -702,6 +715,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         self.assertEqual(set(tensor_dict_out.keys()),
                          set(expected_shapes.keys()))
         for key in expected_shapes:
+          if isinstance(tensor_dict_out[key], list):
+            continue
           self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
 
   @parameterized.parameters(
@@ -748,7 +763,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
               result_tensor_dict['rpn_objectness_predictions_with_background'],
               result_tensor_dict['rpn_features_to_crop'],
               result_tensor_dict['rpn_box_predictor_features'],
-              updates
+              updates,
+              result_tensor_dict['final_anchors'],
              )
 
     image_shape = (batch_size, image_size, image_size, 3)
@@ -785,7 +801,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                 image_size, batch_size, max_num_proposals, initial_crop_size,
                 maxpool_stride, 3),
         'rpn_objectness_predictions_with_background':
-        (2, image_size * image_size * 9, 2)
+        (2, image_size * image_size * 9, 2),
+        'final_anchors': (2, max_num_proposals, 4)
     }
     # TODO(rathodv): Possibly change utils/test_case.py to accept dictionaries
     # and return dicionaries so don't have to rely on the order of tensors.
@@ -805,6 +822,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                         expected_shapes['rpn_features_to_crop'])
     self.assertAllEqual(results[8].shape,
                         expected_shapes['rpn_box_predictor_features'])
+    self.assertAllEqual(results[10].shape,
+                        expected_shapes['final_anchors'])
 
   @parameterized.parameters(
       {'use_static_shapes': False, 'pad_to_max_dimension': None,
@@ -1082,7 +1101,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
               detections['detection_scores'], detections['detection_classes'],
               detections['raw_detection_boxes'],
               detections['raw_detection_scores'],
-              detections['detection_multiclass_scores'])
+              detections['detection_multiclass_scores'],
+              detections['detection_anchor_indices'])
 
     proposal_boxes = np.array(
         [[[1, 1, 2, 3],
@@ -1110,6 +1130,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                                  [images, refined_box_encodings,
                                   class_predictions_with_background,
                                   num_proposals, proposal_boxes])
+    # Note that max_total_detections=5 in the NMS config.
     expected_num_detections = [5, 4]
     expected_detection_classes = [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]]
     expected_detection_scores = [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]]
@@ -1123,6 +1144,10 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                                    [1, 1, 1],
                                    [1, 1, 1],
                                    [0, 0, 0]]]
+    # Note that a single anchor can be used for multiple detections (predictions
+    # are made independently per class).
+    expected_anchor_indices = [[0, 1, 2, 0, 1],
+                               [0, 1, 0, 1]]
 
     h = float(image_shape[1])
     w = float(image_shape[2])
@@ -1143,6 +1168,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                           expected_detection_classes[indx][0:num_proposals])
       self.assertAllClose(results[6][indx][0:num_proposals],
                           expected_multiclass_scores[indx][0:num_proposals])
+      self.assertAllClose(results[7][indx][0:num_proposals],
+                          expected_anchor_indices[indx][0:num_proposals])
 
     self.assertAllClose(results[4], expected_raw_detection_boxes)
     self.assertAllClose(results[5],
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 69318b0c..850140f0 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -82,7 +82,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                clip_anchors_to_image=False,
                use_static_shapes=False,
                resize_masks=False,
-               freeze_batchnorm=False):
+               freeze_batchnorm=False,
+               return_raw_detections_during_predict=False):
     """RFCNMetaArch Constructor.
 
     Args:
@@ -188,6 +189,9 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         training or not. When training with a small batch size (e.g. 1), it is
         desirable to freeze batch norm update and use pretrained batch norm
         params.
+      return_raw_detections_during_predict: Whether to return raw detection
+        boxes in the predict() method. These are decoded boxes that have not
+        been through postprocessing (i.e. NMS). Default False.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`
@@ -234,7 +238,9 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         clip_anchors_to_image,
         use_static_shapes,
         resize_masks,
-        freeze_batchnorm=freeze_batchnorm)
+        freeze_batchnorm=freeze_batchnorm,
+        return_raw_detections_during_predict=(
+            return_raw_detections_during_predict))
 
     self._rfcn_box_predictor = second_stage_rfcn_box_predictor
 
@@ -335,7 +341,11 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         'proposal_boxes': absolute_proposal_boxes,
         'box_classifier_features': box_classifier_features,
         'proposal_boxes_normalized': proposal_boxes_normalized,
+        'final_anchors': absolute_proposal_boxes
     }
+    if self._return_raw_detections_during_predict:
+      prediction_dict.update(self._raw_detections_and_feature_map_inds(
+          refined_box_encodings, absolute_proposal_boxes))
     return prediction_dict
 
   def regularization_losses(self):
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch_test.py b/research/object_detection/meta_architectures/rfcn_meta_arch_test.py
index 829140ac..d99782cc 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch_test.py
@@ -24,7 +24,9 @@ from object_detection.meta_architectures import rfcn_meta_arch
 class RFCNMetaArchTest(
     faster_rcnn_meta_arch_test_lib.FasterRCNNMetaArchTestBase):
 
-  def _get_second_stage_box_predictor_text_proto(self):
+  def _get_second_stage_box_predictor_text_proto(
+      self, share_box_across_classes=False):
+    del share_box_across_classes
     box_predictor_text_proto = """
       rfcn_box_predictor {
         conv_hyperparams {
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 3be4fe92..ee7a3fa2 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -254,13 +254,21 @@ class SSDKerasFeatureExtractor(tf.keras.Model):
       the model graph.
     """
     variables_to_restore = {}
-    for variable in self.variables:
-      # variable.name includes ":0" at the end, but the names in the checkpoint
-      # do not have the suffix ":0". So, we strip it here.
-      var_name = variable.name[:-2]
-      if var_name.startswith(feature_extractor_scope + '/'):
-        var_name = var_name.replace(feature_extractor_scope + '/', '')
-      variables_to_restore[var_name] = variable
+    if tf.executing_eagerly():
+      for variable in self.variables:
+        # variable.name includes ":0" at the end, but the names in the
+        # checkpoint do not have the suffix ":0". So, we strip it here.
+        var_name = variable.name[:-2]
+        if var_name.startswith(feature_extractor_scope + '/'):
+          var_name = var_name.replace(feature_extractor_scope + '/', '')
+        variables_to_restore[var_name] = variable
+    else:
+      # b/137854499: use global_variables.
+      for variable in variables_helper.get_global_variables_safely():
+        var_name = variable.op.name
+        if var_name.startswith(feature_extractor_scope + '/'):
+          var_name = var_name.replace(feature_extractor_scope + '/', '')
+          variables_to_restore[var_name] = variable
 
     return variables_to_restore
 
@@ -295,7 +303,9 @@ class SSDMetaArch(model.DetectionModel):
                expected_loss_weights_fn=None,
                use_confidences_as_targets=False,
                implicit_example_weight=0.5,
-               equalization_loss_config=None):
+               equalization_loss_config=None,
+               return_raw_detections_during_predict=False,
+               nms_on_host=True):
     """SSDMetaArch Constructor.
 
     TODO(rathodv,jonathanhuang): group NMS parameters + score converter into
@@ -371,6 +381,11 @@ class SSDMetaArch(model.DetectionModel):
         for the implicit negative examples.
       equalization_loss_config: a namedtuple that specifies configs for
         computing equalization loss.
+      return_raw_detections_during_predict: Whether to return raw detection
+        boxes in the predict() method. These are decoded boxes that have not
+        been through postprocessing (i.e. NMS). Default False.
+      nms_on_host: boolean (default: True) controlling whether NMS should be
+        carried out on the host (outside of TPU).
     """
     super(SSDMetaArch, self).__init__(num_classes=box_predictor.num_classes)
     self._is_training = is_training
@@ -438,6 +453,10 @@ class SSDMetaArch(model.DetectionModel):
 
     self._equalization_loss_config = equalization_loss_config
 
+    self._return_raw_detections_during_predict = (
+        return_raw_detections_during_predict)
+    self._nms_on_host = nms_on_host
+
   @property
   def anchors(self):
     if not self._anchors:
@@ -475,17 +494,10 @@ class SSDMetaArch(model.DetectionModel):
     Raises:
       ValueError: if inputs tensor does not have type tf.float32
     """
-    if inputs.dtype is not tf.float32:
-      raise ValueError('`preprocess` expects a tf.float32 tensor')
     with tf.name_scope('Preprocessor'):
-      # TODO(jonathanhuang): revisit whether to always use batch size as
-      # the number of parallel iterations vs allow for dynamic batching.
-      outputs = shape_utils.static_or_dynamic_map_fn(
-          self._image_resizer_fn,
-          elems=inputs,
-          dtype=[tf.float32, tf.int32])
-      resized_inputs = outputs[0]
-      true_image_shapes = outputs[1]
+      (resized_inputs,
+       true_image_shapes) = shape_utils.resize_images_and_return_shapes(
+           inputs, self._image_resizer_fn)
 
       return (self._feature_extractor.preprocess(resized_inputs),
               true_image_shapes)
@@ -560,6 +572,14 @@ class SSDMetaArch(model.DetectionModel):
           [batch, height_i, width_i, depth_i].
         5) anchors: 2-D float tensor of shape [num_anchors, 4] containing
           the generated anchors in normalized coordinates.
+        6) final_anchors: 3-D float tensor of shape [batch_size, num_anchors, 4]
+          containing the generated anchors in normalized coordinates.
+        If self._return_raw_detections_during_predict is True, the dictionary
+        will also contain:
+        7) raw_detection_boxes: a 4-D float32 tensor with shape
+          [batch_size, self.max_num_proposals, 4] in normalized coordinates.
+        8) raw_detection_feature_map_indices: a 3-D int32 tensor with shape
+          [batch_size, self.max_num_proposals].
     """
     if self._inplace_batchnorm_update:
       batchnorm_updates_collections = None
@@ -581,11 +601,11 @@ class SSDMetaArch(model.DetectionModel):
         feature_maps)
     image_shape = shape_utils.combined_static_and_dynamic_shape(
         preprocessed_inputs)
-    self._anchors = box_list_ops.concatenate(
-        self._anchor_generator.generate(
-            feature_map_spatial_dims,
-            im_height=image_shape[1],
-            im_width=image_shape[2]))
+    boxlist_list = self._anchor_generator.generate(
+        feature_map_spatial_dims,
+        im_height=image_shape[1],
+        im_width=image_shape[2])
+    self._anchors = box_list_ops.concatenate(boxlist_list)
     if self._box_predictor.is_keras_model:
       predictor_results_dict = self._box_predictor(feature_maps)
     else:
@@ -596,9 +616,15 @@ class SSDMetaArch(model.DetectionModel):
         predictor_results_dict = self._box_predictor.predict(
             feature_maps, self._anchor_generator.num_anchors_per_location())
     predictions_dict = {
-        'preprocessed_inputs': preprocessed_inputs,
-        'feature_maps': feature_maps,
-        'anchors': self._anchors.get()
+        'preprocessed_inputs':
+            preprocessed_inputs,
+        'feature_maps':
+            feature_maps,
+        'anchors':
+            self._anchors.get(),
+        'final_anchors':
+            tf.tile(
+                tf.expand_dims(self._anchors.get(), 0), [image_shape[0], 1, 1])
     }
     for prediction_key, prediction_list in iter(predictor_results_dict.items()):
       prediction = tf.concat(prediction_list, axis=1)
@@ -606,10 +632,29 @@ class SSDMetaArch(model.DetectionModel):
           prediction.shape[2] == 1):
         prediction = tf.squeeze(prediction, axis=2)
       predictions_dict[prediction_key] = prediction
+    if self._return_raw_detections_during_predict:
+      predictions_dict.update(self._raw_detections_and_feature_map_inds(
+          predictions_dict['box_encodings'], boxlist_list))
     self._batched_prediction_tensor_names = [x for x in predictions_dict
                                              if x != 'anchors']
     return predictions_dict
 
+  def _raw_detections_and_feature_map_inds(self, box_encodings, boxlist_list):
+    anchors = self._anchors.get()
+    raw_detection_boxes, _ = self._batch_decode(box_encodings, anchors)
+    batch_size, _, _ = shape_utils.combined_static_and_dynamic_shape(
+        raw_detection_boxes)
+    feature_map_indices = (
+        self._anchor_generator.anchor_index_to_feature_map_index(boxlist_list))
+    feature_map_indices_batched = tf.tile(
+        tf.expand_dims(feature_map_indices, 0),
+        multiples=[batch_size, 1])
+    return {
+        fields.PredictionFields.raw_detection_boxes: raw_detection_boxes,
+        fields.PredictionFields.raw_detection_feature_map_indices:
+            feature_map_indices_batched
+    }
+
   def _get_feature_map_spatial_dims(self, feature_maps):
     """Return list of spatial dimensions for each feature map in a list.
 
@@ -719,7 +764,9 @@ class SSDMetaArch(model.DetectionModel):
           'multiclass_scores': detection_scores_with_background
       }
       if self._anchors is not None:
-        anchor_indices = tf.range(self._anchors.num_boxes_static())
+        num_boxes = (self._anchors.num_boxes_static() or
+                     self._anchors.num_boxes())
+        anchor_indices = tf.range(num_boxes)
         batch_anchor_indices = tf.tile(
             tf.expand_dims(anchor_indices, 0), [batch_size, 1])
         # All additional fields need to be float.
@@ -730,14 +777,30 @@ class SSDMetaArch(model.DetectionModel):
         detection_keypoints = tf.identity(
             detection_keypoints, 'raw_keypoint_locations')
         additional_fields[fields.BoxListFields.keypoints] = detection_keypoints
+
+      def _non_max_suppression_wrapper(kwargs):
+        if self._nms_on_host:
+          # Note: NMS is not memory efficient on TPU. This force the NMS to run
+          # outside of TPU.
+          return tf.contrib.tpu.outside_compilation(
+              lambda x: self._non_max_suppression_fn(**x), kwargs)
+        else:
+          return self._non_max_suppression_fn(**kwargs)
+
       (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-       nmsed_additional_fields, num_detections) = self._non_max_suppression_fn(
+       nmsed_additional_fields,
+       num_detections) = _non_max_suppression_wrapper({
+           'boxes':
            detection_boxes,
+           'scores':
            detection_scores,
-           clip_window=self._compute_clip_window(preprocessed_images,
-                                                 true_image_shapes),
-           additional_fields=additional_fields,
-           masks=prediction_dict.get('mask_predictions'))
+           'clip_window':
+           self._compute_clip_window(preprocessed_images, true_image_shapes),
+           'additional_fields':
+           additional_fields,
+           'masks':
+           prediction_dict.get('mask_predictions')
+       })
       detection_dict = {
           fields.DetectionResultFields.detection_boxes:
               nmsed_boxes,
@@ -1058,6 +1121,15 @@ class SSDMetaArch(model.DetectionModel):
         with rows of the Match objects corresponding to groundtruth boxes
         and columns corresponding to anchors.
     """
+    # TODO(rathodv): Add a test for these summaries.
+    try:
+      # TODO(kaftan): Integrate these summaries into the v2 style loops
+      with tf.compat.v2.init_scope():
+        if tf.compat.v2.executing_eagerly():
+          return
+    except AttributeError:
+      pass
+
     avg_num_gt_boxes = tf.reduce_mean(
         tf.cast(
             tf.stack([tf.shape(x)[0] for x in groundtruth_boxes_list]),
@@ -1078,14 +1150,6 @@ class SSDMetaArch(model.DetectionModel):
         tf.cast(
             tf.stack([match.num_ignored_columns() for match in match_list]),
             dtype=tf.float32))
-    # TODO(rathodv): Add a test for these summaries.
-    try:
-      # TODO(kaftan): Integrate these summaries into the v2 style loops
-      with tf.compat.v2.init_scope():
-        if tf.compat.v2.executing_eagerly():
-          return
-    except AttributeError:
-      pass
 
     tf.summary.scalar('AvgNumGroundtruthBoxesPerImage',
                       avg_num_gt_boxes,
@@ -1232,26 +1296,27 @@ class SSDMetaArch(model.DetectionModel):
       ValueError: if fine_tune_checkpoint_type is neither `classification`
         nor `detection`.
     """
-    if fine_tune_checkpoint_type not in ['detection', 'classification']:
-      raise ValueError('Not supported fine_tune_checkpoint_type: {}'.format(
-          fine_tune_checkpoint_type))
-
     if fine_tune_checkpoint_type == 'classification':
       return self._feature_extractor.restore_from_classification_checkpoint_fn(
           self._extract_features_scope)
 
-    if fine_tune_checkpoint_type == 'detection':
+    elif fine_tune_checkpoint_type == 'detection':
       variables_to_restore = {}
       if tf.executing_eagerly():
-        for variable in self.variables:
-          # variable.name includes ":0" at the end, but the names in the
-          # checkpoint do not have the suffix ":0". So, we strip it here.
-          var_name = variable.name[:-2]
-          if load_all_detection_checkpoint_vars:
+        if load_all_detection_checkpoint_vars:
+          # Grab all detection vars by name
+          for variable in self.variables:
+            # variable.name includes ":0" at the end, but the names in the
+            # checkpoint do not have the suffix ":0". So, we strip it here.
+            var_name = variable.name[:-2]
+            variables_to_restore[var_name] = variable
+        else:
+          # Grab just the feature extractor vars by name
+          for variable in self._feature_extractor.variables:
+            # variable.name includes ":0" at the end, but the names in the
+            # checkpoint do not have the suffix ":0". So, we strip it here.
+            var_name = variable.name[:-2]
             variables_to_restore[var_name] = variable
-          else:
-            if var_name.startswith(self._extract_features_scope):
-              variables_to_restore[var_name] = variable
       else:
         for variable in variables_helper.get_global_variables_safely():
           var_name = variable.op.name
@@ -1261,7 +1326,11 @@ class SSDMetaArch(model.DetectionModel):
             if var_name.startswith(self._extract_features_scope):
               variables_to_restore[var_name] = variable
 
-    return variables_to_restore
+      return variables_to_restore
+
+    else:
+      raise ValueError('Not supported fine_tune_checkpoint_type: {}'.format(
+          fine_tune_checkpoint_type))
 
   def updates(self):
     """Returns a list of update operators for this model.
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index 93b708b9..59946e3a 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -49,7 +49,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       predict_mask=False,
       use_static_shapes=False,
       nms_max_size_per_class=5,
-      calibration_mapping_value=None):
+      calibration_mapping_value=None,
+      return_raw_detections_during_predict=False):
     return super(SsdMetaArchTest, self)._create_model(
         model_fn=ssd_meta_arch.SSDMetaArch,
         apply_hard_mining=apply_hard_mining,
@@ -63,7 +64,9 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         predict_mask=predict_mask,
         use_static_shapes=use_static_shapes,
         nms_max_size_per_class=nms_max_size_per_class,
-        calibration_mapping_value=calibration_mapping_value)
+        calibration_mapping_value=calibration_mapping_value,
+        return_raw_detections_during_predict=(
+            return_raw_detections_during_predict))
 
   def test_preprocess_preserves_shapes_with_dynamic_input_image(
       self, use_keras):
@@ -105,6 +108,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         self.assertIn('class_predictions_with_background', prediction_dict)
         self.assertIn('feature_maps', prediction_dict)
         self.assertIn('anchors', prediction_dict)
+        self.assertIn('final_anchors', prediction_dict)
 
         init_op = tf.global_variables_initializer()
       with self.test_session(graph=tf_graph) as sess:
@@ -121,6 +125,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
 
       self.assertAllEqual(prediction_out['box_encodings'].shape,
                           expected_box_encodings_shape_out)
+      self.assertAllEqual(prediction_out['final_anchors'].shape,
+                          (batch_size, num_anchors, 4))
       self.assertAllEqual(
           prediction_out['class_predictions_with_background'].shape,
           expected_class_predictions_with_background_shape_out)
@@ -137,7 +143,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       return (predictions['box_encodings'],
               predictions['class_predictions_with_background'],
               predictions['feature_maps'],
-              predictions['anchors'])
+              predictions['anchors'], predictions['final_anchors'])
     batch_size = 3
     image_size = 2
     channels = 3
@@ -145,11 +151,83 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                                  channels).astype(np.float32)
     expected_box_encodings_shape = (batch_size, num_anchors, code_size)
     expected_class_predictions_shape = (batch_size, num_anchors, num_classes+1)
-    (box_encodings, class_predictions, _, _) = self.execute(graph_fn,
-                                                            [input_image])
+    final_anchors_shape = (batch_size, num_anchors, 4)
+    (box_encodings, class_predictions, _, _, final_anchors) = self.execute(
+        graph_fn, [input_image])
     self.assertAllEqual(box_encodings.shape, expected_box_encodings_shape)
     self.assertAllEqual(class_predictions.shape,
                         expected_class_predictions_shape)
+    self.assertAllEqual(final_anchors.shape, final_anchors_shape)
+
+  def test_predict_with_raw_output_fields(self, use_keras):
+    with tf.Graph().as_default():
+      _, num_classes, num_anchors, code_size = self._create_model(
+          use_keras=use_keras)
+
+    def graph_fn(input_image):
+      model, _, _, _ = self._create_model(
+          return_raw_detections_during_predict=True)
+      predictions = model.predict(input_image, true_image_shapes=None)
+      return (predictions['box_encodings'],
+              predictions['class_predictions_with_background'],
+              predictions['feature_maps'],
+              predictions['anchors'], predictions['final_anchors'],
+              predictions['raw_detection_boxes'],
+              predictions['raw_detection_feature_map_indices'])
+    batch_size = 3
+    image_size = 2
+    channels = 3
+    input_image = np.random.rand(batch_size, image_size, image_size,
+                                 channels).astype(np.float32)
+    expected_box_encodings_shape = (batch_size, num_anchors, code_size)
+    expected_class_predictions_shape = (batch_size, num_anchors, num_classes+1)
+    final_anchors_shape = (batch_size, num_anchors, 4)
+    expected_raw_detection_boxes_shape = (batch_size, num_anchors, 4)
+    (box_encodings, class_predictions, _, _, final_anchors, raw_detection_boxes,
+     raw_detection_feature_map_indices) = self.execute(
+         graph_fn, [input_image])
+    self.assertAllEqual(box_encodings.shape, expected_box_encodings_shape)
+    self.assertAllEqual(class_predictions.shape,
+                        expected_class_predictions_shape)
+    self.assertAllEqual(final_anchors.shape, final_anchors_shape)
+    self.assertAllEqual(raw_detection_boxes.shape,
+                        expected_raw_detection_boxes_shape)
+    self.assertAllEqual(raw_detection_feature_map_indices,
+                        np.zeros((batch_size, num_anchors)))
+
+  def test_raw_detection_boxes_agree_predict_postprocess(self, use_keras):
+    batch_size = 2
+    image_size = 2
+    input_shapes = [(batch_size, image_size, image_size, 3),
+                    (None, image_size, image_size, 3),
+                    (batch_size, None, None, 3),
+                    (None, None, None, 3)]
+
+    for input_shape in input_shapes:
+      tf_graph = tf.Graph()
+      with tf_graph.as_default():
+        model, _, _, _ = self._create_model(
+            use_keras=use_keras, return_raw_detections_during_predict=True)
+        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
+        preprocessed_inputs, true_image_shapes = model.preprocess(
+            input_placeholder)
+        prediction_dict = model.predict(preprocessed_inputs,
+                                        true_image_shapes)
+        raw_detection_boxes_predict = prediction_dict['raw_detection_boxes']
+        detections = model.postprocess(prediction_dict, true_image_shapes)
+        raw_detection_boxes_postprocess = detections['raw_detection_boxes']
+        init_op = tf.global_variables_initializer()
+      with self.test_session(graph=tf_graph) as sess:
+        sess.run(init_op)
+        raw_detection_boxes_predict_out, raw_detection_boxes_postprocess_out = (
+            sess.run(
+                [raw_detection_boxes_predict, raw_detection_boxes_postprocess],
+                feed_dict={
+                    input_placeholder:
+                        np.random.uniform(size=(batch_size, 2, 2, 3))}))
+
+      self.assertAllEqual(raw_detection_boxes_predict_out,
+                          raw_detection_boxes_postprocess_out)
 
   def test_postprocess_results_are_correct(self, use_keras):
     batch_size = 2
@@ -188,7 +266,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                             [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]]]
     raw_detection_scores = [[[0, 0], [0, 0], [0, 0], [0, 0]],
                             [[0, 0], [0, 0], [0, 0], [0, 0]]]
-    detection_anchor_indices = [[0, 2, 1, 0, 0], [0, 2, 1, 0, 0]]
+    detection_anchor_indices_sets = [[0, 1, 2], [0, 1, 2]]
 
     for input_shape in input_shapes:
       tf_graph = tf.Graph()
@@ -230,8 +308,9 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                           raw_detection_boxes)
       self.assertAllEqual(detections_out['raw_detection_scores'],
                           raw_detection_scores)
-      self.assertAllEqual(detections_out['detection_anchor_indices'],
-                          detection_anchor_indices)
+      for idx in range(batch_size):
+        self.assertSameElements(detections_out['detection_anchor_indices'][idx],
+                                detection_anchor_indices_sets[idx])
 
   def test_postprocess_results_are_correct_static(self, use_keras):
     with tf.Graph().as_default():
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
index 05e03921..ddc3ec0e 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
@@ -129,7 +129,8 @@ class SSDMetaArchTestBase(test_case.TestCase):
       predict_mask=False,
       use_static_shapes=False,
       nms_max_size_per_class=5,
-      calibration_mapping_value=None):
+      calibration_mapping_value=None,
+      return_raw_detections_during_predict=False):
     is_training = False
     num_classes = 1
     mock_anchor_generator = MockAnchorGenerator2x2()
@@ -238,6 +239,8 @@ class SSDMetaArchTestBase(test_case.TestCase):
         add_background_class=add_background_class,
         random_example_sampler=random_example_sampler,
         expected_loss_weights_fn=expected_loss_weights_fn,
+        return_raw_detections_during_predict=(
+            return_raw_detections_during_predict),
         **kwargs)
     return model, num_classes, mock_anchor_generator.num_anchors(), code_size
 
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 67074548..e58be719 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -267,6 +267,13 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
     # Make sure to set the Keras learning phase. True during training,
     # False for inference.
     tf.keras.backend.set_learning_phase(is_training)
+    # Set policy for mixed-precision training with Keras-based models.
+    if use_tpu and train_config.use_bfloat16:
+      from tensorflow.python.keras.engine import base_layer_utils  # pylint: disable=g-import-not-at-top
+      # Enable v2 behavior, as `mixed_bfloat16` is only supported in TF 2.0.
+      base_layer_utils.enable_v2_dtype_behavior()
+      tf.compat.v2.keras.mixed_precision.experimental.set_policy(
+          'mixed_bfloat16')
     detection_model = detection_model_fn(
         is_training=is_training, add_summaries=(not use_tpu))
     scaffold_fn = None
@@ -315,7 +322,8 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
             features[fields.InputDataFields.true_image_shape]))
 
     if mode == tf.estimator.ModeKeys.TRAIN:
-      if train_config.fine_tune_checkpoint and hparams.load_pretrained:
+      load_pretrained = hparams.load_pretrained if hparams else False
+      if train_config.fine_tune_checkpoint and load_pretrained:
         if not train_config.fine_tune_checkpoint_type:
           # train_config.from_detection_checkpoint field is deprecated. For
           # backward compatibility, set train_config.fine_tune_checkpoint_type
@@ -449,6 +457,10 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
           original_image_spatial_shapes=original_image_spatial_shapes,
           true_image_shapes=true_image_shapes)
 
+      if fields.InputDataFields.image_additional_channels in features:
+        eval_dict[fields.InputDataFields.image_additional_channels] = features[
+            fields.InputDataFields.image_additional_channels]
+
       if class_agnostic:
         category_index = label_map_util.create_class_agnostic_category_index()
       else:
diff --git a/research/object_detection/model_lib_v2.py b/research/object_detection/model_lib_v2.py
index 9f40b630..ddac5af0 100644
--- a/research/object_detection/model_lib_v2.py
+++ b/research/object_detection/model_lib_v2.py
@@ -47,9 +47,7 @@ MODEL_BUILD_UTIL_MAP = model_lib.MODEL_BUILD_UTIL_MAP
 
 def _compute_losses_and_predictions_dicts(
     model, features, labels,
-    add_regularization_loss=True,
-    use_tpu=False,
-    use_bfloat16=False):
+    add_regularization_loss=True):
   """Computes the losses dict and predictions dict for a model on inputs.
 
   Args:
@@ -88,8 +86,6 @@ def _compute_losses_and_predictions_dicts(
           float32 tensor containing keypoints for each box.
     add_regularization_loss: Whether or not to include the model's
       regularization loss in the losses dictionary.
-    use_tpu: Whether computation should happen on a TPU.
-    use_bfloat16: Whether computation on a TPU should use bfloat16.
 
   Returns:
     A tuple containing the losses dictionary (with the total loss under
@@ -100,18 +96,10 @@ def _compute_losses_and_predictions_dicts(
   model_lib.provide_groundtruth(model, labels)
   preprocessed_images = features[fields.InputDataFields.image]
 
-  # TODO(kaftan): Check how we're supposed to do this mixed precision stuff
-  ## in TF2 TPUStrategy + Keras
-  if use_tpu and use_bfloat16:
-    with tf.contrib.tpu.bfloat16_scope():
-      prediction_dict = model.predict(
-          preprocessed_images,
-          features[fields.InputDataFields.true_image_shape])
-      prediction_dict = ops.bfloat16_to_float32_nested(prediction_dict)
-  else:
-    prediction_dict = model.predict(
-        preprocessed_images,
-        features[fields.InputDataFields.true_image_shape])
+  prediction_dict = model.predict(
+      preprocessed_images,
+      features[fields.InputDataFields.true_image_shape])
+  prediction_dict = ops.bfloat16_to_float32_nested(prediction_dict)
 
   losses_dict = model.loss(
       prediction_dict, features[fields.InputDataFields.true_image_shape])
@@ -122,6 +110,8 @@ def _compute_losses_and_predictions_dicts(
     ## as well.
     regularization_losses = model.regularization_losses()
     if regularization_losses:
+      regularization_losses = ops.bfloat16_to_float32_nested(
+          regularization_losses)
       regularization_loss = tf.add_n(
           regularization_losses, name='regularization_loss')
       losses.append(regularization_loss)
@@ -146,7 +136,6 @@ def eager_train_step(detection_model,
                      add_regularization_loss=True,
                      clip_gradients_value=None,
                      use_tpu=False,
-                     use_bfloat16=False,
                      global_step=None,
                      num_replicas=1.0):
   """Process a single training batch.
@@ -204,7 +193,6 @@ def eager_train_step(detection_model,
     clip_gradients_value: If this is present, clip the gradients global norm
       at this value using `tf.clip_by_global_norm`.
     use_tpu: Whether computation should happen on a TPU.
-    use_bfloat16: Whether computation on a TPU should use bfloat16.
     global_step: The current training step. Used for TensorBoard logging
       purposes. This step is not updated by this function and must be
       incremented separately.
@@ -226,8 +214,7 @@ def eager_train_step(detection_model,
 
   with tf.GradientTape() as tape:
     losses_dict, _ = _compute_losses_and_predictions_dicts(
-        detection_model, features, labels, add_regularization_loss, use_tpu,
-        use_bfloat16)
+        detection_model, features, labels, add_regularization_loss)
 
     total_loss = losses_dict['Loss/total_loss']
 
@@ -236,9 +223,10 @@ def eager_train_step(detection_model,
                                 tf.constant(num_replicas, dtype=tf.float32))
     losses_dict['Loss/normalized_total_loss'] = total_loss
 
-  for loss_type in losses_dict:
-    tf.compat.v2.summary.scalar(
-        loss_type, losses_dict[loss_type], step=global_step)
+  if not use_tpu:
+    for loss_type in losses_dict:
+      tf.compat.v2.summary.scalar(
+          loss_type, losses_dict[loss_type], step=global_step)
 
   trainable_variables = detection_model.trainable_variables
 
@@ -258,7 +246,7 @@ def eager_train_step(detection_model,
 def load_fine_tune_checkpoint(
     model, checkpoint_path, checkpoint_type,
     load_all_detection_checkpoint_vars, input_dataset,
-    unpad_groundtruth_tensors, use_tpu, use_bfloat16):
+    unpad_groundtruth_tensors):
   """Load a fine tuning classification or detection checkpoint.
 
   To make sure the model variables are all built, this method first executes
@@ -284,8 +272,6 @@ def load_fine_tune_checkpoint(
     input_dataset: The tf.data Dataset the model is being trained on. Needed
       to get the shapes for the dummy loss computation.
     unpad_groundtruth_tensors: A parameter passed to unstack_batch.
-    use_tpu: Whether computation should happen on a TPU.
-    use_bfloat16: Whether computation on a TPU should use bfloat16.
   """
   features, labels = iter(input_dataset).next()
 
@@ -299,9 +285,7 @@ def load_fine_tune_checkpoint(
     return _compute_losses_and_predictions_dicts(
         model,
         features,
-        labels,
-        use_tpu=use_tpu,
-        use_bfloat16=use_bfloat16)
+        labels)
 
   strategy = tf.compat.v2.distribute.get_strategy()
   strategy.experimental_run_v2(
@@ -313,11 +297,10 @@ def load_fine_tune_checkpoint(
       fine_tune_checkpoint_type=checkpoint_type,
       load_all_detection_checkpoint_vars=(
           load_all_detection_checkpoint_vars))
-  available_var_map = (
-      variables_helper.get_variables_available_in_checkpoint(
-          var_map,
-          checkpoint_path,
-          include_global_step=False))
+  available_var_map = variables_helper.get_variables_available_in_checkpoint(
+      var_map,
+      checkpoint_path,
+      include_global_step=False)
   tf.train.init_from_checkpoint(checkpoint_path,
                                 available_var_map)
 
@@ -386,7 +369,6 @@ def train_loop(
   train_input_config = configs['train_input_config']
 
   unpad_groundtruth_tensors = train_config.unpad_groundtruth_tensors
-  use_bfloat16 = train_config.use_bfloat16
   add_regularization_loss = train_config.add_regularization_loss
   clip_gradients_value = None
   if train_config.gradient_clipping_by_norm > 0:
@@ -403,6 +385,9 @@ def train_loop(
       'train_loop: use_tpu %s, export_to_tpu %s', use_tpu,
       export_to_tpu)
 
+  if kwargs['use_bfloat16']:
+    tf.compat.v2.keras.mixed_precision.experimental.set_policy('mixed_bfloat16')
+
   # Parse the checkpoint fine tuning configs
   if hparams.load_pretrained:
     fine_tune_checkpoint_path = train_config.fine_tune_checkpoint
@@ -427,10 +412,8 @@ def train_loop(
     pipeline_config_final = create_pipeline_proto_from_configs(configs)
     config_util.save_pipeline_config(pipeline_config_final, model_dir)
 
-  # TODO(kaftan): Either make strategy a parameter of this method, or
-  ## grab it w/  Distribution strategy's get_scope
   # Build the model, optimizer, and training input
-  strategy = tf.compat.v2.distribute.MirroredStrategy()
+  strategy = tf.compat.v2.distribute.get_strategy()
   with strategy.scope():
     detection_model = model_builder.build(
         model_config=model_config, is_training=True)
@@ -446,7 +429,7 @@ def train_loop(
         train_input.repeat())
 
     global_step = tf.compat.v2.Variable(
-        0, trainable=False, dtype=tf.compat.v2.dtypes.int64)
+        0, trainable=False, dtype=tf.compat.v2.dtypes.int64, name='global_step')
     optimizer, (learning_rate,) = optimizer_builder.build(
         train_config.optimizer, global_step=global_step)
 
@@ -465,8 +448,7 @@ def train_loop(
                                   fine_tune_checkpoint_type,
                                   load_all_detection_checkpoint_vars,
                                   train_input,
-                                  unpad_groundtruth_tensors, use_tpu,
-                                  use_bfloat16)
+                                  unpad_groundtruth_tensors)
 
       ckpt = tf.compat.v2.train.Checkpoint(
           step=global_step, model=detection_model)
@@ -483,7 +465,6 @@ def train_loop(
             unpad_groundtruth_tensors,
             optimizer,
             learning_rate=learning_rate_fn(),
-            use_bfloat16=use_bfloat16,
             add_regularization_loss=add_regularization_loss,
             clip_gradients_value=clip_gradients_value,
             use_tpu=use_tpu,
@@ -512,11 +493,12 @@ def train_loop(
         loss = _dist_train_step(train_input_iter)
         global_step.assign_add(1)
         end_time = time.time()
-        tf.compat.v2.summary.scalar(
-            'steps_per_sec', 1.0 / (end_time - start_time), step=global_step)
+        if not use_tpu:
+          tf.compat.v2.summary.scalar(
+              'steps_per_sec', 1.0 / (end_time - start_time), step=global_step)
         # TODO(kaftan): Remove this print after it is no longer helpful for
         ## debugging.
-        tf.print('Finished step', global_step, end_time, loss)
+        print('Finished step', global_step, end_time, loss)
         if int(global_step.value().numpy()) % checkpoint_every_n == 0:
           manager.save()
 
@@ -552,7 +534,6 @@ def eager_eval_loop(
   train_config = configs['train_config']
   eval_input_config = configs['eval_input_config']
   eval_config = configs['eval_config']
-  use_bfloat16 = train_config.use_bfloat16
   add_regularization_loss = train_config.add_regularization_loss
 
   is_training = False
@@ -594,8 +575,7 @@ def eager_eval_loop(
         labels, unpad_groundtruth_tensors=unpad_groundtruth_tensors)
 
     losses_dict, prediction_dict = _compute_losses_and_predictions_dicts(
-        detection_model, features, labels, add_regularization_loss, use_tpu,
-        use_bfloat16)
+        detection_model, features, labels, add_regularization_loss)
 
     def postprocess_wrapper(args):
       return detection_model.postprocess(args[0], args[1])
@@ -762,6 +742,9 @@ def eval_continuously(
                            eval_on_train_input_config.num_epochs))
     eval_on_train_input_config.num_epochs = 1
 
+  if kwargs['use_bfloat16']:
+    tf.compat.v2.keras.mixed_precision.experimental.set_policy('mixed_bfloat16')
+
   detection_model = model_builder.build(
       model_config=model_config, is_training=True)
 
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 1e4e8dcb..6f9c2685 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -27,6 +27,7 @@ import collections
 import functools
 import tensorflow as tf
 from object_detection.utils import ops
+from object_detection.utils import shape_utils
 slim = tf.contrib.slim
 
 # Activation bound used for TPU v1. Activations will be clipped to
@@ -568,7 +569,7 @@ class KerasFpnTopDownFeatureMaps(tf.keras.Model):
       # TODO (b/128922690): clean-up of ops.nearest_neighbor_upsampling
       if use_native_resize_op:
         def resize_nearest_neighbor(image):
-          image_shape = image.shape.as_list()
+          image_shape = shape_utils.combined_static_and_dynamic_shape(image)
           return tf.image.resize_nearest_neighbor(
               image, [image_shape[1] * 2, image_shape[2] * 2])
         top_down_net.append(tf.keras.layers.Lambda(
@@ -704,7 +705,8 @@ def fpn_top_down_feature_maps(image_features,
       for level in reversed(range(num_levels - 1)):
         if use_native_resize_op:
           with tf.name_scope('nearest_neighbor_upsampling'):
-            top_down_shape = top_down.shape.as_list()
+            top_down_shape = shape_utils.combined_static_and_dynamic_shape(
+                top_down)
             top_down = tf.image.resize_nearest_neighbor(
                 top_down, [top_down_shape[1] * 2, top_down_shape[2] * 2])
         else:
diff --git a/research/object_detection/models/keras_models/mobilenet_v1.py b/research/object_detection/models/keras_models/mobilenet_v1.py
index f9783396..e1bfb328 100644
--- a/research/object_detection/models/keras_models/mobilenet_v1.py
+++ b/research/object_detection/models/keras_models/mobilenet_v1.py
@@ -242,7 +242,7 @@ class _LayersOverride(object):
 
     placeholder_with_default = tf.placeholder_with_default(
         input=input_tensor, shape=[None] + shape)
-    return tf.keras.layers.Input(tensor=placeholder_with_default)
+    return model_utils.input_layer(shape, placeholder_with_default)
 
   # pylint: disable=unused-argument
   def ReLU(self, *args, **kwargs):
diff --git a/research/object_detection/models/keras_models/mobilenet_v2.py b/research/object_detection/models/keras_models/mobilenet_v2.py
index 59f28625..7f8ed508 100644
--- a/research/object_detection/models/keras_models/mobilenet_v2.py
+++ b/research/object_detection/models/keras_models/mobilenet_v2.py
@@ -230,10 +230,7 @@ class _LayersOverride(object):
 
     placeholder_with_default = tf.placeholder_with_default(
         input=input_tensor, shape=[None] + shape)
-    if tf.executing_eagerly():
-      return tf.keras.layers.Input(shape=shape)
-    else:
-      return tf.keras.layers.Input(tensor=placeholder_with_default)
+    return model_utils.input_layer(shape, placeholder_with_default)
 
   # pylint: disable=unused-argument
   def ReLU(self, *args, **kwargs):
diff --git a/research/object_detection/models/keras_models/model_utils.py b/research/object_detection/models/keras_models/model_utils.py
index 981b298d..1576fe95 100644
--- a/research/object_detection/models/keras_models/model_utils.py
+++ b/research/object_detection/models/keras_models/model_utils.py
@@ -20,6 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
+import tensorflow as tf
 
 # This is to specify the custom config of model structures. For example,
 # ConvDefs(conv_name='conv_pw_12', filters=512) for Mobilenet V1 is to specify
@@ -43,3 +44,10 @@ def get_conv_def(conv_defs, layer_name):
     if layer_name == conv_def.conv_name:
       return conv_def.filters
   return None
+
+
+def input_layer(shape, placeholder_with_default):
+  if tf.executing_eagerly():
+    return tf.keras.layers.Input(shape=shape)
+  else:
+    return tf.keras.layers.Input(tensor=placeholder_with_default)
diff --git a/research/object_detection/models/keras_models/resnet_v1.py b/research/object_detection/models/keras_models/resnet_v1.py
index b4c88529..12b71123 100644
--- a/research/object_detection/models/keras_models/resnet_v1.py
+++ b/research/object_detection/models/keras_models/resnet_v1.py
@@ -22,6 +22,7 @@ from __future__ import print_function
 import tensorflow as tf
 
 from object_detection.core import freezable_batch_norm
+from object_detection.models.keras_models import model_utils
 
 
 def _fixed_padding(inputs, kernel_size, rate=1):  # pylint: disable=invalid-name
@@ -216,7 +217,7 @@ class _LayersOverride(object):
 
     placeholder_with_default = tf.placeholder_with_default(
         input=input_tensor, shape=[None] + shape)
-    return tf.keras.layers.Input(tensor=placeholder_with_default)
+    return model_utils.input_layer(shape, placeholder_with_default)
 
   def MaxPooling2D(self, pool_size, **kwargs):
     """Builds a MaxPooling2D layer with default padding as 'SAME'.
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
index b0c149ae..731e467a 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
@@ -52,6 +52,7 @@ class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False):
     """SSD FPN feature extractor based on Mobilenet v1 architecture.
 
@@ -79,6 +80,8 @@ class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
       use_depthwise: Whether to use depthwise convolutions. Default is False.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams_fn`.
@@ -100,6 +103,7 @@ class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     self._conv_defs = None
     if self._use_depthwise:
       self._conv_defs = _create_modified_mobilenet_config()
+    self._use_native_resize_op = use_native_resize_op
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -162,7 +166,8 @@ class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
               [(key, image_features[key]) for key in feature_block_list],
               depth=depth_fn(self._additional_layer_depth),
               use_depthwise=self._use_depthwise,
-              use_explicit_padding=self._use_explicit_padding)
+              use_explicit_padding=self._use_explicit_padding,
+              use_native_resize_op=self._use_native_resize_op)
           feature_maps = []
           for level in range(self._fpn_min_level, base_fpn_max_level + 1):
             feature_maps.append(fpn_features['top_down_{}'.format(
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
index b8090893..d2d27606 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
@@ -49,6 +49,7 @@ class SSDMobileNetV1FpnKerasFeatureExtractor(
                additional_layer_depth=256,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False,
                name=None):
     """SSD Keras based FPN feature extractor Mobilenet v1 architecture.
@@ -84,6 +85,8 @@ class SSDMobileNetV1FpnKerasFeatureExtractor(
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
       use_depthwise: whether to use depthwise convolutions. Default is False.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams`.
@@ -109,6 +112,7 @@ class SSDMobileNetV1FpnKerasFeatureExtractor(
     self._conv_defs = None
     if self._use_depthwise:
       self._conv_defs = _create_modified_mobilenet_config()
+    self._use_native_resize_op = use_native_resize_op
     self._feature_blocks = [
         'Conv2d_3_pointwise', 'Conv2d_5_pointwise', 'Conv2d_11_pointwise',
         'Conv2d_13_pointwise'
@@ -153,6 +157,7 @@ class SSDMobileNetV1FpnKerasFeatureExtractor(
             depth=self._depth_fn(self._additional_layer_depth),
             use_depthwise=self._use_depthwise,
             use_explicit_padding=self._use_explicit_padding,
+            use_native_resize_op=self._use_native_resize_op,
             is_training=self._is_training,
             conv_hyperparams=self._conv_hyperparams,
             freeze_batchnorm=self._freeze_batchnorm,
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
index d3c1f7a2..605930cb 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
@@ -53,6 +53,7 @@ class SSDMobileNetV2FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False):
     """SSD FPN feature extractor based on Mobilenet v2 architecture.
 
@@ -79,6 +80,8 @@ class SSDMobileNetV2FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
       use_depthwise: Whether to use depthwise convolutions. Default is False.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams_fn`.
@@ -100,6 +103,7 @@ class SSDMobileNetV2FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     self._conv_defs = None
     if self._use_depthwise:
       self._conv_defs = _create_modified_mobilenet_config()
+    self._use_native_resize_op = use_native_resize_op
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -159,7 +163,8 @@ class SSDMobileNetV2FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
               [(key, image_features[key]) for key in feature_block_list],
               depth=depth_fn(self._additional_layer_depth),
               use_depthwise=self._use_depthwise,
-              use_explicit_padding=self._use_explicit_padding)
+              use_explicit_padding=self._use_explicit_padding,
+              use_native_resize_op=self._use_native_resize_op)
           feature_maps = []
           for level in range(self._fpn_min_level, base_fpn_max_level + 1):
             feature_maps.append(fpn_features['top_down_{}'.format(
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
index 8976d309..d2927b42 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
@@ -52,6 +52,7 @@ class SSDMobileNetV2FpnKerasFeatureExtractor(
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False,
                name=None):
     """SSD Keras based FPN feature extractor Mobilenet v2 architecture.
@@ -87,6 +88,8 @@ class SSDMobileNetV2FpnKerasFeatureExtractor(
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
       use_depthwise: Whether to use depthwise convolutions. Default is False.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams`.
@@ -112,6 +115,7 @@ class SSDMobileNetV2FpnKerasFeatureExtractor(
     self._conv_defs = None
     if self._use_depthwise:
       self._conv_defs = _create_modified_mobilenet_config()
+    self._use_native_resize_op = use_native_resize_op
     self._feature_blocks = ['layer_4', 'layer_7', 'layer_14', 'layer_19']
     self._mobilenet_v2 = None
     self._fpn_features_generator = None
@@ -151,6 +155,7 @@ class SSDMobileNetV2FpnKerasFeatureExtractor(
             depth=self._depth_fn(self._additional_layer_depth),
             use_depthwise=self._use_depthwise,
             use_explicit_padding=self._use_explicit_padding,
+            use_native_resize_op=self._use_native_resize_op,
             is_training=self._is_training,
             conv_hyperparams=self._conv_hyperparams,
             freeze_batchnorm=self._freeze_batchnorm,
diff --git a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
new file mode 100644
index 00000000..ca7fd6a7
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
@@ -0,0 +1,212 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""SSDFeatureExtractor for MobileNetV3 features."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.utils import context_manager
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+from nets.mobilenet import mobilenet
+from nets.mobilenet import mobilenet_v3
+
+slim = tf.contrib.slim
+
+
+class _SSDMobileNetV3FeatureExtractorBase(ssd_meta_arch.SSDFeatureExtractor):
+  """Base class of SSD feature extractor using MobilenetV3 features."""
+
+  def __init__(self,
+               conv_defs,
+               from_layer,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """MobileNetV3 Feature Extractor for SSD Models.
+
+    MobileNet v3. Details found in:
+    https://arxiv.org/abs/1905.02244
+
+    Args:
+      conv_defs: MobileNetV3 conv defs for backbone.
+      from_layer: A cell of two layer names (string) to connect to the 1st and
+        2nd inputs of the SSD head.
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the base
+        feature extractor.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(_SSDMobileNetV3FeatureExtractorBase, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams
+    )
+    self._conv_defs = conv_defs
+    self._from_layer = from_layer
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    Raises:
+      ValueError if conv_defs is not provided or from_layer does not meet the
+        size requirement.
+    """
+
+    if not self._conv_defs:
+      raise ValueError('Must provide backbone conv defs.')
+
+    if len(self._from_layer) != 2:
+      raise ValueError('SSD input feature names are not provided.')
+
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+
+    feature_map_layout = {
+        'from_layer': [
+            self._from_layer[0], self._from_layer[1], '', '', '', ''
+        ],
+        'layer_depth': [-1, -1, 512, 256, 256, 128],
+        'use_depthwise': self._use_depthwise,
+        'use_explicit_padding': self._use_explicit_padding,
+    }
+
+    with tf.variable_scope('MobilenetV3', reuse=self._reuse_weights) as scope:
+      with slim.arg_scope(
+          mobilenet_v3.training_scope(is_training=None, bn_decay=0.9997)), \
+          slim.arg_scope(
+              [mobilenet.depth_multiplier], min_depth=self._min_depth):
+        with (slim.arg_scope(self._conv_hyperparams_fn())
+              if self._override_base_feature_extractor_hyperparams else
+              context_manager.IdentityContextManager()):
+          # TODO(bochen): switch to v3 modules once v3 is properly refactored.
+          _, image_features = mobilenet_v3.mobilenet_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              conv_defs=self._conv_defs,
+              final_endpoint=self._from_layer[1],
+              depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
+              scope=scope)
+        with slim.arg_scope(self._conv_hyperparams_fn()):
+          feature_maps = feature_map_generators.multi_resolution_feature_maps(
+              feature_map_layout=feature_map_layout,
+              depth_multiplier=self._depth_multiplier,
+              min_depth=self._min_depth,
+              insert_1x1_conv=True,
+              image_features=image_features)
+
+    return feature_maps.values()
+
+
+class SSDMobileNetV3LargeFeatureExtractor(_SSDMobileNetV3FeatureExtractorBase):
+  """Mobilenet V3-Large feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    super(SSDMobileNetV3LargeFeatureExtractor, self).__init__(
+        conv_defs=mobilenet_v3.V3_LARGE_DETECTION,
+        from_layer=['layer_14/expansion_output', 'layer_17'],
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams
+    )
+
+
+class SSDMobileNetV3SmallFeatureExtractor(_SSDMobileNetV3FeatureExtractorBase):
+  """Mobilenet V3-Small feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    super(SSDMobileNetV3SmallFeatureExtractor, self).__init__(
+        conv_defs=mobilenet_v3.V3_SMALL_DETECTION,
+        from_layer=['layer_10/expansion_output', 'layer_13'],
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams
+    )
diff --git a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_test.py
new file mode 100644
index 00000000..ccbef4eb
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_test.py
@@ -0,0 +1,105 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for ssd_mobilenet_v3_feature_extractor."""
+
+import tensorflow as tf
+
+from object_detection.models import ssd_mobilenet_v3_feature_extractor
+from object_detection.models import ssd_mobilenet_v3_feature_extractor_testbase
+
+
+slim = tf.contrib.slim
+
+
+class SsdMobilenetV3LargeFeatureExtractorTest(
+    ssd_mobilenet_v3_feature_extractor_testbase
+    ._SsdMobilenetV3FeatureExtractorTestBase):
+
+  def _get_input_sizes(self):
+    """Return first two input feature map sizes."""
+    return [672, 480]
+
+  def _create_feature_extractor(self,
+                                depth_multiplier,
+                                pad_to_multiple,
+                                use_explicit_padding=False,
+                                use_keras=False):
+    """Constructs a new Mobilenet V3-Large feature extractor.
+
+    Args:
+      depth_multiplier: float depth multiplier for feature extractor
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      use_explicit_padding: use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
+      use_keras: if True builds a keras-based feature extractor, if False builds
+        a slim-based one.
+
+    Returns:
+      an ssd_meta_arch.SSDFeatureExtractor object.
+    """
+    min_depth = 32
+    return (
+        ssd_mobilenet_v3_feature_extractor.SSDMobileNetV3LargeFeatureExtractor(
+            False,
+            depth_multiplier,
+            min_depth,
+            pad_to_multiple,
+            self.conv_hyperparams_fn,
+            use_explicit_padding=use_explicit_padding))
+
+
+class SsdMobilenetV3SmallFeatureExtractorTest(
+    ssd_mobilenet_v3_feature_extractor_testbase
+    ._SsdMobilenetV3FeatureExtractorTestBase):
+
+  def _get_input_sizes(self):
+    """Return first two input feature map sizes."""
+    return [288, 288]
+
+  def _create_feature_extractor(self,
+                                depth_multiplier,
+                                pad_to_multiple,
+                                use_explicit_padding=False,
+                                use_keras=False):
+    """Constructs a new Mobilenet V3-Small feature extractor.
+
+    Args:
+      depth_multiplier: float depth multiplier for feature extractor
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      use_explicit_padding: use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
+      use_keras: if True builds a keras-based feature extractor, if False builds
+        a slim-based one.
+
+    Returns:
+      an ssd_meta_arch.SSDFeatureExtractor object.
+    """
+    min_depth = 32
+    return (
+        ssd_mobilenet_v3_feature_extractor.SSDMobileNetV3SmallFeatureExtractor(
+            False,
+            depth_multiplier,
+            min_depth,
+            pad_to_multiple,
+            self.conv_hyperparams_fn,
+            use_explicit_padding=use_explicit_padding))
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_testbase.py b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_testbase.py
new file mode 100644
index 00000000..63100eea
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_testbase.py
@@ -0,0 +1,115 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Base test class for ssd_mobilenet_v3_feature_extractor."""
+
+import abc
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection.models import ssd_feature_extractor_test
+
+
+slim = tf.contrib.slim
+
+
+class _SsdMobilenetV3FeatureExtractorTestBase(
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
+  """Base class for MobilenetV3 tests."""
+
+  @abc.abstractmethod
+  def _get_input_sizes(self):
+    """Return feature map sizes for the two inputs to SSD head."""
+    pass
+
+  def test_extract_features_returns_correct_shapes_128(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    input_feature_sizes = self._get_input_sizes()
+    expected_feature_map_shape = [(2, 8, 8, input_feature_sizes[0]),
+                                  (2, 4, 4, input_feature_sizes[1]),
+                                  (2, 2, 2, 512), (2, 1, 1, 256), (2, 1, 1,
+                                                                   256),
+                                  (2, 1, 1, 128)]
+    self.check_extract_features_returns_correct_shape(
+        2,
+        image_height,
+        image_width,
+        depth_multiplier,
+        pad_to_multiple,
+        expected_feature_map_shape,
+        use_keras=False)
+
+  def test_extract_features_returns_correct_shapes_299(self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    input_feature_sizes = self._get_input_sizes()
+    expected_feature_map_shape = [(2, 19, 19, input_feature_sizes[0]),
+                                  (2, 10, 10, input_feature_sizes[1]),
+                                  (2, 5, 5, 512), (2, 3, 3, 256), (2, 2, 2,
+                                                                   256),
+                                  (2, 1, 1, 128)]
+    self.check_extract_features_returns_correct_shape(
+        2,
+        image_height,
+        image_width,
+        depth_multiplier,
+        pad_to_multiple,
+        expected_feature_map_shape,
+        use_keras=False)
+
+  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    pad_to_multiple = 32
+    input_feature_sizes = self._get_input_sizes()
+    expected_feature_map_shape = [(2, 20, 20, input_feature_sizes[0]),
+                                  (2, 10, 10, input_feature_sizes[1]),
+                                  (2, 5, 5, 512), (2, 3, 3, 256), (2, 2, 2,
+                                                                   256),
+                                  (2, 1, 1, 128)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_preprocess_returns_correct_value_range(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    test_image = np.random.rand(4, image_height, image_width, 3)
+    feature_extractor = self._create_feature_extractor(
+        depth_multiplier, pad_to_multiple, use_keras=False)
+    preprocessed_image = feature_extractor.preprocess(test_image)
+    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))
+
+  def test_has_fused_batchnorm(self):
+    image_height = 40
+    image_width = 40
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    image_placeholder = tf.placeholder(tf.float32,
+                                       [1, image_height, image_width, 3])
+    feature_extractor = self._create_feature_extractor(
+        depth_multiplier, pad_to_multiple, use_keras=False)
+    preprocessed_image = feature_extractor.preprocess(image_placeholder)
+    _ = feature_extractor.extract_features(preprocessed_image)
+    self.assertTrue(any('FusedBatchNorm' in op.type
+                        for op in tf.get_default_graph().get_operations()))
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index 74434af6..343e8a26 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -47,6 +47,7 @@ class SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False):
     """SSD FPN feature extractor based on Resnet v1 architecture.
 
@@ -77,6 +78,8 @@ class SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
       use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams_fn`.
@@ -103,6 +106,7 @@ class SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     self._fpn_min_level = fpn_min_level
     self._fpn_max_level = fpn_max_level
     self._additional_layer_depth = additional_layer_depth
+    self._use_native_resize_op = use_native_resize_op
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -178,7 +182,8 @@ class SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             feature_block_list.append('block{}'.format(level - 1))
           fpn_features = feature_map_generators.fpn_top_down_feature_maps(
               [(key, image_features[key]) for key in feature_block_list],
-              depth=depth_fn(self._additional_layer_depth))
+              depth=depth_fn(self._additional_layer_depth),
+              use_native_resize_op=self._use_native_resize_op)
           feature_maps = []
           for level in range(self._fpn_min_level, base_fpn_max_level + 1):
             feature_maps.append(
@@ -213,6 +218,7 @@ class SSDResnet50V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False):
     """SSD Resnet50 V1 FPN feature extractor based on Resnet v1 architecture.
 
@@ -232,6 +238,8 @@ class SSDResnet50V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
       use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams_fn`.
@@ -251,6 +259,7 @@ class SSDResnet50V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
         reuse_weights=reuse_weights,
         use_explicit_padding=use_explicit_padding,
         use_depthwise=use_depthwise,
+        use_native_resize_op=use_native_resize_op,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
 
@@ -270,6 +279,7 @@ class SSDResnet101V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False):
     """SSD Resnet101 V1 FPN feature extractor based on Resnet v1 architecture.
 
@@ -289,6 +299,8 @@ class SSDResnet101V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
       use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams_fn`.
@@ -308,6 +320,7 @@ class SSDResnet101V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
         reuse_weights=reuse_weights,
         use_explicit_padding=use_explicit_padding,
         use_depthwise=use_depthwise,
+        use_native_resize_op=use_native_resize_op,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
 
@@ -327,6 +340,7 @@ class SSDResnet152V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
+               use_native_resize_op=False,
                override_base_feature_extractor_hyperparams=False):
     """SSD Resnet152 V1 FPN feature extractor based on Resnet v1 architecture.
 
@@ -346,6 +360,8 @@ class SSDResnet152V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
       use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
+      use_native_resize_op: Whether to use tf.image.nearest_neighbor_resize
+        to do upsampling in FPN. Default is false.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams_fn`.
@@ -365,5 +381,6 @@ class SSDResnet152V1FpnFeatureExtractor(SSDResnetV1FpnFeatureExtractor):
         reuse_weights=reuse_weights,
         use_explicit_padding=use_explicit_padding,
         use_depthwise=use_depthwise,
+        use_native_resize_op=use_native_resize_op,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
index 3db0a897..ef91d283 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
@@ -52,6 +52,7 @@ class SSDResNetV1FpnKerasFeatureExtractor(
                additional_layer_depth=256,
                reuse_weights=None,
                use_explicit_padding=None,
+               use_depthwise=None,
                override_base_feature_extractor_hyperparams=False,
                name=None):
     """SSD Keras based FPN feature extractor Resnet v1 architecture.
@@ -90,6 +91,7 @@ class SSDResNetV1FpnKerasFeatureExtractor(
       use_explicit_padding: whether to use explicit padding when extracting
         features. Default is None, as it's an invalid option and not implemented
         in this feature extractor.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams`.
@@ -105,11 +107,14 @@ class SSDResNetV1FpnKerasFeatureExtractor(
         freeze_batchnorm=freeze_batchnorm,
         inplace_batchnorm_update=inplace_batchnorm_update,
         use_explicit_padding=None,
+        use_depthwise=None,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams,
         name=name)
     if self._use_explicit_padding:
       raise ValueError('Explicit padding is not a valid option.')
+    if self._use_depthwise:
+      raise ValueError('Depthwise is not a valid option.')
     self._fpn_min_level = fpn_min_level
     self._fpn_max_level = fpn_max_level
     self._additional_layer_depth = additional_layer_depth
@@ -251,6 +256,7 @@ class SSDResNet50V1FpnKerasFeatureExtractor(
                additional_layer_depth=256,
                reuse_weights=None,
                use_explicit_padding=None,
+               use_depthwise=None,
                override_base_feature_extractor_hyperparams=False,
                name='ResNet50V1_FPN'):
     """SSD Keras based FPN feature extractor ResnetV1-50 architecture.
@@ -278,7 +284,8 @@ class SSDResNet50V1FpnKerasFeatureExtractor(
       reuse_weights: whether to reuse variables. Default is None.
       use_explicit_padding: whether to use explicit padding when extracting
         features. Default is None, as it's an invalid option and not implemented
-        in this feature extractor
+        in this feature extractor.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams`.
@@ -296,6 +303,7 @@ class SSDResNet50V1FpnKerasFeatureExtractor(
         resnet_v1_base_model=resnet_v1.resnet_v1_50,
         resnet_v1_base_model_name='resnet_v1_50',
         use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams,
         name=name)
@@ -318,6 +326,7 @@ class SSDResNet101V1FpnKerasFeatureExtractor(
                additional_layer_depth=256,
                reuse_weights=None,
                use_explicit_padding=None,
+               use_depthwise=None,
                override_base_feature_extractor_hyperparams=False,
                name='ResNet101V1_FPN'):
     """SSD Keras based FPN feature extractor ResnetV1-101 architecture.
@@ -345,7 +354,8 @@ class SSDResNet101V1FpnKerasFeatureExtractor(
       reuse_weights: whether to reuse variables. Default is None.
       use_explicit_padding: whether to use explicit padding when extracting
         features. Default is None, as it's an invalid option and not implemented
-        in this feature extractor
+        in this feature extractor.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams`.
@@ -363,6 +373,7 @@ class SSDResNet101V1FpnKerasFeatureExtractor(
         resnet_v1_base_model=resnet_v1.resnet_v1_101,
         resnet_v1_base_model_name='resnet_v1_101',
         use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams,
         name=name)
@@ -385,6 +396,7 @@ class SSDResNet152V1FpnKerasFeatureExtractor(
                additional_layer_depth=256,
                reuse_weights=None,
                use_explicit_padding=False,
+               use_depthwise=None,
                override_base_feature_extractor_hyperparams=False,
                name='ResNet152V1_FPN'):
     """SSD Keras based FPN feature extractor ResnetV1-152 architecture.
@@ -412,7 +424,8 @@ class SSDResNet152V1FpnKerasFeatureExtractor(
       reuse_weights: whether to reuse variables. Default is None.
       use_explicit_padding: whether to use explicit padding when extracting
         features. Default is None, as it's an invalid option and not implemented
-        in this feature extractor
+        in this feature extractor.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
       override_base_feature_extractor_hyperparams: Whether to override
         hyperparameters of the base feature extractor with the one from
         `conv_hyperparams`.
@@ -430,6 +443,7 @@ class SSDResNet152V1FpnKerasFeatureExtractor(
         resnet_v1_base_model=resnet_v1.resnet_v1_152,
         resnet_v1_base_model_name='resnet_v1_152',
         use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams,
         name=name)
diff --git a/research/object_detection/object_detection_tutorial.ipynb b/research/object_detection/object_detection_tutorial.ipynb
index 9100ed9b..b6219911 100644
--- a/research/object_detection/object_detection_tutorial.ipynb
+++ b/research/object_detection/object_detection_tutorial.ipynb
@@ -7,8 +7,36 @@
         "id": "V8-yl-s-WKMG"
       },
       "source": [
-        "# Object Detection Demo\n",
-        "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
+        "# Object Detection API Demo\n",
+        "\n",
+        "\u003ctable align=\"left\"\u003e\u003ctd\u003e\n",
+        "  \u003ca target=\"_blank\"  href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\"\u003e\n",
+        "    \u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\n",
+        "  \u003c/a\u003e\n",
+        "\u003c/td\u003e\u003ctd\u003e\n",
+        "  \u003ca target=\"_blank\"  href=\"https://colab.sandbox.google.com/github/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\"\u003e\n",
+        "    \u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
+        "\u003c/td\u003e\u003c/table\u003e"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "3cIrseUv6WKz"
+      },
+      "source": [
+        "Welcome to the [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image."
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "VrJaG0cYN9yh"
+      },
+      "source": [
+        "\u003e **Important**: This tutorial is to help you through the first step towards using [Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to build models. If you just just need an off the shelf model that does the job, see the [TFHub object detection example](https://colab.sandbox.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb)."
       ]
     },
     {
@@ -18,19 +46,151 @@
         "id": "kFSqkTCdWKMI"
       },
       "source": [
-        "# Imports"
+        "# Setup"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "awjrpqy-6MaQ"
+      },
+      "source": [
+        "Important: If you're running on a local machine, be sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md). This notebook includes only what's necessary to run in Colab."
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "p3UGXxUii5Ym"
+      },
+      "source": [
+        "### Install"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
+        "colab_type": "code",
+        "id": "hGL97-GXjSUw"
+      },
+      "outputs": [],
+      "source": [
+        "!pip install -U --pre tensorflow==\"2.*\""
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "n_ap_s9ajTHH"
+      },
+      "source": [
+        "Make sure you have `pycocotools` installed"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "Bg8ZyA47i3pY"
+      },
+      "outputs": [],
+      "source": [
+        "!pip install pycocotools"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "-vsOL3QR6kqs"
+      },
+      "source": [
+        "Get `tensorflow/models` or `cd` to parent directory of the repository."
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "ykA0c-om51s1"
+      },
+      "outputs": [],
+      "source": [
+        "import os\n",
+        "import pathlib\n",
+        "\n",
+        "\n",
+        "if \"models\" in pathlib.Path.cwd().parts:\n",
+        "  while \"models\" in pathlib.Path.cwd().parts:\n",
+        "    os.chdir('..')\n",
+        "elif not pathlib.Path('models').exists():\n",
+        "  !git clone --depth 1 https://github.com/tensorflow/models"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "O219m6yWAj9l"
+      },
+      "source": [
+        "Compile protobufs and install the object_detection package"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "PY41vdYYNlXc"
+      },
+      "outputs": [],
+      "source": [
+        "%%bash\n",
+        "cd models/research/\n",
+        "protoc object_detection/protos/*.proto --python_out=."
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "s62yJyQUcYbp"
+      },
+      "outputs": [],
+      "source": [
+        "%%bash \n",
+        "cd models/research\n",
+        "pip install ."
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "LBdjK2G5ywuc"
+      },
+      "source": [
+        "### Imports"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
         "colab_type": "code",
         "id": "hV4P5gyTWKMI"
       },
@@ -44,78 +204,63 @@
         "import tensorflow as tf\n",
         "import zipfile\n",
         "\n",
-        "from distutils.version import StrictVersion\n",
         "from collections import defaultdict\n",
         "from io import StringIO\n",
         "from matplotlib import pyplot as plt\n",
         "from PIL import Image\n",
-        "\n",
-        "# This is needed since the notebook is stored in the object_detection folder.\n",
-        "sys.path.append(\"..\")\n",
-        "from object_detection.utils import ops as utils_ops\n",
-        "\n",
-        "if StrictVersion(tf.__version__) \u003c StrictVersion('1.12.0'):\n",
-        "  raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')\n"
+        "from IPython.display import display"
       ]
     },
     {
       "cell_type": "markdown",
       "metadata": {
         "colab_type": "text",
-        "id": "Wy72mWwAWKMK"
+        "id": "r5FNuiRPWKMN"
       },
       "source": [
-        "## Env setup"
+        "Import the object detection module."
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "v7m_NY_aWKMK"
+        "id": "4-IMl4b6BdGO"
       },
       "outputs": [],
       "source": [
-        "# This is needed to display the images.\n",
-        "%matplotlib inline"
+        "from object_detection.utils import ops as utils_ops\n",
+        "from object_detection.utils import label_map_util\n",
+        "from object_detection.utils import visualization_utils as vis_util"
       ]
     },
     {
       "cell_type": "markdown",
       "metadata": {
         "colab_type": "text",
-        "id": "r5FNuiRPWKMN"
+        "id": "RYPCiag2iz_q"
       },
       "source": [
-        "## Object detection imports\n",
-        "Here are the imports from the object detection module."
+        "Patches:"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "bm0_uNRnWKMN"
+        "id": "mF-YlMl8c_bM"
       },
       "outputs": [],
       "source": [
-        "from utils import label_map_util\n",
+        "# patch tf1 into `utils.ops`\n",
+        "utils_ops.tf = tf.compat.v1\n",
         "\n",
-        "from utils import visualization_utils as vis_util"
+        "# Patch the location of gfile\n",
+        "tf.gfile = tf.io.gfile"
       ]
     },
     {
@@ -137,288 +282,272 @@
       "source": [
         "## Variables\n",
         "\n",
-        "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
+        "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing the path.\n",
         "\n",
         "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
       ]
     },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "7ai8pLZZWKMS"
+      },
+      "source": [
+        "## Loader"
+      ]
+    },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "VyPz_t8WWKMQ"
+        "id": "zm8xp-0eoItE"
       },
       "outputs": [],
       "source": [
-        "# What model to download.\n",
-        "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
-        "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
-        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
+        "def load_model(model_name):\n",
+        "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
+        "  model_file = model_name + '.tar.gz'\n",
+        "  model_dir = tf.keras.utils.get_file(\n",
+        "    fname=model_name, \n",
+        "    origin=base_url + model_file,\n",
+        "    untar=True)\n",
         "\n",
-        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
-        "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
+        "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
         "\n",
-        "# List of the strings that is used to add correct label for each box.\n",
-        "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')"
+        "  model = tf.saved_model.load(str(model_dir))\n",
+        "  model = model.signatures['serving_default']\n",
+        "\n",
+        "  return model"
       ]
     },
     {
       "cell_type": "markdown",
       "metadata": {
         "colab_type": "text",
-        "id": "7ai8pLZZWKMS"
+        "id": "_1MVVTcLWKMW"
       },
       "source": [
-        "## Download Model"
+        "## Loading label map\n",
+        "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "KILYnwR5WKMS"
+        "id": "hDbpHkiWWKMX"
       },
       "outputs": [],
       "source": [
-        "opener = urllib.request.URLopener()\n",
-        "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
-        "tar_file = tarfile.open(MODEL_FILE)\n",
-        "for file in tar_file.getmembers():\n",
-        "  file_name = os.path.basename(file.name)\n",
-        "  if 'frozen_inference_graph.pb' in file_name:\n",
-        "    tar_file.extract(file, os.getcwd())"
+        "# List of the strings that is used to add correct label for each box.\n",
+        "PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
+        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
       ]
     },
     {
       "cell_type": "markdown",
       "metadata": {
         "colab_type": "text",
-        "id": "YBcB9QHLWKMU"
+        "id": "oVU3U_J6IJVb"
       },
       "source": [
-        "## Load a (frozen) Tensorflow model into memory."
+        "For the sake of simplicity we will test on 2 images:"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "KezjCRVvWKMV"
+        "id": "jG-zn5ykWKMd"
       },
       "outputs": [],
       "source": [
-        "detection_graph = tf.Graph()\n",
-        "with detection_graph.as_default():\n",
-        "  od_graph_def = tf.GraphDef()\n",
-        "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
-        "    serialized_graph = fid.read()\n",
-        "    od_graph_def.ParseFromString(serialized_graph)\n",
-        "    tf.import_graph_def(od_graph_def, name='')"
+        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
+        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('models/research/object_detection/test_images')\n",
+        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
+        "TEST_IMAGE_PATHS"
       ]
     },
     {
       "cell_type": "markdown",
       "metadata": {
         "colab_type": "text",
-        "id": "_1MVVTcLWKMW"
+        "id": "H0_1AGhrWKMc"
       },
       "source": [
-        "## Loading label map\n",
-        "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
+        "# Detection"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "f7aOtOlebK7h"
+      },
+      "source": [
+        "Load an object detection model:"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "hDbpHkiWWKMX"
+        "id": "1XNT0wxybKR6"
       },
       "outputs": [],
       "source": [
-        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
+        "model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
+        "detection_model = load_model(model_name)"
       ]
     },
     {
       "cell_type": "markdown",
       "metadata": {
         "colab_type": "text",
-        "id": "EFsoUHvbWKMZ"
+        "id": "yN1AYfAEJIGp"
       },
       "source": [
-        "## Helper code"
+        "Check the model's input signature, it expects a batch of 3-color images of type uint8: "
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "aSlYc3JkWKMa"
+        "id": "CK4cnry6wsHY"
       },
       "outputs": [],
       "source": [
-        "def load_image_into_numpy_array(image):\n",
-        "  (im_width, im_height) = image.size\n",
-        "  return np.array(image.getdata()).reshape(\n",
-        "      (im_height, im_width, 3)).astype(np.uint8)"
+        "print(detection_model.inputs)"
       ]
     },
     {
       "cell_type": "markdown",
       "metadata": {
         "colab_type": "text",
-        "id": "H0_1AGhrWKMc"
+        "id": "Q8u3BjpMJXZF"
       },
       "source": [
-        "# Detection"
+        "And retuns several outputs:"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "jG-zn5ykWKMd"
+        "id": "oLSZpfaYwuSk"
       },
       "outputs": [],
       "source": [
-        "# For the sake of simplicity we will use only 2 images:\n",
-        "# image1.jpg\n",
-        "# image2.jpg\n",
-        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
-        "PATH_TO_TEST_IMAGES_DIR = 'test_images'\n",
-        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
-        "\n",
-        "# Size, in inches, of the output images.\n",
-        "IMAGE_SIZE = (12, 8)"
+        "detection_model.output_dtypes"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
+        "colab_type": "code",
+        "id": "FZyKUJeuxvpT"
+      },
+      "outputs": [],
+      "source": [
+        "detection_model.output_shapes"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "JP5qZ7sXJpwG"
+      },
+      "source": [
+        "Add a wrapper function to call the model, and cleanup the outputs:"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
         "colab_type": "code",
-        "id": "92BHxzcNWKMf"
+        "id": "ajmR_exWyN76"
       },
       "outputs": [],
       "source": [
-        "def run_inference_for_single_image(image, graph):\n",
-        "  with graph.as_default():\n",
-        "    with tf.Session() as sess:\n",
-        "      # Get handles to input and output tensors\n",
-        "      ops = tf.get_default_graph().get_operations()\n",
-        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
-        "      tensor_dict = {}\n",
-        "      for key in [\n",
-        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
-        "          'detection_classes', 'detection_masks'\n",
-        "      ]:\n",
-        "        tensor_name = key + ':0'\n",
-        "        if tensor_name in all_tensor_names:\n",
-        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
-        "              tensor_name)\n",
-        "      if 'detection_masks' in tensor_dict:\n",
-        "        # The following processing is only for single image\n",
-        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
-        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
-        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
-        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
-        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
-        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
-        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
-        "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
-        "        detection_masks_reframed = tf.cast(\n",
-        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
-        "        # Follow the convention by adding back the batch dimension\n",
-        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
-        "            detection_masks_reframed, 0)\n",
-        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
+        "def run_inference_for_single_image(model, image):\n",
+        "  image = np.asarray(image)\n",
+        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
+        "  input_tensor = tf.convert_to_tensor(image)\n",
+        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
+        "  input_tensor = input_tensor[tf.newaxis,...]\n",
         "\n",
-        "      # Run inference\n",
-        "      output_dict = sess.run(tensor_dict,\n",
-        "                             feed_dict={image_tensor: image})\n",
+        "  # Run inference\n",
+        "  output_dict = model(input_tensor)\n",
         "\n",
-        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
-        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
-        "      output_dict['detection_classes'] = output_dict[\n",
-        "          'detection_classes'][0].astype(np.int64)\n",
-        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
-        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
-        "      if 'detection_masks' in output_dict:\n",
-        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
+        "  # All outputs are batches tensors.\n",
+        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
+        "  # We're only interested in the first num_detections.\n",
+        "  num_detections = int(output_dict.pop('num_detections'))\n",
+        "  output_dict = {key:value[0, :num_detections].numpy() \n",
+        "                 for key,value in output_dict.items()}\n",
+        "  output_dict['num_detections'] = num_detections\n",
+        "\n",
+        "  # detection_classes should be ints.\n",
+        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
+        "   \n",
+        "  # Handle models with masks:\n",
+        "  if 'detection_masks' in output_dict:\n",
+        "    # Reframe the the bbox mask to the image size.\n",
+        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
+        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
+        "               image.shape[0], image.shape[1])      \n",
+        "    detection_masks_reframed = tf.cast(detection_masks_reframed \u003e 0.5,\n",
+        "                                       tf.uint8)\n",
+        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
+        "    \n",
         "  return output_dict"
       ]
     },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "z1wq0LVyMRR_"
+      },
+      "source": [
+        "Run it on each test image and show the results:"
+      ]
+    },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "3a5wMHN8WKMh"
+        "id": "DWh_1zz6aqxs"
       },
       "outputs": [],
       "source": [
-        "for image_path in TEST_IMAGE_PATHS:\n",
-        "  image = Image.open(image_path)\n",
+        "def show_inference(model, image_path):\n",
         "  # the array based representation of the image will be used later in order to prepare the\n",
         "  # result image with boxes and labels on it.\n",
-        "  image_np = load_image_into_numpy_array(image)\n",
-        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
-        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
+        "  image_np = np.array(Image.open(image_path))\n",
         "  # Actual detection.\n",
-        "  output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)\n",
+        "  output_dict = run_inference_for_single_image(model, image_np)\n",
         "  # Visualization of the results of a detection.\n",
         "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
         "      image_np,\n",
@@ -426,25 +555,95 @@
         "      output_dict['detection_classes'],\n",
         "      output_dict['detection_scores'],\n",
         "      category_index,\n",
-        "      instance_masks=output_dict.get('detection_masks'),\n",
+        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
         "      use_normalized_coordinates=True,\n",
         "      line_thickness=8)\n",
-        "  plt.figure(figsize=IMAGE_SIZE)\n",
-        "  plt.imshow(image_np)"
+        "\n",
+        "  display(Image.fromarray(image_np))"
       ]
     },
     {
       "cell_type": "code",
       "execution_count": 0,
       "metadata": {
-        "colab": {
-          "autoexec": {
-            "startup": false,
-            "wait_interval": 0
-          }
-        },
+        "colab": {},
         "colab_type": "code",
-        "id": "LQSEnEsPWKMj"
+        "id": "3a5wMHN8WKMh"
+      },
+      "outputs": [],
+      "source": [
+        "for image_path in TEST_IMAGE_PATHS:\n",
+        "  show_inference(detection_model, image_path)\n"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "DsspMPX3Cssg"
+      },
+      "source": [
+        "## Instance Segmentation"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "CzkVv_n2MxKC"
+      },
+      "outputs": [],
+      "source": [
+        "model_name = \"mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28\"\n",
+        "masking_model = load_model(\"mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28\")"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "0S7aZi8ZOhVV"
+      },
+      "source": [
+        "The instance segmentation model includes a `detection_masks` output:"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "vQ2Sj2VIOZLA"
+      },
+      "outputs": [],
+      "source": [
+        "masking_model.output_shapes"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "AS57rZlnNL7W"
+      },
+      "outputs": [],
+      "source": [
+        "for image_path in TEST_IMAGE_PATHS:\n",
+        "  show_inference(masking_model, image_path)"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {},
+        "colab_type": "code",
+        "id": "nLlmm9JojEKm"
       },
       "outputs": [],
       "source": [
@@ -453,17 +652,58 @@
     }
   ],
   "metadata": {
+    "accelerator": "GPU",
     "colab": {
-      "default_view": {},
-      "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
-      "provenance": [],
-      "version": "0.3.2",
-      "views": {}
+      "collapsed_sections": [],
+      "last_runtime": {
+        "build_target": "//learning/brain/python/client:colab_notebook",
+        "kind": "private"
+      },
+      "name": "object_detection_tutorial.ipynb",
+      "private_outputs": true,
+      "provenance": [
+        {
+          "file_id": "1LNYL6Zsn9Xlil2CVNOTsgDZQSBKeOjCh",
+          "timestamp": 1566498233247
+        },
+        {
+          "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
+          "timestamp": 1566488313397
+        },
+        {
+          "file_id": "/piper/depot/google3/third_party/py/tensorflow_docs/g3doc/en/r2/tutorials/generative/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
+          "timestamp": 1566145894046
+        },
+        {
+          "file_id": "1nBPoWynOV0auSIy40eQcBIk9C6YRSkI8",
+          "timestamp": 1566145841085
+        },
+        {
+          "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
+          "timestamp": 1556295408037
+        },
+        {
+          "file_id": "1layerger-51XwWOwYMY_5zHaCavCeQkO",
+          "timestamp": 1556214267924
+        },
+        {
+          "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
+          "timestamp": 1556207836484
+        },
+        {
+          "file_id": "1w6mqQiNV3liPIX70NOgitOlDF1_4sRMw",
+          "timestamp": 1556154824101
+        },
+        {
+          "file_id": "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb",
+          "timestamp": 1556150293326
+        }
+      ],
+      "version": "0.3.2"
     },
     "kernelspec": {
-      "display_name": "Python 2",
-      "language": "python",
-      "name": "python2"
+      "display_name": "Python 3",
+      "name": "python3"
     }
   },
   "nbformat": 4,
diff --git a/research/object_detection/protos/box_predictor.proto b/research/object_detection/protos/box_predictor.proto
index 75a38bff..0b0fadd7 100644
--- a/research/object_detection/protos/box_predictor.proto
+++ b/research/object_detection/protos/box_predictor.proto
@@ -66,7 +66,7 @@ message ConvolutionalBoxPredictor {
 }
 
 // Configuration proto for weight shared convolutional box predictor.
-// Next id: 18
+// Next id: 19
 message WeightSharedConvolutionalBoxPredictor {
   // Hyperparameters for convolution ops used in the box predictor.
   optional Hyperparams conv_hyperparams = 1;
@@ -122,8 +122,8 @@ message WeightSharedConvolutionalBoxPredictor {
     optional float max = 2;
   }
   optional BoxEncodingsClipRange box_encodings_clip_range = 17;
-}
 
+}
 
 
 // TODO(alirezafathi): Refactor the proto file to be able to configure mask rcnn
@@ -197,3 +197,4 @@ message RfcnBoxPredictor {
 
   optional int32 crop_width = 7 [default = 12];
 }
+
diff --git a/research/object_detection/protos/calibration.proto b/research/object_detection/protos/calibration.proto
index 6cb25a94..60251170 100644
--- a/research/object_detection/protos/calibration.proto
+++ b/research/object_detection/protos/calibration.proto
@@ -21,6 +21,9 @@ message CalibrationConfig {
 
     // Per-class sigmoid calibration.
     ClassIdSigmoidCalibrations class_id_sigmoid_calibrations = 4;
+
+    // Temperature scaling calibration.
+    TemperatureScalingCalibration temperature_scaling_calibration = 5;
   }
 }
 
@@ -50,6 +53,11 @@ message ClassIdSigmoidCalibrations {
   map<int32, SigmoidParameters> class_id_sigmoid_parameters_map = 1;
 }
 
+// Message for Temperature Scaling Calibration.
+message TemperatureScalingCalibration {
+  optional float scaler = 1;
+}
+
 // Description of data used to fit the calibration model. CLASS_SPECIFIC
 // indicates that the calibration parameters are derived from detections
 // pertaining to a single class. ALL_CLASSES indicates that parameters were
diff --git a/research/object_detection/protos/eval.proto b/research/object_detection/protos/eval.proto
index ec05f7ff..d6c5b43b 100644
--- a/research/object_detection/protos/eval.proto
+++ b/research/object_detection/protos/eval.proto
@@ -4,80 +4,85 @@ package object_detection.protos;
 
 // Message for configuring DetectionModel evaluation jobs (eval.py).
 message EvalConfig {
-  optional uint32 batch_size = 25 [default=1];
+  optional uint32 batch_size = 25 [default = 1];
   // Number of visualization images to generate.
-  optional uint32 num_visualizations = 1 [default=10];
+  optional uint32 num_visualizations = 1 [default = 10];
 
   // Number of examples to process of evaluation.
-  optional uint32 num_examples = 2 [default=5000, deprecated=true];
+  optional uint32 num_examples = 2 [default = 5000, deprecated = true];
 
   // How often to run evaluation.
-  optional uint32 eval_interval_secs = 3 [default=300];
+  optional uint32 eval_interval_secs = 3 [default = 300];
 
   // Maximum number of times to run evaluation. If set to 0, will run forever.
-  optional uint32 max_evals = 4 [default=0, deprecated=true];
+  optional uint32 max_evals = 4 [default = 0, deprecated = true];
 
   // Whether the TensorFlow graph used for evaluation should be saved to disk.
-  optional bool save_graph = 5 [default=false];
+  optional bool save_graph = 5 [default = false];
 
   // Path to directory to store visualizations in. If empty, visualization
   // images are not exported (only shown on Tensorboard).
-  optional string visualization_export_dir = 6 [default=""];
+  optional string visualization_export_dir = 6 [default = ""];
 
   // BNS name of the TensorFlow master.
-  optional string eval_master = 7 [default=""];
+  optional string eval_master = 7 [default = ""];
 
   // Type of metrics to use for evaluation.
   repeated string metrics_set = 8;
 
   // Path to export detections to COCO compatible JSON format.
-  optional string export_path = 9 [default=''];
+  optional string export_path = 9 [default =''];
 
   // Option to not read groundtruth labels and only export detections to
   // COCO-compatible JSON file.
-  optional bool ignore_groundtruth = 10 [default=false];
+  optional bool ignore_groundtruth = 10 [default = false];
 
   // Use exponential moving averages of variables for evaluation.
   // TODO(rathodv): When this is false make sure the model is constructed
   // without moving averages in restore_fn.
-  optional bool use_moving_averages = 11 [default=false];
+  optional bool use_moving_averages = 11 [default = false];
 
   // Whether to evaluate instance masks.
   // Note that since there is no evaluation code currently for instance
   // segmenation this option is unused.
-  optional bool eval_instance_masks = 12 [default=false];
+  optional bool eval_instance_masks = 12 [default = false];
 
   // Minimum score threshold for a detected object box to be visualized
-  optional float min_score_threshold = 13 [default=0.5];
+  optional float min_score_threshold = 13 [default = 0.5];
 
   // Maximum number of detections to visualize
-  optional int32 max_num_boxes_to_visualize = 14 [default=20];
+  optional int32 max_num_boxes_to_visualize = 14 [default = 20];
 
   // When drawing a single detection, each label is by default visualized as
   // <label name> : <label score>. One can skip the name or/and score using the
   // following fields:
-  optional bool skip_scores = 15 [default=false];
-  optional bool skip_labels = 16 [default=false];
+  optional bool skip_scores = 15 [default = false];
+  optional bool skip_labels = 16 [default = false];
 
   // Whether to show groundtruth boxes in addition to detected boxes in
   // visualizations.
-  optional bool visualize_groundtruth_boxes = 17 [default=false];
+  optional bool visualize_groundtruth_boxes = 17 [default = false];
 
   // Box color for visualizing groundtruth boxes.
-  optional string groundtruth_box_visualization_color = 18 [default="black"];
+  optional string groundtruth_box_visualization_color = 18 [default = "black"];
 
   // Whether to keep image identifier in filename when exported to
   // visualization_export_dir.
-  optional bool keep_image_id_for_visualization_export = 19 [default=false];
+  optional bool keep_image_id_for_visualization_export = 19 [default = false];
 
   // Whether to retain original images (i.e. not pre-processed) in the tensor
   // dictionary, so that they can be displayed in Tensorboard.
-  optional bool retain_original_images = 23 [default=true];
+  optional bool retain_original_images = 23 [default = true];
 
   // If True, additionally include per-category metrics.
-  optional bool include_metrics_per_category = 24 [default=false];
+  optional bool include_metrics_per_category = 24 [default = false];
 
   // Recall range within which precision should be computed.
   optional float recall_lower_bound = 26 [default = 0.0];
   optional float recall_upper_bound = 27 [default = 1.0];
+
+  // Whether to retain additional channels (i.e. not pre-processed) in the
+  // tensor dictionary, so that they can be displayed in Tensorboard.
+  optional bool retain_original_image_additional_channels = 28
+      [default = false];
 }
diff --git a/research/object_detection/protos/faster_rcnn.proto b/research/object_detection/protos/faster_rcnn.proto
index fb8154de..95324b9f 100644
--- a/research/object_detection/protos/faster_rcnn.proto
+++ b/research/object_detection/protos/faster_rcnn.proto
@@ -169,11 +169,17 @@ message FasterRcnn {
   // running evaluation (specifically not is_training if False).
   optional bool use_static_shapes_for_eval = 37 [default = false];
 
+  // If true, uses implementation of partitioned_non_max_suppression in first
+  // stage.
+  optional bool use_partitioned_nms_in_first_stage = 38 [default = true];
+
+  // Whether to return raw detections (pre NMS).
+  optional bool return_raw_detections_during_predict = 39 [default = false];
+
   // Whether to use tf.image.combined_non_max_suppression.
-  optional bool use_combined_nms_in_first_stage = 38 [default=false];
+  optional bool use_combined_nms_in_first_stage = 40 [default = false];
 }
 
-
 message FasterRcnnFeatureExtractor {
   // Type of Faster R-CNN model (e.g., 'faster_rcnn_resnet101';
   // See builders/model_builder.py for expected types).
diff --git a/research/object_detection/protos/model.proto b/research/object_detection/protos/model.proto
index b699c17b..90b8cdb3 100644
--- a/research/object_detection/protos/model.proto
+++ b/research/object_detection/protos/model.proto
@@ -10,5 +10,15 @@ message DetectionModel {
   oneof model {
     FasterRcnn faster_rcnn = 1;
     Ssd ssd = 2;
+
+    // This can be used to define experimental models. To define your own
+    // experimental meta architecture, populate a key in the
+    // model_builder.EXPERIMENTAL_META_ARCHITECURE_BUILDER_MAP dict and set its
+    // value to a function that builds your model.
+    ExperimentalModel experimental_model = 3;
   }
 }
+
+message ExperimentalModel {
+  optional string name = 1;
+}
diff --git a/research/object_detection/protos/post_processing.proto b/research/object_detection/protos/post_processing.proto
index c4043b37..a286d931 100644
--- a/research/object_detection/protos/post_processing.proto
+++ b/research/object_detection/protos/post_processing.proto
@@ -40,8 +40,11 @@ message BatchNonMaxSuppression {
   // Soft NMS sigma parameter; Bodla et al, https://arxiv.org/abs/1704.04503)
   optional float soft_nms_sigma = 9 [default = 0.0];
 
+  // Whether to use partitioned version of non_max_suppression.
+  optional bool use_partitioned_nms = 10 [default = false];
+
   // Whether to use tf.image.combined_non_max_suppression.
-  optional bool use_combined_nms = 10 [default = false];
+  optional bool use_combined_nms = 11 [default = false];
 }
 
 // Configuration proto for post-processing predicted boxes and
diff --git a/research/object_detection/protos/preprocessor.proto b/research/object_detection/protos/preprocessor.proto
index 74f6aaef..1937decf 100644
--- a/research/object_detection/protos/preprocessor.proto
+++ b/research/object_detection/protos/preprocessor.proto
@@ -39,6 +39,9 @@ message PreprocessingStep {
     AutoAugmentImage autoaugment_image = 31;
     DropLabelProbabilistically drop_label_probabilistically = 32;
     RemapLabels remap_labels = 33;
+    RandomJpegQuality random_jpeg_quality = 34;
+    RandomDownscaleToTargetPixels random_downscale_to_target_pixels = 35;
+    RandomPatchGaussian random_patch_gaussian = 36;
   }
 }
 
@@ -490,3 +493,43 @@ message RemapLabels {
   // Label to map to.
   optional int32 new_label = 2;
 }
+
+// Applies a jpeg encoding with a random quality factor.
+message RandomJpegQuality {
+  // Probability of keeping the original image.
+  optional float random_coef = 1 [default = 0.0];
+
+  // Minimum jpeg quality to use.
+  optional int32 min_jpeg_quality = 2 [default = 0];
+
+  // Maximum jpeg quality to use.
+  optional int32 max_jpeg_quality = 3 [default = 100];
+}
+
+// Randomly shrinks image (keeping aspect ratio) to a target number of pixels.
+// If the image contains less than the chosen target number of pixels, it will
+// not be changed.
+message RandomDownscaleToTargetPixels {
+  // Probability of keeping the original image.
+  optional float random_coef = 1 [default = 0.0];
+
+  // The target number of pixels will be chosen to be in the range
+  // [min_target_pixels, max_target_pixels]
+  optional int32 min_target_pixels = 2 [default = 300000];
+  optional int32 max_target_pixels = 3 [default = 500000];
+}
+
+message RandomPatchGaussian {
+  // Probability of keeping the original image.
+  optional float random_coef = 1 [default = 0.0];
+
+  // The patch size will be chosen to be in the range
+  // [min_patch_size, max_patch_size).
+  optional int32 min_patch_size = 2 [default = 1];
+  optional int32 max_patch_size = 3 [default = 250];
+
+  // The standard deviation of the gaussian noise applied within the patch will
+  // be chosen to be in the range [min_gaussian_stddev, max_gaussian_stddev).
+  optional float min_gaussian_stddev = 4 [default = 0.0];
+  optional float max_gaussian_stddev = 5 [default = 1.0];
+}
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index 16141db4..7465759a 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -13,7 +13,7 @@ import "object_detection/protos/post_processing.proto";
 import "object_detection/protos/region_similarity_calculator.proto";
 
 // Configuration for Single Shot Detection (SSD) models.
-// Next id: 26
+// Next id: 27
 message Ssd {
   // Number of classes to predict.
   optional int32 num_classes = 1;
@@ -96,6 +96,8 @@ message Ssd {
 
   optional float implicit_example_weight = 23 [default = 1.0];
 
+  optional bool return_raw_detections_during_predict = 26 [default = false];
+
   // Configuration proto for MaskHead.
   // Next id: 11
   message MaskHead {
diff --git a/research/object_detection/protos/target_assigner.proto b/research/object_detection/protos/target_assigner.proto
new file mode 100644
index 00000000..dea2d0c6
--- /dev/null
+++ b/research/object_detection/protos/target_assigner.proto
@@ -0,0 +1,14 @@
+syntax = "proto2";
+
+package object_detection.protos;
+
+import "object_detection/protos/box_coder.proto";
+import "object_detection/protos/matcher.proto";
+import "object_detection/protos/region_similarity_calculator.proto";
+
+// Message to configure Target Assigner for object detectors.
+message TargetAssigner {
+  optional Matcher matcher = 1;
+  optional RegionSimilarityCalculator similarity_calculator = 2;
+  optional BoxCoder box_coder = 3;
+}
diff --git a/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config b/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config
new file mode 100644
index 00000000..8bc29ce3
--- /dev/null
+++ b/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config
@@ -0,0 +1,200 @@
+# SSDLite with Mobilenet v3 large feature extractor.
+# Trained on COCO14, initialized from scratch.
+# 3.22M parameters, 1.02B FLOPs
+# TPU-compatible.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 320
+        width: 320
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 3
+        use_depthwise: true
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v3_large'
+      min_depth: 16
+      depth_multiplier: 1.0
+      use_depthwise: true
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 1.0
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+        use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 512
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 32
+  num_steps: 400000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.4
+          total_steps: 400000
+          warmup_learning_rate: 0.13333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
+
diff --git a/research/object_detection/samples/configs/ssdlite_mobilenet_v3_small_320x320_coco.config b/research/object_detection/samples/configs/ssdlite_mobilenet_v3_small_320x320_coco.config
new file mode 100644
index 00000000..b703b552
--- /dev/null
+++ b/research/object_detection/samples/configs/ssdlite_mobilenet_v3_small_320x320_coco.config
@@ -0,0 +1,199 @@
+# SSDLite with Mobilenet v3 small feature extractor.
+# Trained on COCO14, initialized from scratch.
+# TPU-compatible.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 320
+        width: 320
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 3
+        use_depthwise: true
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v3_small'
+      min_depth: 16
+      depth_multiplier: 1.0
+      use_depthwise: true
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 1.0
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+        use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 512
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 32
+  num_steps: 800000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.4
+          total_steps: 800000
+          warmup_learning_rate: 0.13333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
+
diff --git a/research/object_detection/tpu_exporters/faster_rcnn.py b/research/object_detection/tpu_exporters/faster_rcnn.py
index 8657b250..2ea32809 100644
--- a/research/object_detection/tpu_exporters/faster_rcnn.py
+++ b/research/object_detection/tpu_exporters/faster_rcnn.py
@@ -97,7 +97,12 @@ def get_prediction_tensor_shapes(pipeline_config):
   prediction_dict = detection_model.predict(preprocessed_inputs,
                                             true_image_shapes)
 
-  shapes_info = {k: v.shape.as_list() for k, v in prediction_dict.items()}
+  shapes_info = {}
+  for k, v in prediction_dict.items():
+    if isinstance(v, list):
+      shapes_info[k] = [item.shape.as_list() for item in v]
+    else:
+      shapes_info[k] = v.shape.as_list()
   return shapes_info
 
 
@@ -200,7 +205,12 @@ def build_graph(pipeline_config,
   }
 
   for k in prediction_dict:
-    prediction_dict[k].set_shape(shapes_info[k])
+    if isinstance(prediction_dict[k], list):
+      prediction_dict[k] = [
+          prediction_dict[k][idx].set_shape(shapes_info[k][idx])
+          for idx in len(prediction_dict[k])]
+    else:
+      prediction_dict[k].set_shape(shapes_info[k])
 
   if use_bfloat16:
     prediction_dict = utils.bfloat16_to_float32_nested(prediction_dict)
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index 45bdf03e..0c4e6bd9 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -552,6 +552,9 @@ def _maybe_update_config_with_key_value(configs, key, value):
     _update_retain_original_images(configs["eval_config"], value)
   elif field_name == "use_bfloat16":
     _update_use_bfloat16(configs, value)
+  elif field_name == "retain_original_image_additional_channels_in_eval":
+    _update_retain_original_image_additional_channels(configs["eval_config"],
+                                                      value)
   else:
     return False
   return True
@@ -935,3 +938,62 @@ def _update_use_bfloat16(configs, use_bfloat16):
     use_bfloat16: A bool, indicating whether to use bfloat16 for training.
   """
   configs["train_config"].use_bfloat16 = use_bfloat16
+
+
+def _update_retain_original_image_additional_channels(
+    eval_config,
+    retain_original_image_additional_channels):
+  """Updates eval config to retain original image additional channels or not.
+
+  The eval_config object is updated in place, and hence not returned.
+
+  Args:
+    eval_config: A eval_pb2.EvalConfig.
+    retain_original_image_additional_channels: Boolean indicating whether to
+      retain original image additional channels in eval mode.
+  """
+  eval_config.retain_original_image_additional_channels = (
+      retain_original_image_additional_channels)
+
+
+def remove_unecessary_ema(variables_to_restore, no_ema_collection=None):
+  """Remap and Remove EMA variable that are not created during training.
+
+  ExponentialMovingAverage.variables_to_restore() returns a map of EMA names
+  to tf variables to restore. E.g.:
+  {
+      conv/batchnorm/gamma/ExponentialMovingAverage: conv/batchnorm/gamma,
+      conv_4/conv2d_params/ExponentialMovingAverage: conv_4/conv2d_params,
+      global_step: global_step
+  }
+  This function takes care of the extra ExponentialMovingAverage variables
+  that get created during eval but aren't available in the checkpoint, by
+  remapping the key to the shallow copy of the variable itself, and remove
+  the entry of its EMA from the variables to restore. An example resulting
+  dictionary would look like:
+  {
+      conv/batchnorm/gamma: conv/batchnorm/gamma,
+      conv_4/conv2d_params: conv_4/conv2d_params,
+      global_step: global_step
+  }
+  Args:
+    variables_to_restore: A dictionary created by ExponentialMovingAverage.
+      variables_to_restore().
+    no_ema_collection: A list of namescope substrings to match the variables
+      to eliminate EMA.
+
+  Returns:
+    A variables_to_restore dictionary excluding the collection of unwanted
+    EMA mapping.
+  """
+  if no_ema_collection is None:
+    return variables_to_restore
+
+  for key in variables_to_restore:
+    if "ExponentialMovingAverage" in key:
+      for name in no_ema_collection:
+        if name in key:
+          variables_to_restore[key.replace("/ExponentialMovingAverage",
+                                           "")] = variables_to_restore[key]
+          del variables_to_restore[key]
+  return variables_to_restore
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index df72f764..245c9f3f 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -872,6 +872,62 @@ class ConfigUtilTest(tf.test.TestCase):
           field_name="shuffle",
           value=False)
 
+  def testOverWriteRetainOriginalImageAdditionalChannels(self):
+    """Tests that keyword arguments are applied correctly."""
+    original_retain_original_image_additional_channels = True
+    desired_retain_original_image_additional_channels = False
+
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.retain_original_image_additional_channels = (
+        original_retain_original_image_additional_channels)
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    override_dict = {
+        "retain_original_image_additional_channels_in_eval":
+            desired_retain_original_image_additional_channels
+    }
+    configs = config_util.merge_external_params_with_configs(
+        configs, kwargs_dict=override_dict)
+    retain_original_image_additional_channels = configs[
+        "eval_config"].retain_original_image_additional_channels
+    self.assertEqual(desired_retain_original_image_additional_channels,
+                     retain_original_image_additional_channels)
+
+  def testRemoveUnecessaryEma(self):
+    input_dict = {
+        "expanded_conv_10/project/act_quant/min":
+            1,
+        "FeatureExtractor/MobilenetV2_2/expanded_conv_5/expand/act_quant/min":
+            2,
+        "expanded_conv_10/expand/BatchNorm/gamma/min/ExponentialMovingAverage":
+            3,
+        "expanded_conv_3/depthwise/BatchNorm/beta/max/ExponentialMovingAverage":
+            4,
+        "BoxPredictor_1/ClassPredictor_depthwise/act_quant":
+            5
+    }
+
+    no_ema_collection = ["/min", "/max"]
+
+    output_dict = {
+        "expanded_conv_10/project/act_quant/min":
+            1,
+        "FeatureExtractor/MobilenetV2_2/expanded_conv_5/expand/act_quant/min":
+            2,
+        "expanded_conv_10/expand/BatchNorm/gamma/min":
+            3,
+        "expanded_conv_3/depthwise/BatchNorm/beta/max":
+            4,
+        "BoxPredictor_1/ClassPredictor_depthwise/act_quant":
+            5
+    }
+
+    self.assertEqual(
+        output_dict,
+        config_util.remove_unecessary_ema(input_dict, no_ema_collection))
+
 
 if __name__ == "__main__":
   tf.test.main()
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 6ec220de..d180282c 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -227,6 +227,29 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
     ])
     self._build_metric_names()
 
+  def get_internal_state(self):
+    """Returns internal state and image ids that lead to the state.
+
+    Note that only evaluation results will be returned (e.g. not raw predictions
+    or groundtruth.
+    """
+    return self._evaluation.get_internal_state(), self._image_ids
+
+  def merge_internal_state(self, image_ids, state_tuple):
+    """Merges internal state with the existing state of evaluation.
+
+    If image_id is already seen by evaluator, an error will be thrown.
+
+    Args:
+      image_ids: list of images whose state is stored in the tuple.
+      state_tuple: state.
+    """
+    for image_id in image_ids:
+      if image_id in self._image_ids:
+        raise ValueError('Image with id {} already added.'.format(image_id))
+
+    self._evaluation.merge_internal_state(state_tuple)
+
   def _build_metric_names(self):
     """Builds a list with metric names."""
     if self._recall_lower_bound > 0.0 or self._recall_upper_bound < 1.0:
@@ -434,21 +457,23 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
         label_id_offset=self._label_id_offset)
     self._image_ids.clear()
 
-  def get_estimator_eval_metric_ops(self, eval_dict):
-    """Returns dict of metrics to use with `tf.estimator.EstimatorSpec`.
+  def add_eval_dict(self, eval_dict):
+    """Observes an evaluation result dict for a single example.
 
-    Note that this must only be implemented if performing evaluation with a
-    `tf.estimator.Estimator`.
+    When executing eagerly, once all observations have been observed by this
+    method you can use `.evaluate()` to get the final metrics.
+
+    When using `tf.estimator.Estimator` for evaluation this function is used by
+    `get_estimator_eval_metric_ops()` to construct the metric update op.
 
     Args:
       eval_dict: A dictionary that holds tensors for evaluating an object
         detection model, returned from
-        eval_util.result_dict_for_single_example(). It must contain
-        standard_fields.InputDataFields.key.
+        eval_util.result_dict_for_single_example().
 
     Returns:
-      A dictionary of metric names to tuple of value_op and update_op that can
-      be used as eval metric ops in `tf.estimator.EstimatorSpec`.
+      None when executing eagerly, or an update_op that can be used to update
+      the eval metrics in `tf.estimator.EstimatorSpec`.
     """
     # remove unexpected fields
     eval_dict_filtered = dict()
@@ -479,7 +504,25 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
 
     args = [eval_dict_filtered[standard_fields.InputDataFields.key]]
     args.extend(six.itervalues(eval_dict_filtered))
-    update_op = tf.py_func(update_op, args, [])
+    return tf.py_func(update_op, args, [])
+
+  def get_estimator_eval_metric_ops(self, eval_dict):
+    """Returns dict of metrics to use with `tf.estimator.EstimatorSpec`.
+
+    Note that this must only be implemented if performing evaluation with a
+    `tf.estimator.Estimator`.
+
+    Args:
+      eval_dict: A dictionary that holds tensors for evaluating an object
+        detection model, returned from
+        eval_util.result_dict_for_single_example(). It must contain
+        standard_fields.InputDataFields.key.
+
+    Returns:
+      A dictionary of metric names to tuple of value_op and update_op that can
+      be used as eval metric ops in `tf.estimator.EstimatorSpec`.
+    """
+    update_op = self.add_eval_dict(eval_dict)
 
     def first_value_func():
       self._metrics = self.evaluate()
@@ -919,6 +962,16 @@ class OpenImagesInstanceSegmentationChallengeEvaluator(
         group_of_weight=0.0)
 
 
+ObjectDetectionEvaluationState = collections.namedtuple(
+    'ObjectDetectionEvaluationState', [
+        'num_gt_instances_per_class',
+        'scores_per_class',
+        'tp_fp_labels_per_class',
+        'num_gt_imgs_per_class',
+        'num_images_correctly_detected_per_class',
+    ])
+
+
 class ObjectDetectionEvaluation(object):
   """Internal implementation of Pascal object detection metrics."""
 
@@ -996,12 +1049,47 @@ class ObjectDetectionEvaluation(object):
     self.average_precision_per_class.fill(np.nan)
     self.precisions_per_class = [np.nan] * self.num_class
     self.recalls_per_class = [np.nan] * self.num_class
+    self.sum_tp_class = [np.nan] * self.num_class
 
     self.corloc_per_class = np.ones(self.num_class, dtype=float)
 
   def clear_detections(self):
     self._initialize_detections()
 
+  def get_internal_state(self):
+    """Returns internal state of the evaluation.
+
+    NOTE: that only evaluation results will be returned
+    (e.g. no raw predictions or groundtruth).
+    Returns:
+      internal state of the evaluation.
+    """
+    return ObjectDetectionEvaluationState(
+        self.num_gt_instances_per_class, self.scores_per_class,
+        self.tp_fp_labels_per_class, self.num_gt_imgs_per_class,
+        self.num_images_correctly_detected_per_class)
+
+  def merge_internal_state(self, state_tuple):
+    """Merges internal state of the evaluation with the current state.
+
+    Args:
+      state_tuple: state tuple representing evaluation state: should be of type
+        ObjectDetectionEvaluationState.
+    """
+    (num_gt_instances_per_class, scores_per_class, tp_fp_labels_per_class,
+     num_gt_imgs_per_class, num_images_correctly_detected_per_class) = (
+         state_tuple)
+    assert self.num_class == len(num_gt_instances_per_class)
+    assert self.num_class == len(scores_per_class)
+    assert self.num_class == len(tp_fp_labels_per_class)
+    for i in range(self.num_class):
+      self.scores_per_class[i].extend(scores_per_class[i])
+      self.tp_fp_labels_per_class[i].extend(tp_fp_labels_per_class[i])
+      self.num_gt_instances_per_class[i] += num_gt_instances_per_class[i]
+      self.num_gt_imgs_per_class[i] += num_gt_imgs_per_class[i]
+      self.num_images_correctly_detected_per_class[
+          i] += num_images_correctly_detected_per_class[i]
+
   def add_single_ground_truth_image_info(self,
                                          image_key,
                                          groundtruth_boxes,
@@ -1162,9 +1250,9 @@ class ObjectDetectionEvaluation(object):
           ~groundtruth_is_difficult_list
           & ~groundtruth_is_group_of_list] == class_index)
       num_groupof_gt_instances = self.group_of_weight * np.sum(
-          groundtruth_class_labels[groundtruth_is_group_of_list
-                                   & ~groundtruth_is_difficult_list] ==
-          class_index)
+          groundtruth_class_labels[
+              groundtruth_is_group_of_list
+              & ~groundtruth_is_difficult_list] == class_index)
       self.num_gt_instances_per_class[
           class_index] += num_gt_instances + num_groupof_gt_instances
       if np.any(groundtruth_class_labels == class_index):
@@ -1216,6 +1304,7 @@ class ObjectDetectionEvaluation(object):
 
       self.precisions_per_class[class_index] = precision_within_bound
       self.recalls_per_class[class_index] = recall_within_bound
+      self.sum_tp_class[class_index] = tp_fp_labels.sum()
       average_precision = metrics.compute_average_precision(
           precision_within_bound, recall_within_bound)
       self.average_precision_per_class[class_index] = average_precision
diff --git a/research/object_detection/utils/object_detection_evaluation_test.py b/research/object_detection/utils/object_detection_evaluation_test.py
index 6e975932..957826a2 100644
--- a/research/object_detection/utils/object_detection_evaluation_test.py
+++ b/research/object_detection/utils/object_detection_evaluation_test.py
@@ -941,6 +941,34 @@ class ObjectDetectionEvaluationTest(tf.test.TestCase):
     self.assertAlmostEqual(expected_mean_ap, mean_ap)
     self.assertAlmostEqual(expected_mean_corloc, mean_corloc)
 
+  def test_merge_internal_state(self):
+    # Test that if initial state is merged, the results of the evaluation are
+    # the same.
+    od_eval_state = self.od_eval.get_internal_state()
+    copy_od_eval = object_detection_evaluation.ObjectDetectionEvaluation(
+        self.od_eval.num_class)
+    copy_od_eval.merge_internal_state(od_eval_state)
+
+    (average_precision_per_class, mean_ap, precisions_per_class,
+     recalls_per_class, corloc_per_class,
+     mean_corloc) = self.od_eval.evaluate()
+
+    (copy_average_precision_per_class, copy_mean_ap, copy_precisions_per_class,
+     copy_recalls_per_class, copy_corloc_per_class,
+     copy_mean_corloc) = copy_od_eval.evaluate()
+
+    for i in range(self.od_eval.num_class):
+      self.assertTrue(
+          np.allclose(copy_precisions_per_class[i], precisions_per_class[i]))
+      self.assertTrue(
+          np.allclose(copy_recalls_per_class[i], recalls_per_class[i]))
+    self.assertTrue(
+        np.allclose(copy_average_precision_per_class,
+                    average_precision_per_class))
+    self.assertTrue(np.allclose(copy_corloc_per_class, corloc_per_class))
+    self.assertAlmostEqual(copy_mean_ap, mean_ap)
+    self.assertAlmostEqual(copy_mean_corloc, mean_corloc)
+
 
 class ObjectDetectionEvaluatorTest(tf.test.TestCase, parameterized.TestCase):
 
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 28880e75..c4b6101a 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -967,9 +967,8 @@ def nearest_neighbor_upsampling(input_tensor, scale=None, height_scale=None,
     w_scale = scale if width_scale is None else width_scale
     (batch_size, height, width,
      channels) = shape_utils.combined_static_and_dynamic_shape(input_tensor)
-    output_tensor = tf.reshape(
-        input_tensor, [batch_size, height, 1, width, 1, channels]) * tf.ones(
-            [1, 1, h_scale, 1, w_scale, 1], dtype=input_tensor.dtype)
+    output_tensor = tf.stack([input_tensor] * w_scale, axis=3)
+    output_tensor = tf.stack([output_tensor] * h_scale, axis=2)
     return tf.reshape(output_tensor,
                       [batch_size, height * h_scale, width * w_scale, channels])
 
diff --git a/research/object_detection/utils/patch_ops.py b/research/object_detection/utils/patch_ops.py
new file mode 100644
index 00000000..0d1524c8
--- /dev/null
+++ b/research/object_detection/utils/patch_ops.py
@@ -0,0 +1,85 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Operations for image patches."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+def get_patch_mask(y, x, patch_size, image_shape):
+  """Creates a 2D mask array for a square patch of a given size and location.
+
+  The mask is created with its center at the y and x coordinates, which must be
+  within the image. While the mask center must be within the image, the mask
+  itself can be partially outside of it. If patch_size is an even number, then
+  the mask is created with lower-valued coordinates first (top and left).
+
+  Args:
+    y: An integer or scalar int32 tensor. The vertical coordinate of the
+      patch mask center. Must be within the range [0, image_height).
+    x: An integer or scalar int32 tensor. The horizontal coordinate of the
+      patch mask center. Must be within the range [0, image_width).
+    patch_size: An integer or scalar int32 tensor. The square size of the
+      patch mask. Must be at least 1.
+    image_shape: A list or 1D int32 tensor representing the shape of the image
+      to which the mask will correspond, with the first two values being image
+      height and width. For example, [image_height, image_width] or
+      [image_height, image_width, image_channels].
+
+  Returns:
+    Boolean mask tensor of shape [image_height, image_width] with True values
+    for the patch.
+
+  Raises:
+    tf.errors.InvalidArgumentError: if x is not in the range [0, image_width), y
+      is not in the range [0, image_height), or patch_size is not at least 1.
+  """
+  image_hw = image_shape[:2]
+  mask_center_yx = tf.stack([y, x])
+  with tf.control_dependencies([
+      tf.debugging.assert_greater_equal(
+          patch_size, 1,
+          message='Patch size must be >= 1'),
+      tf.debugging.assert_greater_equal(
+          mask_center_yx, 0,
+          message='Patch center (y, x) must be >= (0, 0)'),
+      tf.debugging.assert_less(
+          mask_center_yx, image_hw,
+          message='Patch center (y, x) must be < image (h, w)')
+  ]):
+    mask_center_yx = tf.identity(mask_center_yx)
+
+  half_patch_size = tf.cast(patch_size, dtype=tf.float32) / 2
+  start_yx = mask_center_yx - tf.cast(tf.floor(half_patch_size), dtype=tf.int32)
+  end_yx = mask_center_yx + tf.cast(tf.ceil(half_patch_size), dtype=tf.int32)
+
+  start_yx = tf.maximum(start_yx, 0)
+  end_yx = tf.minimum(end_yx, image_hw)
+
+  start_y = start_yx[0]
+  start_x = start_yx[1]
+  end_y = end_yx[0]
+  end_x = end_yx[1]
+
+  lower_pad = image_hw[0] - end_y
+  upper_pad = start_y
+  left_pad = start_x
+  right_pad = image_hw[1] - end_x
+  mask = tf.ones([end_y - start_y, end_x - start_x], dtype=tf.bool)
+  return tf.pad(mask, [[upper_pad, lower_pad], [left_pad, right_pad]])
diff --git a/research/object_detection/utils/patch_ops_test.py b/research/object_detection/utils/patch_ops_test.py
new file mode 100644
index 00000000..caf3567a
--- /dev/null
+++ b/research/object_detection/utils/patch_ops_test.py
@@ -0,0 +1,139 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.utils.patch_ops."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl.testing import parameterized
+import numpy as np
+import tensorflow as tf
+
+from object_detection.utils import patch_ops
+
+
+class GetPatchMaskTest(tf.test.TestCase, parameterized.TestCase):
+
+  def testMaskShape(self):
+    image_shape = [15, 10]
+    mask = patch_ops.get_patch_mask(
+        10, 5, patch_size=3, image_shape=image_shape)
+    self.assertListEqual(mask.shape.as_list(), image_shape)
+
+  def testHandleImageShapeWithChannels(self):
+    image_shape = [15, 10, 3]
+    mask = patch_ops.get_patch_mask(
+        10, 5, patch_size=3, image_shape=image_shape)
+    self.assertListEqual(mask.shape.as_list(), image_shape[:2])
+
+  def testMaskDType(self):
+    mask = patch_ops.get_patch_mask(2, 3, patch_size=2, image_shape=[6, 7])
+    self.assertDTypeEqual(mask, bool)
+
+  def testMaskAreaWithEvenPatchSize(self):
+    image_shape = [6, 7]
+    mask = patch_ops.get_patch_mask(2, 3, patch_size=2, image_shape=image_shape)
+    expected_mask = np.array([
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 1, 1, 0, 0, 0],
+        [0, 0, 1, 1, 0, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+    ]).reshape(image_shape).astype(bool)
+    self.assertAllEqual(mask, expected_mask)
+
+  def testMaskAreaWithEvenPatchSize4(self):
+    image_shape = [6, 7]
+    mask = patch_ops.get_patch_mask(2, 3, patch_size=4, image_shape=image_shape)
+    expected_mask = np.array([
+        [0, 1, 1, 1, 1, 0, 0],
+        [0, 1, 1, 1, 1, 0, 0],
+        [0, 1, 1, 1, 1, 0, 0],
+        [0, 1, 1, 1, 1, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+    ]).reshape(image_shape).astype(bool)
+    self.assertAllEqual(mask, expected_mask)
+
+  def testMaskAreaWithOddPatchSize(self):
+    image_shape = [6, 7]
+    mask = patch_ops.get_patch_mask(2, 3, patch_size=3, image_shape=image_shape)
+    expected_mask = np.array([
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 1, 1, 1, 0, 0],
+        [0, 0, 1, 1, 1, 0, 0],
+        [0, 0, 1, 1, 1, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+    ]).reshape(image_shape).astype(bool)
+    self.assertAllEqual(mask, expected_mask)
+
+  def testMaskAreaPartiallyOutsideImage(self):
+    image_shape = [6, 7]
+    mask = patch_ops.get_patch_mask(5, 6, patch_size=5, image_shape=image_shape)
+    expected_mask = np.array([
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 0, 0, 0, 0, 0],
+        [0, 0, 0, 0, 1, 1, 1],
+        [0, 0, 0, 0, 1, 1, 1],
+        [0, 0, 0, 0, 1, 1, 1],
+    ]).reshape(image_shape).astype(bool)
+    self.assertAllEqual(mask, expected_mask)
+
+  @parameterized.parameters(
+      {'y': 0, 'x': -1},
+      {'y': -1, 'x': 0},
+      {'y': 0, 'x': 11},
+      {'y': 16, 'x': 0},
+  )
+  def testStaticCoordinatesOutsideImageRaisesError(self, y, x):
+    image_shape = [15, 10]
+    with self.assertRaises(tf.errors.InvalidArgumentError):
+      patch_ops.get_patch_mask(y, x, patch_size=3, image_shape=image_shape)
+
+  def testDynamicCoordinatesOutsideImageRaisesError(self):
+    image_shape = [15, 10]
+    x = tf.random_uniform([], minval=-2, maxval=-1, dtype=tf.int32)
+    y = tf.random_uniform([], minval=0, maxval=1, dtype=tf.int32)
+    mask = patch_ops.get_patch_mask(
+        y, x, patch_size=3, image_shape=image_shape)
+    with self.assertRaises(tf.errors.InvalidArgumentError):
+      self.evaluate(mask)
+
+  @parameterized.parameters(
+      {'patch_size': 0},
+      {'patch_size': -1},
+  )
+  def testStaticNonPositivePatchSizeRaisesError(self, patch_size):
+    image_shape = [6, 7]
+    with self.assertRaises(tf.errors.InvalidArgumentError):
+      patch_ops.get_patch_mask(
+          0, 0, patch_size=patch_size, image_shape=image_shape)
+
+  def testDynamicNonPositivePatchSizeRaisesError(self):
+    image_shape = [6, 7]
+    patch_size = -1 * tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)
+    mask = patch_ops.get_patch_mask(
+        0, 0, patch_size=patch_size, image_shape=image_shape)
+    with self.assertRaises(tf.errors.InvalidArgumentError):
+      self.evaluate(mask)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/shape_utils.py b/research/object_detection/utils/shape_utils.py
index b1af74eb..254719bc 100644
--- a/research/object_detection/utils/shape_utils.py
+++ b/research/object_detection/utils/shape_utils.py
@@ -383,7 +383,7 @@ def flatten_dimensions(inputs, first, last):
 
   Example:
   `inputs` is a tensor with initial shape [10, 5, 20, 20, 3].
-  new_tensor = flatten_dimensions(inputs, last=4, first=2)
+  new_tensor = flatten_dimensions(inputs, first=1, last=3)
   new_tensor.shape -> [10, 100, 20, 3].
 
   Args:
@@ -465,3 +465,34 @@ def expand_first_dimension(inputs, dims):
     inputs_reshaped = tf.reshape(inputs, expanded_shape)
 
   return inputs_reshaped
+
+
+def resize_images_and_return_shapes(inputs, image_resizer_fn):
+  """Resizes images using the given function and returns their true shapes.
+
+  Args:
+    inputs: a float32 Tensor representing a batch of inputs of shape
+      [batch_size, height, width, channels].
+    image_resizer_fn: a function which takes in a single image and outputs
+      a resized image and its original shape.
+
+  Returns:
+    resized_inputs: The inputs resized according to image_resizer_fn.
+    true_image_shapes: A integer tensor of shape [batch_size, 3]
+      representing the height, width and number of channels in inputs.
+  """
+
+  if inputs.dtype is not tf.float32:
+    raise ValueError('`resize_images_and_return_shapes` expects a'
+                     ' tf.float32 tensor')
+
+  # TODO(jonathanhuang): revisit whether to always use batch size as
+  # the number of parallel iterations vs allow for dynamic batching.
+  outputs = static_or_dynamic_map_fn(
+      image_resizer_fn,
+      elems=inputs,
+      dtype=[tf.float32, tf.int32])
+  resized_inputs = outputs[0]
+  true_image_shapes = outputs[1]
+
+  return resized_inputs, true_image_shapes
diff --git a/research/object_detection/utils/spatial_transform_ops.py b/research/object_detection/utils/spatial_transform_ops.py
index 8794cd30..a029a4ac 100644
--- a/research/object_detection/utils/spatial_transform_ops.py
+++ b/research/object_detection/utils/spatial_transform_ops.py
@@ -290,10 +290,11 @@ def multilevel_roi_align(features, boxes, box_levels, output_size,
 
   Args:
     features: A list of 4D float tensors of shape [batch_size, max_height,
-      max_width, channels] containing features.
+      max_width, channels] containing features. Note that each feature map must
+      have the same number of channels.
     boxes: A 3D float tensor of shape [batch_size, num_boxes, 4] containing
       boxes of the form [ymin, xmin, ymax, xmax] in normalized coordinates.
-    box_levels: A 3D int32 tensor of shape [batch_size, num_boxes, 1]
+    box_levels: A 3D int32 tensor of shape [batch_size, num_boxes]
       representing the feature level index for each box.
     output_size: An list of two integers [size_y, size_x] indicating the output
       feature size for each box.
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index 305fd9b9..e22be435 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -536,7 +536,7 @@ def draw_side_by_side_evaluation_image(eval_dict,
   # Add the batch dimension if the eval_dict is for single example.
   if len(eval_dict[detection_fields.detection_classes].shape) == 1:
     for key in eval_dict:
-      if key != input_data_fields.original_image:
+      if key != input_data_fields.original_image and key != input_data_fields.image_additional_channels:
         eval_dict[key] = tf.expand_dims(eval_dict[key], 0)
 
   for indx in range(eval_dict[input_data_fields.original_image].shape[0]):
@@ -600,8 +600,42 @@ def draw_side_by_side_evaluation_image(eval_dict,
         max_boxes_to_draw=None,
         min_score_thresh=0.0,
         use_normalized_coordinates=use_normalized_coordinates)
-    images_with_detections_list.append(
-        tf.concat([images_with_detections, images_with_groundtruth], axis=2))
+    images_to_visualize = tf.concat([images_with_detections,
+                                     images_with_groundtruth], axis=2)
+
+    if input_data_fields.image_additional_channels in eval_dict:
+      images_with_additional_channels_groundtruth = (
+          draw_bounding_boxes_on_image_tensors(
+              tf.expand_dims(
+                  eval_dict[input_data_fields.image_additional_channels][indx],
+                  axis=0),
+              tf.expand_dims(
+                  eval_dict[input_data_fields.groundtruth_boxes][indx], axis=0),
+              tf.expand_dims(
+                  eval_dict[input_data_fields.groundtruth_classes][indx],
+                  axis=0),
+              tf.expand_dims(
+                  tf.ones_like(
+                      eval_dict[input_data_fields.groundtruth_classes][indx],
+                      dtype=tf.float32),
+                  axis=0),
+              category_index,
+              original_image_spatial_shape=tf.expand_dims(
+                  eval_dict[input_data_fields.original_image_spatial_shape]
+                  [indx],
+                  axis=0),
+              true_image_shape=tf.expand_dims(
+                  eval_dict[input_data_fields.true_image_shape][indx], axis=0),
+              instance_masks=groundtruth_instance_masks,
+              keypoints=None,
+              max_boxes_to_draw=None,
+              min_score_thresh=0.0,
+              use_normalized_coordinates=use_normalized_coordinates))
+      images_to_visualize = tf.concat(
+          [images_to_visualize, images_with_additional_channels_groundtruth],
+          axis=2)
+    images_with_detections_list.append(images_to_visualize)
+
   return images_with_detections_list
 
 
diff --git a/research/slim/BUILD b/research/slim/BUILD
index 344bc028..90c06788 100644
--- a/research/slim/BUILD
+++ b/research/slim/BUILD
@@ -1,5 +1,6 @@
 # Description:
 #   Contains files for loading, training and evaluating TF-Slim-based models.
+# load("//devtools/python/blaze:python3.bzl", "py2and3_test")
 
 package(
     default_visibility = ["//visibility:public"],
@@ -91,6 +92,7 @@ sh_binary(
 py_binary(
     name = "build_visualwakewords_data",
     srcs = ["datasets/build_visualwakewords_data.py"],
+    python_version = "PY2",
     deps = [
         ":build_visualwakewords_data_lib",
         # "//tensorflow",
@@ -512,11 +514,34 @@ py_library(
     ],
 )
 
+py_library(
+    name = "mobilenet_common",
+    srcs = [
+        "nets/mobilenet/conv_blocks.py",
+        "nets/mobilenet/mobilenet.py",
+    ],
+    srcs_version = "PY2AND3",
+    deps = [
+        # "//tensorflow",
+    ],
+)
+
 py_library(
     name = "mobilenet_v2",
-    srcs = glob(["nets/mobilenet/*.py"]),
+    srcs = ["nets/mobilenet/mobilenet_v2.py"],
     srcs_version = "PY2AND3",
     deps = [
+        ":mobilenet_common",
+        # "//tensorflow",
+    ],
+)
+
+py_library(
+    name = "mobilenet_v3",
+    srcs = ["nets/mobilenet/mobilenet_v3.py"],
+    srcs_version = "PY2AND3",
+    deps = [
+        ":mobilenet_common",
         # "//tensorflow",
     ],
 )
@@ -532,11 +557,22 @@ py_test(
     ],
 )
 
+py_test(  # py2and3_test
+    name = "mobilenet_v3_test",
+    srcs = ["nets/mobilenet/mobilenet_v3_test.py"],
+    srcs_version = "PY2AND3",
+    deps = [
+        ":mobilenet",
+        # "//tensorflow",
+    ],
+)
+
 py_library(
     name = "mobilenet",
     deps = [
         ":mobilenet_v1",
         ":mobilenet_v2",
+        ":mobilenet_v3",
     ],
 )
 
@@ -709,6 +745,7 @@ py_library(
 py_test(
     name = "resnet_v1_test",
     size = "medium",
+    timeout = "long",
     srcs = ["nets/resnet_v1_test.py"],
     python_version = "PY2",
     shard_count = 2,
diff --git a/research/slim/datasets/download_mscoco.sh b/research/slim/datasets/download_mscoco.sh
old mode 100644
new mode 100755
diff --git a/research/slim/nets/inception_v2.py b/research/slim/nets/inception_v2.py
index da6b822d..a391a6e1 100644
--- a/research/slim/nets/inception_v2.py
+++ b/research/slim/nets/inception_v2.py
@@ -545,6 +545,7 @@ def inception_v2(inputs,
           return net, end_points
         # 1 x 1 x 1024
         net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')
+        end_points['PreLogits'] = net
         logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
                              normalizer_fn=None, scope='Conv2d_1c_1x1')
         if spatial_squeeze:
diff --git a/research/slim/nets/mobilenet/README.md b/research/slim/nets/mobilenet/README.md
index e4699d6a..8b093f98 100644
--- a/research/slim/nets/mobilenet/README.md
+++ b/research/slim/nets/mobilenet/README.md
@@ -1,58 +1,120 @@
-# MobileNetV2
-This folder contains building code for MobileNetV2, based on
-[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)
-
-# Performance
-## Latency
-This is the timing of [MobileNetV1](../mobilenet_v1.md) vs MobileNetV2 using
-TF-Lite on the large core of Pixel 1 phone.
-
-![mnet_v1_vs_v2_pixel1_latency.png](mnet_v1_vs_v2_pixel1_latency.png)
-
-## MACs
-MACs, also sometimes known as MADDs - the number of  multiply-accumulates needed
-to compute an inference on a single image is a common metric to measure the efficiency of the model.
-
-Below is the graph comparing V2 vs a few selected networks. The size
-of each blob represents the number of parameters. Note for [ShuffleNet](https://arxiv.org/abs/1707.01083) there
-are no published size numbers. We estimate it to be comparable to MobileNetV2 numbers.
-
-![madds_top1_accuracy](madds_top1_accuracy.png)
-
-# Pretrained models
-## Imagenet  Checkpoints
-
- Classification Checkpoint | MACs (M)| Parameters (M)| Top 1 Accuracy| Top 5 Accuracy | Mobile CPU  (ms) Pixel 1
----------------------------|---------|---------------|---------|----|-------------
-| [mobilenet_v2_1.4_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz) | 582 | 6.06 | 75.0 | 92.5 | 138.0
-| [mobilenet_v2_1.3_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.3_224.tgz) | 509 | 5.34 | 74.4 | 92.1 | 123.0
-| [mobilenet_v2_1.0_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz) | 300 | 3.47 | 71.8 | 91.0 | 73.8
-| [mobilenet_v2_1.0_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_192.tgz) | 221 | 3.47 | 70.7 | 90.1 | 55.1
-| [mobilenet_v2_1.0_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_160.tgz) | 154 | 3.47 | 68.8 | 89.0 | 40.2
-| [mobilenet_v2_1.0_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_128.tgz) | 99 | 3.47 | 65.3 | 86.9 | 27.6
-| [mobilenet_v2_1.0_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_96.tgz) | 56 | 3.47 | 60.3 | 83.2 | 17.6
-| [mobilenet_v2_0.75_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_224.tgz) | 209 | 2.61 | 69.8 | 89.6 | 55.8
-| [mobilenet_v2_0.75_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_192.tgz) | 153 | 2.61 | 68.7 | 88.9 | 41.6
-| [mobilenet_v2_0.75_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_160.tgz) | 107 | 2.61 | 66.4 | 87.3 | 30.4
-| [mobilenet_v2_0.75_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_128.tgz) | 69 | 2.61 | 63.2 | 85.3 | 21.9
-| [mobilenet_v2_0.75_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_96.tgz) | 39 | 2.61 | 58.8 | 81.6 | 14.2
-| [mobilenet_v2_0.5_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_224.tgz) | 97 | 1.95 | 65.4 | 86.4 | 28.7
-| [mobilenet_v2_0.5_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_192.tgz) | 71 | 1.95 | 63.9 | 85.4 | 21.1
-| [mobilenet_v2_0.5_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_160.tgz) | 50 | 1.95 | 61.0 | 83.2 | 14.9
-| [mobilenet_v2_0.5_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_128.tgz) | 32 | 1.95 | 57.7 | 80.8 | 9.9
-| [mobilenet_v2_0.5_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_96.tgz) | 18 | 1.95 | 51.2 | 75.8 | 6.4
-| [mobilenet_v2_0.35_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_224.tgz) | 59 | 1.66 | 60.3 | 82.9 | 19.7
-| [mobilenet_v2_0.35_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_192.tgz) | 43 | 1.66 | 58.2 | 81.2 | 14.6
-| [mobilenet_v2_0.35_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_160.tgz) | 30 | 1.66 | 55.7 | 79.1 | 10.5
-| [mobilenet_v2_0.35_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_128.tgz) | 20 | 1.66 | 50.8 | 75.0 | 6.9
-| [mobilenet_v2_0.35_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_96.tgz) | 11 | 1.66 | 45.5 | 70.4 | 4.5
-
-# Training
-The numbers above can be reproduced using slim's `train_image_classifier`.
-Below is the set of parameters that achieves 72.0% for full size MobileNetV2, after about 700K when trained on 8 GPU.
-If trained on a single GPU the full convergence is after 5.5M steps. Also note that learning rate and
-num_epochs_per_decay both need to be adjusted depending on how many GPUs are being
-used due to slim's internal averaging.
+# MobilenNet
+
+This folder contains building code for
+[MobileNetV2](https://arxiv.org/abs/1801.04381) and
+[MobilenetV3](https://arxiv.org/abs/1905.02244) networks. The architectural
+definition for each model is located in [mobilenet_v2.py](mobilenet_v2.py) and
+[mobilenet_v3.py](mobilenet_v3.py) respectively.
+
+For MobilenetV1 please refer to this [page](../mobilenet_v1.md)
+
+## Performance
+
+### Mobilenet V3 latency
+
+This is the timing of [MobileNetV2] vs [MobileNetV3] using TF-Lite on the large
+core of Pixel 1 phone.
+
+![Mobilenet V2 and V3 Latency for Pixel 1.png](g3doc/latency_pixel1.png)
+
+### MACs
+
+MACs, also sometimes known as MADDs - the number of multiply-accumulates needed
+to compute an inference on a single image is a common metric to measure the
+efficiency of the model. Full size Mobilenet V3 on image size 224 uses ~215
+Million MADDs (MMadds) while achieving accuracy 75.1%, while Mobilenet V2 uses
+~300MMadds and achieving accuracy 72%. By comparison ResNet-50 uses
+approximately 3500 MMAdds while achieving 76% accuracy.
+
+Below is the graph comparing Mobilenets and a few selected networks. The size of
+each blob represents the number of parameters. Note for
+[ShuffleNet](https://arxiv.org/abs/1707.01083) there are no published size
+numbers. We estimate it to be comparable to MobileNetV2 numbers.
+
+![madds_top1_accuracy](g3doc/madds_top1_accuracy.png)
+
+## Pretrained models
+
+### Mobilenet V3 Imagenet Checkpoints
+
+All mobilenet V3 checkpoints were trained with image resolution 224x224. All
+phone latencies are in milliseconds, measured on large core. In addition to
+large and small models this page also contains so-called minimalistic models,
+these models have the same per-layer dimensions characteristic as MobilenetV3
+however, they don't utilize any of the advanced blocks (squeeze-and-excite
+units, hard-swish, and 5x5 convolutions). While these models are less efficient
+on CPU, we find that they are much more performant on GPU/DSP/EdgeTpu.
+
+| Imagenet Checkpoint | MACs (M) | Params (M) | Top1 | Pixel 1 | Pixel 2 | Pixel 3 |
+| ------------------ | -------- | ---------- | ---- | ------- | ------- | ------- |
+| [Large dm=1 (float)]   | 217      | 5.4        | 75.2 | 51.2    | 61      | 44      |
+| [Large dm=1 (8-bit)] | 217      | 5.4        | 73.9 | 44      | 42.5    | 32      |
+| [Large dm=0.75 (float)] | 155      | 4.0        | 73.3 | 39.8    | 48      | 34      |
+| [Small dm=1 (float)]   | 66       | 2.9        | 67.5 | 15.8    | 19.4    | 14.4    |
+| [Small dm=1 (8-bit)]   | 66       | 2.9        | 64.9 | 15.5    | 15      | 10.7    |
+| [Small dm=0.75 (float)] | 44       | 2.4        | 65.4 | 12.8    | 15.9    | 11.6    |
+
+#### Minimalistic checkpoints:
+
+| Imagenet Checkpoint | MACs (M) | Params (M) | Top1 | Pixel 1 | Pixel 2 | Pixel 3 |
+| -------------- | -------- | ---------- | ---- | ------- | ------- | ------- |
+| [Large minimalistic (float)]   | 209      | 3.9        | 72.3 | 44.1    | 51      | 35      |
+| [Large minimalistic (8-bit)][lm8]   | 209      | 3.9        | 71.3 | 37      | 35      | 27      |
+| [Small minimalistic (float)]   | 65       | 2.0        | 61.9 | 12.2    | 15.1    | 11      |
+
+
+[Small minimalistic (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small-minimalistic_224_1.0_float.tgz
+[Large minimalistic (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large-minimalistic_224_1.0_float.tgz
+[lm8]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large-minimalistic_224_1.0_uint8.tgz
+[Large dm=1 (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large_224_1.0_float.tgz
+[Small dm=1 (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_1.0_float.tgz
+[Large dm=1 (8-bit)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large_224_1.0_uint8.tgz
+[Small dm=1 (8-bit)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_1.0_uint8.tgz
+[Large dm=0.75 (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large_224_0.75_float.tgz
+[Small dm=0.75 (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_0.75_float.tgz
+
+### Mobilenet V2 Imagenet Checkpoints
+
+Classification Checkpoint                                                                                  | MACs (M) | Parameters (M) | Top 1 Accuracy | Top 5 Accuracy | Mobile CPU (ms) Pixel 1
+---------------------------------------------------------------------------------------------------------- | -------- | -------------- | -------------- | -------------- | -----------------------
+[mobilenet_v2_1.4_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz)   | 582      | 6.06           | 75.0           | 92.5           | 138.0
+[mobilenet_v2_1.3_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.3_224.tgz)   | 509      | 5.34           | 74.4           | 92.1           | 123.0
+[mobilenet_v2_1.0_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz)   | 300      | 3.47           | 71.8           | 91.0           | 73.8
+[mobilenet_v2_1.0_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_192.tgz)   | 221      | 3.47           | 70.7           | 90.1           | 55.1
+[mobilenet_v2_1.0_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_160.tgz)   | 154      | 3.47           | 68.8           | 89.0           | 40.2
+[mobilenet_v2_1.0_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_128.tgz)   | 99       | 3.47           | 65.3           | 86.9           | 27.6
+[mobilenet_v2_1.0_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_96.tgz)     | 56       | 3.47           | 60.3           | 83.2           | 17.6
+[mobilenet_v2_0.75_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_224.tgz) | 209      | 2.61           | 69.8           | 89.6           | 55.8
+[mobilenet_v2_0.75_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_192.tgz) | 153      | 2.61           | 68.7           | 88.9           | 41.6
+[mobilenet_v2_0.75_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_160.tgz) | 107      | 2.61           | 66.4           | 87.3           | 30.4
+[mobilenet_v2_0.75_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_128.tgz) | 69       | 2.61           | 63.2           | 85.3           | 21.9
+[mobilenet_v2_0.75_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_96.tgz)   | 39       | 2.61           | 58.8           | 81.6           | 14.2
+[mobilenet_v2_0.5_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_224.tgz)   | 97       | 1.95           | 65.4           | 86.4           | 28.7
+[mobilenet_v2_0.5_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_192.tgz)   | 71       | 1.95           | 63.9           | 85.4           | 21.1
+[mobilenet_v2_0.5_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_160.tgz)   | 50       | 1.95           | 61.0           | 83.2           | 14.9
+[mobilenet_v2_0.5_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_128.tgz)   | 32       | 1.95           | 57.7           | 80.8           | 9.9
+[mobilenet_v2_0.5_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_96.tgz)     | 18       | 1.95           | 51.2           | 75.8           | 6.4
+[mobilenet_v2_0.35_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_224.tgz) | 59       | 1.66           | 60.3           | 82.9           | 19.7
+[mobilenet_v2_0.35_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_192.tgz) | 43       | 1.66           | 58.2           | 81.2           | 14.6
+[mobilenet_v2_0.35_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_160.tgz) | 30       | 1.66           | 55.7           | 79.1           | 10.5
+[mobilenet_v2_0.35_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_128.tgz) | 20       | 1.66           | 50.8           | 75.0           | 6.9
+[mobilenet_v2_0.35_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_96.tgz)   | 11       | 1.66           | 45.5           | 70.4           | 4.5
+
+## Training
+
+### V3
+
+TODO: Add V3 hyperparameters
+
+### V2
+
+The numbers above can be reproduced using slim's
+[`train_image_classifier`](https://github.com/tensorflow/models/blob/master/research/slim/README.md#training-a-model-from-scratch).
+Below is the set of parameters that achieves 72.0% for full size MobileNetV2,
+after about 700K when trained on 8 GPU. If trained on a single GPU the full
+convergence is after 5.5M steps. Also note that learning rate and
+num_epochs_per_decay both need to be adjusted depending on how many GPUs are
+being used due to slim's internal averaging.
 
 ```bash
 --model_name="mobilenet_v2"
@@ -68,6 +130,9 @@ used due to slim's internal averaging.
 
 # Example
 
+See this [ipython notebook](mobilenet_example.ipynb) or open and run the network
+directly in
+[Colaboratory](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb).
 
-See this [ipython notebook](mobilenet_example.ipynb) or open and run the network directly in [Colaboratory](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb).
-
+[MobilenetV2]: https://arxiv.org/abs/1801.04381
+[MobilenetV3]: https://arxiv.org/abs/1905.02244
diff --git a/research/slim/nets/mobilenet/conv_blocks.py b/research/slim/nets/mobilenet/conv_blocks.py
index b86b342b..d4d431e7 100644
--- a/research/slim/nets/mobilenet/conv_blocks.py
+++ b/research/slim/nets/mobilenet/conv_blocks.py
@@ -159,6 +159,50 @@ def expand_input_by_factor(n, divisible_by=8):
   return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)
 
 
+def split_conv(input_tensor,
+               num_outputs,
+               num_ways,
+               scope,
+               divisible_by=8,
+               **kwargs):
+  """Creates a split convolution.
+
+  Split convolution splits the input and output into
+  'num_blocks' blocks of approximately the same size each,
+  and only connects $i$-th input to $i$ output.
+
+  Args:
+    input_tensor: input tensor
+    num_outputs: number of output filters
+    num_ways: num blocks to split by.
+    scope: scope for all the operators.
+    divisible_by: make sure that every part is divisiable by this.
+    **kwargs: will be passed directly into conv2d operator
+  Returns:
+    tensor
+  """
+  b = input_tensor.get_shape().as_list()[3]
+
+  if num_ways == 1 or min(b // num_ways,
+                          num_outputs // num_ways) < divisible_by:
+    # Don't do any splitting if we end up with less than 8 filters
+    # on either side.
+    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)
+
+  outs = []
+  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)
+  output_splits = _split_divisible(
+      num_outputs, num_ways, divisible_by=divisible_by)
+  inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)
+  base = scope
+  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):
+    scope = base + '_part_%d' % (i,)
+    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)
+    n = tf.identity(n, scope + '_output')
+    outs.append(n)
+  return tf.concat(outs, 3, name=scope + '_concat')
+
+
 @slim.add_arg_scope
 def expanded_conv(input_tensor,
                   num_outputs,
@@ -168,7 +212,6 @@ def expanded_conv(input_tensor,
                   kernel_size=(3, 3),
                   residual=True,
                   normalizer_fn=None,
-                  project_activation_fn=tf.identity,
                   split_projection=1,
                   split_expansion=1,
                   split_divisible_by=8,
@@ -178,6 +221,12 @@ def expanded_conv(input_tensor,
                   endpoints=None,
                   use_explicit_padding=False,
                   padding='SAME',
+                  inner_activation_fn=None,
+                  depthwise_activation_fn=None,
+                  project_activation_fn=tf.identity,
+                  depthwise_fn=slim.separable_conv2d,
+                  expansion_fn=split_conv,
+                  projection_fn=split_conv,
                   scope=None):
   """Depthwise Convolution Block with expansion.
 
@@ -197,7 +246,6 @@ def expanded_conv(input_tensor,
     residual: whether to include residual connection between input
       and output.
     normalizer_fn: batchnorm or otherwise
-    project_activation_fn: activation function for the project layer
     split_projection: how many ways to split projection operator
       (that is conv expansion->bottleneck)
     split_expansion: how many ways to split expansion op
@@ -220,6 +268,20 @@ def expanded_conv(input_tensor,
       inputs so that the output dimensions are the same as if 'SAME' padding
       were used.
     padding: Padding type to use if `use_explicit_padding` is not set.
+    inner_activation_fn: activation function to use in all inner convolutions.
+    If none, will rely on slim default scopes.
+    depthwise_activation_fn: activation function to use for deptwhise only.
+      If not provided will rely on slim default scopes. If both
+      inner_activation_fn and depthwise_activation_fn are provided,
+      depthwise_activation_fn takes precedence over inner_activation_fn.
+    project_activation_fn: activation function for the project layer.
+    (note this layer is not affected by inner_activation_fn)
+    depthwise_fn: Depthwise convolution function.
+    expansion_fn: Expansion convolution function. If use custom function then
+      "split_expansion" and "split_divisible_by" will be ignored.
+    projection_fn: Projection convolution function. If use custom function then
+      "split_projection" and "split_divisible_by" will be ignored.
+
     scope: optional scope.
 
   Returns:
@@ -228,8 +290,18 @@ def expanded_conv(input_tensor,
   Raises:
     TypeError: on inval
   """
+  conv_defaults = {}
+  dw_defaults = {}
+  if inner_activation_fn is not None:
+    conv_defaults['activation_fn'] = inner_activation_fn
+    dw_defaults['activation_fn'] = inner_activation_fn
+  if depthwise_activation_fn is not None:
+    dw_defaults['activation_fn'] = depthwise_activation_fn
+  # pylint: disable=g-backslash-continuation
   with tf.variable_scope(scope, default_name='expanded_conv') as s, \
-       tf.name_scope(s.original_name_scope):
+       tf.name_scope(s.original_name_scope), \
+      slim.arg_scope((slim.conv2d,), **conv_defaults), \
+       slim.arg_scope((slim.separable_conv2d,), **dw_defaults):
     prev_depth = input_tensor.get_shape().as_list()[3]
     if  depthwise_location not in [None, 'input', 'output', 'expansion']:
       raise TypeError('%r is unknown value for depthwise_location' %
@@ -240,7 +312,7 @@ def expanded_conv(input_tensor,
                         '"SAME" padding.')
       padding = 'VALID'
     depthwise_func = functools.partial(
-        slim.separable_conv2d,
+        depthwise_fn,
         num_outputs=None,
         kernel_size=kernel_size,
         depth_multiplier=depthwise_channel_multiplier,
@@ -258,6 +330,9 @@ def expanded_conv(input_tensor,
       if use_explicit_padding:
         net = _fixed_padding(net, kernel_size, rate)
       net = depthwise_func(net, activation_fn=None)
+      net = tf.identity(net, name='depthwise_output')
+      if endpoints is not None:
+        endpoints['depthwise_output'] = net
 
     if callable(expansion_size):
       inner_size = expansion_size(num_inputs=prev_depth)
@@ -265,37 +340,43 @@ def expanded_conv(input_tensor,
       inner_size = expansion_size
 
     if inner_size > net.shape[3]:
-      net = split_conv(
+      if expansion_fn == split_conv:
+        expansion_fn = functools.partial(
+            expansion_fn,
+            num_ways=split_expansion,
+            divisible_by=split_divisible_by,
+            stride=1)
+      net = expansion_fn(
           net,
           inner_size,
-          num_ways=split_expansion,
           scope='expand',
-          divisible_by=split_divisible_by,
-          stride=1,
           normalizer_fn=normalizer_fn)
       net = tf.identity(net, 'expansion_output')
-    if endpoints is not None:
-      endpoints['expansion_output'] = net
+      if endpoints is not None:
+        endpoints['expansion_output'] = net
 
     if depthwise_location == 'expansion':
       if use_explicit_padding:
         net = _fixed_padding(net, kernel_size, rate)
       net = depthwise_func(net)
+      net = tf.identity(net, name='depthwise_output')
+      if endpoints is not None:
+        endpoints['depthwise_output'] = net
 
-    net = tf.identity(net, name='depthwise_output')
-    if endpoints is not None:
-      endpoints['depthwise_output'] = net
     if expansion_transform:
       net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)
     # Note in contrast with expansion, we always have
     # projection to produce the desired output size.
-    net = split_conv(
+    if projection_fn == split_conv:
+      projection_fn = functools.partial(
+          projection_fn,
+          num_ways=split_projection,
+          divisible_by=split_divisible_by,
+          stride=1)
+    net = projection_fn(
         net,
         num_outputs,
-        num_ways=split_projection,
-        stride=1,
         scope='project',
-        divisible_by=split_divisible_by,
         normalizer_fn=normalizer_fn,
         activation_fn=project_activation_fn)
     if endpoints is not None:
@@ -304,6 +385,9 @@ def expanded_conv(input_tensor,
       if use_explicit_padding:
         net = _fixed_padding(net, kernel_size, rate)
       net = depthwise_func(net, activation_fn=None)
+      net = tf.identity(net, name='depthwise_output')
+      if endpoints is not None:
+        endpoints['depthwise_output'] = net
 
     if callable(residual):  # custom residual
       net = residual(input_tensor=input_tensor, output_tensor=net)
@@ -318,45 +402,65 @@ def expanded_conv(input_tensor,
     return tf.identity(net, name='output')
 
 
-def split_conv(input_tensor,
-               num_outputs,
-               num_ways,
-               scope,
-               divisible_by=8,
-               **kwargs):
-  """Creates a split convolution.
-
-  Split convolution splits the input and output into
-  'num_blocks' blocks of approximately the same size each,
-  and only connects $i$-th input to $i$ output.
+@slim.add_arg_scope
+def squeeze_excite(input_tensor,
+                   divisible_by=8,
+                   squeeze_factor=3,
+                   inner_activation_fn=tf.nn.relu,
+                   gating_fn=tf.sigmoid,
+                   squeeze_input_tensor=None,
+                   pool=None):
+  """Squeeze excite block for Mobilenet V3.
 
   Args:
-    input_tensor: input tensor
-    num_outputs: number of output filters
-    num_ways: num blocks to split by.
-    scope: scope for all the operators.
-    divisible_by: make sure that every part is divisiable by this.
-    **kwargs: will be passed directly into conv2d operator
+    input_tensor: input tensor to apply SE block to.
+    divisible_by: ensures all inner dimensions are divisible by this number.
+    squeeze_factor: the factor of squeezing in the inner fully connected layer
+    inner_activation_fn: non-linearity to be used in inner layer.
+    gating_fn: non-linearity to be used for final gating function
+    squeeze_input_tensor: custom tensor to use for computing gating activation.
+     If provided the result will be input_tensor * SE(squeeze_input_tensor)
+     instead of input_tensor * SE(input_tensor).
+    pool: if number is  provided will average pool with that kernel size
+      to compute inner tensor, followed by bilinear upsampling.
+
   Returns:
-    tensor
+    Gated input_tensor. (e.g. X * SE(X))
   """
-  b = input_tensor.get_shape().as_list()[3]
-
-  if num_ways == 1 or min(b // num_ways,
-                          num_outputs // num_ways) < divisible_by:
-    # Don't do any splitting if we end up with less than 8 filters
-    # on either side.
-    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)
+  with tf.variable_scope('squeeze_excite'):
+    if squeeze_input_tensor is None:
+      squeeze_input_tensor = input_tensor
+    input_size = input_tensor.shape.as_list()[1:3]
+    pool_height, pool_width = squeeze_input_tensor.shape.as_list()[1:3]
+    stride = 1
+    if pool is not None and pool_height >= pool:
+      pool_height, pool_width, stride = pool, pool, pool
+    input_channels = squeeze_input_tensor.shape.as_list()[3]
+    output_channels = input_tensor.shape.as_list()[3]
+    squeeze_channels = _make_divisible(
+        input_channels / squeeze_factor, divisor=divisible_by)
+
+    pooled = tf.nn.avg_pool(squeeze_input_tensor,
+                            (1, pool_height, pool_width, 1),
+                            strides=(1, stride, stride, 1),
+                            padding='VALID')
+    squeeze = slim.conv2d(
+        pooled,
+        kernel_size=(1, 1),
+        num_outputs=squeeze_channels,
+        normalizer_fn=None,
+        activation_fn=inner_activation_fn)
+    excite_outputs = output_channels
+    excite = slim.conv2d(squeeze, num_outputs=excite_outputs,
+                         kernel_size=[1, 1],
+                         normalizer_fn=None,
+                         activation_fn=gating_fn)
+    if pool is not None:
+      # Note: As of 03/20/2019 only BILINEAR (the default) with
+      # align_corners=True has gradients implemented in TPU.
+      excite = tf.image.resize_images(
+          excite, input_size,
+          align_corners=True)
+    result = input_tensor * excite
+  return result
 
-  outs = []
-  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)
-  output_splits = _split_divisible(
-      num_outputs, num_ways, divisible_by=divisible_by)
-  inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)
-  base = scope
-  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):
-    scope = base + '_part_%d' % (i,)
-    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)
-    n = tf.identity(n, scope + '_output')
-    outs.append(n)
-  return tf.concat(outs, 3, name=scope + '_concat')
diff --git a/research/slim/nets/mobilenet/g3doc/latency_pixel1.png b/research/slim/nets/mobilenet/g3doc/latency_pixel1.png
new file mode 100644
index 00000000..4fda9cf2
Binary files /dev/null and b/research/slim/nets/mobilenet/g3doc/latency_pixel1.png differ
diff --git a/research/slim/nets/mobilenet/g3doc/madds_top1_accuracy.png b/research/slim/nets/mobilenet/g3doc/madds_top1_accuracy.png
new file mode 100644
index 00000000..a2f75bdc
Binary files /dev/null and b/research/slim/nets/mobilenet/g3doc/madds_top1_accuracy.png differ
diff --git a/research/slim/nets/mobilenet/madds_top1_accuracy.png b/research/slim/nets/mobilenet/madds_top1_accuracy.png
deleted file mode 100644
index 6a2d8c07..00000000
Binary files a/research/slim/nets/mobilenet/madds_top1_accuracy.png and /dev/null differ
diff --git a/research/slim/nets/mobilenet/mobilenet.py b/research/slim/nets/mobilenet/mobilenet.py
index a08a55da..31e71846 100644
--- a/research/slim/nets/mobilenet/mobilenet.py
+++ b/research/slim/nets/mobilenet/mobilenet.py
@@ -66,7 +66,7 @@ def _make_divisible(v, divisor, min_value=None):
   # Make sure that round down does not go down by more than 10%.
   if new_v < 0.9 * v:
     new_v += divisor
-  return new_v
+  return int(new_v)
 
 
 @contextlib.contextmanager
diff --git a/research/slim/nets/mobilenet/mobilenet_v2.py b/research/slim/nets/mobilenet/mobilenet_v2.py
index 937eac9d..d2d736a3 100644
--- a/research/slim/nets/mobilenet/mobilenet_v2.py
+++ b/research/slim/nets/mobilenet/mobilenet_v2.py
@@ -81,6 +81,25 @@ V2_DEF = dict(
 )
 # pyformat: enable
 
+# Mobilenet v2 Definition with group normalization.
+V2_DEF_GROUP_NORM = copy.deepcopy(V2_DEF)
+V2_DEF_GROUP_NORM['defaults'] = {
+    (tf.contrib.slim.conv2d, tf.contrib.slim.fully_connected,
+     tf.contrib.slim.separable_conv2d): {
+        'normalizer_fn': tf.contrib.layers.group_norm,  # pylint: disable=C0330
+        'activation_fn': tf.nn.relu6,  # pylint: disable=C0330
+    },  # pylint: disable=C0330
+    (ops.expanded_conv,): {
+        'expansion_size': ops.expand_input_by_factor(6),
+        'split_expansion': 1,
+        'normalizer_fn': tf.contrib.layers.group_norm,
+        'residual': True
+    },
+    (tf.contrib.slim.conv2d, tf.contrib.slim.separable_conv2d): {
+        'padding': 'SAME'
+    }
+}
+
 
 @slim.add_arg_scope
 def mobilenet(input_tensor,
@@ -189,6 +208,19 @@ def mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):
                    base_only=True, **kwargs)
 
 
+@slim.add_arg_scope
+def mobilenet_base_group_norm(input_tensor, depth_multiplier=1.0, **kwargs):
+  """Creates base of the mobilenet (no pooling and no logits) ."""
+  kwargs['conv_defs'] = V2_DEF_GROUP_NORM
+  kwargs['conv_defs']['defaults'].update({
+      (tf.contrib.layers.group_norm,): {
+          'groups': kwargs.pop('groups', 8)
+      }
+  })
+  return mobilenet(
+      input_tensor, depth_multiplier=depth_multiplier, base_only=True, **kwargs)
+
+
 def training_scope(**kwargs):
   """Defines MobilenetV2 training scope.
 
diff --git a/research/slim/nets/mobilenet/mobilenet_v3.py b/research/slim/nets/mobilenet/mobilenet_v3.py
new file mode 100644
index 00000000..3ea393c5
--- /dev/null
+++ b/research/slim/nets/mobilenet/mobilenet_v3.py
@@ -0,0 +1,325 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Mobilenet V3 conv defs and helper functions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import copy
+import functools
+import numpy as np
+
+import tensorflow as tf
+
+from nets.mobilenet import conv_blocks as ops
+from nets.mobilenet import mobilenet as lib
+
+slim = tf.contrib.slim
+op = lib.op
+expand_input = ops.expand_input_by_factor
+
+# Squeeze Excite with all parameters filled-in, we use hard-sigmoid
+# for gating function and relu for inner activation function.
+squeeze_excite = functools.partial(
+    ops.squeeze_excite, squeeze_factor=4,
+    inner_activation_fn=tf.nn.relu,
+    gating_fn=lambda x: tf.nn.relu6(x+3)*0.16667)
+
+# Wrap squeeze excite op as expansion_transform that takes
+# both expansion and input tensor.
+_se4 = lambda expansion_tensor, input_tensor: squeeze_excite(expansion_tensor)
+
+
+def hard_swish(x):
+  with tf.name_scope('hard_swish'):
+    return x * tf.nn.relu6(x + np.float32(3)) * np.float32(1. / 6.)
+
+
+def reduce_to_1x1(input_tensor, default_size=7, **kwargs):
+  h, w = input_tensor.shape.as_list()[1:3]
+  if h is not None and w == h:
+    k = [h, h]
+  else:
+    k = [default_size, default_size]
+  return slim.avg_pool2d(input_tensor, kernel_size=k, **kwargs)
+
+
+def mbv3_op(ef, n, k, s=1, act=tf.nn.relu, se=None):
+  """Defines a single Mobilenet V3 convolution block.
+
+  Args:
+    ef: expansion factor
+    n: number of output channels
+    k: stride of depthwise
+    s: stride
+    act: activation function in inner layers
+    se: squeeze excite function.
+
+  Returns:
+    An object (lib._Op) for inserting in conv_def, representing this operation.
+  """
+  return op(ops.expanded_conv, expansion_size=expand_input(ef),
+            kernel_size=(k, k), stride=s, num_outputs=n,
+            inner_activation_fn=act,
+            expansion_transform=se)
+
+
+mbv3_op_se = functools.partial(mbv3_op, se=_se4)
+
+
+DEFAULTS = {
+    (ops.expanded_conv,):
+        dict(
+            normalizer_fn=slim.batch_norm,
+            residual=True),
+    (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
+        'normalizer_fn': slim.batch_norm,
+        'activation_fn': tf.nn.relu,
+    },
+    (slim.batch_norm,): {
+        'center': True,
+        'scale': True
+    },
+}
+
+# Compatible checkpoint: http://mldash/5511169891790690458#scalars
+V3_LARGE = dict(
+    defaults=dict(DEFAULTS),
+    spec=([
+        # stage 1
+        op(slim.conv2d, stride=2, num_outputs=16, kernel_size=(3, 3),
+           activation_fn=hard_swish),
+        mbv3_op(ef=1, n=16, k=3),
+        mbv3_op(ef=4, n=24, k=3, s=2),
+        mbv3_op(ef=3, n=24, k=3, s=1),
+        mbv3_op_se(ef=3, n=40, k=5, s=2),
+        mbv3_op_se(ef=3, n=40, k=5, s=1),
+        mbv3_op_se(ef=3, n=40, k=5, s=1),
+        mbv3_op(ef=6, n=80, k=3, s=2, act=hard_swish),
+        mbv3_op(ef=2.5, n=80, k=3, s=1, act=hard_swish),
+        mbv3_op(ef=184/80., n=80, k=3, s=1, act=hard_swish),
+        mbv3_op(ef=184/80., n=80, k=3, s=1, act=hard_swish),
+        mbv3_op_se(ef=6, n=112, k=3, s=1, act=hard_swish),
+        mbv3_op_se(ef=6, n=112, k=3, s=1, act=hard_swish),
+        mbv3_op_se(ef=6, n=160, k=5, s=2, act=hard_swish),
+        mbv3_op_se(ef=6, n=160, k=5, s=1, act=hard_swish),
+        mbv3_op_se(ef=6, n=160, k=5, s=1, act=hard_swish),
+        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=960,
+           activation_fn=hard_swish),
+        op(reduce_to_1x1, default_size=7, stride=1, padding='VALID'),
+        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280,
+           normalizer_fn=None, activation_fn=hard_swish)
+    ]))
+
+# 72.2% accuracy.
+V3_LARGE_MINIMALISTIC = dict(
+    defaults=dict(DEFAULTS),
+    spec=([
+        # stage 1
+        op(slim.conv2d, stride=2, num_outputs=16, kernel_size=(3, 3)),
+        mbv3_op(ef=1, n=16, k=3),
+        mbv3_op(ef=4, n=24, k=3, s=2),
+        mbv3_op(ef=3, n=24, k=3, s=1),
+        mbv3_op(ef=3, n=40, k=3, s=2),
+        mbv3_op(ef=3, n=40, k=3, s=1),
+        mbv3_op(ef=3, n=40, k=3, s=1),
+        mbv3_op(ef=6, n=80, k=3, s=2),
+        mbv3_op(ef=2.5, n=80, k=3, s=1),
+        mbv3_op(ef=184 / 80., n=80, k=3, s=1),
+        mbv3_op(ef=184 / 80., n=80, k=3, s=1),
+        mbv3_op(ef=6, n=112, k=3, s=1),
+        mbv3_op(ef=6, n=112, k=3, s=1),
+        mbv3_op(ef=6, n=160, k=3, s=2),
+        mbv3_op(ef=6, n=160, k=3, s=1),
+        mbv3_op(ef=6, n=160, k=3, s=1),
+        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=960),
+        op(reduce_to_1x1, default_size=7, stride=1, padding='VALID'),
+        op(slim.conv2d,
+           stride=1,
+           kernel_size=[1, 1],
+           num_outputs=1280,
+           normalizer_fn=None)
+    ]))
+
+# Compatible run: http://mldash/2023283040014348118#scalars
+V3_SMALL = dict(
+    defaults=dict(DEFAULTS),
+    spec=([
+        # stage 1
+        op(slim.conv2d, stride=2, num_outputs=16, kernel_size=(3, 3),
+           activation_fn=hard_swish),
+        mbv3_op_se(ef=1, n=16, k=3, s=2),
+        mbv3_op(ef=72./16, n=24, k=3, s=2),
+        mbv3_op(ef=(88./24), n=24, k=3, s=1),
+        mbv3_op_se(ef=4, n=40, k=5, s=2, act=hard_swish),
+        mbv3_op_se(ef=6, n=40, k=5, s=1, act=hard_swish),
+        mbv3_op_se(ef=6, n=40, k=5, s=1, act=hard_swish),
+        mbv3_op_se(ef=3, n=48, k=5, s=1, act=hard_swish),
+        mbv3_op_se(ef=3, n=48, k=5, s=1, act=hard_swish),
+        mbv3_op_se(ef=6, n=96, k=5, s=2, act=hard_swish),
+        mbv3_op_se(ef=6, n=96, k=5, s=1, act=hard_swish),
+        mbv3_op_se(ef=6, n=96, k=5, s=1, act=hard_swish),
+        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=576,
+           activation_fn=hard_swish),
+        op(reduce_to_1x1, default_size=7, stride=1, padding='VALID'),
+        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1024,
+           normalizer_fn=None, activation_fn=hard_swish)
+    ]))
+
+# 62% accuracy.
+V3_SMALL_MINIMALISTIC = dict(
+    defaults=dict(DEFAULTS),
+    spec=([
+        # stage 1
+        op(slim.conv2d, stride=2, num_outputs=16, kernel_size=(3, 3)),
+        mbv3_op(ef=1, n=16, k=3, s=2),
+        mbv3_op(ef=72. / 16, n=24, k=3, s=2),
+        mbv3_op(ef=(88. / 24), n=24, k=3, s=1),
+        mbv3_op(ef=4, n=40, k=3, s=2),
+        mbv3_op(ef=6, n=40, k=3, s=1),
+        mbv3_op(ef=6, n=40, k=3, s=1),
+        mbv3_op(ef=3, n=48, k=3, s=1),
+        mbv3_op(ef=3, n=48, k=3, s=1),
+        mbv3_op(ef=6, n=96, k=3, s=2),
+        mbv3_op(ef=6, n=96, k=3, s=1),
+        mbv3_op(ef=6, n=96, k=3, s=1),
+        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=576),
+        op(reduce_to_1x1, default_size=7, stride=1, padding='VALID'),
+        op(slim.conv2d,
+           stride=1,
+           kernel_size=[1, 1],
+           num_outputs=1024,
+           normalizer_fn=None)
+    ]))
+
+
+@slim.add_arg_scope
+def mobilenet(input_tensor,
+              num_classes=1001,
+              depth_multiplier=1.0,
+              scope='MobilenetV3',
+              conv_defs=None,
+              finegrain_classification_mode=False,
+              **kwargs):
+  """Creates mobilenet V3 network.
+
+  Inference mode is created by default. To create training use training_scope
+  below.
+
+  with tf.contrib.slim.arg_scope(mobilenet_v3.training_scope()):
+     logits, endpoints = mobilenet_v3.mobilenet(input_tensor)
+
+  Args:
+    input_tensor: The input tensor
+    num_classes: number of classes
+    depth_multiplier: The multiplier applied to scale number of
+    channels in each layer.
+    scope: Scope of the operator
+    conv_defs: Which version to create. Could be large/small or
+    any conv_def (see mobilenet_v3.py for examples).
+    finegrain_classification_mode: When set to True, the model
+    will keep the last layer large even for small multipliers. Following
+    https://arxiv.org/abs/1801.04381
+    it improves performance for ImageNet-type of problems.
+      *Note* ignored if final_endpoint makes the builder exit earlier.
+    **kwargs: passed directly to mobilenet.mobilenet:
+      prediction_fn- what prediction function to use.
+      reuse-: whether to reuse variables (if reuse set to true, scope
+      must be given).
+  Returns:
+    logits/endpoints pair
+
+  Raises:
+    ValueError: On invalid arguments
+  """
+  if conv_defs is None:
+    conv_defs = V3_LARGE
+  if 'multiplier' in kwargs:
+    raise ValueError('mobilenetv2 doesn\'t support generic '
+                     'multiplier parameter use "depth_multiplier" instead.')
+  if finegrain_classification_mode:
+    conv_defs = copy.deepcopy(conv_defs)
+    conv_defs['spec'][-1] = conv_defs['spec'][-1]._replace(
+        multiplier_func=lambda params, multiplier: params)
+  depth_args = {}
+  with slim.arg_scope((lib.depth_multiplier,), **depth_args):
+    return lib.mobilenet(
+        input_tensor,
+        num_classes=num_classes,
+        conv_defs=conv_defs,
+        scope=scope,
+        multiplier=depth_multiplier,
+        **kwargs)
+
+mobilenet.default_image_size = 224
+training_scope = lib.training_scope
+
+
+@slim.add_arg_scope
+def mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):
+  """Creates base of the mobilenet (no pooling and no logits) ."""
+  return mobilenet(
+      input_tensor, depth_multiplier=depth_multiplier, base_only=True, **kwargs)
+
+
+def wrapped_partial(func, *args, **kwargs):
+  partial_func = functools.partial(func, *args, **kwargs)
+  functools.update_wrapper(partial_func, func)
+  return partial_func
+
+
+large = wrapped_partial(mobilenet, conv_defs=V3_LARGE)
+small = wrapped_partial(mobilenet, conv_defs=V3_SMALL)
+
+
+# Minimalistic model that does not have Squeeze Excite blocks,
+# Hardswish, or 5x5 depthwise convolution.
+# This makes the model very friendly for a wide range of hardware
+large_minimalistic = wrapped_partial(mobilenet, conv_defs=V3_LARGE_MINIMALISTIC)
+small_minimalistic = wrapped_partial(mobilenet, conv_defs=V3_SMALL_MINIMALISTIC)
+
+
+def _reduce_consecutive_layers(conv_defs, start_id, end_id, multiplier=0.5):
+  """Reduce the outputs of consecutive layers with multiplier.
+
+  Args:
+    conv_defs: Mobilenet conv_defs.
+    start_id: 0-based index of the starting conv_def to be reduced.
+    end_id: 0-based index of the last conv_def to be reduced.
+    multiplier: The multiplier by which to reduce the conv_defs.
+
+  Returns:
+    Mobilenet conv_defs where the output sizes from layers [start_id, end_id],
+    inclusive, are reduced by multiplier.
+
+  Raises:
+    ValueError if any layer to be reduced does not have the 'num_outputs'
+    attribute.
+  """
+  defs = copy.deepcopy(conv_defs)
+  for d in defs['spec'][start_id:end_id+1]:
+    d.params.update({
+        'num_outputs': np.int(np.round(d.params['num_outputs'] * multiplier))
+    })
+  return defs
+
+
+V3_LARGE_DETECTION = _reduce_consecutive_layers(V3_LARGE, 13, 16)
+V3_SMALL_DETECTION = _reduce_consecutive_layers(V3_SMALL, 9, 12)
+
+
+__all__ = ['training_scope', 'mobilenet', 'V3_LARGE', 'V3_SMALL', 'large',
+           'small', 'V3_LARGE_DETECTION', 'V3_SMALL_DETECTION']
diff --git a/research/slim/nets/mobilenet/mobilenet_v3_test.py b/research/slim/nets/mobilenet/mobilenet_v3_test.py
new file mode 100644
index 00000000..86c3cf57
--- /dev/null
+++ b/research/slim/nets/mobilenet/mobilenet_v3_test.py
@@ -0,0 +1,57 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for google3.third_party.tensorflow_models.slim.nets.mobilenet.mobilenet_v3."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl.testing import absltest
+import tensorflow as tf
+
+from nets.mobilenet import mobilenet_v3
+
+
+class MobilenetV3Test(absltest.TestCase):
+
+  def setUp(self):
+    super(MobilenetV3Test, self).setUp()
+    tf.reset_default_graph()
+
+  def testMobilenetV3Large(self):
+    logits, endpoints = mobilenet_v3.mobilenet(
+        tf.placeholder(tf.float32, (1, 224, 224, 3)))
+    self.assertEqual(endpoints['layer_19'].shape, [1, 1, 1, 1280])
+    self.assertEqual(logits.shape, [1, 1001])
+
+  def testMobilenetV3Small(self):
+    _, endpoints = mobilenet_v3.mobilenet(
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        conv_defs=mobilenet_v3.V3_SMALL)
+    self.assertEqual(endpoints['layer_15'].shape, [1, 1, 1, 1024])
+
+  def testMobilenetV3BaseOnly(self):
+    result, endpoints = mobilenet_v3.mobilenet(
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        conv_defs=mobilenet_v3.V3_LARGE,
+        base_only=True,
+        final_endpoint='layer_17')
+    # Get the latest layer before average pool.
+    self.assertEqual(endpoints['layer_17'].shape, [1, 7, 7, 960])
+    self.assertEqual(result, endpoints['layer_17'])
+
+
+if __name__ == '__main__':
+  absltest.main()
diff --git a/research/slim/nets/mobilenet_v1.md b/research/slim/nets/mobilenet_v1.md
index c9be8877..badc1be1 100644
--- a/research/slim/nets/mobilenet_v1.md
+++ b/research/slim/nets/mobilenet_v1.md
@@ -1,7 +1,7 @@
-# Mobilenet_v2
-For Mobilenet V2 see this file [mobilenet/README.md](mobilenet/README.md).
+# MobilenetV2 and above
+For MobilenetV2+ see this file [mobilenet/README.md](mobilenet/README_md)
 
-# MobileNet_v1
+# MobileNetV1
 
 [MobileNets](https://arxiv.org/abs/1704.04861) are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices with [TensorFlow Mobile](https://www.tensorflow.org/mobile/).
 
diff --git a/research/slim/nets/vgg.py b/research/slim/nets/vgg.py
index 33a66305..f80a43bc 100644
--- a/research/slim/nets/vgg.py
+++ b/research/slim/nets/vgg.py
@@ -68,6 +68,7 @@ def vgg_a(inputs,
           is_training=True,
           dropout_keep_prob=0.5,
           spatial_squeeze=True,
+          reuse=None,
           scope='vgg_a',
           fc_conv_padding='VALID',
           global_pool=False):
@@ -85,6 +86,8 @@ def vgg_a(inputs,
       layers during training.
     spatial_squeeze: whether or not should squeeze the spatial dimensions of the
       outputs. Useful to remove unnecessary dimensions for classification.
+    reuse: whether or not the network and its variables should be reused. To be
+      able to reuse 'scope' must be given.
     scope: Optional scope for the variables.
     fc_conv_padding: the type of padding to use for the fully connected layer
       that is implemented as a convolutional layer. Use 'SAME' padding if you
@@ -101,7 +104,7 @@ def vgg_a(inputs,
       or the input to the logits layer (if num_classes is 0 or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'vgg_a', [inputs]) as sc:
+  with tf.variable_scope(scope, 'vgg_a', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.max_pool2d],
@@ -146,6 +149,7 @@ def vgg_16(inputs,
            is_training=True,
            dropout_keep_prob=0.5,
            spatial_squeeze=True,
+           reuse=None,
            scope='vgg_16',
            fc_conv_padding='VALID',
            global_pool=False):
@@ -163,6 +167,8 @@ def vgg_16(inputs,
       layers during training.
     spatial_squeeze: whether or not should squeeze the spatial dimensions of the
       outputs. Useful to remove unnecessary dimensions for classification.
+    reuse: whether or not the network and its variables should be reused. To be
+      able to reuse 'scope' must be given.
     scope: Optional scope for the variables.
     fc_conv_padding: the type of padding to use for the fully connected layer
       that is implemented as a convolutional layer. Use 'SAME' padding if you
@@ -179,7 +185,7 @@ def vgg_16(inputs,
       or the input to the logits layer (if num_classes is 0 or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:
+  with tf.variable_scope(scope, 'vgg_16', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
@@ -224,6 +230,7 @@ def vgg_19(inputs,
            is_training=True,
            dropout_keep_prob=0.5,
            spatial_squeeze=True,
+           reuse=None,
            scope='vgg_19',
            fc_conv_padding='VALID',
            global_pool=False):
@@ -241,6 +248,8 @@ def vgg_19(inputs,
       layers during training.
     spatial_squeeze: whether or not should squeeze the spatial dimensions of the
       outputs. Useful to remove unnecessary dimensions for classification.
+    reuse: whether or not the network and its variables should be reused. To be
+      able to reuse 'scope' must be given.
     scope: Optional scope for the variables.
     fc_conv_padding: the type of padding to use for the fully connected layer
       that is implemented as a convolutional layer. Use 'SAME' padding if you
@@ -258,7 +267,7 @@ def vgg_19(inputs,
       None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'vgg_19', [inputs]) as sc:
+  with tf.variable_scope(scope, 'vgg_19', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
