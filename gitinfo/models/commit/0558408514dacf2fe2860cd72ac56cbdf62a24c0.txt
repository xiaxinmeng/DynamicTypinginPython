commit 0558408514dacf2fe2860cd72ac56cbdf62a24c0
Author: pkulzc <lzc@google.com>
Date:   Thu Mar 7 12:19:47 2019 -0800

    Merged commit includes the following changes: (#6315)
    
    236813471  by lzc:
    
        Internal change.
    
    --
    236507310  by lzc:
    
        Fix preprocess.random_resize_method config type issue. The target height and width will be passed as "size" to tf.image.resize_images which only accepts integer.
    
    --
    236409989  by Zhichao Lu:
    
        Config export_to_tpu from function parameter instead of HParams for TPU inference.
    
    --
    236403186  by Zhichao Lu:
    
        Make graph file names optional arguments.
    
    --
    236237072  by Zhichao Lu:
    
        Minor bugfix for keyword args.
    
    --
    236209602  by Zhichao Lu:
    
        Add support for PartitionedVariable to get_variables_available_in_checkpoint.
    
    --
    235828658  by Zhichao Lu:
    
        Automatically stop evaluation jobs when training is finished.
    
    --
    235817964  by Zhichao Lu:
    
        Add an optional process_metrics_fn callback to eval_util, it gets called
        with evaluation results once each evaluation is complete.
    
    --
    235788721  by lzc:
    
        Fix yml file tf runtime version.
    
    --
    235262897  by Zhichao Lu:
    
        Add keypoint support to the random_pad_image preprocessor method.
    
    --
    235257380  by Zhichao Lu:
    
        Support InputDataFields.groundtruth_confidences in retain_groundtruth(), retain_groundtruth_with_positive_classes(), filter_groundtruth_with_crowd_boxes(), filter_groundtruth_with_nan_box_coordinates(), filter_unrecognized_classes().
    
    --
    235109188  by Zhichao Lu:
    
        Fix bug in pad_input_data_to_static_shapes for num_additional_channels > 0; make color-specific data augmentation only touch RGB channels.
    
    --
    235045010  by Zhichao Lu:
    
        Don't slice class_predictions_with_background when add_background_class is false.
    
    --
    235026189  by lzc:
    
        Fix import in g3doc.
    
    --
    234863426  by Zhichao Lu:
    
        Added fixes in exporter to allow writing a checkpoint to a specified temporary directory.
    
    --
    234671886  by lzc:
    
        Internal Change.
    
    --
    234630803  by rathodv:
    
        Internal Change.
    
    --
    233985896  by Zhichao Lu:
    
        Add Neumann optimizer to object detection.
    
    --
    233560911  by Zhichao Lu:
    
        Add NAS-FPN object detection with Resnet and Mobilenet v2.
    
    --
    233513536  by Zhichao Lu:
    
        Export TPU compatible object detection model
    
    --
    233495772  by lzc:
    
        Internal change.
    
    --
    233453557  by Zhichao Lu:
    
        Create Keras-based SSD+MobilenetV1 for object detection.
    
    --
    233220074  by lzc:
    
        Update release notes date.
    
    --
    233165761  by Zhichao Lu:
    
        Support depth_multiplier and min_depth in _SSDResnetV1FpnFeatureExtractor.
    
    --
    233160046  by lzc:
    
        Internal change.
    
    --
    232926599  by Zhichao Lu:
    
        [tf.data] Switching tf.data functions to use `defun`, providing an escape hatch to continue using the legacy `Defun`.
    
        There are subtle differences between the implementation of `defun` and `Defun` (such as resources handling or control flow) and it is possible that input pipelines that use control flow or resources in their functions might be affected by this change. To migrate majority of existing pipelines to the recommended way of creating functions in TF 2.0 world, while allowing (a small number of) existing pipelines to continue relying on the deprecated behavior, this CL provides an escape hatch.
    
        If your input pipeline is affected by this CL, it should apply the escape hatch by replacing `foo.map(...)` with `foo.map_with_legacy_function(...)`.
    
    --
    232891621  by Zhichao Lu:
    
        Modify faster_rcnn meta architecture to normalize raw detections.
    
    --
    232875817  by Zhichao Lu:
    
        Make calibration a post-processing step.
    
        Specifically:
        - Move the calibration config from pipeline.proto --> post_processing.proto
        - Edit post_processing_builder.py to return a calibration function. If no calibration config is provided, it None.
        - Edit SSD and FasterRCNN meta architectures to optionally call the calibration function on detection scores after score conversion and before NMS.
    
    --
    232704481  by Zhichao Lu:
    
        Edit calibration builder to build a function that will be used within a detection model's `postprocess` method, after score conversion and before non-maxima suppression.
    
        Specific Edits:
        - The returned function now accepts class_predictions_with_background as its argument instead of detection_scores and detection_classes.
        - Class-specific calibration was temporarily removed, as it requires more significant refactoring. Will be added later.
    
    --
    232615379  by Zhichao Lu:
    
        Internal change
    
    --
    232483345  by ronnyvotel:
    
        Making the use of bfloat16 restricted to TPUs.
    
    --
    232399572  by Zhichao Lu:
    
        Edit calibration builder and proto to support class-agnostic calibration.
    
        Specifically:
        - Edit calibration protos to include path to relevant label map if required for class-specific calibration. Previously, label maps were inferred from other parts of the pipeline proto; this allows all information required by the builder stay within the calibration proto and remove extraneous information from being passed with class-agnostic calibration.
        - Add class-agnostic protos to the calibration config.
    
        Note that the proto supports sigmoid and linear interpolation parameters, but the builder currently only supports linear interpolation.
    
    --
    231613048  by Zhichao Lu:
    
        Add calibration builder for applying calibration transformations from output of object detection models.
    
        Specifically:
        - Add calibration proto to support sigmoid and isotonic regression (stepwise function) calibration.
        - Add a builder to support calibration from isotonic regression outputs.
    
    --
    231519786  by lzc:
    
        model_builder test refactor.
        - removed proto text boilerplate in each test case and let them call a create_default_proto function instead.
        - consolidated all separate ssd model creation tests into one.
        - consolidated all separate faster rcnn model creation tests into one.
        - used parameterized test for testing mask rcnn models and use_matmul_crop_and_resize
        - added all failures test.
    
    --
    231448169  by Zhichao Lu:
    
        Return static shape as a constant tensor.
    
    --
    231423126  by lzc:
    
        Add a release note for OID v4 models.
    
    --
    231401941  by Zhichao Lu:
    
        Adding correct labelmap for the models trained on Open Images V4 (*oid_v4
        config suffix).
    
    --
    231320357  by Zhichao Lu:
    
        Add scope to Nearest Neighbor Resize op so that it stays in the same name scope as the original resize ops.
    
    --
    231257699  by Zhichao Lu:
    
        Switch to using preserve_aspect_ratio in tf.image.resize_images rather than using a custom implementation.
    
    --
    231247368  by rathodv:
    
        Internal change.
    
    --
    231004874  by lzc:
    
        Update documentations to use tf 1.12 for object detection API.
    
    --
    230999911  by rathodv:
    
        Use tf.batch_gather instead of ops.batch_gather
    
    --
    230999720  by huizhongc:
    
        Fix weight equalization test in ops_test.
    
    --
    230984728  by rathodv:
    
        Internal update.
    
    --
    230929019  by lzc:
    
        Add an option to replace preprocess operation with placeholder for ssd feature extractor.
    
    --
    230845266  by lzc:
    
        Require tensorflow version 1.12 for object detection API and rename keras_applications to keras_models
    
    --
    230392064  by lzc:
    
        Add RetinaNet 101 checkpoint trained on OID v4 to detection model zoo.
    
    --
    230014128  by derekjchow:
    
        This file was re-located below the tensorflow/lite/g3doc/convert
    
    --
    229941449  by lzc:
    
        Update SSD mobilenet v2 quantized model download path.
    
    --
    229843662  by lzc:
    
        Add an option to use native resize tf op in fpn top-down feature map generation.
    
    --
    229636034  by rathodv:
    
        Add deprecation notice to a few old parameters in train.proto
    
    --
    228959078  by derekjchow:
    
        Remove duplicate elif case in _check_and_convert_legacy_input_config_key
    
    --
    228749719  by rathodv:
    
        Minor refactoring to make exporter's `build_detection_graph` method public.
    
    --
    228573828  by rathodv:
    
        Mofity model.postprocess to return raw detections and raw scores.
    
        Modify, post-process methods in core/model.py and the meta architectures to export raw detection (without any non-max suppression) and raw multiclass score logits for those detections.
    
    --
    228420670  by Zhichao Lu:
    
        Add shims for custom architectures for object detection models.
    
    --
    228241692  by Zhichao Lu:
    
        Fix the comment on "losses_mask" in "Loss" class.
    
    --
    228223810  by Zhichao Lu:
    
        Support other_heads' predictions in WeightSharedConvolutionalBoxPredictor. Also remove a few unused parameters and fix a couple of comments in convolutional_box_predictor.py.
    
    --
    228200588  by Zhichao Lu:
    
        Add Expected Calibration Error and an evaluator that calculates the metric for object detections.
    
    --
    228167740  by lzc:
    
        Add option to use bounded activations in FPN top-down feature map generation.
    
    --
    227767700  by rathodv:
    
        Internal.
    
    --
    226295236  by Zhichao Lu:
    
        Add Open Image V4 Resnet101-FPN training config to third_party
    
    --
    226254842  by Zhichao Lu:
    
        Fix typo in documentation.
    
    --
    225833971  by Zhichao Lu:
    
        Option to have no resizer in object detection model.
    
    --
    225824890  by lzc:
    
        Fixes p3 compatibility for model_lib.py
    
    --
    225760897  by menglong:
    
        normalizer should be at least 1.
    
    --
    225559842  by menglong:
    
        Add extra logic filtering unrecognized classes.
    
    --
    225379421  by lzc:
    
        Add faster_rcnn_inception_resnet_v2_atrous_oid_v4 config to third_party
    
    --
    225368337  by Zhichao Lu:
    
        Add extra logic filtering unrecognized classes.
    
    --
    225341095  by Zhichao Lu:
    
        Adding Open Images V4 models to OD API model zoo and corresponding configs to the
        configs.
    
    --
    225218450  by menglong:
    
        Add extra logic filtering unrecognized classes.
    
    --
    225057591  by Zhichao Lu:
    
        Internal change.
    
    --
    224895417  by rathodv:
    
        Internal change.
    
    --
    224209282  by Zhichao Lu:
    
        Add two data augmentations to object detection: (1) Self-concat (2) Absolute pads.
    
    --
    224073762  by Zhichao Lu:
    
        Do not create tf.constant until _generate() is actually called in the object detector.
    
    --
    
    PiperOrigin-RevId: 236813471

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index 2fd23408..b2bcf5db 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -99,6 +99,17 @@ reporting an issue.
 
 ## Release information
 
+### Feb 11, 2019
+
+We have released detection models trained on the [Open Images Dataset V4](https://storage.googleapis.com/openimages/web/challenge.html)
+in our detection model zoo, including
+
+* Faster R-CNN detector with Inception Resnet V2 feature extractor
+* SSD detector with MobileNet V2 feature extractor
+* SSD detector with ResNet 101 FPN feature extractor (aka RetinaNet-101)
+
+<b>Thanks to contributors</b>: Alina Kuznetsova, Yinxiao Li
+
 ### Sep 17, 2018
 
 We have released Faster R-CNN detectors with ResNet-50 / ResNet-101 feature
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator.py b/research/object_detection/anchor_generators/grid_anchor_generator.py
index ba43f013..180b1534 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator.py
@@ -56,13 +56,10 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
     # Handle argument defaults
     if base_anchor_size is None:
       base_anchor_size = [256, 256]
-    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))
     if anchor_stride is None:
       anchor_stride = [16, 16]
-    anchor_stride = tf.to_float(tf.convert_to_tensor(anchor_stride))
     if anchor_offset is None:
       anchor_offset = [0, 0]
-    anchor_offset = tf.to_float(tf.convert_to_tensor(anchor_offset))
 
     self._scales = scales
     self._aspect_ratios = aspect_ratios
@@ -108,6 +105,13 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
     if not all([isinstance(list_item, tuple) and len(list_item) == 2
                 for list_item in feature_map_shape_list]):
       raise ValueError('feature_map_shape_list must be a list of pairs.')
+    self._base_anchor_size = tf.to_float(tf.convert_to_tensor(
+        self._base_anchor_size))
+    self._anchor_stride = tf.to_float(tf.convert_to_tensor(
+        self._anchor_stride))
+    self._anchor_offset = tf.to_float(tf.convert_to_tensor(
+        self._anchor_offset))
+
     grid_height, grid_width = feature_map_shape_list[0]
     scales_grid, aspect_ratios_grid = ops.meshgrid(self._scales,
                                                    self._aspect_ratios)
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index b870adce..015f6ca1 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -60,7 +60,7 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         outside list having the same number of entries as feature_map_shape_list
         (which is passed in at generation time).
       base_anchor_size: base anchor size as [height, width]
-                        (length-2 float tensor, default=[1.0, 1.0]).
+                        (length-2 float numpy or Tensor, default=[1.0, 1.0]).
                         The height and width values are normalized to the
                         minimum dimension of the input height and width, so that
                         when the base anchor height equals the base anchor
@@ -95,7 +95,7 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
       raise ValueError('box_specs_list is expected to be a '
                        'list of lists of pairs')
     if base_anchor_size is None:
-      base_anchor_size = tf.constant([256, 256], dtype=tf.float32)
+      base_anchor_size = [256, 256]
     self._base_anchor_size = base_anchor_size
     self._anchor_strides = anchor_strides
     self._anchor_offsets = anchor_offsets
@@ -211,10 +211,18 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
     min_im_shape = tf.minimum(im_height, im_width)
     scale_height = min_im_shape / im_height
     scale_width = min_im_shape / im_width
-    base_anchor_size = [
-        scale_height * self._base_anchor_size[0],
-        scale_width * self._base_anchor_size[1]
-    ]
+    if not tf.contrib.framework.is_tensor(self._base_anchor_size):
+      base_anchor_size = [
+          scale_height * tf.constant(self._base_anchor_size[0],
+                                     dtype=tf.float32),
+          scale_width * tf.constant(self._base_anchor_size[1],
+                                    dtype=tf.float32)
+      ]
+    else:
+      base_anchor_size = [
+          scale_height * self._base_anchor_size[0],
+          scale_width * self._base_anchor_size[1]
+      ]
     for feature_map_index, (grid_size, scales, aspect_ratios, stride,
                             offset) in enumerate(
                                 zip(feature_map_shape_list, self._scales,
@@ -304,7 +312,6 @@ def create_ssd_anchors(num_layers=6,
   """
   if base_anchor_size is None:
     base_anchor_size = [1.0, 1.0]
-  base_anchor_size = tf.constant(base_anchor_size, dtype=tf.float32)
   box_specs_list = []
   if scales is None or not scales:
     scales = [min_scale + (max_scale - min_scale) * i / (num_layers - 1)
diff --git a/research/object_detection/builders/anchor_generator_builder_test.py b/research/object_detection/builders/anchor_generator_builder_test.py
index 2a23c2d9..abecb0f5 100644
--- a/research/object_detection/builders/anchor_generator_builder_test.py
+++ b/research/object_detection/builders/anchor_generator_builder_test.py
@@ -47,14 +47,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
                                grid_anchor_generator.GridAnchorGenerator))
     self.assertListEqual(anchor_generator_object._scales, [])
     self.assertListEqual(anchor_generator_object._aspect_ratios, [])
-    with self.test_session() as sess:
-      base_anchor_size, anchor_offset, anchor_stride = sess.run(
-          [anchor_generator_object._base_anchor_size,
-           anchor_generator_object._anchor_offset,
-           anchor_generator_object._anchor_stride])
-    self.assertAllEqual(anchor_offset, [0, 0])
-    self.assertAllEqual(anchor_stride, [16, 16])
-    self.assertAllEqual(base_anchor_size, [256, 256])
+    self.assertAllEqual(anchor_generator_object._anchor_offset, [0, 0])
+    self.assertAllEqual(anchor_generator_object._anchor_stride, [16, 16])
+    self.assertAllEqual(anchor_generator_object._base_anchor_size, [256, 256])
 
   def test_build_grid_anchor_generator_with_non_default_parameters(self):
     anchor_generator_text_proto = """
@@ -79,14 +74,9 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
                                   [0.4, 2.2])
     self.assert_almost_list_equal(anchor_generator_object._aspect_ratios,
                                   [0.3, 4.5])
-    with self.test_session() as sess:
-      base_anchor_size, anchor_offset, anchor_stride = sess.run(
-          [anchor_generator_object._base_anchor_size,
-           anchor_generator_object._anchor_offset,
-           anchor_generator_object._anchor_stride])
-    self.assertAllEqual(anchor_offset, [30, 40])
-    self.assertAllEqual(anchor_stride, [10, 20])
-    self.assertAllEqual(base_anchor_size, [128, 512])
+    self.assertAllEqual(anchor_generator_object._anchor_offset, [30, 40])
+    self.assertAllEqual(anchor_generator_object._anchor_stride, [10, 20])
+    self.assertAllEqual(anchor_generator_object._base_anchor_size, [128, 512])
 
   def test_build_ssd_anchor_generator_with_defaults(self):
     anchor_generator_text_proto = """
@@ -114,10 +104,7 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
         list(anchor_generator_object._aspect_ratios),
         [(1.0, 2.0, 0.5)] + 5 * [(1.0, 1.0)]):
       self.assert_almost_list_equal(expected_aspect_ratio, actual_aspect_ratio)
-
-    with self.test_session() as sess:
-      base_anchor_size = sess.run(anchor_generator_object._base_anchor_size)
-    self.assertAllClose(base_anchor_size, [1.0, 1.0])
+    self.assertAllClose(anchor_generator_object._base_anchor_size, [1.0, 1.0])
 
   def test_build_ssd_anchor_generator_with_custom_scales(self):
     anchor_generator_text_proto = """
@@ -194,9 +181,7 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
         6 * [(1.0, 1.0)]):
       self.assert_almost_list_equal(expected_aspect_ratio, actual_aspect_ratio)
 
-    with self.test_session() as sess:
-      base_anchor_size = sess.run(anchor_generator_object._base_anchor_size)
-    self.assertAllClose(base_anchor_size, [1.0, 1.0])
+    self.assertAllClose(anchor_generator_object._base_anchor_size, [1.0, 1.0])
 
   def test_build_ssd_anchor_generator_with_non_default_parameters(self):
     anchor_generator_text_proto = """
@@ -241,9 +226,7 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
         list(anchor_generator_object._anchor_offsets), [(8, 0), (16, 10)]):
       self.assert_almost_list_equal(expected_offsets, actual_offsets)
 
-    with self.test_session() as sess:
-      base_anchor_size = sess.run(anchor_generator_object._base_anchor_size)
-    self.assertAllClose(base_anchor_size, [1.0, 1.0])
+    self.assertAllClose(anchor_generator_object._base_anchor_size, [1.0, 1.0])
 
   def test_raise_value_error_on_empty_anchor_genertor(self):
     anchor_generator_text_proto = """
diff --git a/research/object_detection/builders/calibration_builder.py b/research/object_detection/builders/calibration_builder.py
new file mode 100644
index 00000000..475f4fda
--- /dev/null
+++ b/research/object_detection/builders/calibration_builder.py
@@ -0,0 +1,147 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tensorflow ops to calibrate class predictions and background class."""
+
+import tensorflow as tf
+from object_detection.utils import shape_utils
+
+
+def _find_interval_containing_new_value(x, new_value):
+  """Find the index of x (ascending-ordered) after which new_value occurs."""
+  new_value_shape = shape_utils.combined_static_and_dynamic_shape(new_value)[0]
+  x_shape = shape_utils.combined_static_and_dynamic_shape(x)[0]
+  compare = tf.cast(tf.reshape(new_value, shape=(new_value_shape, 1)) >=
+                    tf.reshape(x, shape=(1, x_shape)),
+                    dtype=tf.int32)
+  diff = compare[:, 1:] - compare[:, :-1]
+  interval_idx = tf.argmin(diff, axis=1)
+  return interval_idx
+
+
+def _tf_linear_interp1d(x_to_interpolate, fn_x, fn_y):
+  """Tensorflow implementation of 1d linear interpolation.
+
+  Args:
+    x_to_interpolate: tf.float32 Tensor of shape (num_examples,) over which 1d
+      linear interpolation is performed.
+    fn_x: Monotonically-increasing, non-repeating tf.float32 Tensor of shape
+      (length,) used as the domain to approximate a function.
+    fn_y: tf.float32 Tensor of shape (length,) used as the range to approximate
+      a function.
+
+  Returns:
+    tf.float32 Tensor of shape (num_examples,)
+  """
+  x_pad = tf.concat([fn_x[:1] - 1, fn_x, fn_x[-1:] + 1], axis=0)
+  y_pad = tf.concat([fn_y[:1], fn_y, fn_y[-1:]], axis=0)
+  interval_idx = _find_interval_containing_new_value(x_pad, x_to_interpolate)
+
+  # Interpolate
+  alpha = (
+      (x_to_interpolate - tf.gather(x_pad, interval_idx)) /
+      (tf.gather(x_pad, interval_idx + 1) - tf.gather(x_pad, interval_idx)))
+  interpolation = ((1 - alpha) * tf.gather(y_pad, interval_idx) +
+                   alpha * tf.gather(y_pad, interval_idx + 1))
+
+  return interpolation
+
+
+def _function_approximation_proto_to_tf_tensors(x_y_pairs_message):
+  """Extracts (x,y) pairs from a XYPairs message.
+
+  Args:
+    x_y_pairs_message: calibration_pb2..XYPairs proto
+  Returns:
+    tf_x: tf.float32 tensor of shape (number_xy_pairs,) for function domain.
+    tf_y: tf.float32 tensor of shape (number_xy_pairs,) for function range.
+  """
+  tf_x = tf.convert_to_tensor([x_y_pair.x
+                               for x_y_pair
+                               in x_y_pairs_message.x_y_pair],
+                              dtype=tf.float32)
+  tf_y = tf.convert_to_tensor([x_y_pair.y
+                               for x_y_pair
+                               in x_y_pairs_message.x_y_pair],
+                              dtype=tf.float32)
+  return tf_x, tf_y
+
+
+def build(calibration_config):
+  """Returns a function that calibrates Tensorflow model scores.
+
+  All returned functions are expected to apply positive monotonic
+  transformations to inputs (i.e. score ordering is strictly preserved or
+  adjacent scores are mapped to the same score, but an input of lower value
+  should never be exceed an input of higher value after transformation).  For
+  class-agnostic calibration, positive monotonicity should hold across all
+  scores. In class-specific cases, positive monotonicity should hold within each
+  class.
+
+  Args:
+    calibration_config: calibration_pb2.CalibrationConfig proto.
+  Returns:
+    Function that that accepts class_predictions_with_background and calibrates
+    the output based on calibration_config's parameters.
+  Raises:
+    ValueError: No calibration builder defined for "Oneof" in
+      calibration_config.
+  """
+
+  # Linear Interpolation (usually used as a result of calibration via
+  # isotonic regression).
+  if calibration_config.WhichOneof('calibrator') == 'function_approximation':
+
+    def calibration_fn(class_predictions_with_background):
+      """Calibrate predictions via 1-d linear interpolation.
+
+      Predictions scores are linearly interpolated based on class-agnostic
+      function approximations. Note that the 0-indexed background class may
+      also transformed.
+
+      Args:
+        class_predictions_with_background: tf.float32 tensor of shape
+          [batch_size, num_anchors, num_classes + 1] containing scores on the
+          interval [0,1]. This is usually produced by a sigmoid or softmax layer
+          and the result of calling the `predict` method of a detection model.
+
+      Returns:
+        tf.float32 tensor of shape [batch_size, num_anchors, num_classes] if
+        background class is not present (else shape is
+        [batch_size, num_anchors, num_classes + 1]) on the interval [0, 1].
+      """
+      # Flattening Tensors and then reshaping at the end.
+      flat_class_predictions_with_background = tf.reshape(
+          class_predictions_with_background, shape=[-1])
+      fn_x, fn_y = _function_approximation_proto_to_tf_tensors(
+          calibration_config.function_approximation.x_y_pairs)
+      updated_scores = _tf_linear_interp1d(
+          flat_class_predictions_with_background, fn_x, fn_y)
+
+      # Un-flatten the scores
+      original_detections_shape = shape_utils.combined_static_and_dynamic_shape(
+          class_predictions_with_background)
+      calibrated_class_predictions_with_background = tf.reshape(
+          updated_scores,
+          shape=original_detections_shape,
+          name='calibrate_scores')
+      return calibrated_class_predictions_with_background
+
+  # TODO(zbeaver): Add sigmoid calibration and per-class isotonic regression.
+  else:
+    raise ValueError('No calibration builder defined for "Oneof" in '
+                     'calibration_config.')
+
+  return calibration_fn
diff --git a/research/object_detection/builders/calibration_builder_test.py b/research/object_detection/builders/calibration_builder_test.py
new file mode 100644
index 00000000..851c0545
--- /dev/null
+++ b/research/object_detection/builders/calibration_builder_test.py
@@ -0,0 +1,148 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for calibration_builder."""
+
+import numpy as np
+from scipy import interpolate
+import tensorflow as tf
+from object_detection.builders import calibration_builder
+from object_detection.protos import calibration_pb2
+
+
+class CalibrationBuilderTest(tf.test.TestCase):
+
+  def test_tf_linear_interp1d_map(self):
+    """Tests TF linear interpolation mapping to a single number."""
+    with self.test_session() as sess:
+      tf_x = tf.constant([0., 0.5, 1.])
+      tf_y = tf.constant([0.5, 0.5, 0.5])
+      new_x = tf.constant([0., 0.25, 0.5, 0.75, 1.])
+      tf_map_outputs = calibration_builder._tf_linear_interp1d(
+          new_x, tf_x, tf_y)
+      tf_map_outputs_np = sess.run([tf_map_outputs])
+    self.assertAllClose(tf_map_outputs_np, [[0.5, 0.5, 0.5, 0.5, 0.5]])
+
+  def test_tf_linear_interp1d_interpolate(self):
+    """Tests TF 1d linear interpolation not mapping to a single number."""
+    with self.test_session() as sess:
+      tf_x = tf.constant([0., 0.5, 1.])
+      tf_y = tf.constant([0.6, 0.7, 1.0])
+      new_x = tf.constant([0., 0.25, 0.5, 0.75, 1.])
+      tf_interpolate_outputs = calibration_builder._tf_linear_interp1d(
+          new_x, tf_x, tf_y)
+      tf_interpolate_outputs_np = sess.run([tf_interpolate_outputs])
+    self.assertAllClose(tf_interpolate_outputs_np, [[0.6, 0.65, 0.7, 0.85, 1.]])
+
+  @staticmethod
+  def _get_scipy_interp1d(new_x, x, y):
+    """Helper performing 1d linear interpolation using SciPy."""
+    interpolation1d_fn = interpolate.interp1d(x, y)
+    return interpolation1d_fn(new_x)
+
+  def _get_tf_interp1d(self, new_x, x, y):
+    """Helper performing 1d linear interpolation using Tensorflow."""
+    with self.test_session() as sess:
+      tf_interp_outputs = calibration_builder._tf_linear_interp1d(
+          tf.convert_to_tensor(new_x, dtype=tf.float32),
+          tf.convert_to_tensor(x, dtype=tf.float32),
+          tf.convert_to_tensor(y, dtype=tf.float32))
+      np_tf_interp_outputs = sess.run(tf_interp_outputs)
+    return np_tf_interp_outputs
+
+  def test_tf_linear_interp1d_against_scipy_map(self):
+    """Tests parity of TF linear interpolation with SciPy for simple mapping."""
+    length = 10
+    np_x = np.linspace(0, 1, length)
+
+    # Mapping all numbers to 0.5
+    np_y_map = np.repeat(0.5, length)
+
+    # Scipy and TF interpolations
+    test_data_np = np.linspace(0, 1, length * 10)
+    scipy_map_outputs = self._get_scipy_interp1d(test_data_np, np_x, np_y_map)
+    np_tf_map_outputs = self._get_tf_interp1d(test_data_np, np_x, np_y_map)
+    self.assertAllClose(scipy_map_outputs, np_tf_map_outputs)
+
+  def test_tf_linear_interp1d_against_scipy_interpolate(self):
+    """Tests parity of TF linear interpolation with SciPy."""
+    length = 10
+    np_x = np.linspace(0, 1, length)
+
+    # Requires interpolation over 0.5 to 1 domain
+    np_y_interp = np.linspace(0.5, 1, length)
+
+    # Scipy interpolation for comparison
+    test_data_np = np.linspace(0, 1, length * 10)
+    scipy_interp_outputs = self._get_scipy_interp1d(test_data_np, np_x,
+                                                    np_y_interp)
+    np_tf_interp_outputs = self._get_tf_interp1d(test_data_np, np_x,
+                                                 np_y_interp)
+    self.assertAllClose(scipy_interp_outputs, np_tf_interp_outputs)
+
+  @staticmethod
+  def _add_function_approximation_to_calibration_proto(calibration_proto,
+                                                       x_array,
+                                                       y_array,
+                                                       class_label):
+    """Adds a function approximation to calibration proto for a class label."""
+    # Per-class calibration.
+    if class_label:
+      label_function_approximation = (calibration_proto
+                                      .label_function_approximations
+                                      .label_xy_pairs_map[class_label])
+    # Class-agnostic calibration.
+    else:
+      label_function_approximation = (calibration_proto
+                                      .function_approximation
+                                      .x_y_pairs)
+    for x, y in zip(x_array, y_array):
+      x_y_pair_message = label_function_approximation.x_y_pair.add()
+      x_y_pair_message.x = x
+      x_y_pair_message.y = y
+
+  def test_class_agnostic_function_approximation(self):
+    """Ensures that calibration appropriate values, regardless of class."""
+    # Generate fake calibration proto. For this interpolation, any input on
+    # [0.0, 0.5] should be divided by 2 and any input on (0.5, 1.0] should have
+    # 0.25 subtracted from it.
+    class_agnostic_x = np.asarray([0.0, 0.5, 1.0])
+    class_agnostic_y = np.asarray([0.0, 0.25, 0.75])
+    calibration_config = calibration_pb2.CalibrationConfig()
+    self._add_function_approximation_to_calibration_proto(calibration_config,
+                                                          class_agnostic_x,
+                                                          class_agnostic_y,
+                                                          class_label=None)
+
+    od_graph = tf.Graph()
+    with self.test_session(graph=od_graph) as sess:
+      calibration_fn = calibration_builder.build(calibration_config)
+      # batch_size = 2, num_classes = 2, num_anchors = 2.
+      class_predictions_with_background = tf.constant(
+          [[[0.1, 0.2, 0.3],
+            [0.4, 0.5, 0.0]],
+           [[0.6, 0.7, 0.8],
+            [0.9, 1.0, 1.0]]], dtype=tf.float32)
+
+      # Everything should map to 0.5 if classes are ignored.
+      calibrated_scores = calibration_fn(class_predictions_with_background)
+      calibrated_scores_np = sess.run(calibrated_scores)
+    self.assertAllClose(calibrated_scores_np, [[[0.05, 0.1, 0.15],
+                                                [0.2, 0.25, 0.0]],
+                                               [[0.35, 0.45, 0.55],
+                                                [0.65, 0.75, 0.75]]])
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index b46d5f7f..74c811c4 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -117,6 +117,7 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
       label_map_proto_file = input_reader_config.label_map_path
     decoder = tf_example_decoder.TfExampleDecoder(
         load_instance_masks=input_reader_config.load_instance_masks,
+        load_multiclass_scores=input_reader_config.load_multiclass_scores,
         instance_mask_type=input_reader_config.mask_type,
         label_map_proto_file=label_map_proto_file,
         use_display_name=input_reader_config.use_display_name,
@@ -140,9 +141,12 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
       num_parallel_calls = batch_size * input_reader_config.num_parallel_batches
     else:
       num_parallel_calls = input_reader_config.num_parallel_map_calls
-    dataset = dataset.map(
-        process_fn,
-        num_parallel_calls=num_parallel_calls)
+    # TODO(b/123952794): Migrate to V2 function.
+    if hasattr(dataset, 'map_with_legacy_function'):
+      data_map_fn = dataset.map_with_legacy_function
+    else:
+      data_map_fn = dataset.map
+    dataset = data_map_fn(process_fn, num_parallel_calls=num_parallel_calls)
     if batch_size:
       dataset = dataset.apply(
           tf.contrib.data.batch_and_drop_remainder(batch_size))
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index 243c84dd..529065ce 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -102,6 +102,14 @@ def build(image_resizer_config):
         method=method)
     if not fixed_shape_resizer_config.convert_to_grayscale:
       return image_resizer_fn
+  elif image_resizer_oneof == 'identity_resizer':
+    def image_resizer_fn(image, masks=None, **kwargs):
+      del kwargs
+      if masks is None:
+        return [image, tf.shape(image)]
+      else:
+        return [image, masks, tf.shape(image)]
+    return image_resizer_fn
   else:
     raise ValueError(
         'Invalid image resizer option: \'%s\'.' % image_resizer_oneof)
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index f7da1912..54a73c8d 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -104,6 +104,17 @@ class ImageResizerBuilderTest(tf.test.TestCase):
         input_shape, image_resizer_text_proto)
     self.assertEqual(output_shape, expected_output_shape)
 
+  def test_identity_resizer_returns_expected_shape(self):
+    image_resizer_text_proto = """
+      identity_resizer {
+      }
+    """
+    input_shape = (10, 20, 3)
+    expected_output_shape = (10, 20, 3)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
   def test_raises_error_on_invalid_input(self):
     invalid_input = 'invalid_input'
     with self.assertRaises(ValueError):
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 83b709ae..fcb7f215 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -44,6 +44,7 @@ from object_detection.models.ssd_inception_v2_feature_extractor import SSDIncept
 from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
+from object_detection.models.ssd_mobilenet_v1_keras_feature_extractor import SSDMobileNetV1KerasFeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_fpn_feature_extractor import SSDMobileNetV2FpnFeatureExtractor
@@ -76,6 +77,7 @@ SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
 }
 
 SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
+    'ssd_mobilenet_v1_keras': SSDMobileNetV1KerasFeatureExtractor,
     'ssd_mobilenet_v2_keras': SSDMobileNetV2KerasFeatureExtractor
 }
 
@@ -187,6 +189,12 @@ def _build_ssd_feature_extractor(feature_extractor_config,
           override_base_feature_extractor_hyperparams
   }
 
+  if feature_extractor_config.HasField('replace_preprocessor_with_placeholder'):
+    kwargs.update({
+        'replace_preprocessor_with_placeholder':
+            feature_extractor_config.replace_preprocessor_with_placeholder
+    })
+
   if is_keras_extractor:
     kwargs.update({
         'conv_hyperparams': conv_hyperparams,
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
index 87ac0a8b..1cf5d8d6 100644
--- a/research/object_detection/builders/model_builder_test.py
+++ b/research/object_detection/builders/model_builder_test.py
@@ -24,69 +24,29 @@ from object_detection.builders import model_builder
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.meta_architectures import rfcn_meta_arch
 from object_detection.meta_architectures import ssd_meta_arch
-from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
-from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
-from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
-from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
-from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
 from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
-from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn
-from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
-from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_fpn_feature_extractor import SSDMobileNetV2FpnFeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_keras_feature_extractor import SSDMobileNetV2KerasFeatureExtractor
-from object_detection.predictors import convolutional_box_predictor
-from object_detection.predictors import convolutional_keras_box_predictor
+from object_detection.protos import hyperparams_pb2
+from object_detection.protos import losses_pb2
 from object_detection.protos import model_pb2
 
-FRCNN_RESNET_FEAT_MAPS = {
-    'faster_rcnn_resnet50':
-    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,
-    'faster_rcnn_resnet101':
-    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,
-    'faster_rcnn_resnet152':
-    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor
-}
-
-SSD_RESNET_V1_FPN_FEAT_MAPS = {
-    'ssd_resnet50_v1_fpn':
-    ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
-    'ssd_resnet101_v1_fpn':
-    ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
-    'ssd_resnet152_v1_fpn':
-    ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
-}
-
-SSD_RESNET_V1_PPN_FEAT_MAPS = {
-    'ssd_resnet50_v1_ppn':
-    ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,
-    'ssd_resnet101_v1_ppn':
-    ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,
-    'ssd_resnet152_v1_ppn':
-    ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor
-}
-
 
 class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
 
-  def create_model(self, model_config):
+  def create_model(self, model_config, is_training=True):
     """Builds a DetectionModel based on the model config.
 
     Args:
       model_config: A model.proto object containing the config for the desired
         DetectionModel.
+      is_training: True if this model is being built for training purposes.
 
     Returns:
       DetectionModel based on the config.
     """
-    return model_builder.build(model_config, is_training=True)
+    return model_builder.build(model_config, is_training=is_training)
 
-  def test_create_ssd_inception_v2_model_from_config(self):
+  def create_default_ssd_model_proto(self):
+    """Creates a DetectionModel proto with ssd model fields populated."""
     model_text_proto = """
       ssd {
         feature_extractor {
@@ -153,56 +113,46 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
       }"""
     model_proto = model_pb2.DetectionModel()
     text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDInceptionV2FeatureExtractor)
-    self.assertIsNone(model._expected_loss_weights_fn)
+    return model_proto
 
-
-
-  def test_create_ssd_inception_v3_model_from_config(self):
+  def create_default_faster_rcnn_model_proto(self):
+    """Creates a DetectionModel proto with FasterRCNN model fields populated."""
     model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_inception_v3'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-          override_base_feature_extractor_hyperparams: true
-        }
-        box_coder {
-          faster_rcnn_box_coder {
+      faster_rcnn {
+        inplace_batchnorm_update: false
+        num_classes: 3
+        image_resizer {
+          keep_aspect_ratio_resizer {
+            min_dimension: 600
+            max_dimension: 1024
           }
         }
-        matcher {
-          argmax_matcher {
-          }
+        feature_extractor {
+          type: 'faster_rcnn_resnet101'
         }
-        similarity_calculator {
-          iou_similarity {
+        first_stage_anchor_generator {
+          grid_anchor_generator {
+            scales: [0.25, 0.5, 1.0, 2.0]
+            aspect_ratios: [0.5, 1.0, 2.0]
+            height_stride: 16
+            width_stride: 16
           }
         }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
+        first_stage_box_predictor_conv_hyperparams {
+          regularizer {
+            l2_regularizer {
+            }
           }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
+          initializer {
+            truncated_normal_initializer {
+            }
           }
         }
-        box_predictor {
-          convolutional_box_predictor {
+        initial_crop_size: 14
+        maxpool_kernel_size: 2
+        maxpool_stride: 2
+        second_stage_box_predictor {
+          mask_rcnn_box_predictor {
             conv_hyperparams {
               regularizer {
                 l2_regularizer {
@@ -213,1357 +163,169 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
                 }
               }
             }
-          }
-        }
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDInceptionV3FeatureExtractor)
-
-  def test_create_ssd_resnet_v1_fpn_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_resnet50_v1_fpn'
-          fpn {
-            min_level: 3
-            max_level: 7
-          }
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        encode_background_as_zeros: true
-        anchor_generator {
-          multiscale_anchor_generator {
-            aspect_ratios: [1.0, 2.0, 0.5]
-            scales_per_octave: 2
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          weight_shared_convolutional_box_predictor {
-            depth: 32
-            conv_hyperparams {
+            fc_hyperparams {
+              op: FC
               regularizer {
                 l2_regularizer {
                 }
               }
               initializer {
-                random_normal_initializer {
+                truncated_normal_initializer {
                 }
               }
             }
-            num_layers_before_predictor: 1
           }
         }
-        normalize_loss_by_num_matches: true
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_sigmoid_focal {
-              alpha: 0.25
-              gamma: 2.0
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-              delta: 0.1
-            }
+        second_stage_post_processing {
+          batch_non_max_suppression {
+            score_threshold: 0.01
+            iou_threshold: 0.6
+            max_detections_per_class: 100
+            max_total_detections: 300
           }
-          classification_weight: 1.0
-          localization_weight: 1.0
+          score_converter: SOFTMAX
         }
       }"""
     model_proto = model_pb2.DetectionModel()
     text_format.Merge(model_text_proto, model_proto)
+    return model_proto
 
-    for extractor_type, extractor_class in SSD_RESNET_V1_FPN_FEAT_MAPS.items():
+  def test_create_ssd_models_from_config(self):
+    model_proto = self.create_default_ssd_model_proto()
+    ssd_feature_extractor_map = {}
+    ssd_feature_extractor_map.update(
+        model_builder.SSD_FEATURE_EXTRACTOR_CLASS_MAP)
+    ssd_feature_extractor_map.update(
+        model_builder.SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP)
+
+    for extractor_type, extractor_class in ssd_feature_extractor_map.items():
       model_proto.ssd.feature_extractor.type = extractor_type
       model = model_builder.build(model_proto, is_training=True)
       self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
       self.assertIsInstance(model._feature_extractor, extractor_class)
 
-  def test_create_ssd_resnet_v1_ppn_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_resnet_v1_50_ppn'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          mean_stddev_box_coder {
-          }
-        }
-        matcher {
-          bipartite_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          weight_shared_convolutional_box_predictor {
-            depth: 1024
-            class_prediction_bias_init: -4.6
-            conv_hyperparams {
-              activation: RELU_6,
-              regularizer {
-                l2_regularizer {
-                  weight: 0.0004
-                }
-              }
-              initializer {
-                variance_scaling_initializer {
-                }
-              }
-            }
-            num_layers_before_predictor: 2
-            kernel_size: 1
-          }
-        }
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_l2 {
-            }
-          }
-          classification_weight: 1.0
-          localization_weight: 1.0
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
+  def test_create_ssd_fpn_model_from_config(self):
+    model_proto = self.create_default_ssd_model_proto()
+    model_proto.ssd.feature_extractor.type = 'ssd_resnet101_v1_fpn'
+    model_proto.ssd.feature_extractor.fpn.min_level = 3
+    model_proto.ssd.feature_extractor.fpn.max_level = 7
+    model = model_builder.build(model_proto, is_training=True)
+    self.assertIsInstance(model._feature_extractor,
+                          ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor)
+    self.assertEqual(model._feature_extractor._fpn_min_level, 3)
+    self.assertEqual(model._feature_extractor._fpn_max_level, 7)
+
+
+  @parameterized.named_parameters(
+      {
+          'testcase_name': 'mask_rcnn_with_matmul',
+          'use_matmul_crop_and_resize': False,
+          'enable_mask_prediction': True
+      },
+      {
+          'testcase_name': 'mask_rcnn_without_matmul',
+          'use_matmul_crop_and_resize': True,
+          'enable_mask_prediction': True
+      },
+      {
+          'testcase_name': 'faster_rcnn_with_matmul',
+          'use_matmul_crop_and_resize': False,
+          'enable_mask_prediction': False
+      },
+      {
+          'testcase_name': 'faster_rcnn_without_matmul',
+          'use_matmul_crop_and_resize': True,
+          'enable_mask_prediction': False
+      },
+  )
+  def test_create_faster_rcnn_models_from_config(
+      self, use_matmul_crop_and_resize, enable_mask_prediction):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    faster_rcnn_config = model_proto.faster_rcnn
+    faster_rcnn_config.use_matmul_crop_and_resize = use_matmul_crop_and_resize
+    if enable_mask_prediction:
+      faster_rcnn_config.second_stage_mask_prediction_loss_weight = 3.0
+      mask_predictor_config = (
+          faster_rcnn_config.second_stage_box_predictor.mask_rcnn_box_predictor)
+      mask_predictor_config.predict_instance_masks = True
+
+    for extractor_type, extractor_class in (
+        model_builder.FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP.items()):
+      faster_rcnn_config.feature_extractor.type = extractor_type
+      model = model_builder.build(model_proto, is_training=True)
+      self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
+      self.assertIsInstance(model._feature_extractor, extractor_class)
+      if enable_mask_prediction:
+        self.assertAlmostEqual(model._second_stage_mask_loss_weight, 3.0)
 
-    for extractor_type, extractor_class in SSD_RESNET_V1_PPN_FEAT_MAPS.items():
-      model_proto.ssd.feature_extractor.type = extractor_type
+  def test_create_faster_rcnn_model_from_config_with_example_miner(self):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    model_proto.faster_rcnn.hard_example_miner.num_hard_examples = 64
+    model = model_builder.build(model_proto, is_training=True)
+    self.assertIsNotNone(model._hard_example_miner)
+
+  def test_create_rfcn_model_from_config(self):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    rfcn_predictor_config = (
+        model_proto.faster_rcnn.second_stage_box_predictor.rfcn_box_predictor)
+    rfcn_predictor_config.conv_hyperparams.op = hyperparams_pb2.Hyperparams.CONV
+    for extractor_type, extractor_class in (
+        model_builder.FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP.items()):
+      model_proto.faster_rcnn.feature_extractor.type = extractor_type
       model = model_builder.build(model_proto, is_training=True)
-      self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
+      self.assertIsInstance(model, rfcn_meta_arch.RFCNMetaArch)
       self.assertIsInstance(model._feature_extractor, extractor_class)
 
-  def test_create_ssd_mobilenet_v1_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        freeze_batchnorm: true
-        inplace_batchnorm_update: true
-        feature_extractor {
-          type: 'ssd_mobilenet_v1'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV1FeatureExtractor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-    self.assertTrue(model._freeze_batchnorm)
-    self.assertTrue(model._inplace_batchnorm_update)
-
-  def test_create_ssd_mobilenet_v1_fpn_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        freeze_batchnorm: true
-        inplace_batchnorm_update: true
-        feature_extractor {
-          type: 'ssd_mobilenet_v1_fpn'
-          fpn {
-            min_level: 3
-            max_level: 7
-          }
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV1FpnFeatureExtractor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-    self.assertTrue(model._freeze_batchnorm)
-    self.assertTrue(model._inplace_batchnorm_update)
-
-  def test_create_ssd_mobilenet_v1_ppn_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        freeze_batchnorm: true
-        inplace_batchnorm_update: true
-        feature_extractor {
-          type: 'ssd_mobilenet_v1_ppn'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV1PpnFeatureExtractor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-    self.assertTrue(model._freeze_batchnorm)
-    self.assertTrue(model._inplace_batchnorm_update)
-
-  def test_create_ssd_mobilenet_v2_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_mobilenet_v2'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV2FeatureExtractor)
-    self.assertIsInstance(model._box_predictor,
-                          convolutional_box_predictor.ConvolutionalBoxPredictor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-
-  def test_create_ssd_mobilenet_v2_keras_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_mobilenet_v2_keras'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV2KerasFeatureExtractor)
-    self.assertIsInstance(
-        model._box_predictor,
-        convolutional_keras_box_predictor.ConvolutionalBoxPredictor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-
-  def test_create_ssd_mobilenet_v2_fpn_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        freeze_batchnorm: true
-        inplace_batchnorm_update: true
-        feature_extractor {
-          type: 'ssd_mobilenet_v2_fpn'
-          fpn {
-            min_level: 3
-            max_level: 7
-          }
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV2FpnFeatureExtractor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-    self.assertTrue(model._freeze_batchnorm)
-    self.assertTrue(model._inplace_batchnorm_update)
-
-  def test_create_ssd_mobilenet_v2_fpnlite_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        freeze_batchnorm: true
-        inplace_batchnorm_update: true
-        feature_extractor {
-          type: 'ssd_mobilenet_v2_fpn'
-          use_depthwise: true
-          fpn {
-            min_level: 3
-            max_level: 7
-            additional_layer_depth: 128
-          }
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV2FpnFeatureExtractor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-    self.assertTrue(model._freeze_batchnorm)
-    self.assertTrue(model._inplace_batchnorm_update)
-
-  def test_create_embedded_ssd_mobilenet_v1_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'embedded_ssd_mobilenet_v1'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 256
-            width: 256
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          EmbeddedSSDMobileNetV1FeatureExtractor)
-
-  def test_create_faster_rcnn_resnet_v1_models_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        inplace_batchnorm_update: false
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_resnet101'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-
-    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
-      model_proto.faster_rcnn.feature_extractor.type = extractor_type
-      model = model_builder.build(model_proto, is_training=True)
-      self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-      self.assertIsInstance(model._feature_extractor, extractor_class)
-
-  @parameterized.parameters(
-      {'use_matmul_crop_and_resize': False},
-      {'use_matmul_crop_and_resize': True},
-  )
-  def test_create_faster_rcnn_resnet101_with_mask_prediction_enabled(
-      self, use_matmul_crop_and_resize):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_resnet101'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-            predict_instance_masks: true
-          }
-        }
-        second_stage_mask_prediction_loss_weight: 3.0
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model_proto.faster_rcnn.use_matmul_crop_and_resize = (
-        use_matmul_crop_and_resize)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertAlmostEqual(model._second_stage_mask_loss_weight, 3.0)
-
-  def test_create_faster_rcnn_nas_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_nas'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 17
-        maxpool_kernel_size: 1
-        maxpool_stride: 1
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(
-        model._feature_extractor,
-        frcnn_nas.FasterRCNNNASFeatureExtractor)
-
-  def test_create_faster_rcnn_pnas_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_pnas'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 17
-        maxpool_kernel_size: 1
-        maxpool_stride: 1
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(
-        model._feature_extractor,
-        frcnn_pnas.FasterRCNNPNASFeatureExtractor)
-
-  def test_create_faster_rcnn_inception_resnet_v2_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_inception_resnet_v2'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 17
-        maxpool_kernel_size: 1
-        maxpool_stride: 1
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(
-        model._feature_extractor,
-        frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor)
-
-  def test_create_faster_rcnn_inception_v2_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_inception_v2'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor)
-
-  def test_create_faster_rcnn_model_from_config_with_example_miner(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        feature_extractor {
-          type: 'faster_rcnn_inception_resnet_v2'
-        }
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        hard_example_miner {
-          num_hard_examples: 10
-          iou_threshold: 0.99
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsNotNone(model._hard_example_miner)
+  def test_invalid_model_config_proto(self):
+    model_proto = ''
+    with self.assertRaisesRegexp(
+        ValueError, 'model_config not of type model_pb2.DetectionModel.'):
+      model_builder.build(model_proto, is_training=True)
 
-  def test_create_rfcn_resnet_v1_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_resnet101'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          rfcn_box_predictor {
-            conv_hyperparams {
-              op: CONV
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
+  def test_unknown_meta_architecture(self):
     model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
-      model_proto.faster_rcnn.feature_extractor.type = extractor_type
-      model = model_builder.build(model_proto, is_training=True)
-      self.assertIsInstance(model, rfcn_meta_arch.RFCNMetaArch)
-      self.assertIsInstance(model._feature_extractor, extractor_class)
+    with self.assertRaisesRegexp(ValueError, 'Unknown meta architecture'):
+      model_builder.build(model_proto, is_training=True)
+
+  def test_unknown_ssd_feature_extractor(self):
+    model_proto = self.create_default_ssd_model_proto()
+    model_proto.ssd.feature_extractor.type = 'unknown_feature_extractor'
+    with self.assertRaisesRegexp(ValueError, 'Unknown ssd feature_extractor'):
+      model_builder.build(model_proto, is_training=True)
+
+  def test_unknown_faster_rcnn_feature_extractor(self):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    model_proto.faster_rcnn.feature_extractor.type = 'unknown_feature_extractor'
+    with self.assertRaisesRegexp(ValueError,
+                                 'Unknown Faster R-CNN feature_extractor'):
+      model_builder.build(model_proto, is_training=True)
+
+  def test_invalid_first_stage_nms_iou_threshold(self):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    model_proto.faster_rcnn.first_stage_nms_iou_threshold = 1.1
+    with self.assertRaisesRegexp(ValueError,
+                                 r'iou_threshold not in \[0, 1\.0\]'):
+      model_builder.build(model_proto, is_training=True)
+    model_proto.faster_rcnn.first_stage_nms_iou_threshold = -0.1
+    with self.assertRaisesRegexp(ValueError,
+                                 r'iou_threshold not in \[0, 1\.0\]'):
+      model_builder.build(model_proto, is_training=True)
+
+  def test_invalid_second_stage_batch_size(self):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    model_proto.faster_rcnn.first_stage_max_proposals = 1
+    model_proto.faster_rcnn.second_stage_batch_size = 2
+    with self.assertRaisesRegexp(
+        ValueError, 'second_stage_batch_size should be no greater '
+        'than first_stage_max_proposals.'):
+      model_builder.build(model_proto, is_training=True)
+
+  def test_invalid_faster_rcnn_batchnorm_update(self):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    model_proto.faster_rcnn.inplace_batchnorm_update = True
+    with self.assertRaisesRegexp(ValueError,
+                                 'inplace batchnorm updates not supported'):
+      model_builder.build(model_proto, is_training=True)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/builders/optimizer_builder.py b/research/object_detection/builders/optimizer_builder.py
index ce64bfe6..8049001f 100644
--- a/research/object_detection/builders/optimizer_builder.py
+++ b/research/object_detection/builders/optimizer_builder.py
@@ -16,6 +16,8 @@
 """Functions to build DetectionModel training optimizers."""
 
 import tensorflow as tf
+
+
 from object_detection.utils import learning_schedules
 
 
@@ -59,6 +61,7 @@ def build(optimizer_config):
     summary_vars.append(learning_rate)
     optimizer = tf.train.AdamOptimizer(learning_rate)
 
+
   if optimizer is None:
     raise ValueError('Optimizer %s not supported.' % optimizer_type)
 
diff --git a/research/object_detection/builders/post_processing_builder.py b/research/object_detection/builders/post_processing_builder.py
index b48ed7ef..a77165b4 100644
--- a/research/object_detection/builders/post_processing_builder.py
+++ b/research/object_detection/builders/post_processing_builder.py
@@ -1,4 +1,4 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -17,6 +17,7 @@
 import functools
 
 import tensorflow as tf
+from object_detection.builders import calibration_builder
 from object_detection.core import post_processing
 from object_detection.protos import post_processing_pb2
 
@@ -24,8 +25,8 @@ from object_detection.protos import post_processing_pb2
 def build(post_processing_config):
   """Builds callables for post-processing operations.
 
-  Builds callables for non-max suppression and score conversion based on the
-  configuration.
+  Builds callables for non-max suppression, score conversion, and (optionally)
+  calibration based on the configuration.
 
   Non-max suppression callable takes `boxes`, `scores`, and optionally
   `clip_window`, `parallel_iterations` `masks, and `scope` as inputs. It returns
@@ -35,8 +36,10 @@ def build(post_processing_config):
 
   Score converter callable should be called with `input` tensor. The callable
   returns the output from one of 3 tf operations based on the configuration -
-  tf.identity, tf.sigmoid or tf.nn.softmax. See tensorflow documentation for
-  argument and return value descriptions.
+  tf.identity, tf.sigmoid or tf.nn.softmax. If a calibration config is provided,
+  score_converter also applies calibration transformations, as defined in
+  calibration_builder.py. See tensorflow documentation for argument and return
+  value descriptions.
 
   Args:
     post_processing_config: post_processing.proto object containing the
@@ -57,6 +60,10 @@ def build(post_processing_config):
   score_converter_fn = _build_score_converter(
       post_processing_config.score_converter,
       post_processing_config.logit_scale)
+  if post_processing_config.HasField('calibration_config'):
+    score_converter_fn = _build_calibrated_score_converter(
+        score_converter_fn,
+        post_processing_config.calibration_config)
   return non_max_suppressor_fn, score_converter_fn
 
 
@@ -122,3 +129,32 @@ def _build_score_converter(score_converter_config, logit_scale):
   if score_converter_config == post_processing_pb2.PostProcessing.SOFTMAX:
     return _score_converter_fn_with_logit_scale(tf.nn.softmax, logit_scale)
   raise ValueError('Unknown score converter.')
+
+
+def _build_calibrated_score_converter(score_converter_fn, calibration_config):
+  """Wraps a score_converter_fn, adding a calibration step.
+
+  Builds a score converter function witha calibration transformation according
+  to calibration_builder.py. Calibration applies positive monotonic
+  transformations to inputs (i.e. score ordering is strictly preserved or
+  adjacent scores are mapped to the same score). When calibration is
+  class-agnostic, the highest-scoring class remains unchanged, unless two
+  adjacent scores are mapped to the same value and one class arbitrarily
+  selected to break the tie. In per-class calibration, it's possible (though
+  rare in practice) that the highest-scoring class will change, since positive
+  monotonicity is only required to hold within each class.
+
+  Args:
+    score_converter_fn: callable that takes logit scores as input.
+    calibration_config: post_processing_pb2.PostProcessing.calibration_config.
+
+  Returns:
+    Callable calibrated score coverter op.
+  """
+  calibration_fn = calibration_builder.build(calibration_config)
+  def calibrated_score_converter_fn(logits):
+    converted_logits = score_converter_fn(logits)
+    return calibration_fn(converted_logits)
+  calibrated_score_converter_fn.__name__ = (
+      'calibrate_with_%s' % calibration_config.WhichOneof('calibrator'))
+  return calibrated_score_converter_fn
diff --git a/research/object_detection/builders/post_processing_builder_test.py b/research/object_detection/builders/post_processing_builder_test.py
index c39fbfb4..e49303ec 100644
--- a/research/object_detection/builders/post_processing_builder_test.py
+++ b/research/object_detection/builders/post_processing_builder_test.py
@@ -47,7 +47,8 @@ class PostProcessingBuilderTest(tf.test.TestCase):
     """
     post_processing_config = post_processing_pb2.PostProcessing()
     text_format.Merge(post_processing_text_proto, post_processing_config)
-    _, score_converter = post_processing_builder.build(post_processing_config)
+    _, score_converter = post_processing_builder.build(
+        post_processing_config)
     self.assertEqual(score_converter.__name__, 'identity_with_logit_scale')
 
     inputs = tf.constant([1, 1], tf.float32)
@@ -102,6 +103,36 @@ class PostProcessingBuilderTest(tf.test.TestCase):
     _, score_converter = post_processing_builder.build(post_processing_config)
     self.assertEqual(score_converter.__name__, 'softmax_with_logit_scale')
 
+  def test_build_calibrator_with_nonempty_config(self):
+    """Test that identity function used when no calibration_config specified."""
+    # Calibration config maps all scores to 0.5.
+    post_processing_text_proto = """
+      score_converter: SOFTMAX
+      calibration_config {
+        function_approximation {
+          x_y_pairs {
+              x_y_pair {
+                x: 0.0
+                y: 0.5
+              }
+              x_y_pair {
+                x: 1.0
+                y: 0.5
+              }}}}"""
+    post_processing_config = post_processing_pb2.PostProcessing()
+    text_format.Merge(post_processing_text_proto, post_processing_config)
+    _, calibrated_score_conversion_fn = post_processing_builder.build(
+        post_processing_config)
+    self.assertEqual(calibrated_score_conversion_fn.__name__,
+                     'calibrate_with_function_approximation')
+
+    input_scores = tf.constant([1, 1], tf.float32)
+    outputs = calibrated_score_conversion_fn(input_scores)
+    with self.test_session() as sess:
+      calibrated_scores = sess.run(outputs)
+      expected_calibrated_scores = sess.run(tf.constant([0.5, 0.5], tf.float32))
+      self.assertAllClose(calibrated_scores, expected_calibrated_scores)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/preprocessor_builder.py b/research/object_detection/builders/preprocessor_builder.py
index 1d03b641..633205e3 100644
--- a/research/object_detection/builders/preprocessor_builder.py
+++ b/research/object_detection/builders/preprocessor_builder.py
@@ -191,10 +191,10 @@ def build(preprocessor_step_config):
 
     pad_color = config.pad_color or None
     if pad_color:
-      if len(pad_color) == 3:
-        pad_color = tf.to_float([x for x in config.pad_color])
-      else:
-        raise ValueError('pad_color should have 3 elements (RGB) if set!')
+      if len(pad_color) != 3:
+        tf.logging.warn('pad_color should have 3 elements (RGB) if set!')
+
+      pad_color = tf.to_float([x for x in config.pad_color])
     return (preprocessor.random_pad_image,
             {
                 'min_image_size': min_image_size,
@@ -202,6 +202,25 @@ def build(preprocessor_step_config):
                 'pad_color': pad_color,
             })
 
+  if step_type == 'random_absolute_pad_image':
+    config = preprocessor_step_config.random_absolute_pad_image
+
+    max_height_padding = config.max_height_padding or 1
+    max_width_padding = config.max_width_padding or 1
+
+    pad_color = config.pad_color or None
+    if pad_color:
+      if len(pad_color) != 3:
+        tf.logging.warn('pad_color should have 3 elements (RGB) if set!')
+
+      pad_color = tf.to_float([x for x in config.pad_color])
+
+    return (preprocessor.random_absolute_pad_image,
+            {
+                'max_height_padding': max_height_padding,
+                'max_width_padding': max_width_padding,
+                'pad_color': pad_color,
+            })
   if step_type == 'random_crop_pad_image':
     config = preprocessor_step_config.random_crop_pad_image
     min_padded_size_ratio = config.min_padded_size_ratio
@@ -210,9 +229,13 @@ def build(preprocessor_step_config):
     max_padded_size_ratio = config.max_padded_size_ratio
     if max_padded_size_ratio and len(max_padded_size_ratio) != 2:
       raise ValueError('max_padded_size_ratio should have 2 elements if set!')
-    pad_color = config.pad_color
-    if pad_color and len(pad_color) != 3:
-      raise ValueError('pad_color should have 3 elements if set!')
+    pad_color = config.pad_color or None
+    if pad_color:
+      if len(pad_color) != 3:
+        tf.logging.warn('pad_color should have 3 elements (RGB) if set!')
+
+      pad_color = tf.to_float([x for x in config.pad_color])
+
     kwargs = {
         'min_object_covered': config.min_object_covered,
         'aspect_ratio_range': (config.min_aspect_ratio,
@@ -221,13 +244,12 @@ def build(preprocessor_step_config):
         'overlap_thresh': config.overlap_thresh,
         'clip_boxes': config.clip_boxes,
         'random_coef': config.random_coef,
+        'pad_color': pad_color,
     }
     if min_padded_size_ratio:
       kwargs['min_padded_size_ratio'] = tuple(min_padded_size_ratio)
     if max_padded_size_ratio:
       kwargs['max_padded_size_ratio'] = tuple(max_padded_size_ratio)
-    if pad_color:
-      kwargs['pad_color'] = tuple(pad_color)
     return (preprocessor.random_crop_pad_image, kwargs)
 
   if step_type == 'random_resize_method':
@@ -247,6 +269,13 @@ def build(preprocessor_step_config):
                 'method': method
             })
 
+  if step_type == 'random_self_concat_image':
+    config = preprocessor_step_config.random_self_concat_image
+    return (preprocessor.random_self_concat_image, {
+        'concat_vertical_probability': config.concat_vertical_probability,
+        'concat_horizontal_probability': config.concat_horizontal_probability
+    })
+
   if step_type == 'ssd_random_crop':
     config = preprocessor_step_config.ssd_random_crop
     if config.operations:
diff --git a/research/object_detection/builders/preprocessor_builder_test.py b/research/object_detection/builders/preprocessor_builder_test.py
index 89de2b43..bd176f96 100644
--- a/research/object_detection/builders/preprocessor_builder_test.py
+++ b/research/object_detection/builders/preprocessor_builder_test.py
@@ -254,6 +254,23 @@ class PreprocessorBuilderTest(tf.test.TestCase):
         'pad_color': None,
     })
 
+  def test_build_random_absolute_pad_image(self):
+    preprocessor_text_proto = """
+    random_absolute_pad_image {
+      max_height_padding: 50
+      max_width_padding: 100
+    }
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Merge(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.random_absolute_pad_image)
+    self.assertEqual(args, {
+        'max_height_padding': 50,
+        'max_width_padding': 100,
+        'pad_color': None,
+    })
+
   def test_build_random_crop_pad_image(self):
     preprocessor_text_proto = """
     random_crop_pad_image {
@@ -278,6 +295,7 @@ class PreprocessorBuilderTest(tf.test.TestCase):
         'overlap_thresh': 0.5,
         'clip_boxes': False,
         'random_coef': 0.125,
+        'pad_color': None,
     })
 
   def test_build_random_crop_pad_image_with_optional_parameters(self):
@@ -295,9 +313,6 @@ class PreprocessorBuilderTest(tf.test.TestCase):
       min_padded_size_ratio: 0.75
       max_padded_size_ratio: 0.5
       max_padded_size_ratio: 0.75
-      pad_color: 0.5
-      pad_color: 0.5
-      pad_color: 1.0
     }
     """
     preprocessor_proto = preprocessor_pb2.PreprocessingStep()
@@ -313,7 +328,7 @@ class PreprocessorBuilderTest(tf.test.TestCase):
         'random_coef': 0.125,
         'min_padded_size_ratio': (0.5, 0.75),
         'max_padded_size_ratio': (0.5, 0.75),
-        'pad_color': (0.5, 0.5, 1.0)
+        'pad_color': None,
     })
 
   def test_build_random_crop_to_aspect_ratio(self):
@@ -409,6 +424,20 @@ class PreprocessorBuilderTest(tf.test.TestCase):
     self.assertEqual(function, preprocessor.subtract_channel_mean)
     self.assertEqual(args, {'means': [1.0, 2.0, 3.0]})
 
+  def test_random_self_concat_image(self):
+    preprocessor_text_proto = """
+    random_self_concat_image {
+      concat_vertical_probability: 0.5
+      concat_horizontal_probability: 0.25
+    }
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Merge(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.random_self_concat_image)
+    self.assertEqual(args, {'concat_vertical_probability': 0.5,
+                            'concat_horizontal_probability': 0.25})
+
   def test_build_ssd_random_crop(self):
     preprocessor_text_proto = """
     ssd_random_crop {
diff --git a/research/object_detection/builders/region_similarity_calculator_builder.py b/research/object_detection/builders/region_similarity_calculator_builder.py
index 157c94fb..8f35087f 100644
--- a/research/object_detection/builders/region_similarity_calculator_builder.py
+++ b/research/object_detection/builders/region_similarity_calculator_builder.py
@@ -53,7 +53,7 @@ def build(region_similarity_calculator_config):
     return region_similarity_calculator.NegSqDistSimilarity()
   if similarity_calculator == 'thresholded_iou_similarity':
     return region_similarity_calculator.ThresholdedIouSimilarity(
-        region_similarity_calculator_config.thresholded_iou_similarity.threshold
-    )
+        region_similarity_calculator_config.thresholded_iou_similarity
+        .iou_threshold)
 
   raise ValueError('Unknown region similarity calculator.')
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index c7a85ac3..b7d81fcb 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -61,8 +61,8 @@ class Loss(object):
         shouldn't be factored into the loss.
       losses_mask: A [batch] boolean tensor that indicates whether losses should
         be applied to individual images in the batch. For elements that
-        are True, corresponding prediction, target, and weight tensors will be
-        removed prior to loss computation. If None, no filtering will take place
+        are False, corresponding prediction, target, and weight tensors will not
+        contribute to loss computation. If None, no filtering will take place
         prior to loss computation.
       scope: Op scope name. Defaults to 'Loss' if None.
       **params: Additional keyword arguments for specific implementations of
diff --git a/research/object_detection/core/model.py b/research/object_detection/core/model.py
index 91e31813..6dda1e7a 100644
--- a/research/object_detection/core/model.py
+++ b/research/object_detection/core/model.py
@@ -54,15 +54,14 @@ By default, DetectionModels produce bounding box detections; However, we support
 a handful of auxiliary annotations associated with each bounding box, namely,
 instance masks and keypoints.
 """
-from abc import ABCMeta
-from abc import abstractmethod
+import abc
 
 from object_detection.core import standard_fields as fields
 
 
 class DetectionModel(object):
   """Abstract base class for detection models."""
-  __metaclass__ = ABCMeta
+  __metaclass__ = abc.ABCMeta
 
   def __init__(self, num_classes):
     """Constructor.
@@ -112,7 +111,7 @@ class DetectionModel(object):
     """
     return field in self._groundtruth_lists
 
-  @abstractmethod
+  @abc.abstractmethod
   def preprocess(self, inputs):
     """Input preprocessing.
 
@@ -155,7 +154,7 @@ class DetectionModel(object):
     """
     pass
 
-  @abstractmethod
+  @abc.abstractmethod
   def predict(self, preprocessed_inputs, true_image_shapes):
     """Predict prediction tensors from inputs tensor.
 
@@ -175,10 +174,14 @@ class DetectionModel(object):
     """
     pass
 
-  @abstractmethod
+  @abc.abstractmethod
   def postprocess(self, prediction_dict, true_image_shapes, **params):
     """Convert predicted output tensors to final detections.
 
+    This stage typically performs a few things such as
+    * Non-Max Suppression to remove overlapping detection boxes.
+    * Score conversion and background class removal.
+
     Outputs adhere to the following conventions:
     * Classes are integers in [0, num_classes); background classes are removed
       and the first non-background class is mapped to 0. If the model produces
@@ -212,10 +215,20 @@ class DetectionModel(object):
           (optional)
         keypoints: [batch, max_detections, num_keypoints, 2] (optional)
         num_detections: [batch]
+
+        In addition to the above fields this stage also outputs the following
+        raw tensors:
+
+        raw_detection_boxes: [batch, total_detections, 4] tensor containing
+          all detection boxes from `prediction_dict` in the format
+          [ymin, xmin, ymax, xmax] and normalized co-ordinates.
+        raw_detection_scores: [batch, total_detections,
+          num_classes_with_background] tensor of class score logits for
+          raw detection boxes.
     """
     pass
 
-  @abstractmethod
+  @abc.abstractmethod
   def loss(self, prediction_dict, true_image_shapes):
     """Compute scalar loss tensors with respect to provided groundtruth.
 
@@ -296,7 +309,7 @@ class DetectionModel(object):
       self._groundtruth_lists[
           fields.InputDataFields.is_annotated] = is_annotated_list
 
-  @abstractmethod
+  @abc.abstractmethod
   def regularization_losses(self):
     """Returns a list of regularization losses for this model.
 
@@ -308,7 +321,7 @@ class DetectionModel(object):
     """
     pass
 
-  @abstractmethod
+  @abc.abstractmethod
   def restore_map(self, fine_tune_checkpoint_type='detection'):
     """Returns a map of variables to load from a foreign checkpoint.
 
@@ -332,7 +345,7 @@ class DetectionModel(object):
     """
     pass
 
-  @abstractmethod
+  @abc.abstractmethod
   def updates(self):
     """Returns a list of update operators for this model.
 
diff --git a/research/object_detection/core/post_processing_test.py b/research/object_detection/core/post_processing_test.py
index c69aacd9..ca8f1fa5 100644
--- a/research/object_detection/core/post_processing_test.py
+++ b/research/object_detection/core/post_processing_test.py
@@ -57,7 +57,53 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
       self.assertAllClose(nms_scores_output, exp_nms_scores)
       self.assertAllClose(nms_classes_output, exp_nms_classes)
 
-  # TODO(bhattad): Remove conditional after CMLE moves to TF 1.9
+  def test_multiclass_nms_select_with_shared_boxes_pad_to_max_output_size(self):
+    boxes = np.array([[[0, 0, 1, 1]],
+                      [[0, 0.1, 1, 1.1]],
+                      [[0, -0.1, 1, 0.9]],
+                      [[0, 10, 1, 11]],
+                      [[0, 10.1, 1, 11.1]],
+                      [[0, 100, 1, 101]],
+                      [[0, 1000, 1, 1002]],
+                      [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05],
+                       [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01],
+                       [.01, .85], [.01, .5]], np.float32)
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_size_per_class = 4
+    max_output_size = 5
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+
+    def graph_fn(boxes, scores):
+      nms, num_valid_nms_boxes = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_size_per_class,
+          max_total_size=max_output_size,
+          pad_to_max_output_size=True)
+      return [nms.get(), nms.get_field(fields.BoxListFields.scores),
+              nms.get_field(fields.BoxListFields.classes), num_valid_nms_boxes]
+
+    [nms_corners_output, nms_scores_output, nms_classes_output,
+     num_valid_nms_boxes] = self.execute(graph_fn, [boxes, scores])
+
+    self.assertEqual(num_valid_nms_boxes, 4)
+    self.assertAllClose(nms_corners_output[0:num_valid_nms_boxes],
+                        exp_nms_corners)
+    self.assertAllClose(nms_scores_output[0:num_valid_nms_boxes],
+                        exp_nms_scores)
+    self.assertAllClose(nms_classes_output[0:num_valid_nms_boxes],
+                        exp_nms_classes)
 
   def test_multiclass_nms_select_with_shared_boxes_given_keypoints(self):
     boxes = tf.constant([[[0, 0, 1, 1]],
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 4d8f60b0..37c6e889 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -826,6 +826,14 @@ def random_image_scale(image,
     return tuple(result)
 
 
+def _augment_only_rgb_channels(image, augment_function):
+  """Augments only the RGB slice of an image with additional channels."""
+  rgb_slice = image[:, :, :3]
+  augmented_rgb_slice = augment_function(rgb_slice)
+  image = tf.concat([augmented_rgb_slice, image[:, :, 3:]], -1)
+  return image
+
+
 def random_rgb_to_gray(image,
                        probability=0.1,
                        seed=None,
@@ -860,7 +868,7 @@ def random_rgb_to_gray(image,
 
     image = tf.cond(
         tf.greater(do_gray_random, probability), lambda: image,
-        lambda: _image_to_gray(image))
+        lambda: _augment_only_rgb_channels(image, _image_to_gray))
 
   return image
 
@@ -895,8 +903,12 @@ def random_adjust_brightness(image,
         preprocessor_cache.PreprocessorCache.ADJUST_BRIGHTNESS,
         preprocess_vars_cache)
 
-    image = tf.image.adjust_brightness(image / 255, delta) * 255
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+    def _adjust_brightness(image):
+      image = tf.image.adjust_brightness(image / 255, delta) * 255
+      image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+      return image
+
+    image = _augment_only_rgb_channels(image, _adjust_brightness)
     return image
 
 
@@ -932,8 +944,12 @@ def random_adjust_contrast(image,
         generator_func,
         preprocessor_cache.PreprocessorCache.ADJUST_CONTRAST,
         preprocess_vars_cache)
-    image = tf.image.adjust_contrast(image / 255, contrast_factor) * 255
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+
+    def _adjust_contrast(image):
+      image = tf.image.adjust_contrast(image / 255, contrast_factor) * 255
+      image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+      return image
+    image = _augment_only_rgb_channels(image, _adjust_contrast)
     return image
 
 
@@ -964,8 +980,11 @@ def random_adjust_hue(image,
     delta = _get_or_create_preprocess_rand_vars(
         generator_func, preprocessor_cache.PreprocessorCache.ADJUST_HUE,
         preprocess_vars_cache)
-    image = tf.image.adjust_hue(image / 255, delta) * 255
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+    def _adjust_hue(image):
+      image = tf.image.adjust_hue(image / 255, delta) * 255
+      image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+      return image
+    image = _augment_only_rgb_channels(image, _adjust_hue)
     return image
 
 
@@ -1001,8 +1020,11 @@ def random_adjust_saturation(image,
         generator_func,
         preprocessor_cache.PreprocessorCache.ADJUST_SATURATION,
         preprocess_vars_cache)
-    image = tf.image.adjust_saturation(image / 255, saturation_factor) * 255
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+    def _adjust_saturation(image):
+      image = tf.image.adjust_saturation(image / 255, saturation_factor) * 255
+      image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)
+      return image
+    image = _augment_only_rgb_channels(image, _adjust_saturation)
     return image
 
 
@@ -1423,6 +1445,7 @@ def random_crop_image(image,
 
 def random_pad_image(image,
                      boxes,
+                     keypoints=None,
                      min_image_size=None,
                      max_image_size=None,
                      pad_color=None,
@@ -1444,15 +1467,18 @@ def random_pad_image(image,
            Boxes are in normalized form meaning their coordinates vary
            between [0, 1].
            Each row is in the form of [ymin, xmin, ymax, xmax].
+    keypoints: (optional) rank 3 float32 tensor with shape
+               [N, num_keypoints, 2]. The keypoints are in y-x normalized
+               coordinates.
     min_image_size: a tensor of size [min_height, min_width], type tf.int32.
                     If passed as None, will be set to image size
                     [height, width].
     max_image_size: a tensor of size [max_height, max_width], type tf.int32.
                     If passed as None, will be set to twice the
                     image [height * 2, width * 2].
-    pad_color: padding color. A rank 1 tensor of [3] with dtype=tf.float32.
-               if set as None, it will be set to average color of the input
-               image.
+    pad_color: padding color. A rank 1 tensor of [channels] with dtype=
+               tf.float32. if set as None, it will be set to average color of
+               the input image.
     seed: random seed.
     preprocess_vars_cache: PreprocessorCache object that records previously
                            performed augmentations. Updated in-place. If this
@@ -1463,6 +1489,9 @@ def random_pad_image(image,
     image: Image shape will be [new_height, new_width, channels].
     boxes: boxes which is the same rank as input boxes. Boxes are in normalized
            form.
+
+    if keypoints is not None, the function also returns:
+    keypoints: rank 3 float32 tensor with shape [N, num_keypoints, 2]
   """
   if pad_color is None:
     pad_color = tf.reduce_mean(image, axis=[0, 1])
@@ -1537,7 +1566,62 @@ def random_pad_image(image,
   new_boxlist = box_list_ops.change_coordinate_frame(boxlist, new_window)
   new_boxes = new_boxlist.get()
 
-  return new_image, new_boxes
+  result = [new_image, new_boxes]
+
+  if keypoints is not None:
+    new_keypoints = keypoint_ops.change_coordinate_frame(keypoints, new_window)
+    result.append(new_keypoints)
+
+  return tuple(result)
+
+
+def random_absolute_pad_image(image,
+                              boxes,
+                              max_height_padding,
+                              max_width_padding,
+                              pad_color=None,
+                              seed=None,
+                              preprocess_vars_cache=None):
+  """Randomly pads the image by small absolute amounts.
+
+  As random_pad_image above, but the padding is of size [0, max_height_padding]
+  or [0, max_width_padding] instead of padding to a fixed size of
+  max_height_padding for all images.
+
+  Args:
+    image: rank 3 float32 tensor containing 1 image -> [height, width, channels]
+           with pixel values varying between [0, 1].
+    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].
+           Boxes are in normalized form meaning their coordinates vary
+           between [0, 1].
+           Each row is in the form of [ymin, xmin, ymax, xmax].
+    max_height_padding: a scalar tf.int32 tensor denoting the maximum amount of
+                        height padding. The padding will be chosen uniformly at
+                        random from [0, max_height_padding).
+    max_width_padding: a scalar tf.int32 tensor denoting the maximum amount of
+                       width padding. The padding will be chosen uniformly at
+                       random from [0, max_width_padding).
+    pad_color: padding color. A rank 1 tensor of [3] with dtype=tf.float32.
+               if set as None, it will be set to average color of the input
+               image.
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
+
+  Returns:
+    image: Image shape will be [new_height, new_width, channels].
+    boxes: boxes which is the same rank as input boxes. Boxes are in normalized
+           form.
+  """
+  min_image_size = tf.shape(image)[:2]
+  max_image_size = min_image_size + tf.to_int32(
+      [max_height_padding, max_width_padding])
+  return random_pad_image(image, boxes, min_image_size=min_image_size,
+                          max_image_size=max_image_size, pad_color=pad_color,
+                          seed=seed,
+                          preprocess_vars_cache=preprocess_vars_cache)
 
 
 def random_crop_pad_image(image,
@@ -2101,80 +2185,6 @@ def random_resize_method(image, target_size, preprocess_vars_cache=None):
   return resized_image
 
 
-def _compute_new_static_size(image, min_dimension, max_dimension):
-  """Compute new static shape for resize_to_range method."""
-  image_shape = image.get_shape().as_list()
-  orig_height = image_shape[0]
-  orig_width = image_shape[1]
-  num_channels = image_shape[2]
-  orig_min_dim = min(orig_height, orig_width)
-  # Calculates the larger of the possible sizes
-  large_scale_factor = min_dimension / float(orig_min_dim)
-  # Scaling orig_(height|width) by large_scale_factor will make the smaller
-  # dimension equal to min_dimension, save for floating point rounding errors.
-  # For reasonably-sized images, taking the nearest integer will reliably
-  # eliminate this error.
-  large_height = int(round(orig_height * large_scale_factor))
-  large_width = int(round(orig_width * large_scale_factor))
-  large_size = [large_height, large_width]
-  if max_dimension:
-    # Calculates the smaller of the possible sizes, use that if the larger
-    # is too big.
-    orig_max_dim = max(orig_height, orig_width)
-    small_scale_factor = max_dimension / float(orig_max_dim)
-    # Scaling orig_(height|width) by small_scale_factor will make the larger
-    # dimension equal to max_dimension, save for floating point rounding
-    # errors. For reasonably-sized images, taking the nearest integer will
-    # reliably eliminate this error.
-    small_height = int(round(orig_height * small_scale_factor))
-    small_width = int(round(orig_width * small_scale_factor))
-    small_size = [small_height, small_width]
-    new_size = large_size
-    if max(large_size) > max_dimension:
-      new_size = small_size
-  else:
-    new_size = large_size
-  return tf.constant(new_size + [num_channels])
-
-
-def _compute_new_dynamic_size(image, min_dimension, max_dimension):
-  """Compute new dynamic shape for resize_to_range method."""
-  image_shape = tf.shape(image)
-  orig_height = tf.to_float(image_shape[0])
-  orig_width = tf.to_float(image_shape[1])
-  num_channels = image_shape[2]
-  orig_min_dim = tf.minimum(orig_height, orig_width)
-  # Calculates the larger of the possible sizes
-  min_dimension = tf.constant(min_dimension, dtype=tf.float32)
-  large_scale_factor = min_dimension / orig_min_dim
-  # Scaling orig_(height|width) by large_scale_factor will make the smaller
-  # dimension equal to min_dimension, save for floating point rounding errors.
-  # For reasonably-sized images, taking the nearest integer will reliably
-  # eliminate this error.
-  large_height = tf.to_int32(tf.round(orig_height * large_scale_factor))
-  large_width = tf.to_int32(tf.round(orig_width * large_scale_factor))
-  large_size = tf.stack([large_height, large_width])
-  if max_dimension:
-    # Calculates the smaller of the possible sizes, use that if the larger
-    # is too big.
-    orig_max_dim = tf.maximum(orig_height, orig_width)
-    max_dimension = tf.constant(max_dimension, dtype=tf.float32)
-    small_scale_factor = max_dimension / orig_max_dim
-    # Scaling orig_(height|width) by small_scale_factor will make the larger
-    # dimension equal to max_dimension, save for floating point rounding
-    # errors. For reasonably-sized images, taking the nearest integer will
-    # reliably eliminate this error.
-    small_height = tf.to_int32(tf.round(orig_height * small_scale_factor))
-    small_width = tf.to_int32(tf.round(orig_width * small_scale_factor))
-    small_size = tf.stack([small_height, small_width])
-    new_size = tf.cond(
-        tf.to_float(tf.reduce_max(large_size)) > max_dimension,
-        lambda: small_size, lambda: large_size)
-  else:
-    new_size = large_size
-  return tf.stack(tf.unstack(new_size) + [num_channels])
-
-
 def resize_to_range(image,
                     masks=None,
                     min_dimension=None,
@@ -2228,13 +2238,31 @@ def resize_to_range(image,
   if len(image.get_shape()) != 3:
     raise ValueError('Image should be 3D tensor')
 
+  def _resize_landscape_image(image):
+    # resize a landscape image
+    return tf.image.resize_images(
+        image, tf.stack([min_dimension, max_dimension]), method=method,
+        align_corners=align_corners, preserve_aspect_ratio=True)
+
+  def _resize_portrait_image(image):
+    # resize a portrait image
+    return tf.image.resize_images(
+        image, tf.stack([max_dimension, min_dimension]), method=method,
+        align_corners=align_corners, preserve_aspect_ratio=True)
+
   with tf.name_scope('ResizeToRange', values=[image, min_dimension]):
     if image.get_shape().is_fully_defined():
-      new_size = _compute_new_static_size(image, min_dimension, max_dimension)
+      if image.get_shape()[0] < image.get_shape()[1]:
+        new_image = _resize_landscape_image(image)
+      else:
+        new_image = _resize_portrait_image(image)
+      new_size = tf.constant(new_image.get_shape().as_list())
     else:
-      new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)
-    new_image = tf.image.resize_images(
-        image, new_size[:-1], method=method, align_corners=align_corners)
+      new_image = tf.cond(
+          tf.less(tf.shape(image)[0], tf.shape(image)[1]),
+          lambda: _resize_landscape_image(image),
+          lambda: _resize_portrait_image(image))
+      new_size = tf.shape(new_image)
 
     if pad_to_max_dimension:
       channels = tf.unstack(new_image, axis=2)
@@ -2480,6 +2508,115 @@ def rgb_to_gray(image):
   return _rgb_to_grayscale(image)
 
 
+def random_self_concat_image(
+    image, boxes, labels, label_weights, label_confidences=None,
+    multiclass_scores=None, concat_vertical_probability=0.1,
+    concat_horizontal_probability=0.1, seed=None,
+    preprocess_vars_cache=None):
+  """Randomly concatenates the image with itself.
+
+  This function randomly concatenates the image with itself; the random
+  variables for vertical and horizontal concatenation are independent.
+  Afterwards, we adjust the old bounding boxes, and add new bounding boxes
+  for the new objects.
+
+  Args:
+    image: rank 3 float32 tensor containing 1 image -> [height, width, channels]
+           with pixel values varying between [0, 1].
+    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].
+           Boxes are in normalized form meaning their coordinates vary
+           between [0, 1].
+           Each row is in the form of [ymin, xmin, ymax, xmax].
+    labels: rank 1 int32 tensor containing the object classes.
+    label_weights: rank 1 float32 containing the label weights.
+    label_confidences: (optional) rank 1 float32 containing the label
+                       confidences.
+    multiclass_scores: (optional) float32 tensor of shape
+                       [num_instances, num_classes] representing the score for
+                       each box for each class.
+    concat_vertical_probability: (optional) a tf.float32 scalar denoting the
+                                 probability of a vertical concatenation.
+    concat_horizontal_probability: (optional) a tf.float32 scalar denoting the
+                                   probability of a horizontal concatenation.
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
+
+  Returns:
+    image: Image shape will be [new_height, new_width, channels].
+    boxes: boxes which is the same rank as input boxes. Boxes are in normalized
+           form.
+    if label_confidences is not None also returns:
+    maybe_concat_label_confidences: cropped label weights.
+    if multiclass_scores is not None also returns:
+    maybe_concat_multiclass_scores: cropped_multiclass_scores.
+  """
+
+  concat_vertical = (tf.random_uniform([], seed=seed) <
+                     concat_vertical_probability)
+  # Note the seed + 1 so we get some semblance of independence even with
+  # fixed seeds.
+  concat_horizontal = (tf.random_uniform([], seed=seed + 1 if seed else None)
+                       < concat_horizontal_probability)
+
+  gen_func = lambda: (concat_vertical, concat_horizontal)
+  params = _get_or_create_preprocess_rand_vars(
+      gen_func, preprocessor_cache.PreprocessorCache.SELF_CONCAT_IMAGE,
+      preprocess_vars_cache)
+  concat_vertical, concat_horizontal = params
+
+  def _concat_image(image, boxes, labels, label_weights, axis):
+    """Concats the image to itself on `axis`."""
+    output_images = tf.concat([image, image], axis=axis)
+
+    if axis == 0:
+      # Concat vertically, so need to reduce the y coordinates.
+      old_scaling = tf.to_float([0.5, 1.0, 0.5, 1.0])
+      new_translation = tf.to_float([0.5, 0.0, 0.5, 0.0])
+    elif axis == 1:
+      old_scaling = tf.to_float([1.0, 0.5, 1.0, 0.5])
+      new_translation = tf.to_float([0.0, 0.5, 0.0, 0.5])
+
+    old_boxes = old_scaling * boxes
+    new_boxes = old_boxes + new_translation
+    all_boxes = tf.concat([old_boxes, new_boxes], axis=0)
+
+    return [output_images, all_boxes, tf.tile(labels, [2]), tf.tile(
+        label_weights, [2])]
+
+  image, boxes, labels, label_weights = tf.cond(
+      concat_vertical,
+      lambda: _concat_image(image, boxes, labels, label_weights, axis=0),
+      lambda: [image, boxes, labels, label_weights],
+      strict=True)
+
+  outputs = tf.cond(
+      concat_horizontal,
+      lambda: _concat_image(image, boxes, labels, label_weights, axis=1),
+      lambda: [image, boxes, labels, label_weights],
+      strict=True)
+
+  if label_confidences is not None:
+    label_confidences = tf.cond(concat_vertical,
+                                lambda: tf.tile(label_confidences, [2]),
+                                lambda: label_confidences)
+    outputs.append(tf.cond(concat_horizontal,
+                           lambda: tf.tile(label_confidences, [2]),
+                           lambda: label_confidences))
+
+  if multiclass_scores is not None:
+    multiclass_scores = tf.cond(concat_vertical,
+                                lambda: tf.tile(multiclass_scores, [2, 1]),
+                                lambda: multiclass_scores)
+    outputs.append(tf.cond(concat_horizontal,
+                           lambda: tf.tile(multiclass_scores, [2, 1]),
+                           lambda: multiclass_scores))
+
+  return outputs
+
+
 def ssd_random_crop(image,
                     boxes,
                     labels,
@@ -2804,7 +2941,7 @@ def ssd_random_crop_fixed_aspect_ratio(
            Boxes are in normalized form.
     labels: new labels.
 
-    If mulitclass_scores, masks, or keypoints is not None, the function also
+    If multiclass_scores, masks, or keypoints is not None, the function also
       returns:
 
     multiclass_scores: rank 2 float32 tensor with shape
@@ -3132,9 +3269,13 @@ def get_default_func_arg_map(include_label_weights=True,
                           groundtruth_label_weights,
                           groundtruth_label_confidences,
                           multiclass_scores,
-                          groundtruth_instance_masks, groundtruth_keypoints),
+                          groundtruth_instance_masks,
+                          groundtruth_keypoints),
       random_pad_image: (fields.InputDataFields.image,
-                         fields.InputDataFields.groundtruth_boxes),
+                         fields.InputDataFields.groundtruth_boxes,
+                         groundtruth_keypoints),
+      random_absolute_pad_image: (fields.InputDataFields.image,
+                                  fields.InputDataFields.groundtruth_boxes),
       random_crop_pad_image: (fields.InputDataFields.image,
                               fields.InputDataFields.groundtruth_boxes,
                               fields.InputDataFields.groundtruth_classes,
@@ -3189,6 +3330,12 @@ def get_default_func_arg_map(include_label_weights=True,
       subtract_channel_mean: (fields.InputDataFields.image,),
       one_hot_encoding: (fields.InputDataFields.groundtruth_image_classes,),
       rgb_to_gray: (fields.InputDataFields.image,),
+      random_self_concat_image: (fields.InputDataFields.image,
+                                 fields.InputDataFields.groundtruth_boxes,
+                                 fields.InputDataFields.groundtruth_classes,
+                                 groundtruth_label_weights,
+                                 groundtruth_label_confidences,
+                                 multiclass_scores),
       ssd_random_crop: (fields.InputDataFields.image,
                         fields.InputDataFields.groundtruth_boxes,
                         fields.InputDataFields.groundtruth_classes,
@@ -3302,6 +3449,7 @@ def preprocess(tensor_dict,
     if (preprocess_vars_cache is not None and
         'preprocess_vars_cache' in inspect.getargspec(func).args):
       params['preprocess_vars_cache'] = preprocess_vars_cache
+
     results = func(*args, **params)
     if not isinstance(results, (list, tuple)):
       results = (results,)
diff --git a/research/object_detection/core/preprocessor_cache.py b/research/object_detection/core/preprocessor_cache.py
index 4294ad1c..13471fe4 100644
--- a/research/object_detection/core/preprocessor_cache.py
+++ b/research/object_detection/core/preprocessor_cache.py
@@ -51,6 +51,7 @@ class PreprocessorCache(object):
   ADD_BLACK_PATCH = 'add_black_patch'
   SELECTOR = 'selector'
   SELECTOR_TUPLES = 'selector_tuples'
+  SELF_CONCAT_IMAGE = 'self_concat_image'
   SSD_CROP_SELECTOR_ID = 'ssd_crop_selector_id'
   SSD_CROP_PAD_SELECTOR_ID = 'ssd_crop_pad_selector_id'
 
@@ -60,7 +61,8 @@ class PreprocessorCache(object):
                 ADJUST_HUE, ADJUST_SATURATION, DISTORT_COLOR, STRICT_CROP_IMAGE,
                 CROP_IMAGE, PAD_IMAGE, CROP_TO_ASPECT_RATIO, RESIZE_METHOD,
                 PAD_TO_ASPECT_RATIO, BLACK_PATCHES, ADD_BLACK_PATCH, SELECTOR,
-                SELECTOR_TUPLES, SSD_CROP_SELECTOR_ID, SSD_CROP_PAD_SELECTOR_ID]
+                SELECTOR_TUPLES, SELF_CONCAT_IMAGE, SSD_CROP_SELECTOR_ID,
+                SSD_CROP_PAD_SELECTOR_ID]
 
   def __init__(self):
     self._history = defaultdict(dict)
@@ -99,4 +101,3 @@ class PreprocessorCache(object):
     if function_id not in self._VALID_FNS:
       raise ValueError('Function id not recognized: %s.' % str(function_id))
     self._history[function_id][key] = value
-
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 4786f807..9bd03e35 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -2071,6 +2071,96 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
           padded_boxes_[:, 3] - padded_boxes_[:, 1])))
 
+  def testRandomPadImageWithKeypoints(self):
+    preprocessing_options = [(preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    })]
+
+    images = self.createTestImages()
+    boxes = self.createTestBoxes()
+    labels = self.createTestLabels()
+    keypoints = self.createTestKeypoints()
+    tensor_dict = {
+        fields.InputDataFields.image: images,
+        fields.InputDataFields.groundtruth_boxes: boxes,
+        fields.InputDataFields.groundtruth_classes: labels,
+        fields.InputDataFields.groundtruth_keypoints: keypoints,
+    }
+    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+    images = tensor_dict[fields.InputDataFields.image]
+
+    preprocessing_options = [(preprocessor.random_pad_image, {})]
+    padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                 preprocessing_options)
+
+    padded_images = padded_tensor_dict[fields.InputDataFields.image]
+    padded_boxes = padded_tensor_dict[
+        fields.InputDataFields.groundtruth_boxes]
+    padded_keypoints = padded_tensor_dict[
+        fields.InputDataFields.groundtruth_keypoints]
+    boxes_shape = tf.shape(boxes)
+    padded_boxes_shape = tf.shape(padded_boxes)
+    keypoints_shape = tf.shape(keypoints)
+    padded_keypoints_shape = tf.shape(padded_keypoints)
+    images_shape = tf.shape(images)
+    padded_images_shape = tf.shape(padded_images)
+
+    with self.test_session() as sess:
+      (boxes_shape_, padded_boxes_shape_, keypoints_shape_,
+       padded_keypoints_shape_, images_shape_, padded_images_shape_, boxes_,
+       padded_boxes_, keypoints_, padded_keypoints_) = sess.run(
+           [boxes_shape, padded_boxes_shape, keypoints_shape,
+            padded_keypoints_shape, images_shape, padded_images_shape, boxes,
+            padded_boxes, keypoints, padded_keypoints])
+      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
+      self.assertAllEqual(keypoints_shape_, padded_keypoints_shape_)
+      self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
+      self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
+      self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
+      self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
+      self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
+          padded_boxes_[:, 2] - padded_boxes_[:, 0])))
+      self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
+          padded_boxes_[:, 3] - padded_boxes_[:, 1])))
+      self.assertTrue(np.all((keypoints_[1, :, 0] - keypoints_[0, :, 0]) >= (
+          padded_keypoints_[1, :, 0] - padded_keypoints_[0, :, 0])))
+      self.assertTrue(np.all((keypoints_[1, :, 1] - keypoints_[0, :, 1]) >= (
+          padded_keypoints_[1, :, 1] - padded_keypoints_[0, :, 1])))
+
+  def testRandomAbsolutePadImage(self):
+    images = self.createTestImages()
+    boxes = self.createTestBoxes()
+    labels = self.createTestLabels()
+    tensor_dict = {
+        fields.InputDataFields.image: tf.to_float(images),
+        fields.InputDataFields.groundtruth_boxes: boxes,
+        fields.InputDataFields.groundtruth_classes: labels,
+    }
+
+    height_padding = 10
+    width_padding = 20
+    preprocessing_options = [(preprocessor.random_absolute_pad_image, {
+        'max_height_padding': height_padding,
+        'max_width_padding': width_padding})]
+    padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                 preprocessing_options)
+
+    original_shape = tf.shape(images)
+    final_shape = tf.shape(padded_tensor_dict[fields.InputDataFields.image])
+
+    with self.test_session() as sess:
+      _, height, width, _ = sess.run(original_shape)
+      for _ in range(100):
+        output_shape = sess.run(final_shape)
+
+        self.assertTrue(output_shape[1] >= height)
+        self.assertTrue(output_shape[1] < height + height_padding)
+        self.assertTrue(output_shape[2] >= width)
+        self.assertTrue(output_shape[2] < width + width_padding)
+
   def testRandomCropPadImageWithCache(self):
     preprocess_options = [(preprocessor.normalize_image, {
         'original_minval': 0,
@@ -2693,6 +2783,95 @@ class PreprocessorTest(tf.test.TestCase):
 
       self.assertAllEqual([0, 1, 1, 0, 1], one_hot)
 
+  def testRandomSelfConcatImage(self):
+    tf.set_random_seed(24601)
+
+    images = self.createTestImages()
+    boxes = self.createTestBoxes()
+    labels = self.createTestLabels()
+    weights = self.createTestGroundtruthWeights()
+    confidences = weights
+    scores = self.createTestMultiClassScores()
+
+    tensor_dict = {
+        fields.InputDataFields.image: tf.to_float(images),
+        fields.InputDataFields.groundtruth_boxes: boxes,
+        fields.InputDataFields.groundtruth_classes: labels,
+        fields.InputDataFields.groundtruth_weights: weights,
+        fields.InputDataFields.groundtruth_confidences: confidences,
+        fields.InputDataFields.multiclass_scores: scores,
+    }
+
+    preprocessing_options = [(preprocessor.random_self_concat_image, {
+        'concat_vertical_probability': 0.5,
+        'concat_horizontal_probability': 0.5,
+        'seed': 24601,
+    })]
+    func_arg_map = preprocessor.get_default_func_arg_map(
+        True, True, True)
+    output_tensor_dict = preprocessor.preprocess(
+        tensor_dict, preprocessing_options, func_arg_map=func_arg_map)
+
+    final_shape = tf.shape(output_tensor_dict[fields.InputDataFields.image])[
+        1:3]
+
+    with self.test_session() as sess:
+      outputs = []
+
+      augment_height_only = False
+      augment_width_only = False
+
+      for _ in range(50):
+        original_boxes = sess.run(boxes)
+        shape, new_boxes, new_labels, new_confidences, new_scores = sess.run(
+            [final_shape,
+             output_tensor_dict[fields.InputDataFields.groundtruth_boxes],
+             output_tensor_dict[fields.InputDataFields.groundtruth_classes],
+             output_tensor_dict[fields.InputDataFields.groundtruth_confidences],
+             output_tensor_dict[fields.InputDataFields.multiclass_scores],
+            ])
+        shape = np.array(shape)
+        outputs.append(shape)
+
+        if np.array_equal(shape, [8, 4]):
+          augment_height_only = True
+          self.assertEqual(
+              new_boxes.shape[0], 2 * boxes.shape[0])
+
+          self.assertAllClose(new_boxes[:2, :] * [2.0, 1.0, 2.0, 1.0],
+                              original_boxes)
+          self.assertAllClose(
+              (new_boxes[2:, :] - [0.5, 0.0, 0.5, 0.0]) * [
+                  2.0, 1.0, 2.0, 1.0],
+              original_boxes)
+        elif np.array_equal(shape, [4, 8]):
+          augment_width_only = True
+          self.assertEqual(
+              new_boxes.shape[0], 2 * boxes.shape[0])
+
+          self.assertAllClose(new_boxes[:2, :] * [1.0, 2.0, 1.0, 2.0],
+                              original_boxes)
+          self.assertAllClose(
+              (new_boxes[2:, :] - [0.0, 0.5, 0.0, 0.5]) * [
+                  1.0, 2.0, 1.0, 2.0],
+              original_boxes)
+
+        augmentation_factor = new_boxes.shape[0] / boxes.shape[0].value
+        self.assertEqual(new_labels.shape[0],
+                         labels.shape[0].value * augmentation_factor)
+        self.assertEqual(new_confidences.shape[0],
+                         confidences.shape[0].value * augmentation_factor)
+        self.assertEqual(new_scores.shape[0],
+                         scores.shape[0].value * augmentation_factor)
+
+      max_height = max(x[0] for x in outputs)
+      max_width = max(x[1] for x in outputs)
+
+      self.assertEqual(max_height, 8)
+      self.assertEqual(max_width, 8)
+      self.assertEqual(augment_height_only, True)
+      self.assertEqual(augment_width_only, True)
+
   def testSSDRandomCropWithCache(self):
     preprocess_options = [
         (preprocessor.normalize_image, {
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index cc0da39a..c4f9fef0 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -114,6 +114,9 @@ class DetectionResultFields(object):
     detection_boundaries: contains an object boundary for each detection box.
     detection_keypoints: contains detection keypoints for each detection box.
     num_detections: number of detections in the batch.
+    raw_detection_boxes: contains decoded detection boxes without Non-Max
+      suppression.
+    raw_detection_scores: contains class score logits for raw detection boxes.
   """
 
   source_id = 'source_id'
@@ -125,6 +128,8 @@ class DetectionResultFields(object):
   detection_boundaries = 'detection_boundaries'
   detection_keypoints = 'detection_keypoints'
   num_detections = 'num_detections'
+  raw_detection_boxes = 'raw_detection_boxes'
+  raw_detection_scores = 'raw_detection_scores'
 
 
 class BoxListFields(object):
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 0bb3b613..664926bc 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -166,6 +166,10 @@ class TargetAssigner(object):
         num_gt_boxes = groundtruth_boxes.num_boxes()
       groundtruth_weights = tf.ones([num_gt_boxes], dtype=tf.float32)
 
+    # set scores on the gt boxes
+    scores = 1 - groundtruth_labels[:, 0]
+    groundtruth_boxes.add_field(fields.BoxListFields.scores, scores)
+
     with tf.control_dependencies(
         [unmatched_shape_assert, labels_and_box_shapes_assert]):
       match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes,
diff --git a/research/object_detection/data/oid_v4_label_map.pbtxt b/research/object_detection/data/oid_v4_label_map.pbtxt
new file mode 100644
index 00000000..643b9e8e
--- /dev/null
+++ b/research/object_detection/data/oid_v4_label_map.pbtxt
@@ -0,0 +1,3005 @@
+item {
+  name: "/m/011k07"
+  id: 1
+  display_name: "Tortoise"
+}
+item {
+  name: "/m/011q46kg"
+  id: 2
+  display_name: "Container"
+}
+item {
+  name: "/m/012074"
+  id: 3
+  display_name: "Magpie"
+}
+item {
+  name: "/m/0120dh"
+  id: 4
+  display_name: "Sea turtle"
+}
+item {
+  name: "/m/01226z"
+  id: 5
+  display_name: "Football"
+}
+item {
+  name: "/m/012n7d"
+  id: 6
+  display_name: "Ambulance"
+}
+item {
+  name: "/m/012w5l"
+  id: 7
+  display_name: "Ladder"
+}
+item {
+  name: "/m/012xff"
+  id: 8
+  display_name: "Toothbrush"
+}
+item {
+  name: "/m/012ysf"
+  id: 9
+  display_name: "Syringe"
+}
+item {
+  name: "/m/0130jx"
+  id: 10
+  display_name: "Sink"
+}
+item {
+  name: "/m/0138tl"
+  id: 11
+  display_name: "Toy"
+}
+item {
+  name: "/m/013y1f"
+  id: 12
+  display_name: "Organ"
+}
+item {
+  name: "/m/01432t"
+  id: 13
+  display_name: "Cassette deck"
+}
+item {
+  name: "/m/014j1m"
+  id: 14
+  display_name: "Apple"
+}
+item {
+  name: "/m/014sv8"
+  id: 15
+  display_name: "Human eye"
+}
+item {
+  name: "/m/014trl"
+  id: 16
+  display_name: "Cosmetics"
+}
+item {
+  name: "/m/014y4n"
+  id: 17
+  display_name: "Paddle"
+}
+item {
+  name: "/m/0152hh"
+  id: 18
+  display_name: "Snowman"
+}
+item {
+  name: "/m/01599"
+  id: 19
+  display_name: "Beer"
+}
+item {
+  name: "/m/01_5g"
+  id: 20
+  display_name: "Chopsticks"
+}
+item {
+  name: "/m/015h_t"
+  id: 21
+  display_name: "Human beard"
+}
+item {
+  name: "/m/015p6"
+  id: 22
+  display_name: "Bird"
+}
+item {
+  name: "/m/015qbp"
+  id: 23
+  display_name: "Parking meter"
+}
+item {
+  name: "/m/015qff"
+  id: 24
+  display_name: "Traffic light"
+}
+item {
+  name: "/m/015wgc"
+  id: 25
+  display_name: "Croissant"
+}
+item {
+  name: "/m/015x4r"
+  id: 26
+  display_name: "Cucumber"
+}
+item {
+  name: "/m/015x5n"
+  id: 27
+  display_name: "Radish"
+}
+item {
+  name: "/m/0162_1"
+  id: 28
+  display_name: "Towel"
+}
+item {
+  name: "/m/0167gd"
+  id: 29
+  display_name: "Doll"
+}
+item {
+  name: "/m/016m2d"
+  id: 30
+  display_name: "Skull"
+}
+item {
+  name: "/m/0174k2"
+  id: 31
+  display_name: "Washing machine"
+}
+item {
+  name: "/m/0174n1"
+  id: 32
+  display_name: "Glove"
+}
+item {
+  name: "/m/0175cv"
+  id: 33
+  display_name: "Tick"
+}
+item {
+  name: "/m/0176mf"
+  id: 34
+  display_name: "Belt"
+}
+item {
+  name: "/m/017ftj"
+  id: 35
+  display_name: "Sunglasses"
+}
+item {
+  name: "/m/018j2"
+  id: 36
+  display_name: "Banjo"
+}
+item {
+  name: "/m/018p4k"
+  id: 37
+  display_name: "Cart"
+}
+item {
+  name: "/m/018xm"
+  id: 38
+  display_name: "Ball"
+}
+item {
+  name: "/m/01940j"
+  id: 39
+  display_name: "Backpack"
+}
+item {
+  name: "/m/0199g"
+  id: 40
+  display_name: "Bicycle"
+}
+item {
+  name: "/m/019dx1"
+  id: 41
+  display_name: "Home appliance"
+}
+item {
+  name: "/m/019h78"
+  id: 42
+  display_name: "Centipede"
+}
+item {
+  name: "/m/019jd"
+  id: 43
+  display_name: "Boat"
+}
+item {
+  name: "/m/019w40"
+  id: 44
+  display_name: "Surfboard"
+}
+item {
+  name: "/m/01b638"
+  id: 45
+  display_name: "Boot"
+}
+item {
+  name: "/m/01b7fy"
+  id: 46
+  display_name: "Headphones"
+}
+item {
+  name: "/m/01b9xk"
+  id: 47
+  display_name: "Hot dog"
+}
+item {
+  name: "/m/01bfm9"
+  id: 48
+  display_name: "Shorts"
+}
+item {
+  name: "/m/01_bhs"
+  id: 49
+  display_name: "Fast food"
+}
+item {
+  name: "/m/01bjv"
+  id: 50
+  display_name: "Bus"
+}
+item {
+  name: "/m/01bl7v"
+  id: 51
+  display_name: "Boy"
+}
+item {
+  name: "/m/01bms0"
+  id: 52
+  display_name: "Screwdriver"
+}
+item {
+  name: "/m/01bqk0"
+  id: 53
+  display_name: "Bicycle wheel"
+}
+item {
+  name: "/m/01btn"
+  id: 54
+  display_name: "Barge"
+}
+item {
+  name: "/m/01c648"
+  id: 55
+  display_name: "Laptop"
+}
+item {
+  name: "/m/01cmb2"
+  id: 56
+  display_name: "Miniskirt"
+}
+item {
+  name: "/m/01d380"
+  id: 57
+  display_name: "Drill"
+}
+item {
+  name: "/m/01d40f"
+  id: 58
+  display_name: "Dress"
+}
+item {
+  name: "/m/01dws"
+  id: 59
+  display_name: "Bear"
+}
+item {
+  name: "/m/01dwsz"
+  id: 60
+  display_name: "Waffle"
+}
+item {
+  name: "/m/01dwwc"
+  id: 61
+  display_name: "Pancake"
+}
+item {
+  name: "/m/01dxs"
+  id: 62
+  display_name: "Brown bear"
+}
+item {
+  name: "/m/01dy8n"
+  id: 63
+  display_name: "Woodpecker"
+}
+item {
+  name: "/m/01f8m5"
+  id: 64
+  display_name: "Blue jay"
+}
+item {
+  name: "/m/01f91_"
+  id: 65
+  display_name: "Pretzel"
+}
+item {
+  name: "/m/01fb_0"
+  id: 66
+  display_name: "Bagel"
+}
+item {
+  name: "/m/01fdzj"
+  id: 67
+  display_name: "Tower"
+}
+item {
+  name: "/m/01fh4r"
+  id: 68
+  display_name: "Teapot"
+}
+item {
+  name: "/m/01g317"
+  id: 69
+  display_name: "Person"
+}
+item {
+  name: "/m/01g3x7"
+  id: 70
+  display_name: "Bow and arrow"
+}
+item {
+  name: "/m/01gkx_"
+  id: 71
+  display_name: "Swimwear"
+}
+item {
+  name: "/m/01gllr"
+  id: 72
+  display_name: "Beehive"
+}
+item {
+  name: "/m/01gmv2"
+  id: 73
+  display_name: "Brassiere"
+}
+item {
+  name: "/m/01h3n"
+  id: 74
+  display_name: "Bee"
+}
+item {
+  name: "/m/01h44"
+  id: 75
+  display_name: "Bat"
+}
+item {
+  name: "/m/01h8tj"
+  id: 76
+  display_name: "Starfish"
+}
+item {
+  name: "/m/01hrv5"
+  id: 77
+  display_name: "Popcorn"
+}
+item {
+  name: "/m/01j3zr"
+  id: 78
+  display_name: "Burrito"
+}
+item {
+  name: "/m/01j4z9"
+  id: 79
+  display_name: "Chainsaw"
+}
+item {
+  name: "/m/01j51"
+  id: 80
+  display_name: "Balloon"
+}
+item {
+  name: "/m/01j5ks"
+  id: 81
+  display_name: "Wrench"
+}
+item {
+  name: "/m/01j61q"
+  id: 82
+  display_name: "Tent"
+}
+item {
+  name: "/m/01jfm_"
+  id: 83
+  display_name: "Vehicle registration plate"
+}
+item {
+  name: "/m/01jfsr"
+  id: 84
+  display_name: "Lantern"
+}
+item {
+  name: "/m/01k6s3"
+  id: 85
+  display_name: "Toaster"
+}
+item {
+  name: "/m/01kb5b"
+  id: 86
+  display_name: "Flashlight"
+}
+item {
+  name: "/m/01knjb"
+  id: 87
+  display_name: "Billboard"
+}
+item {
+  name: "/m/01krhy"
+  id: 88
+  display_name: "Tiara"
+}
+item {
+  name: "/m/01lcw4"
+  id: 89
+  display_name: "Limousine"
+}
+item {
+  name: "/m/01llwg"
+  id: 90
+  display_name: "Necklace"
+}
+item {
+  name: "/m/01lrl"
+  id: 91
+  display_name: "Carnivore"
+}
+item {
+  name: "/m/01lsmm"
+  id: 92
+  display_name: "Scissors"
+}
+item {
+  name: "/m/01lynh"
+  id: 93
+  display_name: "Stairs"
+}
+item {
+  name: "/m/01m2v"
+  id: 94
+  display_name: "Computer keyboard"
+}
+item {
+  name: "/m/01m4t"
+  id: 95
+  display_name: "Printer"
+}
+item {
+  name: "/m/01mqdt"
+  id: 96
+  display_name: "Traffic sign"
+}
+item {
+  name: "/m/01mzpv"
+  id: 97
+  display_name: "Chair"
+}
+item {
+  name: "/m/01n4qj"
+  id: 98
+  display_name: "Shirt"
+}
+item {
+  name: "/m/01n5jq"
+  id: 99
+  display_name: "Poster"
+}
+item {
+  name: "/m/01nkt"
+  id: 100
+  display_name: "Cheese"
+}
+item {
+  name: "/m/01nq26"
+  id: 101
+  display_name: "Sock"
+}
+item {
+  name: "/m/01pns0"
+  id: 102
+  display_name: "Fire hydrant"
+}
+item {
+  name: "/m/01prls"
+  id: 103
+  display_name: "Land vehicle"
+}
+item {
+  name: "/m/01r546"
+  id: 104
+  display_name: "Earrings"
+}
+item {
+  name: "/m/01rkbr"
+  id: 105
+  display_name: "Tie"
+}
+item {
+  name: "/m/01rzcn"
+  id: 106
+  display_name: "Watercraft"
+}
+item {
+  name: "/m/01s105"
+  id: 107
+  display_name: "Cabinetry"
+}
+item {
+  name: "/m/01s55n"
+  id: 108
+  display_name: "Suitcase"
+}
+item {
+  name: "/m/01tcjp"
+  id: 109
+  display_name: "Muffin"
+}
+item {
+  name: "/m/01vbnl"
+  id: 110
+  display_name: "Bidet"
+}
+item {
+  name: "/m/01ww8y"
+  id: 111
+  display_name: "Snack"
+}
+item {
+  name: "/m/01x3jk"
+  id: 112
+  display_name: "Snowmobile"
+}
+item {
+  name: "/m/01x3z"
+  id: 113
+  display_name: "Clock"
+}
+item {
+  name: "/m/01xgg_"
+  id: 114
+  display_name: "Medical equipment"
+}
+item {
+  name: "/m/01xq0k1"
+  id: 115
+  display_name: "Cattle"
+}
+item {
+  name: "/m/01xqw"
+  id: 116
+  display_name: "Cello"
+}
+item {
+  name: "/m/01xs3r"
+  id: 117
+  display_name: "Jet ski"
+}
+item {
+  name: "/m/01x_v"
+  id: 118
+  display_name: "Camel"
+}
+item {
+  name: "/m/01xygc"
+  id: 119
+  display_name: "Coat"
+}
+item {
+  name: "/m/01xyhv"
+  id: 120
+  display_name: "Suit"
+}
+item {
+  name: "/m/01y9k5"
+  id: 121
+  display_name: "Desk"
+}
+item {
+  name: "/m/01yrx"
+  id: 122
+  display_name: "Cat"
+}
+item {
+  name: "/m/01yx86"
+  id: 123
+  display_name: "Bronze sculpture"
+}
+item {
+  name: "/m/01z1kdw"
+  id: 124
+  display_name: "Juice"
+}
+item {
+  name: "/m/02068x"
+  id: 125
+  display_name: "Gondola"
+}
+item {
+  name: "/m/020jm"
+  id: 126
+  display_name: "Beetle"
+}
+item {
+  name: "/m/020kz"
+  id: 127
+  display_name: "Cannon"
+}
+item {
+  name: "/m/020lf"
+  id: 128
+  display_name: "Computer mouse"
+}
+item {
+  name: "/m/021mn"
+  id: 129
+  display_name: "Cookie"
+}
+item {
+  name: "/m/021sj1"
+  id: 130
+  display_name: "Office building"
+}
+item {
+  name: "/m/0220r2"
+  id: 131
+  display_name: "Fountain"
+}
+item {
+  name: "/m/0242l"
+  id: 132
+  display_name: "Coin"
+}
+item {
+  name: "/m/024d2"
+  id: 133
+  display_name: "Calculator"
+}
+item {
+  name: "/m/024g6"
+  id: 134
+  display_name: "Cocktail"
+}
+item {
+  name: "/m/02522"
+  id: 135
+  display_name: "Computer monitor"
+}
+item {
+  name: "/m/025dyy"
+  id: 136
+  display_name: "Box"
+}
+item {
+  name: "/m/025fsf"
+  id: 137
+  display_name: "Stapler"
+}
+item {
+  name: "/m/025nd"
+  id: 138
+  display_name: "Christmas tree"
+}
+item {
+  name: "/m/025rp__"
+  id: 139
+  display_name: "Cowboy hat"
+}
+item {
+  name: "/m/0268lbt"
+  id: 140
+  display_name: "Hiking equipment"
+}
+item {
+  name: "/m/026qbn5"
+  id: 141
+  display_name: "Studio couch"
+}
+item {
+  name: "/m/026t6"
+  id: 142
+  display_name: "Drum"
+}
+item {
+  name: "/m/0270h"
+  id: 143
+  display_name: "Dessert"
+}
+item {
+  name: "/m/0271qf7"
+  id: 144
+  display_name: "Wine rack"
+}
+item {
+  name: "/m/0271t"
+  id: 145
+  display_name: "Drink"
+}
+item {
+  name: "/m/027pcv"
+  id: 146
+  display_name: "Zucchini"
+}
+item {
+  name: "/m/027rl48"
+  id: 147
+  display_name: "Ladle"
+}
+item {
+  name: "/m/0283dt1"
+  id: 148
+  display_name: "Human mouth"
+}
+item {
+  name: "/m/0284d"
+  id: 149
+  display_name: "Dairy"
+}
+item {
+  name: "/m/029b3"
+  id: 150
+  display_name: "Dice"
+}
+item {
+  name: "/m/029bxz"
+  id: 151
+  display_name: "Oven"
+}
+item {
+  name: "/m/029tx"
+  id: 152
+  display_name: "Dinosaur"
+}
+item {
+  name: "/m/02bm9n"
+  id: 153
+  display_name: "Ratchet"
+}
+item {
+  name: "/m/02crq1"
+  id: 154
+  display_name: "Couch"
+}
+item {
+  name: "/m/02ctlc"
+  id: 155
+  display_name: "Cricket ball"
+}
+item {
+  name: "/m/02cvgx"
+  id: 156
+  display_name: "Winter melon"
+}
+item {
+  name: "/m/02d1br"
+  id: 157
+  display_name: "Spatula"
+}
+item {
+  name: "/m/02d9qx"
+  id: 158
+  display_name: "Whiteboard"
+}
+item {
+  name: "/m/02ddwp"
+  id: 159
+  display_name: "Pencil sharpener"
+}
+item {
+  name: "/m/02dgv"
+  id: 160
+  display_name: "Door"
+}
+item {
+  name: "/m/02dl1y"
+  id: 161
+  display_name: "Hat"
+}
+item {
+  name: "/m/02f9f_"
+  id: 162
+  display_name: "Shower"
+}
+item {
+  name: "/m/02fh7f"
+  id: 163
+  display_name: "Eraser"
+}
+item {
+  name: "/m/02fq_6"
+  id: 164
+  display_name: "Fedora"
+}
+item {
+  name: "/m/02g30s"
+  id: 165
+  display_name: "Guacamole"
+}
+item {
+  name: "/m/02gzp"
+  id: 166
+  display_name: "Dagger"
+}
+item {
+  name: "/m/02h19r"
+  id: 167
+  display_name: "Scarf"
+}
+item {
+  name: "/m/02hj4"
+  id: 168
+  display_name: "Dolphin"
+}
+item {
+  name: "/m/02jfl0"
+  id: 169
+  display_name: "Sombrero"
+}
+item {
+  name: "/m/02jnhm"
+  id: 170
+  display_name: "Tin can"
+}
+item {
+  name: "/m/02jvh9"
+  id: 171
+  display_name: "Mug"
+}
+item {
+  name: "/m/02jz0l"
+  id: 172
+  display_name: "Tap"
+}
+item {
+  name: "/m/02l8p9"
+  id: 173
+  display_name: "Harbor seal"
+}
+item {
+  name: "/m/02lbcq"
+  id: 174
+  display_name: "Stretcher"
+}
+item {
+  name: "/m/02mqfb"
+  id: 175
+  display_name: "Can opener"
+}
+item {
+  name: "/m/02_n6y"
+  id: 176
+  display_name: "Goggles"
+}
+item {
+  name: "/m/02p0tk3"
+  id: 177
+  display_name: "Human body"
+}
+item {
+  name: "/m/02p3w7d"
+  id: 178
+  display_name: "Roller skates"
+}
+item {
+  name: "/m/02p5f1q"
+  id: 179
+  display_name: "Coffee cup"
+}
+item {
+  name: "/m/02pdsw"
+  id: 180
+  display_name: "Cutting board"
+}
+item {
+  name: "/m/02pjr4"
+  id: 181
+  display_name: "Blender"
+}
+item {
+  name: "/m/02pkr5"
+  id: 182
+  display_name: "Plumbing fixture"
+}
+item {
+  name: "/m/02pv19"
+  id: 183
+  display_name: "Stop sign"
+}
+item {
+  name: "/m/02rdsp"
+  id: 184
+  display_name: "Office supplies"
+}
+item {
+  name: "/m/02rgn06"
+  id: 185
+  display_name: "Volleyball"
+}
+item {
+  name: "/m/02s195"
+  id: 186
+  display_name: "Vase"
+}
+item {
+  name: "/m/02tsc9"
+  id: 187
+  display_name: "Slow cooker"
+}
+item {
+  name: "/m/02vkqh8"
+  id: 188
+  display_name: "Wardrobe"
+}
+item {
+  name: "/m/02vqfm"
+  id: 189
+  display_name: "Coffee"
+}
+item {
+  name: "/m/02vwcm"
+  id: 190
+  display_name: "Whisk"
+}
+item {
+  name: "/m/02w3r3"
+  id: 191
+  display_name: "Paper towel"
+}
+item {
+  name: "/m/02w3_ws"
+  id: 192
+  display_name: "Personal care"
+}
+item {
+  name: "/m/02wbm"
+  id: 193
+  display_name: "Food"
+}
+item {
+  name: "/m/02wbtzl"
+  id: 194
+  display_name: "Sun hat"
+}
+item {
+  name: "/m/02wg_p"
+  id: 195
+  display_name: "Tree house"
+}
+item {
+  name: "/m/02wmf"
+  id: 196
+  display_name: "Flying disc"
+}
+item {
+  name: "/m/02wv6h6"
+  id: 197
+  display_name: "Skirt"
+}
+item {
+  name: "/m/02wv84t"
+  id: 198
+  display_name: "Gas stove"
+}
+item {
+  name: "/m/02x8cch"
+  id: 199
+  display_name: "Salt and pepper shakers"
+}
+item {
+  name: "/m/02x984l"
+  id: 200
+  display_name: "Mechanical fan"
+}
+item {
+  name: "/m/02xb7qb"
+  id: 201
+  display_name: "Face powder"
+}
+item {
+  name: "/m/02xqq"
+  id: 202
+  display_name: "Fax"
+}
+item {
+  name: "/m/02xwb"
+  id: 203
+  display_name: "Fruit"
+}
+item {
+  name: "/m/02y6n"
+  id: 204
+  display_name: "French fries"
+}
+item {
+  name: "/m/02z51p"
+  id: 205
+  display_name: "Nightstand"
+}
+item {
+  name: "/m/02zn6n"
+  id: 206
+  display_name: "Barrel"
+}
+item {
+  name: "/m/02zt3"
+  id: 207
+  display_name: "Kite"
+}
+item {
+  name: "/m/02zvsm"
+  id: 208
+  display_name: "Tart"
+}
+item {
+  name: "/m/030610"
+  id: 209
+  display_name: "Treadmill"
+}
+item {
+  name: "/m/0306r"
+  id: 210
+  display_name: "Fox"
+}
+item {
+  name: "/m/03120"
+  id: 211
+  display_name: "Flag"
+}
+item {
+  name: "/m/0319l"
+  id: 212
+  display_name: "Horn"
+}
+item {
+  name: "/m/031b6r"
+  id: 213
+  display_name: "Window blind"
+}
+item {
+  name: "/m/031n1"
+  id: 214
+  display_name: "Human foot"
+}
+item {
+  name: "/m/0323sq"
+  id: 215
+  display_name: "Golf cart"
+}
+item {
+  name: "/m/032b3c"
+  id: 216
+  display_name: "Jacket"
+}
+item {
+  name: "/m/033cnk"
+  id: 217
+  display_name: "Egg"
+}
+item {
+  name: "/m/033rq4"
+  id: 218
+  display_name: "Street light"
+}
+item {
+  name: "/m/0342h"
+  id: 219
+  display_name: "Guitar"
+}
+item {
+  name: "/m/034c16"
+  id: 220
+  display_name: "Pillow"
+}
+item {
+  name: "/m/035r7c"
+  id: 221
+  display_name: "Human leg"
+}
+item {
+  name: "/m/035vxb"
+  id: 222
+  display_name: "Isopod"
+}
+item {
+  name: "/m/0388q"
+  id: 223
+  display_name: "Grape"
+}
+item {
+  name: "/m/039xj_"
+  id: 224
+  display_name: "Human ear"
+}
+item {
+  name: "/m/03bbps"
+  id: 225
+  display_name: "Power plugs and sockets"
+}
+item {
+  name: "/m/03bj1"
+  id: 226
+  display_name: "Panda"
+}
+item {
+  name: "/m/03bk1"
+  id: 227
+  display_name: "Giraffe"
+}
+item {
+  name: "/m/03bt1vf"
+  id: 228
+  display_name: "Woman"
+}
+item {
+  name: "/m/03c7gz"
+  id: 229
+  display_name: "Door handle"
+}
+item {
+  name: "/m/03d443"
+  id: 230
+  display_name: "Rhinoceros"
+}
+item {
+  name: "/m/03dnzn"
+  id: 231
+  display_name: "Bathtub"
+}
+item {
+  name: "/m/03fj2"
+  id: 232
+  display_name: "Goldfish"
+}
+item {
+  name: "/m/03fp41"
+  id: 233
+  display_name: "Houseplant"
+}
+item {
+  name: "/m/03fwl"
+  id: 234
+  display_name: "Goat"
+}
+item {
+  name: "/m/03g8mr"
+  id: 235
+  display_name: "Baseball bat"
+}
+item {
+  name: "/m/03grzl"
+  id: 236
+  display_name: "Baseball glove"
+}
+item {
+  name: "/m/03hj559"
+  id: 237
+  display_name: "Mixing bowl"
+}
+item {
+  name: "/m/03hl4l9"
+  id: 238
+  display_name: "Marine invertebrates"
+}
+item {
+  name: "/m/03hlz0c"
+  id: 239
+  display_name: "Kitchen utensil"
+}
+item {
+  name: "/m/03jbxj"
+  id: 240
+  display_name: "Light switch"
+}
+item {
+  name: "/m/03jm5"
+  id: 241
+  display_name: "House"
+}
+item {
+  name: "/m/03k3r"
+  id: 242
+  display_name: "Horse"
+}
+item {
+  name: "/m/03kt2w"
+  id: 243
+  display_name: "Stationary bicycle"
+}
+item {
+  name: "/m/03l9g"
+  id: 244
+  display_name: "Hammer"
+}
+item {
+  name: "/m/03ldnb"
+  id: 245
+  display_name: "Ceiling fan"
+}
+item {
+  name: "/m/03m3pdh"
+  id: 246
+  display_name: "Sofa bed"
+}
+item {
+  name: "/m/03m3vtv"
+  id: 247
+  display_name: "Adhesive tape"
+}
+item {
+  name: "/m/03m5k"
+  id: 248
+  display_name: "Harp"
+}
+item {
+  name: "/m/03nfch"
+  id: 249
+  display_name: "Sandal"
+}
+item {
+  name: "/m/03p3bw"
+  id: 250
+  display_name: "Bicycle helmet"
+}
+item {
+  name: "/m/03q5c7"
+  id: 251
+  display_name: "Saucer"
+}
+item {
+  name: "/m/03q5t"
+  id: 252
+  display_name: "Harpsichord"
+}
+item {
+  name: "/m/03q69"
+  id: 253
+  display_name: "Human hair"
+}
+item {
+  name: "/m/03qhv5"
+  id: 254
+  display_name: "Heater"
+}
+item {
+  name: "/m/03qjg"
+  id: 255
+  display_name: "Harmonica"
+}
+item {
+  name: "/m/03qrc"
+  id: 256
+  display_name: "Hamster"
+}
+item {
+  name: "/m/03rszm"
+  id: 257
+  display_name: "Curtain"
+}
+item {
+  name: "/m/03ssj5"
+  id: 258
+  display_name: "Bed"
+}
+item {
+  name: "/m/03s_tn"
+  id: 259
+  display_name: "Kettle"
+}
+item {
+  name: "/m/03tw93"
+  id: 260
+  display_name: "Fireplace"
+}
+item {
+  name: "/m/03txqz"
+  id: 261
+  display_name: "Scale"
+}
+item {
+  name: "/m/03v5tg"
+  id: 262
+  display_name: "Drinking straw"
+}
+item {
+  name: "/m/03vt0"
+  id: 263
+  display_name: "Insect"
+}
+item {
+  name: "/m/03wvsk"
+  id: 264
+  display_name: "Hair dryer"
+}
+item {
+  name: "/m/03_wxk"
+  id: 265
+  display_name: "Kitchenware"
+}
+item {
+  name: "/m/03wym"
+  id: 266
+  display_name: "Indoor rower"
+}
+item {
+  name: "/m/03xxp"
+  id: 267
+  display_name: "Invertebrate"
+}
+item {
+  name: "/m/03y6mg"
+  id: 268
+  display_name: "Food processor"
+}
+item {
+  name: "/m/03__z0"
+  id: 269
+  display_name: "Bookcase"
+}
+item {
+  name: "/m/040b_t"
+  id: 270
+  display_name: "Refrigerator"
+}
+item {
+  name: "/m/04169hn"
+  id: 271
+  display_name: "Wood-burning stove"
+}
+item {
+  name: "/m/0420v5"
+  id: 272
+  display_name: "Punching bag"
+}
+item {
+  name: "/m/043nyj"
+  id: 273
+  display_name: "Common fig"
+}
+item {
+  name: "/m/0440zs"
+  id: 274
+  display_name: "Cocktail shaker"
+}
+item {
+  name: "/m/0449p"
+  id: 275
+  display_name: "Jaguar"
+}
+item {
+  name: "/m/044r5d"
+  id: 276
+  display_name: "Golf ball"
+}
+item {
+  name: "/m/0463sg"
+  id: 277
+  display_name: "Fashion accessory"
+}
+item {
+  name: "/m/046dlr"
+  id: 278
+  display_name: "Alarm clock"
+}
+item {
+  name: "/m/047j0r"
+  id: 279
+  display_name: "Filing cabinet"
+}
+item {
+  name: "/m/047v4b"
+  id: 280
+  display_name: "Artichoke"
+}
+item {
+  name: "/m/04bcr3"
+  id: 281
+  display_name: "Table"
+}
+item {
+  name: "/m/04brg2"
+  id: 282
+  display_name: "Tableware"
+}
+item {
+  name: "/m/04c0y"
+  id: 283
+  display_name: "Kangaroo"
+}
+item {
+  name: "/m/04cp_"
+  id: 284
+  display_name: "Koala"
+}
+item {
+  name: "/m/04ctx"
+  id: 285
+  display_name: "Knife"
+}
+item {
+  name: "/m/04dr76w"
+  id: 286
+  display_name: "Bottle"
+}
+item {
+  name: "/m/04f5ws"
+  id: 287
+  display_name: "Bottle opener"
+}
+item {
+  name: "/m/04g2r"
+  id: 288
+  display_name: "Lynx"
+}
+item {
+  name: "/m/04gth"
+  id: 289
+  display_name: "Lavender"
+}
+item {
+  name: "/m/04h7h"
+  id: 290
+  display_name: "Lighthouse"
+}
+item {
+  name: "/m/04h8sr"
+  id: 291
+  display_name: "Dumbbell"
+}
+item {
+  name: "/m/04hgtk"
+  id: 292
+  display_name: "Human head"
+}
+item {
+  name: "/m/04kkgm"
+  id: 293
+  display_name: "Bowl"
+}
+item {
+  name: "/m/04lvq_"
+  id: 294
+  display_name: "Humidifier"
+}
+item {
+  name: "/m/04m6gz"
+  id: 295
+  display_name: "Porch"
+}
+item {
+  name: "/m/04m9y"
+  id: 296
+  display_name: "Lizard"
+}
+item {
+  name: "/m/04p0qw"
+  id: 297
+  display_name: "Billiard table"
+}
+item {
+  name: "/m/04rky"
+  id: 298
+  display_name: "Mammal"
+}
+item {
+  name: "/m/04rmv"
+  id: 299
+  display_name: "Mouse"
+}
+item {
+  name: "/m/04_sv"
+  id: 300
+  display_name: "Motorcycle"
+}
+item {
+  name: "/m/04szw"
+  id: 301
+  display_name: "Musical instrument"
+}
+item {
+  name: "/m/04tn4x"
+  id: 302
+  display_name: "Swim cap"
+}
+item {
+  name: "/m/04v6l4"
+  id: 303
+  display_name: "Frying pan"
+}
+item {
+  name: "/m/04vv5k"
+  id: 304
+  display_name: "Snowplow"
+}
+item {
+  name: "/m/04y4h8h"
+  id: 305
+  display_name: "Bathroom cabinet"
+}
+item {
+  name: "/m/04ylt"
+  id: 306
+  display_name: "Missile"
+}
+item {
+  name: "/m/04yqq2"
+  id: 307
+  display_name: "Bust"
+}
+item {
+  name: "/m/04yx4"
+  id: 308
+  display_name: "Man"
+}
+item {
+  name: "/m/04z4wx"
+  id: 309
+  display_name: "Waffle iron"
+}
+item {
+  name: "/m/04zpv"
+  id: 310
+  display_name: "Milk"
+}
+item {
+  name: "/m/04zwwv"
+  id: 311
+  display_name: "Ring binder"
+}
+item {
+  name: "/m/050gv4"
+  id: 312
+  display_name: "Plate"
+}
+item {
+  name: "/m/050k8"
+  id: 313
+  display_name: "Mobile phone"
+}
+item {
+  name: "/m/052lwg6"
+  id: 314
+  display_name: "Baked goods"
+}
+item {
+  name: "/m/052sf"
+  id: 315
+  display_name: "Mushroom"
+}
+item {
+  name: "/m/05441v"
+  id: 316
+  display_name: "Crutch"
+}
+item {
+  name: "/m/054fyh"
+  id: 317
+  display_name: "Pitcher"
+}
+item {
+  name: "/m/054_l"
+  id: 318
+  display_name: "Mirror"
+}
+item {
+  name: "/m/054xkw"
+  id: 319
+  display_name: "Lifejacket"
+}
+item {
+  name: "/m/05_5p_0"
+  id: 320
+  display_name: "Table tennis racket"
+}
+item {
+  name: "/m/05676x"
+  id: 321
+  display_name: "Pencil case"
+}
+item {
+  name: "/m/057cc"
+  id: 322
+  display_name: "Musical keyboard"
+}
+item {
+  name: "/m/057p5t"
+  id: 323
+  display_name: "Scoreboard"
+}
+item {
+  name: "/m/0584n8"
+  id: 324
+  display_name: "Briefcase"
+}
+item {
+  name: "/m/058qzx"
+  id: 325
+  display_name: "Kitchen knife"
+}
+item {
+  name: "/m/05bm6"
+  id: 326
+  display_name: "Nail"
+}
+item {
+  name: "/m/05ctyq"
+  id: 327
+  display_name: "Tennis ball"
+}
+item {
+  name: "/m/05gqfk"
+  id: 328
+  display_name: "Plastic bag"
+}
+item {
+  name: "/m/05kms"
+  id: 329
+  display_name: "Oboe"
+}
+item {
+  name: "/m/05kyg_"
+  id: 330
+  display_name: "Chest of drawers"
+}
+item {
+  name: "/m/05n4y"
+  id: 331
+  display_name: "Ostrich"
+}
+item {
+  name: "/m/05r5c"
+  id: 332
+  display_name: "Piano"
+}
+item {
+  name: "/m/05r655"
+  id: 333
+  display_name: "Girl"
+}
+item {
+  name: "/m/05s2s"
+  id: 334
+  display_name: "Plant"
+}
+item {
+  name: "/m/05vtc"
+  id: 335
+  display_name: "Potato"
+}
+item {
+  name: "/m/05w9t9"
+  id: 336
+  display_name: "Hair spray"
+}
+item {
+  name: "/m/05y5lj"
+  id: 337
+  display_name: "Sports equipment"
+}
+item {
+  name: "/m/05z55"
+  id: 338
+  display_name: "Pasta"
+}
+item {
+  name: "/m/05z6w"
+  id: 339
+  display_name: "Penguin"
+}
+item {
+  name: "/m/05zsy"
+  id: 340
+  display_name: "Pumpkin"
+}
+item {
+  name: "/m/061_f"
+  id: 341
+  display_name: "Pear"
+}
+item {
+  name: "/m/061hd_"
+  id: 342
+  display_name: "Infant bed"
+}
+item {
+  name: "/m/0633h"
+  id: 343
+  display_name: "Polar bear"
+}
+item {
+  name: "/m/063rgb"
+  id: 344
+  display_name: "Mixer"
+}
+item {
+  name: "/m/0642b4"
+  id: 345
+  display_name: "Cupboard"
+}
+item {
+  name: "/m/065h6l"
+  id: 346
+  display_name: "Jacuzzi"
+}
+item {
+  name: "/m/0663v"
+  id: 347
+  display_name: "Pizza"
+}
+item {
+  name: "/m/06_72j"
+  id: 348
+  display_name: "Digital clock"
+}
+item {
+  name: "/m/068zj"
+  id: 349
+  display_name: "Pig"
+}
+item {
+  name: "/m/06bt6"
+  id: 350
+  display_name: "Reptile"
+}
+item {
+  name: "/m/06c54"
+  id: 351
+  display_name: "Rifle"
+}
+item {
+  name: "/m/06c7f7"
+  id: 352
+  display_name: "Lipstick"
+}
+item {
+  name: "/m/06_fw"
+  id: 353
+  display_name: "Skateboard"
+}
+item {
+  name: "/m/06j2d"
+  id: 354
+  display_name: "Raven"
+}
+item {
+  name: "/m/06k2mb"
+  id: 355
+  display_name: "High heels"
+}
+item {
+  name: "/m/06l9r"
+  id: 356
+  display_name: "Red panda"
+}
+item {
+  name: "/m/06m11"
+  id: 357
+  display_name: "Rose"
+}
+item {
+  name: "/m/06mf6"
+  id: 358
+  display_name: "Rabbit"
+}
+item {
+  name: "/m/06msq"
+  id: 359
+  display_name: "Sculpture"
+}
+item {
+  name: "/m/06ncr"
+  id: 360
+  display_name: "Saxophone"
+}
+item {
+  name: "/m/06nrc"
+  id: 361
+  display_name: "Shotgun"
+}
+item {
+  name: "/m/06nwz"
+  id: 362
+  display_name: "Seafood"
+}
+item {
+  name: "/m/06pcq"
+  id: 363
+  display_name: "Submarine sandwich"
+}
+item {
+  name: "/m/06__v"
+  id: 364
+  display_name: "Snowboard"
+}
+item {
+  name: "/m/06y5r"
+  id: 365
+  display_name: "Sword"
+}
+item {
+  name: "/m/06z37_"
+  id: 366
+  display_name: "Picture frame"
+}
+item {
+  name: "/m/07030"
+  id: 367
+  display_name: "Sushi"
+}
+item {
+  name: "/m/0703r8"
+  id: 368
+  display_name: "Loveseat"
+}
+item {
+  name: "/m/071p9"
+  id: 369
+  display_name: "Ski"
+}
+item {
+  name: "/m/071qp"
+  id: 370
+  display_name: "Squirrel"
+}
+item {
+  name: "/m/073bxn"
+  id: 371
+  display_name: "Tripod"
+}
+item {
+  name: "/m/073g6"
+  id: 372
+  display_name: "Stethoscope"
+}
+item {
+  name: "/m/074d1"
+  id: 373
+  display_name: "Submarine"
+}
+item {
+  name: "/m/0755b"
+  id: 374
+  display_name: "Scorpion"
+}
+item {
+  name: "/m/076bq"
+  id: 375
+  display_name: "Segway"
+}
+item {
+  name: "/m/076lb9"
+  id: 376
+  display_name: "Training bench"
+}
+item {
+  name: "/m/078jl"
+  id: 377
+  display_name: "Snake"
+}
+item {
+  name: "/m/078n6m"
+  id: 378
+  display_name: "Coffee table"
+}
+item {
+  name: "/m/079cl"
+  id: 379
+  display_name: "Skyscraper"
+}
+item {
+  name: "/m/07bgp"
+  id: 380
+  display_name: "Sheep"
+}
+item {
+  name: "/m/07c52"
+  id: 381
+  display_name: "Television"
+}
+item {
+  name: "/m/07c6l"
+  id: 382
+  display_name: "Trombone"
+}
+item {
+  name: "/m/07clx"
+  id: 383
+  display_name: "Tea"
+}
+item {
+  name: "/m/07cmd"
+  id: 384
+  display_name: "Tank"
+}
+item {
+  name: "/m/07crc"
+  id: 385
+  display_name: "Taco"
+}
+item {
+  name: "/m/07cx4"
+  id: 386
+  display_name: "Telephone"
+}
+item {
+  name: "/m/07dd4"
+  id: 387
+  display_name: "Torch"
+}
+item {
+  name: "/m/07dm6"
+  id: 388
+  display_name: "Tiger"
+}
+item {
+  name: "/m/07fbm7"
+  id: 389
+  display_name: "Strawberry"
+}
+item {
+  name: "/m/07gql"
+  id: 390
+  display_name: "Trumpet"
+}
+item {
+  name: "/m/07j7r"
+  id: 391
+  display_name: "Tree"
+}
+item {
+  name: "/m/07j87"
+  id: 392
+  display_name: "Tomato"
+}
+item {
+  name: "/m/07jdr"
+  id: 393
+  display_name: "Train"
+}
+item {
+  name: "/m/07k1x"
+  id: 394
+  display_name: "Tool"
+}
+item {
+  name: "/m/07kng9"
+  id: 395
+  display_name: "Picnic basket"
+}
+item {
+  name: "/m/07mcwg"
+  id: 396
+  display_name: "Cooking spray"
+}
+item {
+  name: "/m/07mhn"
+  id: 397
+  display_name: "Trousers"
+}
+item {
+  name: "/m/07pj7bq"
+  id: 398
+  display_name: "Bowling equipment"
+}
+item {
+  name: "/m/07qxg_"
+  id: 399
+  display_name: "Football helmet"
+}
+item {
+  name: "/m/07r04"
+  id: 400
+  display_name: "Truck"
+}
+item {
+  name: "/m/07v9_z"
+  id: 401
+  display_name: "Measuring cup"
+}
+item {
+  name: "/m/07xyvk"
+  id: 402
+  display_name: "Coffeemaker"
+}
+item {
+  name: "/m/07y_7"
+  id: 403
+  display_name: "Violin"
+}
+item {
+  name: "/m/07yv9"
+  id: 404
+  display_name: "Vehicle"
+}
+item {
+  name: "/m/080hkjn"
+  id: 405
+  display_name: "Handbag"
+}
+item {
+  name: "/m/080n7g"
+  id: 406
+  display_name: "Paper cutter"
+}
+item {
+  name: "/m/081qc"
+  id: 407
+  display_name: "Wine"
+}
+item {
+  name: "/m/083kb"
+  id: 408
+  display_name: "Weapon"
+}
+item {
+  name: "/m/083wq"
+  id: 409
+  display_name: "Wheel"
+}
+item {
+  name: "/m/084hf"
+  id: 410
+  display_name: "Worm"
+}
+item {
+  name: "/m/084rd"
+  id: 411
+  display_name: "Wok"
+}
+item {
+  name: "/m/084zz"
+  id: 412
+  display_name: "Whale"
+}
+item {
+  name: "/m/0898b"
+  id: 413
+  display_name: "Zebra"
+}
+item {
+  name: "/m/08dz3q"
+  id: 414
+  display_name: "Auto part"
+}
+item {
+  name: "/m/08hvt4"
+  id: 415
+  display_name: "Jug"
+}
+item {
+  name: "/m/08ks85"
+  id: 416
+  display_name: "Pizza cutter"
+}
+item {
+  name: "/m/08p92x"
+  id: 417
+  display_name: "Cream"
+}
+item {
+  name: "/m/08pbxl"
+  id: 418
+  display_name: "Monkey"
+}
+item {
+  name: "/m/096mb"
+  id: 419
+  display_name: "Lion"
+}
+item {
+  name: "/m/09728"
+  id: 420
+  display_name: "Bread"
+}
+item {
+  name: "/m/099ssp"
+  id: 421
+  display_name: "Platter"
+}
+item {
+  name: "/m/09b5t"
+  id: 422
+  display_name: "Chicken"
+}
+item {
+  name: "/m/09csl"
+  id: 423
+  display_name: "Eagle"
+}
+item {
+  name: "/m/09ct_"
+  id: 424
+  display_name: "Helicopter"
+}
+item {
+  name: "/m/09d5_"
+  id: 425
+  display_name: "Owl"
+}
+item {
+  name: "/m/09ddx"
+  id: 426
+  display_name: "Duck"
+}
+item {
+  name: "/m/09dzg"
+  id: 427
+  display_name: "Turtle"
+}
+item {
+  name: "/m/09f20"
+  id: 428
+  display_name: "Hippopotamus"
+}
+item {
+  name: "/m/09f_2"
+  id: 429
+  display_name: "Crocodile"
+}
+item {
+  name: "/m/09g1w"
+  id: 430
+  display_name: "Toilet"
+}
+item {
+  name: "/m/09gtd"
+  id: 431
+  display_name: "Toilet paper"
+}
+item {
+  name: "/m/09gys"
+  id: 432
+  display_name: "Squid"
+}
+item {
+  name: "/m/09j2d"
+  id: 433
+  display_name: "Clothing"
+}
+item {
+  name: "/m/09j5n"
+  id: 434
+  display_name: "Footwear"
+}
+item {
+  name: "/m/09k_b"
+  id: 435
+  display_name: "Lemon"
+}
+item {
+  name: "/m/09kmb"
+  id: 436
+  display_name: "Spider"
+}
+item {
+  name: "/m/09kx5"
+  id: 437
+  display_name: "Deer"
+}
+item {
+  name: "/m/09ld4"
+  id: 438
+  display_name: "Frog"
+}
+item {
+  name: "/m/09qck"
+  id: 439
+  display_name: "Banana"
+}
+item {
+  name: "/m/09rvcxw"
+  id: 440
+  display_name: "Rocket"
+}
+item {
+  name: "/m/09tvcd"
+  id: 441
+  display_name: "Wine glass"
+}
+item {
+  name: "/m/0b3fp9"
+  id: 442
+  display_name: "Countertop"
+}
+item {
+  name: "/m/0bh9flk"
+  id: 443
+  display_name: "Tablet computer"
+}
+item {
+  name: "/m/0bjyj5"
+  id: 444
+  display_name: "Waste container"
+}
+item {
+  name: "/m/0b_rs"
+  id: 445
+  display_name: "Swimming pool"
+}
+item {
+  name: "/m/0bt9lr"
+  id: 446
+  display_name: "Dog"
+}
+item {
+  name: "/m/0bt_c3"
+  id: 447
+  display_name: "Book"
+}
+item {
+  name: "/m/0bwd_0j"
+  id: 448
+  display_name: "Elephant"
+}
+item {
+  name: "/m/0by6g"
+  id: 449
+  display_name: "Shark"
+}
+item {
+  name: "/m/0c06p"
+  id: 450
+  display_name: "Candle"
+}
+item {
+  name: "/m/0c29q"
+  id: 451
+  display_name: "Leopard"
+}
+item {
+  name: "/m/0c2jj"
+  id: 452
+  display_name: "Axe"
+}
+item {
+  name: "/m/0c3m8g"
+  id: 453
+  display_name: "Hand dryer"
+}
+item {
+  name: "/m/0c3mkw"
+  id: 454
+  display_name: "Soap dispenser"
+}
+item {
+  name: "/m/0c568"
+  id: 455
+  display_name: "Porcupine"
+}
+item {
+  name: "/m/0c9ph5"
+  id: 456
+  display_name: "Flower"
+}
+item {
+  name: "/m/0ccs93"
+  id: 457
+  display_name: "Canary"
+}
+item {
+  name: "/m/0cd4d"
+  id: 458
+  display_name: "Cheetah"
+}
+item {
+  name: "/m/0cdl1"
+  id: 459
+  display_name: "Palm tree"
+}
+item {
+  name: "/m/0cdn1"
+  id: 460
+  display_name: "Hamburger"
+}
+item {
+  name: "/m/0cffdh"
+  id: 461
+  display_name: "Maple"
+}
+item {
+  name: "/m/0cgh4"
+  id: 462
+  display_name: "Building"
+}
+item {
+  name: "/m/0ch_cf"
+  id: 463
+  display_name: "Fish"
+}
+item {
+  name: "/m/0cjq5"
+  id: 464
+  display_name: "Lobster"
+}
+item {
+  name: "/m/0cjs7"
+  id: 465
+  display_name: "Asparagus"
+}
+item {
+  name: "/m/0c_jw"
+  id: 466
+  display_name: "Furniture"
+}
+item {
+  name: "/m/0cl4p"
+  id: 467
+  display_name: "Hedgehog"
+}
+item {
+  name: "/m/0cmf2"
+  id: 468
+  display_name: "Airplane"
+}
+item {
+  name: "/m/0cmx8"
+  id: 469
+  display_name: "Spoon"
+}
+item {
+  name: "/m/0cn6p"
+  id: 470
+  display_name: "Otter"
+}
+item {
+  name: "/m/0cnyhnx"
+  id: 471
+  display_name: "Bull"
+}
+item {
+  name: "/m/0_cp5"
+  id: 472
+  display_name: "Oyster"
+}
+item {
+  name: "/m/0cqn2"
+  id: 473
+  display_name: "Horizontal bar"
+}
+item {
+  name: "/m/0crjs"
+  id: 474
+  display_name: "Convenience store"
+}
+item {
+  name: "/m/0ct4f"
+  id: 475
+  display_name: "Bomb"
+}
+item {
+  name: "/m/0cvnqh"
+  id: 476
+  display_name: "Bench"
+}
+item {
+  name: "/m/0cxn2"
+  id: 477
+  display_name: "Ice cream"
+}
+item {
+  name: "/m/0cydv"
+  id: 478
+  display_name: "Caterpillar"
+}
+item {
+  name: "/m/0cyf8"
+  id: 479
+  display_name: "Butterfly"
+}
+item {
+  name: "/m/0cyfs"
+  id: 480
+  display_name: "Parachute"
+}
+item {
+  name: "/m/0cyhj_"
+  id: 481
+  display_name: "Orange"
+}
+item {
+  name: "/m/0czz2"
+  id: 482
+  display_name: "Antelope"
+}
+item {
+  name: "/m/0d20w4"
+  id: 483
+  display_name: "Beaker"
+}
+item {
+  name: "/m/0d_2m"
+  id: 484
+  display_name: "Moths and butterflies"
+}
+item {
+  name: "/m/0d4v4"
+  id: 485
+  display_name: "Window"
+}
+item {
+  name: "/m/0d4w1"
+  id: 486
+  display_name: "Closet"
+}
+item {
+  name: "/m/0d5gx"
+  id: 487
+  display_name: "Castle"
+}
+item {
+  name: "/m/0d8zb"
+  id: 488
+  display_name: "Jellyfish"
+}
+item {
+  name: "/m/0dbvp"
+  id: 489
+  display_name: "Goose"
+}
+item {
+  name: "/m/0dbzx"
+  id: 490
+  display_name: "Mule"
+}
+item {
+  name: "/m/0dftk"
+  id: 491
+  display_name: "Swan"
+}
+item {
+  name: "/m/0dj6p"
+  id: 492
+  display_name: "Peach"
+}
+item {
+  name: "/m/0djtd"
+  id: 493
+  display_name: "Coconut"
+}
+item {
+  name: "/m/0dkzw"
+  id: 494
+  display_name: "Seat belt"
+}
+item {
+  name: "/m/0dq75"
+  id: 495
+  display_name: "Raccoon"
+}
+item {
+  name: "/m/0_dqb"
+  id: 496
+  display_name: "Chisel"
+}
+item {
+  name: "/m/0dt3t"
+  id: 497
+  display_name: "Fork"
+}
+item {
+  name: "/m/0dtln"
+  id: 498
+  display_name: "Lamp"
+}
+item {
+  name: "/m/0dv5r"
+  id: 499
+  display_name: "Camera"
+}
+item {
+  name: "/m/0dv77"
+  id: 500
+  display_name: "Squash"
+}
+item {
+  name: "/m/0dv9c"
+  id: 501
+  display_name: "Racket"
+}
+item {
+  name: "/m/0dzct"
+  id: 502
+  display_name: "Human face"
+}
+item {
+  name: "/m/0dzf4"
+  id: 503
+  display_name: "Human arm"
+}
+item {
+  name: "/m/0f4s2w"
+  id: 504
+  display_name: "Vegetable"
+}
+item {
+  name: "/m/0f571"
+  id: 505
+  display_name: "Diaper"
+}
+item {
+  name: "/m/0f6nr"
+  id: 506
+  display_name: "Unicycle"
+}
+item {
+  name: "/m/0f6wt"
+  id: 507
+  display_name: "Falcon"
+}
+item {
+  name: "/m/0f8s22"
+  id: 508
+  display_name: "Chime"
+}
+item {
+  name: "/m/0f9_l"
+  id: 509
+  display_name: "Snail"
+}
+item {
+  name: "/m/0fbdv"
+  id: 510
+  display_name: "Shellfish"
+}
+item {
+  name: "/m/0fbw6"
+  id: 511
+  display_name: "Cabbage"
+}
+item {
+  name: "/m/0fj52s"
+  id: 512
+  display_name: "Carrot"
+}
+item {
+  name: "/m/0fldg"
+  id: 513
+  display_name: "Mango"
+}
+item {
+  name: "/m/0fly7"
+  id: 514
+  display_name: "Jeans"
+}
+item {
+  name: "/m/0fm3zh"
+  id: 515
+  display_name: "Flowerpot"
+}
+item {
+  name: "/m/0fp6w"
+  id: 516
+  display_name: "Pineapple"
+}
+item {
+  name: "/m/0fqfqc"
+  id: 517
+  display_name: "Drawer"
+}
+item {
+  name: "/m/0fqt361"
+  id: 518
+  display_name: "Stool"
+}
+item {
+  name: "/m/0frqm"
+  id: 519
+  display_name: "Envelope"
+}
+item {
+  name: "/m/0fszt"
+  id: 520
+  display_name: "Cake"
+}
+item {
+  name: "/m/0ft9s"
+  id: 521
+  display_name: "Dragonfly"
+}
+item {
+  name: "/m/0ftb8"
+  id: 522
+  display_name: "Sunflower"
+}
+item {
+  name: "/m/0fx9l"
+  id: 523
+  display_name: "Microwave oven"
+}
+item {
+  name: "/m/0fz0h"
+  id: 524
+  display_name: "Honeycomb"
+}
+item {
+  name: "/m/0gd2v"
+  id: 525
+  display_name: "Marine mammal"
+}
+item {
+  name: "/m/0gd36"
+  id: 526
+  display_name: "Sea lion"
+}
+item {
+  name: "/m/0gj37"
+  id: 527
+  display_name: "Ladybug"
+}
+item {
+  name: "/m/0gjbg72"
+  id: 528
+  display_name: "Shelf"
+}
+item {
+  name: "/m/0gjkl"
+  id: 529
+  display_name: "Watch"
+}
+item {
+  name: "/m/0gm28"
+  id: 530
+  display_name: "Candy"
+}
+item {
+  name: "/m/0grw1"
+  id: 531
+  display_name: "Salad"
+}
+item {
+  name: "/m/0gv1x"
+  id: 532
+  display_name: "Parrot"
+}
+item {
+  name: "/m/0gxl3"
+  id: 533
+  display_name: "Handgun"
+}
+item {
+  name: "/m/0h23m"
+  id: 534
+  display_name: "Sparrow"
+}
+item {
+  name: "/m/0h2r6"
+  id: 535
+  display_name: "Van"
+}
+item {
+  name: "/m/0h8jyh6"
+  id: 536
+  display_name: "Grinder"
+}
+item {
+  name: "/m/0h8kx63"
+  id: 537
+  display_name: "Spice rack"
+}
+item {
+  name: "/m/0h8l4fh"
+  id: 538
+  display_name: "Light bulb"
+}
+item {
+  name: "/m/0h8lkj8"
+  id: 539
+  display_name: "Corded phone"
+}
+item {
+  name: "/m/0h8mhzd"
+  id: 540
+  display_name: "Sports uniform"
+}
+item {
+  name: "/m/0h8my_4"
+  id: 541
+  display_name: "Tennis racket"
+}
+item {
+  name: "/m/0h8mzrc"
+  id: 542
+  display_name: "Wall clock"
+}
+item {
+  name: "/m/0h8n27j"
+  id: 543
+  display_name: "Serving tray"
+}
+item {
+  name: "/m/0h8n5zk"
+  id: 544
+  display_name: "Kitchen & dining room table"
+}
+item {
+  name: "/m/0h8n6f9"
+  id: 545
+  display_name: "Dog bed"
+}
+item {
+  name: "/m/0h8n6ft"
+  id: 546
+  display_name: "Cake stand"
+}
+item {
+  name: "/m/0h8nm9j"
+  id: 547
+  display_name: "Cat furniture"
+}
+item {
+  name: "/m/0h8nr_l"
+  id: 548
+  display_name: "Bathroom accessory"
+}
+item {
+  name: "/m/0h8nsvg"
+  id: 549
+  display_name: "Facial tissue holder"
+}
+item {
+  name: "/m/0h8ntjv"
+  id: 550
+  display_name: "Pressure cooker"
+}
+item {
+  name: "/m/0h99cwc"
+  id: 551
+  display_name: "Kitchen appliance"
+}
+item {
+  name: "/m/0h9mv"
+  id: 552
+  display_name: "Tire"
+}
+item {
+  name: "/m/0hdln"
+  id: 553
+  display_name: "Ruler"
+}
+item {
+  name: "/m/0hf58v5"
+  id: 554
+  display_name: "Luggage and bags"
+}
+item {
+  name: "/m/0hg7b"
+  id: 555
+  display_name: "Microphone"
+}
+item {
+  name: "/m/0hkxq"
+  id: 556
+  display_name: "Broccoli"
+}
+item {
+  name: "/m/0hnnb"
+  id: 557
+  display_name: "Umbrella"
+}
+item {
+  name: "/m/0hnyx"
+  id: 558
+  display_name: "Pastry"
+}
+item {
+  name: "/m/0hqkz"
+  id: 559
+  display_name: "Grapefruit"
+}
+item {
+  name: "/m/0j496"
+  id: 560
+  display_name: "Band-aid"
+}
+item {
+  name: "/m/0jbk"
+  id: 561
+  display_name: "Animal"
+}
+item {
+  name: "/m/0jg57"
+  id: 562
+  display_name: "Bell pepper"
+}
+item {
+  name: "/m/0jly1"
+  id: 563
+  display_name: "Turkey"
+}
+item {
+  name: "/m/0jqgx"
+  id: 564
+  display_name: "Lily"
+}
+item {
+  name: "/m/0jwn_"
+  id: 565
+  display_name: "Pomegranate"
+}
+item {
+  name: "/m/0jy4k"
+  id: 566
+  display_name: "Doughnut"
+}
+item {
+  name: "/m/0jyfg"
+  id: 567
+  display_name: "Glasses"
+}
+item {
+  name: "/m/0k0pj"
+  id: 568
+  display_name: "Human nose"
+}
+item {
+  name: "/m/0k1tl"
+  id: 569
+  display_name: "Pen"
+}
+item {
+  name: "/m/0_k2"
+  id: 570
+  display_name: "Ant"
+}
+item {
+  name: "/m/0k4j"
+  id: 571
+  display_name: "Car"
+}
+item {
+  name: "/m/0k5j"
+  id: 572
+  display_name: "Aircraft"
+}
+item {
+  name: "/m/0k65p"
+  id: 573
+  display_name: "Human hand"
+}
+item {
+  name: "/m/0km7z"
+  id: 574
+  display_name: "Skunk"
+}
+item {
+  name: "/m/0kmg4"
+  id: 575
+  display_name: "Teddy bear"
+}
+item {
+  name: "/m/0kpqd"
+  id: 576
+  display_name: "Watermelon"
+}
+item {
+  name: "/m/0kpt_"
+  id: 577
+  display_name: "Cantaloupe"
+}
+item {
+  name: "/m/0ky7b"
+  id: 578
+  display_name: "Dishwasher"
+}
+item {
+  name: "/m/0l14j_"
+  id: 579
+  display_name: "Flute"
+}
+item {
+  name: "/m/0l3ms"
+  id: 580
+  display_name: "Balance beam"
+}
+item {
+  name: "/m/0l515"
+  id: 581
+  display_name: "Sandwich"
+}
+item {
+  name: "/m/0ll1f78"
+  id: 582
+  display_name: "Shrimp"
+}
+item {
+  name: "/m/0llzx"
+  id: 583
+  display_name: "Sewing machine"
+}
+item {
+  name: "/m/0lt4_"
+  id: 584
+  display_name: "Binoculars"
+}
+item {
+  name: "/m/0m53l"
+  id: 585
+  display_name: "Rays and skates"
+}
+item {
+  name: "/m/0mcx2"
+  id: 586
+  display_name: "Ipod"
+}
+item {
+  name: "/m/0mkg"
+  id: 587
+  display_name: "Accordion"
+}
+item {
+  name: "/m/0mw_6"
+  id: 588
+  display_name: "Willow"
+}
+item {
+  name: "/m/0n28_"
+  id: 589
+  display_name: "Crab"
+}
+item {
+  name: "/m/0nl46"
+  id: 590
+  display_name: "Crown"
+}
+item {
+  name: "/m/0nybt"
+  id: 591
+  display_name: "Seahorse"
+}
+item {
+  name: "/m/0p833"
+  id: 592
+  display_name: "Perfume"
+}
+item {
+  name: "/m/0pcr"
+  id: 593
+  display_name: "Alpaca"
+}
+item {
+  name: "/m/0pg52"
+  id: 594
+  display_name: "Taxi"
+}
+item {
+  name: "/m/0ph39"
+  id: 595
+  display_name: "Canoe"
+}
+item {
+  name: "/m/0qjjc"
+  id: 596
+  display_name: "Remote control"
+}
+item {
+  name: "/m/0qmmr"
+  id: 597
+  display_name: "Wheelchair"
+}
+item {
+  name: "/m/0wdt60w"
+  id: 598
+  display_name: "Rugby ball"
+}
+item {
+  name: "/m/0xfy"
+  id: 599
+  display_name: "Armadillo"
+}
+item {
+  name: "/m/0xzly"
+  id: 600
+  display_name: "Maracas"
+}
+item {
+  name: "/m/0zvk5"
+  id: 601
+  display_name: "Helmet"
+}
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index c844e3dc..83c56b9b 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -131,7 +131,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
                use_display_name=False,
                dct_method='',
                num_keypoints=0,
-               num_additional_channels=0):
+               num_additional_channels=0,
+               load_multiclass_scores=False):
     """Constructor sets keys_to_features and items_to_handlers.
 
     Args:
@@ -153,6 +154,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         example, the jpeg library does not have that specific option.
       num_keypoints: the number of keypoints per object.
       num_additional_channels: how many additional channels to use.
+      load_multiclass_scores: Whether to load multiclass scores associated with
+        boxes.
 
     Raises:
       ValueError: If `instance_mask_type` option is not one of
@@ -205,6 +208,7 @@ class TfExampleDecoder(data_decoder.DataDecoder):
             tf.VarLenFeature(tf.int64),
         'image/object/weight':
             tf.VarLenFeature(tf.float32),
+
     }
     # We are checking `dct_method` instead of passing it directly in order to
     # ensure TF version 1.6 compatibility.
@@ -251,7 +255,13 @@ class TfExampleDecoder(data_decoder.DataDecoder):
             slim_example_decoder.Tensor('image/object/group_of')),
         fields.InputDataFields.groundtruth_weights: (
             slim_example_decoder.Tensor('image/object/weight')),
+
     }
+    if load_multiclass_scores:
+      self.keys_to_features[
+          'image/object/class/multiclass_scores'] = tf.VarLenFeature(tf.float32)
+      self.items_to_handlers[fields.InputDataFields.multiclass_scores] = (
+          slim_example_decoder.Tensor('image/object/class/multiclass_scores'))
     if num_additional_channels > 0:
       self.keys_to_features[
           'image/additional_channels/encoded'] = tf.FixedLenFeature(
@@ -355,6 +365,9 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         shape [None, None, None] containing instance masks.
       fields.InputDataFields.groundtruth_image_classes - 1D uint64 of shape
         [None] containing classes for the boxes.
+      fields.InputDataFields.multiclass_scores - 1D float32 tensor of shape
+        [None * num_classes] containing flattened multiclass scores for
+        groundtruth boxes.
     """
     serialized_example = tf.reshape(tf_example_string_tensor, shape=[])
     decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index 91fa8693..e761f7f1 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -374,6 +374,43 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(bbox_classes,
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
+  def testDecodeMultiClassScores(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_ymins = [0.0, 4.0]
+    bbox_xmins = [1.0, 5.0]
+    bbox_ymaxs = [2.0, 6.0]
+    bbox_xmaxs = [3.0, 7.0]
+    flattened_multiclass_scores = [100., 50.] + [20., 30.]
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature('jpeg'),
+                'image/object/class/multiclass_scores':
+                    dataset_util.float_list_feature(flattened_multiclass_scores
+                                                   ),
+                'image/object/bbox/ymin':
+                    dataset_util.float_list_feature(bbox_ymins),
+                'image/object/bbox/xmin':
+                    dataset_util.float_list_feature(bbox_xmins),
+                'image/object/bbox/ymax':
+                    dataset_util.float_list_feature(bbox_ymaxs),
+                'image/object/bbox/xmax':
+                    dataset_util.float_list_feature(bbox_xmaxs),
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        load_multiclass_scores=True)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual(flattened_multiclass_scores,
+                        tensor_dict[fields.InputDataFields.multiclass_scores])
+
   def testDecodeObjectLabelNoText(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -417,6 +454,51 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(bbox_classes,
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
+  def testDecodeObjectLabelWithText(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_classes_text = ['cat', 'dog']
+    # Annotation label gets overridden by labelmap id.
+    annotated_bbox_classes = [3, 4]
+    expected_bbox_classes = [1, 2]
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature('jpeg'),
+                'image/object/class/text':
+                    dataset_util.bytes_list_feature(bbox_classes_text),
+                'image/object/class/label':
+                    dataset_util.int64_list_feature(annotated_bbox_classes),
+            })).SerializeToString()
+    label_map_string = """
+      item {
+        id:1
+        name:'cat'
+      }
+      item {
+        id:2
+        name:'dog'
+      }
+    """
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        label_map_proto_file=label_map_path)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    init = tf.tables_initializer()
+    with self.test_session() as sess:
+      sess.run(init)
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual(expected_bbox_classes,
+                        tensor_dict[fields.InputDataFields.groundtruth_classes])
+
   def testDecodeObjectLabelUnrecognizedName(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -501,6 +583,50 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual([3, 1],
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
+  def testDecodeObjectLabelUnrecognizedNameWithMappingWithDisplayName(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_classes_text = ['cat', 'cheetah']
+    bbox_classes_id = [5, 6]
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature('jpeg'),
+                'image/object/class/text':
+                    dataset_util.bytes_list_feature(bbox_classes_text),
+                'image/object/class/label':
+                    dataset_util.int64_list_feature(bbox_classes_id),
+            })).SerializeToString()
+
+    label_map_string = """
+      item {
+        name:'/m/cat'
+        id:3
+        display_name:'cat'
+      }
+      item {
+        name:'/m/dog'
+        id:1
+        display_name:'dog'
+      }
+    """
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        label_map_proto_file=label_map_path)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    with self.test_session() as sess:
+      sess.run(tf.tables_initializer())
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual([3, -1],
+                        tensor_dict[fields.InputDataFields.groundtruth_classes])
+
   def testDecodeObjectLabelWithMappingWithName(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index f7145511..a88fbc7b 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -15,6 +15,7 @@
 """Common utility functions for evaluation."""
 import collections
 import os
+import re
 import time
 
 import numpy as np
@@ -233,7 +234,8 @@ def _run_checkpoint_once(tensor_dict,
                          save_graph=False,
                          save_graph_dir='',
                          losses_dict=None,
-                         eval_export_path=None):
+                         eval_export_path=None,
+                         process_metrics_fn=None):
   """Evaluates metrics defined in evaluators and returns summaries.
 
   This function loads the latest checkpoint in checkpoint_dirs and evaluates
@@ -275,6 +277,12 @@ def _run_checkpoint_once(tensor_dict,
     losses_dict: optional dictionary of scalar detection losses.
     eval_export_path: Path for saving a json file that contains the detection
       results in json format.
+    process_metrics_fn: a callback called with evaluation results after each
+      evaluation is done.  It could be used e.g. to back up checkpoints with
+      best evaluation scores, or to call an external system to update evaluation
+      results in order to drive best hyper-parameter search.  Parameters are:
+      int checkpoint_number, Dict[str, ObjectDetectionEvalMetrics] metrics,
+      str checkpoint_file path.
 
   Returns:
     global_step: the count of global steps.
@@ -291,6 +299,7 @@ def _run_checkpoint_once(tensor_dict,
   sess.run(tf.global_variables_initializer())
   sess.run(tf.local_variables_initializer())
   sess.run(tf.tables_initializer())
+  checkpoint_file = None
   if restore_fn:
     restore_fn(sess)
   else:
@@ -370,6 +379,15 @@ def _run_checkpoint_once(tensor_dict,
 
       for key, value in iter(aggregate_result_losses_dict.items()):
         all_evaluator_metrics['Losses/' + key] = np.mean(value)
+      if process_metrics_fn and checkpoint_file:
+        m = re.search(r'model.ckpt-(\d+)$', checkpoint_file)
+        if not m:
+          tf.logging.error('Failed to parse checkpoint number from: %s',
+                           checkpoint_file)
+        else:
+          checkpoint_number = int(m.group(1))
+          process_metrics_fn(checkpoint_number, all_evaluator_metrics,
+                             checkpoint_file)
   sess.close()
   return (global_step, all_evaluator_metrics)
 
@@ -385,11 +403,13 @@ def repeated_checkpoint_run(tensor_dict,
                             num_batches=1,
                             eval_interval_secs=120,
                             max_number_of_evaluations=None,
+                            max_evaluation_global_step=None,
                             master='',
                             save_graph=False,
                             save_graph_dir='',
                             losses_dict=None,
-                            eval_export_path=None):
+                            eval_export_path=None,
+                            process_metrics_fn=None):
   """Periodically evaluates desired tensors using checkpoint_dirs or restore_fn.
 
   This function repeatedly loads a checkpoint and evaluates a desired
@@ -425,6 +445,7 @@ def repeated_checkpoint_run(tensor_dict,
     eval_interval_secs: the number of seconds between each evaluation run.
     max_number_of_evaluations: the max number of iterations of the evaluation.
       If the value is left as None the evaluation continues indefinitely.
+    max_evaluation_global_step: global step when evaluation stops.
     master: the location of the Tensorflow session.
     save_graph: whether or not the Tensorflow graph is saved as a pbtxt file.
     save_graph_dir: where to save on disk the Tensorflow graph. If store_graph
@@ -432,6 +453,12 @@ def repeated_checkpoint_run(tensor_dict,
     losses_dict: optional dictionary of scalar detection losses.
     eval_export_path: Path for saving a json file that contains the detection
       results in json format.
+    process_metrics_fn: a callback called with evaluation results after each
+      evaluation is done.  It could be used e.g. to back up checkpoints with
+      best evaluation scores, or to call an external system to update evaluation
+      results in order to drive best hyper-parameter search.  Parameters are:
+      int checkpoint_number, Dict[str, ObjectDetectionEvalMetrics] metrics,
+      str checkpoint_file path.
 
   Returns:
     metrics: A dictionary containing metric names and values in the latest
@@ -443,7 +470,10 @@ def repeated_checkpoint_run(tensor_dict,
   """
   if max_number_of_evaluations and max_number_of_evaluations <= 0:
     raise ValueError(
-        '`number_of_steps` must be either None or a positive number.')
+        '`max_number_of_evaluations` must be either None or a positive number.')
+  if max_evaluation_global_step and max_evaluation_global_step <= 0:
+    raise ValueError(
+        '`max_evaluation_global_step` must be either None or positive.')
 
   if not checkpoint_dirs:
     raise ValueError('`checkpoint_dirs` must have at least one entry.')
@@ -475,8 +505,13 @@ def repeated_checkpoint_run(tensor_dict,
           save_graph,
           save_graph_dir,
           losses_dict=losses_dict,
-          eval_export_path=eval_export_path)
+          eval_export_path=eval_export_path,
+          process_metrics_fn=process_metrics_fn)
       write_metrics(metrics, global_step, summary_dir)
+      if (max_evaluation_global_step and
+          global_step >= max_evaluation_global_step):
+        tf.logging.info('Finished evaluation!')
+        break
     number_of_evaluations += 1
 
     if (max_number_of_evaluations and
diff --git a/research/object_detection/export_inference_graph.py b/research/object_detection/export_inference_graph.py
index 6b5257be..73af8fcc 100644
--- a/research/object_detection/export_inference_graph.py
+++ b/research/object_detection/export_inference_graph.py
@@ -39,6 +39,12 @@ and the following output nodes returned by the model.postprocess(..):
       [batch, num_boxes] containing class scores for the detections.
   * `detection_classes`: Outputs float32 tensors of the form
       [batch, num_boxes] containing classes for the detections.
+  * `raw_detection_boxes`: Outputs float32 tensors of the form
+      [batch, raw_num_boxes, 4] containing detection boxes without
+      post-processing.
+  * `raw_detection_scores`: Outputs float32 tensors of the form
+      [batch, raw_num_boxes, num_classes_with_background] containing class score
+      logits for raw detection boxes.
   * `detection_masks`: Outputs float32 tensors of the form
       [batch, num_boxes, mask_height, mask_width] containing predicted instance
       masks for each box if its present in the dictionary of postprocessed
diff --git a/research/object_detection/export_tflite_ssd_graph_lib.py b/research/object_detection/export_tflite_ssd_graph_lib.py
index 05075f17..bb1b0f85 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib.py
@@ -154,7 +154,9 @@ def export_tflite_graph(pipeline_config,
                         max_detections,
                         max_classes_per_detection,
                         detections_per_class=100,
-                        use_regular_nms=False):
+                        use_regular_nms=False,
+                        binary_graph_name='tflite_graph.pb',
+                        txt_graph_name='tflite_graph.pbtxt'):
   """Exports a tflite compatible graph and anchors for ssd detection model.
 
   Anchors are written to a tensor and tflite compatible graph
@@ -174,6 +176,8 @@ def export_tflite_graph(pipeline_config,
     for NonMaxSuppression per class
     use_regular_nms: Flag to set postprocessing op to use Regular NMS instead
       of Fast NMS.
+    binary_graph_name: Name of the exported graph file in binary format.
+    txt_graph_name: Name of the exported graph file in text format.
 
   Raises:
     ValueError: if the pipeline config contains models other than ssd or uses an
@@ -304,9 +308,9 @@ def export_tflite_graph(pipeline_config,
     # Return frozen without adding post-processing custom op
     transformed_graph_def = frozen_graph_def
 
-  binary_graph = os.path.join(output_dir, 'tflite_graph.pb')
+  binary_graph = os.path.join(output_dir, binary_graph_name)
   with tf.gfile.GFile(binary_graph, 'wb') as f:
     f.write(transformed_graph_def.SerializeToString())
-  txt_graph = os.path.join(output_dir, 'tflite_graph.pbtxt')
+  txt_graph = os.path.join(output_dir, txt_graph_name)
   with tf.gfile.GFile(txt_graph, 'w') as f:
     f.write(str(transformed_graph_def))
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index 9aaf012b..79e6bc09 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -19,11 +19,7 @@ import tempfile
 import tensorflow as tf
 from tensorflow.contrib.quantize.python import graph_matcher
 from tensorflow.core.protobuf import saver_pb2
-from tensorflow.python.client import session
-from tensorflow.python.platform import gfile
-from tensorflow.python.saved_model import signature_constants
-from tensorflow.python.tools import freeze_graph
-from tensorflow.python.training import saver as saver_lib
+from tensorflow.python.tools import freeze_graph  # pylint: disable=g-direct-tensorflow-import
 from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
 from object_detection.core import standard_fields as fields
@@ -73,7 +69,8 @@ def rewrite_nn_resize_op(is_quantized=False):
     nn_resize = tf.image.resize_nearest_neighbor(
         projection_op.outputs[0],
         add_op.outputs[0].shape.dims[1:3],
-        align_corners=False)
+        align_corners=False,
+        name=os.path.split(reshape_2_op.name)[0] + '/resize_nearest_neighbor')
 
     for index, op_input in enumerate(add_op.inputs):
       if op_input == reshape_2_op.outputs[0]:
@@ -207,6 +204,8 @@ def add_output_tensor_nodes(postprocessed_tensors,
   label_id_offset = 1
   boxes = postprocessed_tensors.get(detection_fields.detection_boxes)
   scores = postprocessed_tensors.get(detection_fields.detection_scores)
+  raw_boxes = postprocessed_tensors.get(detection_fields.raw_detection_boxes)
+  raw_scores = postprocessed_tensors.get(detection_fields.raw_detection_scores)
   classes = postprocessed_tensors.get(
       detection_fields.detection_classes) + label_id_offset
   keypoints = postprocessed_tensors.get(detection_fields.detection_keypoints)
@@ -221,6 +220,12 @@ def add_output_tensor_nodes(postprocessed_tensors,
       classes, name=detection_fields.detection_classes)
   outputs[detection_fields.num_detections] = tf.identity(
       num_detections, name=detection_fields.num_detections)
+  if raw_boxes is not None:
+    outputs[detection_fields.raw_detection_boxes] = tf.identity(
+        raw_boxes, name=detection_fields.raw_detection_boxes)
+  if raw_scores is not None:
+    outputs[detection_fields.raw_detection_scores] = tf.identity(
+        raw_scores, name=detection_fields.raw_detection_scores)
   if keypoints is not None:
     outputs[detection_fields.detection_keypoints] = tf.identity(
         keypoints, name=detection_fields.detection_keypoints)
@@ -252,7 +257,7 @@ def write_saved_model(saved_model_path,
     outputs: A tensor dictionary containing the outputs of a DetectionModel.
   """
   with tf.Graph().as_default():
-    with session.Session() as sess:
+    with tf.Session() as sess:
 
       tf.import_graph_def(frozen_graph_def, name='')
 
@@ -268,12 +273,15 @@ def write_saved_model(saved_model_path,
           tf.saved_model.signature_def_utils.build_signature_def(
               inputs=tensor_info_inputs,
               outputs=tensor_info_outputs,
-              method_name=signature_constants.PREDICT_METHOD_NAME))
+              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
+          ))
 
       builder.add_meta_graph_and_variables(
-          sess, [tf.saved_model.tag_constants.SERVING],
+          sess,
+          [tf.saved_model.tag_constants.SERVING],
           signature_def_map={
-              signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
+              tf.saved_model.signature_constants
+              .DEFAULT_SERVING_SIGNATURE_DEF_KEY:
                   detection_signature,
           },
       )
@@ -289,9 +297,9 @@ def write_graph_and_checkpoint(inference_graph_def,
     node.device = ''
   with tf.Graph().as_default():
     tf.import_graph_def(inference_graph_def, name='')
-    with session.Session() as sess:
-      saver = saver_lib.Saver(saver_def=input_saver_def,
-                              save_relative_paths=True)
+    with tf.Session() as sess:
+      saver = tf.train.Saver(
+          saver_def=input_saver_def, save_relative_paths=True)
       saver.restore(sess, trained_checkpoint_prefix)
       saver.save(sess, model_path)
 
@@ -308,8 +316,8 @@ def _get_outputs_from_inputs(input_tensors, detection_model,
                                  output_collection_name)
 
 
-def _build_detection_graph(input_type, detection_model, input_shape,
-                           output_collection_name, graph_hook_fn):
+def build_detection_graph(input_type, detection_model, input_shape,
+                          output_collection_name, graph_hook_fn):
   """Build the detection graph."""
   if input_type not in input_placeholder_fn_map:
     raise ValueError('Unknown input type: {}'.format(input_type))
@@ -343,7 +351,8 @@ def _export_inference_graph(input_type,
                             input_shape=None,
                             output_collection_name='inference_op',
                             graph_hook_fn=None,
-                            write_inference_graph=False):
+                            write_inference_graph=False,
+                            temp_checkpoint_prefix=''):
   """Export helper."""
   tf.gfile.MakeDirs(output_directory)
   frozen_graph_path = os.path.join(output_directory,
@@ -351,7 +360,7 @@ def _export_inference_graph(input_type,
   saved_model_path = os.path.join(output_directory, 'saved_model')
   model_path = os.path.join(output_directory, 'model.ckpt')
 
-  outputs, placeholder_tensor = _build_detection_graph(
+  outputs, placeholder_tensor = build_detection_graph(
       input_type=input_type,
       detection_model=detection_model,
       input_shape=input_shape,
@@ -361,12 +370,13 @@ def _export_inference_graph(input_type,
   profile_inference_graph(tf.get_default_graph())
   saver_kwargs = {}
   if use_moving_averages:
-    # This check is to be compatible with both version of SaverDef.
-    if os.path.isfile(trained_checkpoint_prefix):
-      saver_kwargs['write_version'] = saver_pb2.SaverDef.V1
-      temp_checkpoint_prefix = tempfile.NamedTemporaryFile().name
-    else:
-      temp_checkpoint_prefix = tempfile.mkdtemp()
+    if not temp_checkpoint_prefix:
+      # This check is to be compatible with both version of SaverDef.
+      if os.path.isfile(trained_checkpoint_prefix):
+        saver_kwargs['write_version'] = saver_pb2.SaverDef.V1
+        temp_checkpoint_prefix = tempfile.NamedTemporaryFile().name
+      else:
+        temp_checkpoint_prefix = tempfile.mkdtemp()
     replace_variable_values_with_moving_averages(
         tf.get_default_graph(), trained_checkpoint_prefix,
         temp_checkpoint_prefix)
@@ -388,7 +398,7 @@ def _export_inference_graph(input_type,
                                         'inference_graph.pbtxt')
     for node in inference_graph_def.node:
       node.device = ''
-    with gfile.GFile(inference_graph_path, 'wb') as f:
+    with tf.gfile.GFile(inference_graph_path, 'wb') as f:
       f.write(str(inference_graph_def))
 
   if additional_output_tensor_names is not None:
@@ -486,4 +496,3 @@ def profile_inference_graph(graph):
   tf.contrib.tfprof.model_analyzer.print_model_analysis(
       graph,
       tfprof_options=tfprof_flops_option)
-
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index 5d2bd9ba..c647e44d 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -61,7 +61,14 @@ class FakeModel(model.DetectionModel):
                                            [0.9, 0.0]], tf.float32),
           'detection_classes': tf.constant([[0, 1],
                                             [1, 0]], tf.float32),
-          'num_detections': tf.constant([2, 1], tf.float32)
+          'num_detections': tf.constant([2, 1], tf.float32),
+          'raw_detection_boxes': tf.constant([[[0.0, 0.0, 0.5, 0.5],
+                                               [0.5, 0.5, 0.8, 0.8]],
+                                              [[0.5, 0.5, 1.0, 1.0],
+                                               [0.0, 0.5, 0.0, 0.5]]],
+                                             tf.float32),
+          'raw_detection_scores': tf.constant([[0.7, 0.6],
+                                               [0.9, 0.5]], tf.float32),
       }
       if self._add_detection_keypoints:
         postprocessed_tensors['detection_keypoints'] = tf.constant(
@@ -612,7 +619,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       pipeline_config.eval_config.use_moving_averages = False
       detection_model = model_builder.build(pipeline_config.model,
                                             is_training=False)
-      outputs, _ = exporter._build_detection_graph(
+      outputs, _ = exporter.build_detection_graph(
           input_type='tf_example',
           detection_model=detection_model,
           input_shape=None,
@@ -760,7 +767,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       pipeline_config.eval_config.use_moving_averages = False
       detection_model = model_builder.build(pipeline_config.model,
                                             is_training=False)
-      outputs, placeholder_tensor = exporter._build_detection_graph(
+      outputs, placeholder_tensor = exporter.build_detection_graph(
           input_type='tf_example',
           detection_model=detection_model,
           input_shape=None,
@@ -893,7 +900,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       pipeline_config.eval_config.use_moving_averages = False
       detection_model = model_builder.build(pipeline_config.model,
                                             is_training=False)
-      exporter._build_detection_graph(
+      exporter.build_detection_graph(
           input_type='tf_example',
           detection_model=detection_model,
           input_shape=None,
@@ -917,13 +924,16 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         tf_example = od_graph.get_tensor_by_name('tf_example:0')
         boxes = od_graph.get_tensor_by_name('detection_boxes:0')
         scores = od_graph.get_tensor_by_name('detection_scores:0')
+        raw_boxes = od_graph.get_tensor_by_name('raw_detection_boxes:0')
+        raw_scores = od_graph.get_tensor_by_name('raw_detection_scores:0')
         classes = od_graph.get_tensor_by_name('detection_classes:0')
         keypoints = od_graph.get_tensor_by_name('detection_keypoints:0')
         masks = od_graph.get_tensor_by_name('detection_masks:0')
         num_detections = od_graph.get_tensor_by_name('num_detections:0')
-        (boxes_np, scores_np, classes_np, keypoints_np, masks_np,
-         num_detections_np) = sess.run(
-             [boxes, scores, classes, keypoints, masks, num_detections],
+        (boxes_np, scores_np, raw_boxes_np, raw_scores_np, classes_np,
+         keypoints_np, masks_np, num_detections_np) = sess.run(
+             [boxes, scores, raw_boxes, raw_scores, classes, keypoints, masks,
+              num_detections],
              feed_dict={tf_example: tf_example_np})
         self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],
                                         [0.5, 0.5, 0.8, 0.8]],
@@ -931,6 +941,12 @@ class ExportInferenceGraphTest(tf.test.TestCase):
                                         [0.0, 0.0, 0.0, 0.0]]])
         self.assertAllClose(scores_np, [[0.7, 0.6],
                                         [0.9, 0.0]])
+        self.assertAllClose(raw_boxes_np, [[[0.0, 0.0, 0.5, 0.5],
+                                            [0.5, 0.5, 0.8, 0.8]],
+                                           [[0.5, 0.5, 1.0, 1.0],
+                                            [0.0, 0.5, 0.0, 0.5]]])
+        self.assertAllClose(raw_scores_np, [[0.7, 0.6],
+                                            [0.9, 0.5]])
         self.assertAllClose(classes_np, [[1, 2],
                                          [2, 1]])
         self.assertAllClose(keypoints_np, np.arange(48).reshape([2, 2, 6, 2]))
diff --git a/research/object_detection/g3doc/detection_model_zoo.md b/research/object_detection/g3doc/detection_model_zoo.md
index ed8d698f..fd5642ae 100644
--- a/research/object_detection/g3doc/detection_model_zoo.md
+++ b/research/object_detection/g3doc/detection_model_zoo.md
@@ -57,7 +57,7 @@ Some remarks on frozen inference graphs:
   a detector (and discarding the part past that point), which negatively impacts
   standard mAP metrics.
 * Our frozen inference graphs are generated using the
-  [v1.8.0](https://github.com/tensorflow/tensorflow/tree/v1.8.0)
+  [v1.12.0](https://github.com/tensorflow/tensorflow/tree/v1.12.0)
   release version of Tensorflow and we do not guarantee that these will work
   with other versions; this being said, each frozen inference graph can be
   regenerated using your current version of Tensorflow by re-running the
@@ -78,7 +78,7 @@ Some remarks on frozen inference graphs:
 | [ssd_mobilenet_v1_fpn_coco ](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz) | 56 | 32 | Boxes |
 | [ssd_resnet_50_fpn_coco ](http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz) | 76 | 35 | Boxes |
 | [ssd_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz) | 31 | 22 | Boxes |
-| [ssd_mobilenet_v2_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2018_09_14.tar.gz) | 29 | 22 | Boxes |
+| [ssd_mobilenet_v2_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz) | 29 | 22 | Boxes |
 | [ssdlite_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) | 27 | 22 | Boxes |
 | [ssd_inception_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz) | 42 | 24 | Boxes |
 | [faster_rcnn_inception_v2_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz) | 58 | 28 | Boxes |
@@ -110,10 +110,15 @@ Model name
 
 Model name                                                                                                                                                                                    | Speed (ms) | Open Images mAP@0.5[^2] | Outputs
 --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------: | :---------------------: | :-----:
-[faster_rcnn_inception_resnet_v2_atrous_oidv2](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_2018_01_28.tar.gz)                           | 727        | 37                      | Boxes
+[faster_rcnn_inception_resnet_v2_atrous_oidv2](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_2018_01_28.tar.gz)                           | 727        | 37                     | Boxes
 [faster_rcnn_inception_resnet_v2_atrous_lowproposals_oidv2](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid_2018_01_28.tar.gz) | 347        |                         | Boxes
 [facessd_mobilenet_v2_quantized_open_image_v4](http://download.tensorflow.org/models/object_detection/facessd_mobilenet_v2_quantized_320x320_open_image_v4.tar.gz) [^3]                       | 20         | 73 (faces)              | Boxes
 
+Model name                                                                                                                                                                                    | Speed (ms) | Open Images mAP@0.5[^4] | Outputs
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------: | :---------------------: | :-----:
+[faster_rcnn_inception_resnet_v2_atrous_oidv4](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_v4_2018_12_12.tar.gz)                         | 425        | 54                  | Boxes
+[ssd_mobilenetv2_oidv4](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_oid_v4_2018_12_12.tar.gz)                                                                       | 89         | 36                | Boxes
+[ssd_resnet_101_fpn_oidv4](http://download.tensorflow.org/models/object_detection/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync_2019_01_20.tar.gz)                                                                       | 237         | 38                | Boxes
 ## iNaturalist Species-trained models
 
 Model name                                                                                                                                                        | Speed (ms) | Pascal mAP@0.5 | Outputs
@@ -129,8 +134,10 @@ Model name
 [faster_rcnn_resnet101_ava_v2.1](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_ava_v2.1_2018_04_30.tar.gz) | 93  | 11              | Boxes
 
 
-[^1]: See [MSCOCO evaluation protocol](http://cocodataset.org/#detections-eval).
+[^1]: See [MSCOCO evaluation protocol](http://cocodataset.org/#detections-eval). The COCO mAP numbers here are evaluated on COCO 14 minival set (note that our split is different from COCO 17 Val). A full list of image ids used in our split could be fould [here](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_minival_ids.txt).
+
 
-[^2]: This is PASCAL mAP with a slightly different way of true positives computation: see [Open Images evaluation protocol](evaluation_protocols.md#open-images).
+[^2]: This is PASCAL mAP with a slightly different way of true positives computation: see [Open Images evaluation protocols](evaluation_protocols.md), oid_V2_detection_metrics.
 [^3]: Non-face boxes are dropped during training and non-face groundtruth boxes are ignored when evaluating.
+[^4]: This is Open Images Challenge metric: see [Open Images evaluation protocols](evaluation_protocols.md), oid_challenge_detection_metrics.
 
diff --git a/research/object_detection/g3doc/installation.md b/research/object_detection/g3doc/installation.md
index 206304d1..c4f05b40 100644
--- a/research/object_detection/g3doc/installation.md
+++ b/research/object_detection/g3doc/installation.md
@@ -11,7 +11,7 @@ Tensorflow Object Detection API depends on the following libraries:
 *   tf Slim (which is included in the "tensorflow/models/research/" checkout)
 *   Jupyter notebook
 *   Matplotlib
-*   Tensorflow (>=1.9.0)
+*   Tensorflow (>=1.12.0)
 *   Cython
 *   contextlib2
 *   cocoapi
diff --git a/research/object_detection/g3doc/running_on_cloud.md b/research/object_detection/g3doc/running_on_cloud.md
index fb2dd53c..5ee5d87a 100644
--- a/research/object_detection/g3doc/running_on_cloud.md
+++ b/research/object_detection/g3doc/running_on_cloud.md
@@ -44,7 +44,7 @@ job using GPUs. A sample YAML file is given below:
 
 ```
 trainingInput:
-  runtimeVersion: "1.9"
+  runtimeVersion: "1.12"
   scaleTier: CUSTOM
   masterType: standard_gpu
   workerCount: 9
@@ -73,7 +73,7 @@ following command:
 ```bash
 # From tensorflow/models/research/
 gcloud ml-engine jobs submit training object_detection_`date +%m_%d_%Y_%H_%M_%S` \
-    --runtime-version 1.9 \
+    --runtime-version 1.12 \
     --job-dir=gs://${MODEL_DIR} \
     --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \
     --module-name object_detection.model_main \
@@ -93,7 +93,7 @@ Google Cloud Storage.
 Users can monitor the progress of their training job on the [ML Engine
 Dashboard](https://console.cloud.google.com/mlengine/jobs).
 
-Note: This sample is supported for use with 1.9 runtime version.
+Note: This sample is supported for use with 1.12 runtime version.
 
 ## Running a TPU Training Job on CMLE
 
@@ -105,7 +105,7 @@ gcloud ml-engine jobs submit training `whoami`_object_detection_`date +%m_%d_%Y_
 --job-dir=gs://${MODEL_DIR} \
 --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \
 --module-name object_detection.model_tpu_main \
---runtime-version 1.9 \
+--runtime-version 1.12 \
 --scale-tier BASIC_TPU \
 --region us-central1 \
 -- \
@@ -133,7 +133,7 @@ job:
 
 ```bash
 gcloud ml-engine jobs submit training object_detection_eval_`date +%m_%d_%Y_%H_%M_%S` \
-    --runtime-version 1.9 \
+    --runtime-version 1.12 \
     --job-dir=gs://${MODEL_DIR} \
     --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \
     --module-name object_detection.model_main \
diff --git a/research/object_detection/g3doc/running_pets.md b/research/object_detection/g3doc/running_pets.md
index bbfcdf7d..bb62db56 100644
--- a/research/object_detection/g3doc/running_pets.md
+++ b/research/object_detection/g3doc/running_pets.md
@@ -208,7 +208,7 @@ For running the training Cloud ML job, we'll configure the cluster to use 5
 training jobs and three parameters servers. The
 configuration file can be found at `object_detection/samples/cloud/cloud.yml`.
 
-Note: The code sample below is supported for use with 1.9 runtime version.
+Note: The code sample below is supported for use with 1.12 runtime version.
 
 To start training and evaluation, execute the following command from the
 `tensorflow/models/research/` directory:
@@ -216,7 +216,7 @@ To start training and evaluation, execute the following command from the
 ```bash
 # From tensorflow/models/research/
 gcloud ml-engine jobs submit training `whoami`_object_detection_pets_`date +%m_%d_%Y_%H_%M_%S` \
-    --runtime-version 1.9 \
+    --runtime-version 1.12 \
     --job-dir=gs://${YOUR_GCS_BUCKET}/model_dir \
     --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \
     --module-name object_detection.model_main \
diff --git a/research/object_detection/g3doc/using_your_own_dataset.md b/research/object_detection/g3doc/using_your_own_dataset.md
index 077b6543..23222f26 100644
--- a/research/object_detection/g3doc/using_your_own_dataset.md
+++ b/research/object_detection/g3doc/using_your_own_dataset.md
@@ -76,7 +76,7 @@ def create_cat_tf_example(encoded_cat_image_data):
       'image/width': dataset_util.int64_feature(width),
       'image/filename': dataset_util.bytes_feature(filename),
       'image/source_id': dataset_util.bytes_feature(filename),
-      'image/encoded': dataset_util.bytes_feature(encoded_cat_image_data),
+      'image/encoded': dataset_util.bytes_feature(encoded_image_data),
       'image/format': dataset_util.bytes_feature(image_format),
       'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
       'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 3598e23b..916afc70 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -53,6 +53,7 @@ def transform_input_data(tensor_dict,
                          data_augmentation_fn=None,
                          merge_multiple_boxes=False,
                          retain_original_image=False,
+                         use_multiclass_scores=False,
                          use_bfloat16=False):
   """A single function that is responsible for all input data transformations.
 
@@ -87,25 +88,37 @@ def transform_input_data(tensor_dict,
       and classes for a given image if the boxes are exactly the same.
     retain_original_image: (optional) whether to retain original image in the
       output dictionary.
+    use_multiclass_scores: whether to use multiclass scores as
+      class targets instead of one-hot encoding of `groundtruth_classes`.
     use_bfloat16: (optional) a bool, whether to use bfloat16 in training.
 
   Returns:
     A dictionary keyed by fields.InputDataFields containing the tensors obtained
     after applying all the transformations.
   """
+  # Reshape flattened multiclass scores tensor into a 2D tensor of shape
+  # [num_boxes, num_classes].
+  if fields.InputDataFields.multiclass_scores in tensor_dict:
+    tensor_dict[fields.InputDataFields.multiclass_scores] = tf.reshape(
+        tensor_dict[fields.InputDataFields.multiclass_scores], [
+            tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0],
+            num_classes
+        ])
   if fields.InputDataFields.groundtruth_boxes in tensor_dict:
     tensor_dict = util_ops.filter_groundtruth_with_nan_box_coordinates(
         tensor_dict)
-  if fields.InputDataFields.image_additional_channels in tensor_dict:
-    channels = tensor_dict[fields.InputDataFields.image_additional_channels]
-    tensor_dict[fields.InputDataFields.image] = tf.concat(
-        [tensor_dict[fields.InputDataFields.image], channels], axis=2)
+    tensor_dict = util_ops.filter_unrecognized_classes(tensor_dict)
 
   if retain_original_image:
     tensor_dict[fields.InputDataFields.original_image] = tf.cast(
         image_resizer_fn(tensor_dict[fields.InputDataFields.image], None)[0],
         tf.uint8)
 
+  if fields.InputDataFields.image_additional_channels in tensor_dict:
+    channels = tensor_dict[fields.InputDataFields.image_additional_channels]
+    tensor_dict[fields.InputDataFields.image] = tf.concat(
+        [tensor_dict[fields.InputDataFields.image], channels], axis=2)
+
   # Apply data augmentation ops.
   if data_augmentation_fn is not None:
     tensor_dict = data_augmentation_fn(tensor_dict)
@@ -136,6 +149,11 @@ def transform_input_data(tensor_dict,
   tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.one_hot(
       zero_indexed_groundtruth_classes, num_classes)
 
+  if use_multiclass_scores:
+    tensor_dict[fields.InputDataFields.groundtruth_classes] = tensor_dict[
+        fields.InputDataFields.multiclass_scores]
+    tensor_dict.pop(fields.InputDataFields.multiclass_scores, None)
+
   if fields.InputDataFields.groundtruth_confidences in tensor_dict:
     groundtruth_confidences = tensor_dict[
         fields.InputDataFields.groundtruth_confidences]
@@ -172,6 +190,9 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
                                     spatial_image_shape=None):
   """Pads input tensors to static shapes.
 
+  In case num_additional_channels > 0, we assume that the additional channels
+  have already been concatenated to the base image.
+
   Args:
     tensor_dict: Tensor dictionary of input data
     max_num_boxes: Max number of groundtruth boxes needed to compute shapes for
@@ -186,7 +207,8 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
     tensors in the dataset.
 
   Raises:
-    ValueError: If groundtruth classes is neither rank 1 nor rank 2.
+    ValueError: If groundtruth classes is neither rank 1 nor rank 2, or if we
+      detect that additional channels have not been concatenated yet.
   """
 
   if not spatial_image_shape or spatial_image_shape == [-1, -1]:
@@ -198,14 +220,27 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
   if fields.InputDataFields.image_additional_channels in tensor_dict:
     num_additional_channels = tensor_dict[
         fields.InputDataFields.image_additional_channels].shape[2].value
-  num_image_channels = 3
+
+  # We assume that if num_additional_channels > 0, then it has already been
+  # concatenated to the base image (but not the ground truth).
+  num_channels = 3
   if fields.InputDataFields.image in tensor_dict:
-    num_image_channels = tensor_dict[fields.InputDataFields
-                                     .image].shape[2].value
+    num_channels = tensor_dict[fields.InputDataFields.image].shape[2].value
+
+  if num_additional_channels:
+    if num_additional_channels >= num_channels:
+      raise ValueError(
+          'Image must be already concatenated with additional channels.')
+
+    if (fields.InputDataFields.original_image in tensor_dict and
+        tensor_dict[fields.InputDataFields.original_image].shape[2].value ==
+        num_channels):
+      raise ValueError(
+          'Image must be already concatenated with additional channels.')
+
   padding_shapes = {
-      # Additional channels are merged before batching.
       fields.InputDataFields.image: [
-          height, width, num_image_channels + num_additional_channels
+          height, width, num_channels
       ],
       fields.InputDataFields.original_image_spatial_shape: [2],
       fields.InputDataFields.image_additional_channels: [
@@ -231,16 +266,14 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
       fields.InputDataFields.groundtruth_label_types: [max_num_boxes],
       fields.InputDataFields.groundtruth_label_weights: [max_num_boxes],
       fields.InputDataFields.true_image_shape: [3],
-      fields.InputDataFields.multiclass_scores: [
-          max_num_boxes, num_classes + 1 if num_classes is not None else None
-      ],
       fields.InputDataFields.groundtruth_image_classes: [num_classes],
       fields.InputDataFields.groundtruth_image_confidences: [num_classes],
   }
 
   if fields.InputDataFields.original_image in tensor_dict:
     padding_shapes[fields.InputDataFields.original_image] = [
-        height, width, num_image_channels + num_additional_channels
+        height, width, tensor_dict[fields.InputDataFields.
+                                   original_image].shape[2].value
     ]
   if fields.InputDataFields.groundtruth_keypoints in tensor_dict:
     tensor_shape = (
@@ -294,11 +327,14 @@ def augment_input_data(tensor_dict, data_augmentation_options):
                            in tensor_dict)
   include_label_confidences = (fields.InputDataFields.groundtruth_confidences
                                in tensor_dict)
+  include_multiclass_scores = (fields.InputDataFields.multiclass_scores in
+                               tensor_dict)
   tensor_dict = preprocessor.preprocess(
       tensor_dict, data_augmentation_options,
       func_arg_map=preprocessor.get_default_func_arg_map(
           include_label_weights=include_label_weights,
           include_label_confidences=include_label_confidences,
+          include_multiclass_scores=include_multiclass_scores,
           include_instance_masks=include_instance_masks,
           include_keypoints=include_keypoints))
   tensor_dict[fields.InputDataFields.image] = tf.squeeze(
@@ -472,6 +508,7 @@ def create_train_input_fn(train_config, train_input_config,
           data_augmentation_fn=data_augmentation_fn,
           merge_multiple_boxes=train_config.merge_multiple_label_boxes,
           retain_original_image=train_config.retain_original_images,
+          use_multiclass_scores=train_config.use_multiclass_scores,
           use_bfloat16=train_config.use_bfloat16)
 
       tensor_dict = pad_input_data_to_static_shapes(
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index e0a85301..c096cea7 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -105,6 +105,48 @@ class InputsTest(test_case.TestCase, parameterized.TestCase):
         tf.float32,
         labels[fields.InputDataFields.groundtruth_confidences].dtype)
 
+  def test_faster_rcnn_resnet50_train_input_with_additional_channels(self):
+    """Tests the training input function for FasterRcnnResnet50."""
+    configs = _get_configs_for_model('faster_rcnn_resnet50_pets')
+    model_config = configs['model']
+    configs['train_input_config'].num_additional_channels = 2
+    configs['train_config'].retain_original_images = True
+    model_config.faster_rcnn.num_classes = 37
+    train_input_fn = inputs.create_train_input_fn(
+        configs['train_config'], configs['train_input_config'], model_config)
+    features, labels = _make_initializable_iterator(train_input_fn()).get_next()
+
+    self.assertAllEqual([1, None, None, 5],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertAllEqual(
+        [1, None, None, 3],
+        features[fields.InputDataFields.original_image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual([1],
+                        features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [1, 100, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [1, 100, model_config.faster_rcnn.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [1, 100],
+        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_weights].dtype)
+    self.assertAllEqual(
+        [1, 100, model_config.faster_rcnn.num_classes],
+        labels[fields.InputDataFields.groundtruth_confidences].shape.as_list())
+    self.assertEqual(
+        tf.float32,
+        labels[fields.InputDataFields.groundtruth_confidences].dtype)
+
   @parameterized.parameters(
       {'eval_batch_size': 1},
       {'eval_batch_size': 8}
@@ -595,6 +637,72 @@ class DataTransformationFnTest(test_case.TestCase):
         transformed_inputs[fields.InputDataFields.groundtruth_confidences],
         [[0, 0, 1], [1, 0, 0]])
 
+  def test_returns_correct_labels_with_unrecognized_class(self):
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(4, 4, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.constant(
+                np.array([[0, 0, 1, 1], [.2, .2, 4, 4], [.5, .5, 1, 1]],
+                         np.float32)),
+        fields.InputDataFields.groundtruth_area:
+            tf.constant(np.array([.5, .4, .3])),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([3, -1, 1], np.int32)),
+        fields.InputDataFields.groundtruth_keypoints:
+            tf.constant(
+                np.array([[[.1, .1]], [[.2, .2]], [[.5, .5]]],
+                         np.float32)),
+        fields.InputDataFields.groundtruth_keypoint_visibilities:
+            tf.constant([True, False, True]),
+        fields.InputDataFields.groundtruth_instance_masks:
+            tf.constant(np.random.rand(3, 4, 4).astype(np.float32)),
+        fields.InputDataFields.groundtruth_is_crowd:
+            tf.constant([False, True, False]),
+        fields.InputDataFields.groundtruth_difficult:
+            tf.constant(np.array([0, 0, 1], np.int32))
+    }
+
+    num_classes = 3
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_model_preprocessor_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes)
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_classes],
+        [[0, 0, 1], [1, 0, 0]])
+    self.assertAllEqual(
+        transformed_inputs[fields.InputDataFields.num_groundtruth_boxes], 2)
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_area], [.5, .3])
+    self.assertAllEqual(
+        transformed_inputs[fields.InputDataFields.groundtruth_confidences],
+        [[0, 0, 1], [1, 0, 0]])
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_boxes],
+        [[0, 0, 1, 1], [.5, .5, 1, 1]])
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_keypoints],
+        [[[.1, .1]], [[.5, .5]]])
+    self.assertAllEqual(
+        transformed_inputs[
+            fields.InputDataFields.groundtruth_keypoint_visibilities],
+        [True, True])
+    self.assertAllEqual(
+        transformed_inputs[
+            fields.InputDataFields.groundtruth_instance_masks].shape, [2, 4, 4])
+    self.assertAllEqual(
+        transformed_inputs[fields.InputDataFields.groundtruth_is_crowd],
+        [False, False])
+    self.assertAllEqual(
+        transformed_inputs[fields.InputDataFields.groundtruth_difficult],
+        [0, 1])
+
   def test_returns_correct_merged_boxes(self):
     tensor_dict = {
         fields.InputDataFields.image:
@@ -885,7 +993,7 @@ class PadInputDataToStaticShapesFnTest(test_case.TestCase):
   def test_images_and_additional_channels(self):
     input_tensor_dict = {
         fields.InputDataFields.image:
-            tf.placeholder(tf.float32, [None, None, 3]),
+            tf.placeholder(tf.float32, [None, None, 5]),
         fields.InputDataFields.image_additional_channels:
             tf.placeholder(tf.float32, [None, None, 2]),
     }
@@ -895,6 +1003,8 @@ class PadInputDataToStaticShapesFnTest(test_case.TestCase):
         num_classes=3,
         spatial_image_shape=[5, 6])
 
+    # pad_input_data_to_static_shape assumes that image is already concatenated
+    # with additional channels.
     self.assertAllEqual(
         padded_tensor_dict[fields.InputDataFields.image].shape.as_list(),
         [5, 6, 5])
@@ -902,6 +1012,22 @@ class PadInputDataToStaticShapesFnTest(test_case.TestCase):
         padded_tensor_dict[fields.InputDataFields.image_additional_channels]
         .shape.as_list(), [5, 6, 2])
 
+  def test_images_and_additional_channels_errors(self):
+    input_tensor_dict = {
+        fields.InputDataFields.image:
+            tf.placeholder(tf.float32, [None, None, 3]),
+        fields.InputDataFields.image_additional_channels:
+            tf.placeholder(tf.float32, [None, None, 2]),
+        fields.InputDataFields.original_image:
+            tf.placeholder(tf.float32, [None, None, 3]),
+    }
+    with self.assertRaises(ValueError):
+      _ = inputs.pad_input_data_to_static_shapes(
+          tensor_dict=input_tensor_dict,
+          max_num_boxes=3,
+          num_classes=3,
+          spatial_image_shape=[5, 6])
+
   def test_gray_images(self):
     input_tensor_dict = {
         fields.InputDataFields.image:
@@ -920,10 +1046,12 @@ class PadInputDataToStaticShapesFnTest(test_case.TestCase):
   def test_gray_images_and_additional_channels(self):
     input_tensor_dict = {
         fields.InputDataFields.image:
-            tf.placeholder(tf.float32, [None, None, 1]),
+            tf.placeholder(tf.float32, [None, None, 3]),
         fields.InputDataFields.image_additional_channels:
             tf.placeholder(tf.float32, [None, None, 2]),
     }
+    # pad_input_data_to_static_shape assumes that image is already concatenated
+    # with additional channels.
     padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
         tensor_dict=input_tensor_dict,
         max_num_boxes=3,
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 95cbba07..22ece5ae 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -92,8 +92,8 @@ configured in the meta architecture:
   non-max suppression and normalize them. In this case, the `postprocess` method
   skips both `_postprocess_rpn` and `_postprocess_box_classifier`.
 """
-from abc import abstractmethod
-from functools import partial
+import abc
+import functools
 import tensorflow as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
@@ -138,7 +138,7 @@ class FasterRCNNFeatureExtractor(object):
     self._reuse_weights = reuse_weights
     self._weight_decay = weight_decay
 
-  @abstractmethod
+  @abc.abstractmethod
   def preprocess(self, resized_inputs):
     """Feature-extractor specific preprocessing (minus image resizing)."""
     pass
@@ -162,7 +162,7 @@ class FasterRCNNFeatureExtractor(object):
     with tf.variable_scope(scope, values=[preprocessed_inputs]):
       return self._extract_proposal_features(preprocessed_inputs, scope)
 
-  @abstractmethod
+  @abc.abstractmethod
   def _extract_proposal_features(self, preprocessed_inputs, scope):
     """Extracts first stage RPN features, to be overridden."""
     pass
@@ -185,7 +185,7 @@ class FasterRCNNFeatureExtractor(object):
         scope, values=[proposal_feature_maps], reuse=tf.AUTO_REUSE):
       return self._extract_box_classifier_features(proposal_feature_maps, scope)
 
-  @abstractmethod
+  @abc.abstractmethod
   def _extract_box_classifier_features(self, proposal_feature_maps, scope):
     """Extracts second stage box classifier features, to be overridden."""
     pass
@@ -770,7 +770,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           representing the features for each proposal.
     """
     image_shape_2d = self._image_batch_shape_2d(image_shape)
-    proposal_boxes_normalized, _, num_proposals = self._postprocess_rpn(
+    proposal_boxes_normalized, _, num_proposals, _, _ = self._postprocess_rpn(
         rpn_box_encodings, rpn_objectness_predictions_with_background,
         anchors, image_shape_2d, true_image_shapes)
 
@@ -1080,7 +1080,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         anchors_boxlist, clip_window)
     def _batch_gather_kept_indices(predictions_tensor):
       return shape_utils.static_or_dynamic_map_fn(
-          partial(tf.gather, indices=keep_indices),
+          functools.partial(tf.gather, indices=keep_indices),
           elems=predictions_tensor,
           dtype=tf.float32,
           parallel_iterations=self._parallel_iterations,
@@ -1148,17 +1148,22 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     with tf.name_scope('FirstStagePostprocessor'):
       if self._number_of_stages == 1:
-        proposal_boxes, proposal_scores, num_proposals = self._postprocess_rpn(
-            prediction_dict['rpn_box_encodings'],
-            prediction_dict['rpn_objectness_predictions_with_background'],
-            prediction_dict['anchors'],
-            true_image_shapes,
-            true_image_shapes)
+        (proposal_boxes, proposal_scores, num_proposals, raw_proposal_boxes,
+         raw_proposal_scores) = self._postprocess_rpn(
+             prediction_dict['rpn_box_encodings'],
+             prediction_dict['rpn_objectness_predictions_with_background'],
+             prediction_dict['anchors'], true_image_shapes, true_image_shapes)
         return {
-            fields.DetectionResultFields.detection_boxes: proposal_boxes,
-            fields.DetectionResultFields.detection_scores: proposal_scores,
+            fields.DetectionResultFields.detection_boxes:
+                proposal_boxes,
+            fields.DetectionResultFields.detection_scores:
+                proposal_scores,
             fields.DetectionResultFields.num_detections:
                 tf.to_float(num_proposals),
+            fields.DetectionResultFields.raw_detection_boxes:
+                raw_proposal_boxes,
+            fields.DetectionResultFields.raw_detection_scores:
+                raw_proposal_scores
         }
 
     # TODO(jrru): Remove mask_predictions from _post_process_box_classifier.
@@ -1266,6 +1271,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
       num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]
         representing the number of proposals predicted for each image in
         the batch.
+      raw_detection_boxes: [batch, total_detections, 4] tensor with decoded
+        proposal boxes before Non-Max Suppression.
+      raw_detection_score: [batch, total_detections,
+        num_classes_with_background] tensor of class score logits for
+        raw proposal boxes.
     """
     rpn_box_encodings_batch = tf.expand_dims(rpn_box_encodings_batch, axis=2)
     rpn_encodings_shape = shape_utils.combined_static_and_dynamic_shape(
@@ -1274,13 +1284,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
         tf.expand_dims(anchors, 0), [rpn_encodings_shape[0], 1, 1])
     proposal_boxes = self._batch_decode_boxes(rpn_box_encodings_batch,
                                               tiled_anchor_boxes)
-    proposal_boxes = tf.squeeze(proposal_boxes, axis=2)
+    raw_proposal_boxes = tf.squeeze(proposal_boxes, axis=2)
     rpn_objectness_softmax_without_background = tf.nn.softmax(
         rpn_objectness_predictions_with_background_batch)[:, :, 1]
     clip_window = self._compute_clip_window(image_shapes)
     (proposal_boxes, proposal_scores, _, _, _,
      num_proposals) = self._first_stage_nms_fn(
-         tf.expand_dims(proposal_boxes, axis=2),
+         tf.expand_dims(raw_proposal_boxes, axis=2),
          tf.expand_dims(rpn_objectness_softmax_without_background, axis=2),
          clip_window=clip_window)
     if self._is_training:
@@ -1304,7 +1314,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
       return normalized_boxes_per_image
     normalized_proposal_boxes = shape_utils.static_or_dynamic_map_fn(
         normalize_boxes, elems=[proposal_boxes, image_shapes], dtype=tf.float32)
-    return normalized_proposal_boxes, proposal_scores, num_proposals
+    raw_normalized_proposal_boxes = shape_utils.static_or_dynamic_map_fn(
+        normalize_boxes,
+        elems=[raw_proposal_boxes, image_shapes],
+        dtype=tf.float32)
+    return (normalized_proposal_boxes, proposal_scores, num_proposals,
+            raw_normalized_proposal_boxes,
+            rpn_objectness_predictions_with_background_batch)
 
   def _sample_box_classifier_batch(
       self,
@@ -1576,6 +1592,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
           (optional) [batch, max_detections, mask_height, mask_width]. Note
           that a pixel-wise sigmoid score converter is applied to the detection
           masks.
+        `raw_detection_boxes`: [batch, total_detections, 4] tensor with decoded
+          detection boxes before Non-Max Suppression.
+        `raw_detection_score`: [batch, total_detections,
+          num_classes_with_background] tensor of multi-class score logits for
+          raw detection boxes.
     """
     refined_box_encodings_batch = tf.reshape(
         refined_box_encodings,
@@ -1589,11 +1610,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
     )
     refined_decoded_boxes_batch = self._batch_decode_boxes(
         refined_box_encodings_batch, proposal_boxes)
-    class_predictions_with_background_batch = (
+    class_predictions_with_background_batch_normalized = (
         self._second_stage_score_conversion_fn(
             class_predictions_with_background_batch))
     class_predictions_batch = tf.reshape(
-        tf.slice(class_predictions_with_background_batch,
+        tf.slice(class_predictions_with_background_batch_normalized,
                  [0, 0, 1], [-1, -1, -1]),
         [-1, self.max_num_proposals, self.num_classes])
     clip_window = self._compute_clip_window(image_shapes)
@@ -1614,11 +1635,51 @@ class FasterRCNNMetaArch(model.DetectionModel):
          change_coordinate_frame=True,
          num_valid_boxes=num_proposals,
          masks=mask_predictions_batch)
+    if refined_decoded_boxes_batch.shape[2] > 1:
+      class_ids = tf.expand_dims(
+          tf.argmax(class_predictions_with_background_batch[:, :, 1:], axis=2,
+                    output_type=tf.int32),
+          axis=-1)
+      raw_detection_boxes = tf.squeeze(
+          tf.batch_gather(refined_decoded_boxes_batch, class_ids), axis=2)
+    else:
+      raw_detection_boxes = tf.squeeze(refined_decoded_boxes_batch, axis=2)
+
+    def normalize_and_clip_boxes(args):
+      """Normalize and clip boxes."""
+      boxes_per_image = args[0]
+      image_shape = args[1]
+      normalized_boxes_per_image = box_list_ops.to_normalized_coordinates(
+          box_list.BoxList(boxes_per_image),
+          image_shape[0],
+          image_shape[1],
+          check_range=False).get()
+
+      normalized_boxes_per_image = box_list_ops.clip_to_window(
+          box_list.BoxList(normalized_boxes_per_image),
+          tf.constant([0.0, 0.0, 1.0, 1.0], tf.float32),
+          filter_nonoverlapping=False).get()
+
+      return normalized_boxes_per_image
+
+    raw_normalized_detection_boxes = shape_utils.static_or_dynamic_map_fn(
+        normalize_and_clip_boxes,
+        elems=[raw_detection_boxes, image_shapes],
+        dtype=tf.float32)
+
     detections = {
-        fields.DetectionResultFields.detection_boxes: nmsed_boxes,
-        fields.DetectionResultFields.detection_scores: nmsed_scores,
-        fields.DetectionResultFields.detection_classes: nmsed_classes,
-        fields.DetectionResultFields.num_detections: tf.to_float(num_detections)
+        fields.DetectionResultFields.detection_boxes:
+            nmsed_boxes,
+        fields.DetectionResultFields.detection_scores:
+            nmsed_scores,
+        fields.DetectionResultFields.detection_classes:
+            nmsed_classes,
+        fields.DetectionResultFields.num_detections:
+            tf.to_float(num_detections),
+        fields.DetectionResultFields.raw_detection_boxes:
+            raw_normalized_detection_boxes,
+        fields.DetectionResultFields.raw_detection_scores:
+            class_predictions_with_background_batch
     }
     if nmsed_masks is not None:
       detections[fields.DetectionResultFields.detection_masks] = nmsed_masks
@@ -1769,7 +1830,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
           back_prop=True))
 
       # Normalize by number of examples in sampled minibatch
-      normalizer = tf.reduce_sum(batch_sampled_indices, axis=1)
+      normalizer = tf.maximum(
+          tf.reduce_sum(batch_sampled_indices, axis=1), 1.0)
       batch_one_hot_targets = tf.one_hot(
           tf.to_int32(batch_cls_targets), depth=2)
       sampled_reg_indices = tf.multiply(batch_sampled_indices,
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index 2c701d28..810d65a2 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -85,6 +85,68 @@ class FasterRCNNMetaArchTest(
       self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
       self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
 
+  def test_postprocess_second_stage_only_inference_mode_with_calibration(self):
+    model = self._build_model(
+        is_training=False, number_of_stages=2, second_stage_batch_size=6,
+        calibration_mapping_value=0.5)
+
+    batch_size = 2
+    total_num_padded_proposals = batch_size * model.max_num_proposals
+    proposal_boxes = tf.constant(
+        [[[1, 1, 2, 3],
+          [0, 0, 1, 1],
+          [.5, .5, .6, .6],
+          4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
+         [[2, 3, 6, 8],
+          [1, 2, 5, 3],
+          4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)
+    num_proposals = tf.constant([3, 2], dtype=tf.int32)
+    refined_box_encodings = tf.zeros(
+        [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
+    class_predictions_with_background = tf.ones(
+        [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)
+    image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
+
+    mask_height = 2
+    mask_width = 2
+    mask_predictions = 30. * tf.ones(
+        [total_num_padded_proposals, model.num_classes,
+         mask_height, mask_width], dtype=tf.float32)
+    exp_detection_masks = np.array([[[[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]]],
+                                    [[[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]],
+                                     [[0, 0], [0, 0]]]])
+
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+    detections = model.postprocess({
+        'refined_box_encodings': refined_box_encodings,
+        'class_predictions_with_background': class_predictions_with_background,
+        'num_proposals': num_proposals,
+        'proposal_boxes': proposal_boxes,
+        'image_shape': image_shape,
+        'mask_predictions': mask_predictions
+    }, true_image_shapes)
+    with self.test_session() as sess:
+      detections_out = sess.run(detections)
+      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
+      # All scores map to 0.5, except for the final one, which is pruned.
+      self.assertAllClose(detections_out['detection_scores'],
+                          [[0.5, 0.5, 0.5, 0.5, 0.5],
+                           [0.5, 0.5, 0.5, 0.5, 0.0]])
+      self.assertAllClose(detections_out['detection_classes'],
+                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
+      self.assertAllClose(detections_out['num_detections'], [5, 4])
+      self.assertAllClose(detections_out['detection_masks'],
+                          exp_detection_masks)
+      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
+      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
+
   def test_postprocess_second_stage_only_inference_mode_with_shared_boxes(self):
     model = self._build_model(
         is_training=False, number_of_stages=2, second_stage_batch_size=6)
@@ -190,6 +252,7 @@ class FasterRCNNMetaArchTest(
               set([
                   'detection_boxes', 'detection_scores', 'detection_classes',
                   'detection_masks', 'num_detections', 'mask_predictions',
+                  'raw_detection_boxes', 'raw_detection_scores'
               ])))
       for key in expected_shapes:
         self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
@@ -276,7 +339,7 @@ class FasterRCNNMetaArchTest(
           self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
 
         anchors_shape_out = tensor_dict_out['anchors'].shape
-        self.assertEqual(2, len(anchors_shape_out))
+        self.assertLen(anchors_shape_out, 2)
         self.assertEqual(4, anchors_shape_out[1])
         num_anchors_out = anchors_shape_out[0]
         self.assertAllEqual(tensor_dict_out['rpn_box_encodings'].shape,
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 655a44fe..6de390a6 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -165,7 +165,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                    use_matmul_crop_and_resize=False,
                    clip_anchors_to_image=False,
                    use_matmul_gather_in_matcher=False,
-                   use_static_shapes=False):
+                   use_static_shapes=False,
+                   calibration_mapping_value=None):
 
     def image_resizer_fn(image, masks=None):
       """Fake image resizer function."""
@@ -244,7 +245,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     first_stage_localization_loss_weight = 1.0
     first_stage_objectness_loss_weight = 1.0
 
+    post_processing_config = post_processing_pb2.PostProcessing()
     post_processing_text_proto = """
+      score_converter: IDENTITY
       batch_non_max_suppression {
         score_threshold: -20.0
         iou_threshold: 1.0
@@ -253,18 +256,31 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         use_static_shapes: """ +'{}'.format(use_static_shapes) + """
       }
     """
-    post_processing_config = post_processing_pb2.PostProcessing()
+    if calibration_mapping_value:
+      calibration_text_proto = """
+      calibration_config {
+        function_approximation {
+          x_y_pairs {
+            x_y_pair {
+              x: 0.0
+              y: %f
+            }
+            x_y_pair {
+              x: 1.0
+              y: %f
+              }}}}""" % (calibration_mapping_value, calibration_mapping_value)
+      post_processing_text_proto = (post_processing_text_proto
+                                    + ' ' + calibration_text_proto)
     text_format.Merge(post_processing_text_proto, post_processing_config)
+    second_stage_non_max_suppression_fn, second_stage_score_conversion_fn = (
+        post_processing_builder.build(post_processing_config))
 
     second_stage_target_assigner = target_assigner.create_target_assigner(
         'FasterRCNN', 'detection',
         use_matmul_gather=use_matmul_gather_in_matcher)
-    second_stage_non_max_suppression_fn, _ = post_processing_builder.build(
-        post_processing_config)
     second_stage_sampler = sampler.BalancedPositiveNegativeSampler(
         positive_fraction=1.0, is_static=use_static_shapes)
 
-    second_stage_score_conversion_fn = tf.identity
     second_stage_localization_loss_weight = 1.0
     second_stage_classification_loss_weight = 1.0
     if softmax_second_stage_classification_loss:
@@ -336,6 +352,10 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
             predict_masks=predict_masks,
             masks_are_class_agnostic=masks_are_class_agnostic), **common_kwargs)
 
+  @parameterized.parameters(
+      {'use_static_shapes': False},
+      {'use_static_shapes': True}
+  )
   def test_predict_gives_correct_shapes_in_inference_mode_first_stage_only(
       self, use_static_shapes=False):
     batch_size = 2
@@ -457,6 +477,10 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
             prediction_out['rpn_objectness_predictions_with_background'].shape,
             (batch_size, num_anchors_out, 2))
 
+  @parameterized.parameters(
+      {'use_static_shapes': False},
+      {'use_static_shapes': True}
+  )
   def test_predict_correct_shapes_in_inference_mode_two_stages(
       self, use_static_shapes=False):
 
@@ -578,6 +602,10 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         for key in expected_shapes:
           self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
 
+  @parameterized.parameters(
+      {'use_static_shapes': False},
+      {'use_static_shapes': True}
+  )
   def test_predict_gives_correct_shapes_in_train_mode_both_stages(
       self,
       use_static_shapes=False):
@@ -670,6 +698,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertAllEqual(results[8].shape,
                         expected_shapes['rpn_box_predictor_features'])
 
+  @parameterized.parameters(
+      {'use_static_shapes': False, 'pad_to_max_dimension': None},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56}
+  )
   def test_postprocess_first_stage_only_inference_mode(
       self, use_static_shapes=False, pad_to_max_dimension=None):
     batch_size = 2
@@ -696,9 +730,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           rpn_objectness_predictions_with_background,
           'rpn_features_to_crop': rpn_features_to_crop,
           'anchors': anchors}, true_image_shapes)
-      return (proposals['num_detections'],
-              proposals['detection_boxes'],
-              proposals['detection_scores'])
+      return (proposals['num_detections'], proposals['detection_boxes'],
+              proposals['detection_scores'], proposals['raw_detection_boxes'],
+              proposals['raw_detection_scores'])
 
     anchors = np.array(
         [[0, 0, 16, 16],
@@ -741,6 +775,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     expected_proposal_scores = [[1, 1, 0, 0, 0, 0, 0, 0],
                                 [1, 1, 0, 0, 0, 0, 0, 0]]
     expected_num_proposals = [4, 4]
+    expected_raw_proposal_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                                    [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]],
+                                   [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                                    [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]]]
+    expected_raw_scores = [[[-10., 13.], [10., -10.], [10., -11.], [-10., 12.]],
+                           [[10., -10.], [-10., 13.], [-10., 12.], [10., -11.]]]
 
     self.assertAllClose(results[0], expected_num_proposals)
     for indx, num_proposals in enumerate(expected_num_proposals):
@@ -748,6 +788,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                           expected_proposal_boxes[indx][0:num_proposals])
       self.assertAllClose(results[2][indx][0:num_proposals],
                           expected_proposal_scores[indx][0:num_proposals])
+    self.assertAllClose(results[3], expected_raw_proposal_boxes)
+    self.assertAllClose(results[4], expected_raw_scores)
 
   def _test_postprocess_first_stage_only_train_mode(self,
                                                     pad_to_max_dimension=None):
@@ -801,9 +843,17 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     expected_proposal_scores = [[1, 1],
                                 [1, 1]]
     expected_num_proposals = [2, 2]
-
-    expected_output_keys = set(['detection_boxes', 'detection_scores',
-                                'num_detections'])
+    expected_raw_proposal_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                                    [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]],
+                                   [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                                    [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]]]
+    expected_raw_scores = [[[-10., 13.], [-10., 12.], [-10., 11.], [-10., 10.]],
+                           [[-10., 13.], [-10., 12.], [-10., 11.], [-10., 10.]]]
+
+    expected_output_keys = set([
+        'detection_boxes', 'detection_scores', 'num_detections',
+        'raw_detection_boxes', 'raw_detection_scores'
+    ])
     self.assertEqual(set(proposals.keys()), expected_output_keys)
 
     with self.test_session() as sess:
@@ -817,6 +867,10 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                           expected_proposal_scores)
       self.assertAllEqual(proposals_out['num_detections'],
                           expected_num_proposals)
+    self.assertAllClose(proposals_out['raw_detection_boxes'],
+                        expected_raw_proposal_boxes)
+    self.assertAllClose(proposals_out['raw_detection_scores'],
+                        expected_raw_scores)
 
   def test_postprocess_first_stage_only_train_mode(self):
     self._test_postprocess_first_stage_only_train_mode()
@@ -824,6 +878,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
   def test_postprocess_first_stage_only_train_mode_padded_image(self):
     self._test_postprocess_first_stage_only_train_mode(pad_to_max_dimension=56)
 
+  @parameterized.parameters(
+      {'use_static_shapes': False, 'pad_to_max_dimension': None},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56}
+  )
   def test_postprocess_second_stage_only_inference_mode(
       self, use_static_shapes=False, pad_to_max_dimension=None):
     batch_size = 2
@@ -854,10 +914,10 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           'num_proposals': num_proposals,
           'proposal_boxes': proposal_boxes,
       }, true_image_shapes)
-      return (detections['num_detections'],
-              detections['detection_boxes'],
-              detections['detection_scores'],
-              detections['detection_classes'])
+      return (detections['num_detections'], detections['detection_boxes'],
+              detections['detection_scores'], detections['detection_classes'],
+              detections['raw_detection_boxes'],
+              detections['raw_detection_scores'])
 
     proposal_boxes = np.array(
         [[[1, 1, 2, 3],
@@ -867,6 +927,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
          [[2, 3, 6, 8],
           [1, 2, 5, 3],
           4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=np.float32)
+
     num_proposals = np.array([3, 2], dtype=np.int32)
     refined_box_encodings = np.zeros(
         [total_num_padded_proposals, num_classes, 4], dtype=np.float32)
@@ -887,6 +948,15 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     expected_num_detections = [5, 4]
     expected_detection_classes = [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]]
     expected_detection_scores = [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]]
+    h = float(image_shape[1])
+    w = float(image_shape[2])
+    expected_raw_detection_boxes = np.array(
+        [[[1 / h, 1 / w, 2 / h, 3 / w], [0, 0, 1 / h, 1 / w],
+          [.5 / h, .5 / w, .6 / h, .6 / w], 4 * [0], 4 * [0], 4 * [0], 4 * [0],
+          4 * [0]],
+         [[2 / h, 3 / w, 6 / h, 8 / w], [1 / h, 2 / w, 5 / h, 3 / w], 4 * [0],
+          4 * [0], 4 * [0], 4 * [0], 4 * [0], 4 * [0]]],
+        dtype=np.float32)
 
     self.assertAllClose(results[0], expected_num_detections)
 
@@ -896,6 +966,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       self.assertAllClose(results[3][indx][0:num_proposals],
                           expected_detection_classes[indx][0:num_proposals])
 
+    self.assertAllClose(results[4], expected_raw_detection_boxes)
+    self.assertAllClose(results[5],
+                        class_predictions_with_background.reshape([-1, 8, 3]))
     if not use_static_shapes:
       self.assertAllEqual(results[1].shape, [2, 5, 4])
 
@@ -1268,6 +1341,13 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           'Loss/BoxClassifierLoss/classification_loss'], 0)
       self.assertAllClose(loss_dict_out['Loss/BoxClassifierLoss/mask_loss'], 0)
 
+  @parameterized.parameters(
+      {'use_static_shapes': False, 'shared_boxes': False},
+      {'use_static_shapes': False, 'shared_boxes': True},
+
+      {'use_static_shapes': True, 'shared_boxes': False},
+      {'use_static_shapes': True, 'shared_boxes': True},
+  )
   def test_loss_full_zero_padded_proposals_nonzero_loss_with_two_images(
       self, use_static_shapes=False, shared_boxes=False):
     batch_size = 2
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 0c0052fe..8dfaa453 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -288,7 +288,7 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
     """
     image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0),
                              [image_shape[0], 1])
-    proposal_boxes_normalized, _, num_proposals = self._postprocess_rpn(
+    proposal_boxes_normalized, _, num_proposals, _, _ = self._postprocess_rpn(
         rpn_box_encodings, rpn_objectness_predictions_with_background,
         anchors, image_shape_2d, true_image_shapes)
 
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index b618dae9..bcbedd89 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -17,8 +17,7 @@
 General tensorflow implementation of convolutional Multibox/SSD detection
 models.
 """
-from abc import abstractmethod
-
+import abc
 import tensorflow as tf
 
 from object_detection.core import box_list
@@ -80,7 +79,7 @@ class SSDFeatureExtractor(object):
   def is_keras_model(self):
     return False
 
-  @abstractmethod
+  @abc.abstractmethod
   def preprocess(self, resized_inputs):
     """Preprocesses images for feature extraction (minus image resizing).
 
@@ -98,7 +97,7 @@ class SSDFeatureExtractor(object):
     """
     pass
 
-  @abstractmethod
+  @abc.abstractmethod
   def extract_features(self, preprocessed_inputs):
     """Extracts features from preprocessed inputs.
 
@@ -196,7 +195,7 @@ class SSDKerasFeatureExtractor(tf.keras.Model):
   def is_keras_model(self):
     return True
 
-  @abstractmethod
+  @abc.abstractmethod
   def preprocess(self, resized_inputs):
     """Preprocesses images for feature extraction (minus image resizing).
 
@@ -214,7 +213,7 @@ class SSDKerasFeatureExtractor(tf.keras.Model):
     """
     raise NotImplementedError
 
-  @abstractmethod
+  @abc.abstractmethod
   def _extract_features(self, preprocessed_inputs):
     """Extracts features from preprocessed inputs.
 
@@ -552,8 +551,10 @@ class SSDMetaArch(model.DetectionModel):
         5) anchors: 2-D float tensor of shape [num_anchors, 4] containing
           the generated anchors in normalized coordinates.
     """
-    batchnorm_updates_collections = (None if self._inplace_batchnorm_update
-                                     else tf.GraphKeys.UPDATE_OPS)
+    if self._inplace_batchnorm_update:
+      batchnorm_updates_collections = None
+    else:
+      batchnorm_updates_collections = tf.GraphKeys.UPDATE_OPS
     if self._feature_extractor.is_keras_model:
       feature_maps = self._feature_extractor(preprocessed_inputs)
     else:
@@ -648,14 +649,22 @@ class SSDMetaArch(model.DetectionModel):
 
     Returns:
       detections: a dictionary containing the following fields
-        detection_boxes: [batch, max_detections, 4]
-        detection_scores: [batch, max_detections]
-        detection_classes: [batch, max_detections]
+        detection_boxes: [batch, max_detections, 4] tensor with post-processed
+          detection boxes.
+        detection_scores: [batch, max_detections] tensor with scalar scores for
+          post-processed detection boxes.
+        detection_classes: [batch, max_detections] tensor with classes for
+          post-processed detection classes.
         detection_keypoints: [batch, max_detections, num_keypoints, 2] (if
           encoded in the prediction_dict 'box_encodings')
         detection_masks: [batch_size, max_detections, mask_height, mask_width]
           (optional)
         num_detections: [batch]
+        raw_detection_boxes: [batch, total_detections, 4] tensor with decoded
+          detection boxes before Non-Max Suppression.
+        raw_detection_score: [batch, total_detections,
+          num_classes_with_background] tensor of multi-class score logits for
+          raw detection boxes.
     Raises:
       ValueError: if prediction_dict does not contain `box_encodings` or
         `class_predictions_with_background` fields.
@@ -700,11 +709,18 @@ class SSDMetaArch(model.DetectionModel):
            additional_fields=additional_fields,
            masks=prediction_dict.get('mask_predictions'))
       detection_dict = {
-          fields.DetectionResultFields.detection_boxes: nmsed_boxes,
-          fields.DetectionResultFields.detection_scores: nmsed_scores,
-          fields.DetectionResultFields.detection_classes: nmsed_classes,
+          fields.DetectionResultFields.detection_boxes:
+              nmsed_boxes,
+          fields.DetectionResultFields.detection_scores:
+              nmsed_scores,
+          fields.DetectionResultFields.detection_classes:
+              nmsed_classes,
           fields.DetectionResultFields.num_detections:
-              tf.to_float(num_detections)
+              tf.to_float(num_detections),
+          fields.DetectionResultFields.raw_detection_boxes:
+              tf.squeeze(detection_boxes, axis=2),
+          fields.DetectionResultFields.raw_detection_scores:
+              class_predictions
       }
       if (nmsed_additional_fields is not None and
           fields.BoxListFields.keypoints in nmsed_additional_fields):
@@ -1049,9 +1065,9 @@ class SSDMetaArch(model.DetectionModel):
       mined_cls_loss: a float scalar with sum of classification losses from
         selected hard examples.
     """
-    class_predictions = tf.slice(
-        prediction_dict['class_predictions_with_background'], [0, 0,
-                                                               1], [-1, -1, -1])
+    class_predictions = prediction_dict['class_predictions_with_background']
+    if self._add_background_class:
+      class_predictions = tf.slice(class_predictions, [0, 0, 1], [-1, -1, -1])
 
     decoded_boxes, _ = self._batch_decode(prediction_dict['box_encodings'])
     decoded_box_tensors_list = tf.unstack(decoded_boxes)
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index a4014574..46d032f9 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -48,7 +48,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       use_keras=False,
       predict_mask=False,
       use_static_shapes=False,
-      nms_max_size_per_class=5):
+      nms_max_size_per_class=5,
+      calibration_mapping_value=None):
     return super(SsdMetaArchTest, self)._create_model(
         model_fn=ssd_meta_arch.SSDMetaArch,
         apply_hard_mining=apply_hard_mining,
@@ -61,7 +62,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         use_keras=use_keras,
         predict_mask=predict_mask,
         use_static_shapes=use_static_shapes,
-        nms_max_size_per_class=nms_max_size_per_class)
+        nms_max_size_per_class=nms_max_size_per_class,
+        calibration_mapping_value=calibration_mapping_value)
 
   def test_preprocess_preserves_shapes_with_dynamic_input_image(
       self, use_keras):
@@ -177,6 +179,13 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     expected_classes = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
     expected_num_detections = np.array([3, 3])
 
+    raw_detection_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]],
+                           [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]]]
+    raw_detection_scores = [[[0, 0], [0, 0], [0, 0], [0, 0]],
+                            [[0, 0], [0, 0], [0, 0], [0, 0]]]
+
     for input_shape in input_shapes:
       tf_graph = tf.Graph()
       with tf_graph.as_default():
@@ -191,6 +200,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         self.assertIn('detection_scores', detections)
         self.assertIn('detection_classes', detections)
         self.assertIn('num_detections', detections)
+        self.assertIn('raw_detection_boxes', detections)
+        self.assertIn('raw_detection_scores', detections)
         init_op = tf.global_variables_initializer()
       with self.test_session(graph=tf_graph) as sess:
         sess.run(init_op)
@@ -208,7 +219,139 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       self.assertAllClose(detections_out['detection_classes'], expected_classes)
       self.assertAllClose(detections_out['num_detections'],
                           expected_num_detections)
+      self.assertAllEqual(detections_out['raw_detection_boxes'],
+                          raw_detection_boxes)
+      self.assertAllEqual(detections_out['raw_detection_scores'],
+                          raw_detection_scores)
+
+  def test_postprocess_results_are_correct_static(self, use_keras):
+    with tf.Graph().as_default():
+      _, _, _, _ = self._create_model(use_keras=use_keras)
+    def graph_fn(input_image):
+      model, _, _, _ = self._create_model(use_static_shapes=True,
+                                          nms_max_size_per_class=4)
+      preprocessed_inputs, true_image_shapes = model.preprocess(input_image)
+      prediction_dict = model.predict(preprocessed_inputs,
+                                      true_image_shapes)
+      detections = model.postprocess(prediction_dict, true_image_shapes)
+      return (detections['detection_boxes'], detections['detection_scores'],
+              detections['detection_classes'], detections['num_detections'])
+
+    batch_size = 2
+    image_size = 2
+    channels = 3
+    input_image = np.random.rand(batch_size, image_size, image_size,
+                                 channels).astype(np.float32)
+    expected_boxes = [
+        [
+            [0, 0, .5, .5],
+            [0, .5, .5, 1],
+            [.5, 0, 1, .5],
+            [0, 0, 0, 0]
+        ],  # padding
+        [
+            [0, 0, .5, .5],
+            [0, .5, .5, 1],
+            [.5, 0, 1, .5],
+            [0, 0, 0, 0]
+        ]
+    ]  # padding
+    expected_scores = [[0, 0, 0, 0], [0, 0, 0, 0]]
+    expected_classes = [[0, 0, 0, 0], [0, 0, 0, 0]]
+    expected_num_detections = np.array([3, 3])
 
+    (detection_boxes, detection_scores, detection_classes,
+     num_detections) = self.execute(graph_fn, [input_image])
+    for image_idx in range(batch_size):
+      self.assertTrue(test_utils.first_rows_close_as_set(
+          detection_boxes[image_idx][
+              0:expected_num_detections[image_idx]].tolist(),
+          expected_boxes[image_idx][0:expected_num_detections[image_idx]]))
+      self.assertAllClose(
+          detection_scores[image_idx][0:expected_num_detections[image_idx]],
+          expected_scores[image_idx][0:expected_num_detections[image_idx]])
+      self.assertAllClose(
+          detection_classes[image_idx][0:expected_num_detections[image_idx]],
+          expected_classes[image_idx][0:expected_num_detections[image_idx]])
+    self.assertAllClose(num_detections,
+                        expected_num_detections)
+
+  def test_postprocess_results_are_correct_with_calibration(self, use_keras):
+    batch_size = 2
+    image_size = 2
+    input_shapes = [(batch_size, image_size, image_size, 3),
+                    (None, image_size, image_size, 3),
+                    (batch_size, None, None, 3),
+                    (None, None, None, 3)]
+
+    expected_boxes = [
+        [
+            [0, 0, .5, .5],
+            [0, .5, .5, 1],
+            [.5, 0, 1, .5],
+            [0, 0, 0, 0],  # pruned prediction
+            [0, 0, 0, 0]
+        ],  # padding
+        [
+            [0, 0, .5, .5],
+            [0, .5, .5, 1],
+            [.5, 0, 1, .5],
+            [0, 0, 0, 0],  # pruned prediction
+            [0, 0, 0, 0]
+        ]
+    ]  # padding
+    # Calibration mapping value below is set to map all scores to 0.5, except
+    # for the last two detections in each batch (see expected number of
+    # detections below.
+    expected_scores = [[0.5, 0.5, 0.5, 0., 0.], [0.5, 0.5, 0.5, 0., 0.]]
+    expected_classes = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
+    expected_num_detections = np.array([3, 3])
+
+    raw_detection_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]],
+                           [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]]]
+    raw_detection_scores = [[[0, 0], [0, 0], [0, 0], [0, 0]],
+                            [[0, 0], [0, 0], [0, 0], [0, 0]]]
+
+    for input_shape in input_shapes:
+      tf_graph = tf.Graph()
+      with tf_graph.as_default():
+        model, _, _, _ = self._create_model(use_keras=use_keras,
+                                            calibration_mapping_value=0.5)
+        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
+        preprocessed_inputs, true_image_shapes = model.preprocess(
+            input_placeholder)
+        prediction_dict = model.predict(preprocessed_inputs,
+                                        true_image_shapes)
+        detections = model.postprocess(prediction_dict, true_image_shapes)
+        self.assertIn('detection_boxes', detections)
+        self.assertIn('detection_scores', detections)
+        self.assertIn('detection_classes', detections)
+        self.assertIn('num_detections', detections)
+        self.assertIn('raw_detection_boxes', detections)
+        self.assertIn('raw_detection_scores', detections)
+        init_op = tf.global_variables_initializer()
+      with self.test_session(graph=tf_graph) as sess:
+        sess.run(init_op)
+        detections_out = sess.run(detections,
+                                  feed_dict={
+                                      input_placeholder:
+                                      np.random.uniform(
+                                          size=(batch_size, 2, 2, 3))})
+      for image_idx in range(batch_size):
+        self.assertTrue(
+            test_utils.first_rows_close_as_set(
+                detections_out['detection_boxes'][image_idx].tolist(),
+                expected_boxes[image_idx]))
+      self.assertAllClose(detections_out['detection_scores'], expected_scores)
+      self.assertAllClose(detections_out['detection_classes'], expected_classes)
+      self.assertAllClose(detections_out['num_detections'],
+                          expected_num_detections)
+      self.assertAllEqual(detections_out['raw_detection_boxes'],
+                          raw_detection_boxes)
+      self.assertAllEqual(detections_out['raw_detection_scores'],
+                          raw_detection_scores)
 
   def test_loss_results_are_correct(self, use_keras):
 
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
index 9d9adc6b..05e03921 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
@@ -16,7 +16,9 @@
 
 import functools
 import tensorflow as tf
+from google.protobuf import text_format
 
+from object_detection.builders import post_processing_builder
 from object_detection.core import anchor_generator
 from object_detection.core import balanced_positive_negative_sampler as sampler
 from object_detection.core import box_list
@@ -25,6 +27,7 @@ from object_detection.core import post_processing
 from object_detection.core import region_similarity_calculator as sim_calc
 from object_detection.core import target_assigner
 from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.protos import calibration_pb2
 from object_detection.protos import model_pb2
 from object_detection.utils import ops
 from object_detection.utils import test_case
@@ -125,7 +128,8 @@ class SSDMetaArchTestBase(test_case.TestCase):
       use_keras=False,
       predict_mask=False,
       use_static_shapes=False,
-      nms_max_size_per_class=5):
+      nms_max_size_per_class=5,
+      calibration_mapping_value=None):
     is_training = False
     num_classes = 1
     mock_anchor_generator = MockAnchorGenerator2x2()
@@ -156,6 +160,24 @@ class SSDMetaArchTestBase(test_case.TestCase):
         max_size_per_class=nms_max_size_per_class,
         max_total_size=nms_max_size_per_class,
         use_static_shapes=use_static_shapes)
+    score_conversion_fn = tf.identity
+    calibration_config = calibration_pb2.CalibrationConfig()
+    if calibration_mapping_value:
+      calibration_text_proto = """
+      function_approximation {
+        x_y_pairs {
+            x_y_pair {
+              x: 0.0
+              y: %f
+            }
+            x_y_pair {
+              x: 1.0
+              y: %f
+            }}}""" % (calibration_mapping_value, calibration_mapping_value)
+      text_format.Merge(calibration_text_proto, calibration_config)
+      score_conversion_fn = (
+          post_processing_builder._build_calibrated_score_converter(  # pylint: disable=protected-access
+              tf.identity, calibration_config))
     classification_loss_weight = 1.0
     localization_loss_weight = 1.0
     negative_class_weight = 1.0
@@ -201,7 +223,7 @@ class SSDMetaArchTestBase(test_case.TestCase):
         encode_background_as_zeros=encode_background_as_zeros,
         image_resizer_fn=image_resizer_fn,
         non_max_suppression_fn=non_max_suppression_fn,
-        score_conversion_fn=tf.identity,
+        score_conversion_fn=score_conversion_fn,
         classification_loss=classification_loss,
         localization_loss=localization_loss,
         classification_loss_weight=classification_loss_weight,
diff --git a/research/object_detection/metrics/calibration_evaluation.py b/research/object_detection/metrics/calibration_evaluation.py
new file mode 100644
index 00000000..928c16ad
--- /dev/null
+++ b/research/object_detection/metrics/calibration_evaluation.py
@@ -0,0 +1,228 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Class for evaluating object detections with calibration metrics."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from object_detection.box_coders import mean_stddev_box_coder
+from object_detection.core import box_list
+from object_detection.core import region_similarity_calculator
+from object_detection.core import standard_fields
+from object_detection.core import target_assigner
+from object_detection.matchers import argmax_matcher
+from object_detection.metrics import calibration_metrics
+from object_detection.utils import object_detection_evaluation
+
+
+# TODO(zbeaver): Implement metrics per category.
+class CalibrationDetectionEvaluator(
+    object_detection_evaluation.DetectionEvaluator):
+  """Class to evaluate calibration detection metrics."""
+
+  def __init__(self,
+               categories,
+               iou_threshold=0.5):
+    """Constructor.
+
+    Args:
+      categories: A list of dicts, each of which has the following keys -
+        'id': (required) an integer id uniquely identifying this category.
+        'name': (required) string representing category name e.g., 'cat', 'dog'.
+      iou_threshold: Threshold above which to consider a box as matched during
+        evaluation.
+    """
+    super(CalibrationDetectionEvaluator, self).__init__(categories)
+
+    # Constructing target_assigner to match detections to groundtruth.
+    similarity_calc = region_similarity_calculator.IouSimilarity()
+    matcher = argmax_matcher.ArgMaxMatcher(
+        matched_threshold=iou_threshold, unmatched_threshold=iou_threshold)
+    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
+    self._target_assigner = target_assigner.TargetAssigner(
+        similarity_calc, matcher, box_coder)
+
+  def match_single_image_info(self, image_info):
+    """Match detections to groundtruth for a single image.
+
+    Detections are matched to available groundtruth in the image based on the
+    IOU threshold from the constructor.  The classes of the detections and
+    groundtruth matches are then compared. Detections that do not have IOU above
+    the required threshold or have different classes from their match are
+    considered negative matches. All inputs in `image_info` originate or are
+    inferred from the eval_dict passed to class method
+    `get_estimator_eval_metric_ops`.
+
+    Args:
+      image_info: a tuple or list containing the following (in order):
+        - gt_boxes: tf.float32 tensor of groundtruth boxes.
+        - gt_classes: tf.int64 tensor of groundtruth classes associated with
+            groundtruth boxes.
+        - num_gt_box: scalar indicating the number of groundtruth boxes per
+            image.
+        - det_boxes: tf.float32 tensor of detection boxes.
+        - det_classes: tf.int64 tensor of detection classes associated with
+            detection boxes.
+        - num_det_box: scalar indicating the number of detection boxes per
+            image.
+    Returns:
+      is_class_matched: tf.int64 tensor identical in shape to det_boxes,
+        indicating whether detection boxes matched with and had the same
+        class as groundtruth annotations.
+    """
+    (gt_boxes, gt_classes, num_gt_box, det_boxes, det_classes,
+     num_det_box) = image_info
+    detection_boxes = det_boxes[:num_det_box]
+    detection_classes = det_classes[:num_det_box]
+    groundtruth_boxes = gt_boxes[:num_gt_box]
+    groundtruth_classes = gt_classes[:num_gt_box]
+    det_boxlist = box_list.BoxList(detection_boxes)
+    gt_boxlist = box_list.BoxList(groundtruth_boxes)
+
+    # Target assigner requires classes in one-hot format. An additional
+    # dimension is required since gt_classes are 1-indexed; the zero index is
+    # provided to all non-matches.
+    one_hot_depth = tf.cast(tf.add(tf.reduce_max(groundtruth_classes), 1),
+                            dtype=tf.int32)
+    gt_classes_one_hot = tf.one_hot(
+        groundtruth_classes, one_hot_depth, dtype=tf.float32)
+    one_hot_cls_targets, _, _, _, _ = self._target_assigner.assign(
+        det_boxlist,
+        gt_boxlist,
+        gt_classes_one_hot,
+        unmatched_class_label=tf.zeros(shape=one_hot_depth, dtype=tf.float32))
+    # Transform from one-hot back to indexes.
+    cls_targets = tf.argmax(one_hot_cls_targets, axis=1)
+    is_class_matched = tf.cast(
+        tf.equal(tf.cast(cls_targets, tf.int64), detection_classes),
+        dtype=tf.int64)
+    return is_class_matched
+
+  def get_estimator_eval_metric_ops(self, eval_dict):
+    """Returns a dictionary of eval metric ops.
+
+    Note that once value_op is called, the detections and groundtruth added via
+    update_op are cleared.
+
+    This function can take in groundtruth and detections for a batch of images,
+    or for a single image. For the latter case, the batch dimension for input
+    tensors need not be present.
+
+    Args:
+      eval_dict: A dictionary that holds tensors for evaluating object detection
+        performance. For single-image evaluation, this dictionary may be
+        produced from eval_util.result_dict_for_single_example(). If multi-image
+        evaluation, `eval_dict` should contain the fields
+        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to
+        properly unpad the tensors from the batch.
+
+    Returns:
+      a dictionary of metric names to tuple of value_op and update_op that can
+      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all
+      update ops must be run together and similarly all value ops must be run
+      together to guarantee correct behaviour.
+    """
+    # Unpack items from the evaluation dictionary.
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    image_id = eval_dict[input_data_fields.key]
+    groundtruth_boxes = eval_dict[input_data_fields.groundtruth_boxes]
+    groundtruth_classes = eval_dict[input_data_fields.groundtruth_classes]
+    detection_boxes = eval_dict[detection_fields.detection_boxes]
+    detection_scores = eval_dict[detection_fields.detection_scores]
+    detection_classes = eval_dict[detection_fields.detection_classes]
+    num_gt_boxes_per_image = eval_dict.get(
+        'num_groundtruth_boxes_per_image', None)
+    num_det_boxes_per_image = eval_dict.get('num_det_boxes_per_image', None)
+    is_annotated_batched = eval_dict.get('is_annotated', None)
+
+    if not image_id.shape.as_list():
+      # Apply a batch dimension to all tensors.
+      image_id = tf.expand_dims(image_id, 0)
+      groundtruth_boxes = tf.expand_dims(groundtruth_boxes, 0)
+      groundtruth_classes = tf.expand_dims(groundtruth_classes, 0)
+      detection_boxes = tf.expand_dims(detection_boxes, 0)
+      detection_scores = tf.expand_dims(detection_scores, 0)
+      detection_classes = tf.expand_dims(detection_classes, 0)
+
+      if num_gt_boxes_per_image is None:
+        num_gt_boxes_per_image = tf.shape(groundtruth_boxes)[1:2]
+      else:
+        num_gt_boxes_per_image = tf.expand_dims(num_gt_boxes_per_image, 0)
+
+      if num_det_boxes_per_image is None:
+        num_det_boxes_per_image = tf.shape(detection_boxes)[1:2]
+      else:
+        num_det_boxes_per_image = tf.expand_dims(num_det_boxes_per_image, 0)
+
+      if is_annotated_batched is None:
+        is_annotated_batched = tf.constant([True])
+      else:
+        is_annotated_batched = tf.expand_dims(is_annotated_batched, 0)
+    else:
+      if num_gt_boxes_per_image is None:
+        num_gt_boxes_per_image = tf.tile(
+            tf.shape(groundtruth_boxes)[1:2],
+            multiples=tf.shape(groundtruth_boxes)[0:1])
+      if num_det_boxes_per_image is None:
+        num_det_boxes_per_image = tf.tile(
+            tf.shape(detection_boxes)[1:2],
+            multiples=tf.shape(detection_boxes)[0:1])
+      if is_annotated_batched is None:
+        is_annotated_batched = tf.ones_like(image_id, dtype=tf.bool)
+
+    # Filter images based on is_annotated_batched and match detections.
+    image_info = [tf.boolean_mask(tensor, is_annotated_batched) for tensor in
+                  [groundtruth_boxes, groundtruth_classes,
+                   num_gt_boxes_per_image, detection_boxes, detection_classes,
+                   num_det_boxes_per_image]]
+    is_class_matched = tf.map_fn(
+        self.match_single_image_info, image_info, dtype=tf.int64)
+    y_true = tf.squeeze(is_class_matched)
+    y_pred = tf.squeeze(tf.boolean_mask(detection_scores, is_annotated_batched))
+    ece, update_op = calibration_metrics.expected_calibration_error(
+        y_true, y_pred)
+    return {'CalibrationError/ExpectedCalibrationError': (ece, update_op)}
+
+  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
+    """Adds groundtruth for a single image to be used for evaluation.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      groundtruth_dict: A dictionary of groundtruth numpy arrays required
+        for evaluations.
+    """
+    raise NotImplementedError
+
+  def add_single_detected_image_info(self, image_id, detections_dict):
+    """Adds detections for a single image to be used for evaluation.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      detections_dict: A dictionary of detection numpy arrays required for
+        evaluation.
+    """
+    raise NotImplementedError
+
+  def evaluate(self):
+    """Evaluates detections and returns a dictionary of metrics."""
+    raise NotImplementedError
+
+  def clear(self):
+    """Clears the state to prepare for a fresh evaluation."""
+    raise NotImplementedError
diff --git a/research/object_detection/metrics/calibration_evaluation_test.py b/research/object_detection/metrics/calibration_evaluation_test.py
new file mode 100644
index 00000000..422567e0
--- /dev/null
+++ b/research/object_detection/metrics/calibration_evaluation_test.py
@@ -0,0 +1,200 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for tensorflow_models.object_detection.metrics.calibration_evaluation."""  # pylint: disable=line-too-long
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from object_detection.core import standard_fields
+from object_detection.metrics import calibration_evaluation
+
+
+def _get_categories_list():
+  return [{
+      'id': 1,
+      'name': 'person'
+  }, {
+      'id': 2,
+      'name': 'dog'
+  }, {
+      'id': 3,
+      'name': 'cat'
+  }]
+
+
+class CalibrationDetectionEvaluationTest(tf.test.TestCase):
+
+  def _get_ece(self, ece_op, update_op):
+    """Return scalar expected calibration error."""
+    with self.test_session() as sess:
+      metrics_vars = tf.get_collection(tf.GraphKeys.METRIC_VARIABLES)
+      sess.run(tf.variables_initializer(var_list=metrics_vars))
+      _ = sess.run(update_op)
+    return sess.run(ece_op)
+
+  def testGetECEWithMatchingGroundtruthAndDetections(self):
+    """Tests that ECE is calculated correctly when box matches exist."""
+    calibration_evaluator = calibration_evaluation.CalibrationDetectionEvaluator(
+        _get_categories_list(), iou_threshold=0.5)
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    # All gt and detection boxes match.
+    base_eval_dict = {
+        input_data_fields.key:
+            tf.constant(['image_1', 'image_2', 'image_3']),
+        input_data_fields.groundtruth_boxes:
+            tf.constant([[[100., 100., 200., 200.]],
+                         [[50., 50., 100., 100.]],
+                         [[25., 25., 50., 50.]]],
+                        dtype=tf.float32),
+        detection_fields.detection_boxes:
+            tf.constant([[[100., 100., 200., 200.]],
+                         [[50., 50., 100., 100.]],
+                         [[25., 25., 50., 50.]]],
+                        dtype=tf.float32),
+        input_data_fields.groundtruth_classes:
+            tf.constant([[1], [2], [3]], dtype=tf.int64),
+        # Note that, in the zero ECE case, the detection class for image_2
+        # should NOT match groundtruth, since the detection score is zero.
+        detection_fields.detection_scores:
+            tf.constant([[1.0], [0.0], [1.0]], dtype=tf.float32)
+    }
+
+    # Zero ECE (perfectly calibrated).
+    zero_ece_eval_dict = base_eval_dict.copy()
+    zero_ece_eval_dict[detection_fields.detection_classes] = tf.constant(
+        [[1], [1], [3]], dtype=tf.int64)
+    zero_ece_op, zero_ece_update_op = (
+        calibration_evaluator.get_estimator_eval_metric_ops(zero_ece_eval_dict)
+        ['CalibrationError/ExpectedCalibrationError'])
+    zero_ece = self._get_ece(zero_ece_op, zero_ece_update_op)
+    self.assertAlmostEqual(zero_ece, 0.0)
+
+    # ECE of 1 (poorest calibration).
+    one_ece_eval_dict = base_eval_dict.copy()
+    one_ece_eval_dict[detection_fields.detection_classes] = tf.constant(
+        [[3], [2], [1]], dtype=tf.int64)
+    one_ece_op, one_ece_update_op = (
+        calibration_evaluator.get_estimator_eval_metric_ops(one_ece_eval_dict)
+        ['CalibrationError/ExpectedCalibrationError'])
+    one_ece = self._get_ece(one_ece_op, one_ece_update_op)
+    self.assertAlmostEqual(one_ece, 1.0)
+
+  def testGetECEWithUnmatchedGroundtruthAndDetections(self):
+    """Tests that ECE is correctly calculated when boxes are unmatched."""
+    calibration_evaluator = calibration_evaluation.CalibrationDetectionEvaluator(
+        _get_categories_list(), iou_threshold=0.5)
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    # No gt and detection boxes match.
+    eval_dict = {
+        input_data_fields.key:
+            tf.constant(['image_1', 'image_2', 'image_3']),
+        input_data_fields.groundtruth_boxes:
+            tf.constant([[[100., 100., 200., 200.]],
+                         [[50., 50., 100., 100.]],
+                         [[25., 25., 50., 50.]]],
+                        dtype=tf.float32),
+        detection_fields.detection_boxes:
+            tf.constant([[[50., 50., 100., 100.]],
+                         [[25., 25., 50., 50.]],
+                         [[100., 100., 200., 200.]]],
+                        dtype=tf.float32),
+        input_data_fields.groundtruth_classes:
+            tf.constant([[1], [2], [3]], dtype=tf.int64),
+        detection_fields.detection_classes:
+            tf.constant([[1], [1], [3]], dtype=tf.int64),
+        # Detection scores of zero when boxes are unmatched = ECE of zero.
+        detection_fields.detection_scores:
+            tf.constant([[0.0], [0.0], [0.0]], dtype=tf.float32)
+    }
+
+    ece_op, update_op = calibration_evaluator.get_estimator_eval_metric_ops(
+        eval_dict)['CalibrationError/ExpectedCalibrationError']
+    ece = self._get_ece(ece_op, update_op)
+    self.assertAlmostEqual(ece, 0.0)
+
+  def testGetECEWithBatchedDetections(self):
+    """Tests that ECE is correct with multiple detections per image."""
+    calibration_evaluator = calibration_evaluation.CalibrationDetectionEvaluator(
+        _get_categories_list(), iou_threshold=0.5)
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    # Note that image_2 has mismatched classes and detection scores but should
+    # still produce ECE of 0 because detection scores are also 0.
+    eval_dict = {
+        input_data_fields.key:
+            tf.constant(['image_1', 'image_2', 'image_3']),
+        input_data_fields.groundtruth_boxes:
+            tf.constant([[[100., 100., 200., 200.], [50., 50., 100., 100.]],
+                         [[50., 50., 100., 100.], [100., 100., 200., 200.]],
+                         [[25., 25., 50., 50.], [100., 100., 200., 200.]]],
+                        dtype=tf.float32),
+        detection_fields.detection_boxes:
+            tf.constant([[[100., 100., 200., 200.], [50., 50., 100., 100.]],
+                         [[50., 50., 100., 100.], [25., 25., 50., 50.]],
+                         [[25., 25., 50., 50.], [100., 100., 200., 200.]]],
+                        dtype=tf.float32),
+        input_data_fields.groundtruth_classes:
+            tf.constant([[1, 2], [2, 3], [3, 1]], dtype=tf.int64),
+        detection_fields.detection_classes:
+            tf.constant([[1, 2], [1, 1], [3, 1]], dtype=tf.int64),
+        detection_fields.detection_scores:
+            tf.constant([[1.0, 1.0], [0.0, 0.0], [1.0, 1.0]], dtype=tf.float32)
+    }
+
+    ece_op, update_op = calibration_evaluator.get_estimator_eval_metric_ops(
+        eval_dict)['CalibrationError/ExpectedCalibrationError']
+    ece = self._get_ece(ece_op, update_op)
+    self.assertAlmostEqual(ece, 0.0)
+
+  def testGetECEWhenImagesFilteredByIsAnnotated(self):
+    """Tests that ECE is correct when detections filtered by is_annotated."""
+    calibration_evaluator = calibration_evaluation.CalibrationDetectionEvaluator(
+        _get_categories_list(), iou_threshold=0.5)
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    # ECE will be 0 only if the third image is filtered by is_annotated.
+    eval_dict = {
+        input_data_fields.key:
+            tf.constant(['image_1', 'image_2', 'image_3']),
+        input_data_fields.groundtruth_boxes:
+            tf.constant([[[100., 100., 200., 200.]],
+                         [[50., 50., 100., 100.]],
+                         [[25., 25., 50., 50.]]],
+                        dtype=tf.float32),
+        detection_fields.detection_boxes:
+            tf.constant([[[100., 100., 200., 200.]],
+                         [[50., 50., 100., 100.]],
+                         [[25., 25., 50., 50.]]],
+                        dtype=tf.float32),
+        input_data_fields.groundtruth_classes:
+            tf.constant([[1], [2], [1]], dtype=tf.int64),
+        detection_fields.detection_classes:
+            tf.constant([[1], [1], [3]], dtype=tf.int64),
+        detection_fields.detection_scores:
+            tf.constant([[1.0], [0.0], [1.0]], dtype=tf.float32),
+        'is_annotated': tf.constant([True, True, False], dtype=tf.bool)
+    }
+
+    ece_op, update_op = calibration_evaluator.get_estimator_eval_metric_ops(
+        eval_dict)['CalibrationError/ExpectedCalibrationError']
+    ece = self._get_ece(ece_op, update_op)
+    self.assertAlmostEqual(ece, 0.0)
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/metrics/calibration_metrics.py b/research/object_detection/metrics/calibration_metrics.py
new file mode 100644
index 00000000..6c90d033
--- /dev/null
+++ b/research/object_detection/metrics/calibration_metrics.py
@@ -0,0 +1,115 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Object detection calibration metrics.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from tensorflow.python.ops import metrics_impl
+
+
+def _safe_div(numerator, denominator):
+  """Divides two tensors element-wise, returning 0 if the denominator is <= 0.
+
+  Args:
+    numerator: A real `Tensor`.
+    denominator: A real `Tensor`, with dtype matching `numerator`.
+
+  Returns:
+    0 if `denominator` <= 0, else `numerator` / `denominator`
+  """
+  t = tf.truediv(numerator, denominator)
+  zero = tf.zeros_like(t, dtype=denominator.dtype)
+  condition = tf.greater(denominator, zero)
+  zero = tf.cast(zero, t.dtype)
+  return tf.where(condition, t, zero)
+
+
+def _ece_from_bins(bin_counts, bin_true_sum, bin_preds_sum, name):
+  """Calculates Expected Calibration Error from accumulated statistics."""
+  bin_accuracies = _safe_div(bin_true_sum, bin_counts)
+  bin_confidences = _safe_div(bin_preds_sum, bin_counts)
+  abs_bin_errors = tf.abs(bin_accuracies - bin_confidences)
+  bin_weights = _safe_div(bin_counts, tf.reduce_sum(bin_counts))
+  return tf.reduce_sum(abs_bin_errors * bin_weights, name=name)
+
+
+def expected_calibration_error(y_true, y_pred, nbins=20):
+  """Calculates Expected Calibration Error (ECE).
+
+  ECE is a scalar summary statistic of calibration error. It is the
+  sample-weighted average of the difference between the predicted and true
+  probabilities of a positive detection across uniformly-spaced model
+  confidences [0, 1]. See referenced paper for a thorough explanation.
+
+  Reference:
+    Guo, et. al, "On Calibration of Modern Neural Networks"
+    Page 2, Expected Calibration Error (ECE).
+    https://arxiv.org/pdf/1706.04599.pdf
+
+  This function creates three local variables, `bin_counts`, `bin_true_sum`, and
+  `bin_preds_sum` that are used to compute ECE.  For estimation of the metric
+  over a stream of data, the function creates an `update_op` operation that
+  updates these variables and returns the ECE.
+
+  Args:
+    y_true: 1-D tf.int64 Tensor of binarized ground truth, corresponding to each
+      prediction in y_pred.
+    y_pred: 1-D tf.float32 tensor of model confidence scores in range
+      [0.0, 1.0].
+    nbins: int specifying the number of uniformly-spaced bins into which y_pred
+      will be bucketed.
+
+  Returns:
+    value_op: A value metric op that returns ece.
+    update_op: An operation that increments the `bin_counts`, `bin_true_sum`,
+      and `bin_preds_sum` variables appropriately and whose value matches `ece`.
+
+  Raises:
+    InvalidArgumentError: if y_pred is not in [0.0, 1.0].
+  """
+  bin_counts = metrics_impl.metric_variable(
+      [nbins], tf.float32, name='bin_counts')
+  bin_true_sum = metrics_impl.metric_variable(
+      [nbins], tf.float32, name='true_sum')
+  bin_preds_sum = metrics_impl.metric_variable(
+      [nbins], tf.float32, name='preds_sum')
+
+  with tf.control_dependencies([
+      tf.assert_greater_equal(y_pred, 0.0),
+      tf.assert_less_equal(y_pred, 1.0),
+  ]):
+    bin_ids = tf.histogram_fixed_width_bins(y_pred, [0.0, 1.0], nbins=nbins)
+
+  with tf.control_dependencies([bin_ids]):
+    update_bin_counts_op = tf.assign_add(
+        bin_counts, tf.to_float(tf.bincount(bin_ids, minlength=nbins)))
+    update_bin_true_sum_op = tf.assign_add(
+        bin_true_sum,
+        tf.to_float(tf.bincount(bin_ids, weights=y_true, minlength=nbins)))
+    update_bin_preds_sum_op = tf.assign_add(
+        bin_preds_sum,
+        tf.to_float(tf.bincount(bin_ids, weights=y_pred, minlength=nbins)))
+
+  ece_update_op = _ece_from_bins(
+      update_bin_counts_op,
+      update_bin_true_sum_op,
+      update_bin_preds_sum_op,
+      name='update_op')
+  ece = _ece_from_bins(bin_counts, bin_true_sum, bin_preds_sum, name='value')
+  return ece, ece_update_op
diff --git a/research/object_detection/metrics/calibration_metrics_test.py b/research/object_detection/metrics/calibration_metrics_test.py
new file mode 100644
index 00000000..4518293c
--- /dev/null
+++ b/research/object_detection/metrics/calibration_metrics_test.py
@@ -0,0 +1,109 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for calibration_metrics."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+from object_detection.metrics import calibration_metrics
+
+
+class CalibrationLibTest(tf.test.TestCase):
+
+  @staticmethod
+  def _get_calibration_placeholders():
+    """Returns TF placeholders for y_true and y_pred."""
+    return (tf.placeholder(tf.int64, shape=(None)),
+            tf.placeholder(tf.float32, shape=(None)))
+
+  def test_expected_calibration_error_all_bins_filled(self):
+    """Test expected calibration error when all bins contain predictions."""
+    y_true, y_pred = self._get_calibration_placeholders()
+    expected_ece_op, update_op = calibration_metrics.expected_calibration_error(
+        y_true, y_pred, nbins=2)
+    with self.test_session() as sess:
+      metrics_vars = tf.get_collection(tf.GraphKeys.METRIC_VARIABLES)
+      sess.run(tf.variables_initializer(var_list=metrics_vars))
+      # Bin calibration errors (|confidence - accuracy| * bin_weight):
+      # - [0,0.5): |0.2 - 0.333| * (3/5) = 0.08
+      # - [0.5, 1]: |0.75 - 0.5| * (2/5) = 0.1
+      sess.run(
+          update_op,
+          feed_dict={
+              y_pred: np.array([0., 0.2, 0.4, 0.5, 1.0]),
+              y_true: np.array([0, 0, 1, 0, 1])
+          })
+    actual_ece = 0.08 + 0.1
+    expected_ece = sess.run(expected_ece_op)
+    self.assertAlmostEqual(actual_ece, expected_ece)
+
+  def test_expected_calibration_error_all_bins_not_filled(self):
+    """Test expected calibration error when no predictions for one bin."""
+    y_true, y_pred = self._get_calibration_placeholders()
+    expected_ece_op, update_op = calibration_metrics.expected_calibration_error(
+        y_true, y_pred, nbins=2)
+    with self.test_session() as sess:
+      metrics_vars = tf.get_collection(tf.GraphKeys.METRIC_VARIABLES)
+      sess.run(tf.variables_initializer(var_list=metrics_vars))
+      # Bin calibration errors (|confidence - accuracy| * bin_weight):
+      # - [0,0.5): |0.2 - 0.333| * (3/5) = 0.08
+      # - [0.5, 1]: |0.75 - 0.5| * (2/5) = 0.1
+      sess.run(
+          update_op,
+          feed_dict={
+              y_pred: np.array([0., 0.2, 0.4]),
+              y_true: np.array([0, 0, 1])
+          })
+    actual_ece = np.abs(0.2 - (1 / 3.))
+    expected_ece = sess.run(expected_ece_op)
+    self.assertAlmostEqual(actual_ece, expected_ece)
+
+  def test_expected_calibration_error_with_multiple_data_streams(self):
+    """Test expected calibration error when multiple data batches provided."""
+    y_true, y_pred = self._get_calibration_placeholders()
+    expected_ece_op, update_op = calibration_metrics.expected_calibration_error(
+        y_true, y_pred, nbins=2)
+    with self.test_session() as sess:
+      metrics_vars = tf.get_collection(tf.GraphKeys.METRIC_VARIABLES)
+      sess.run(tf.variables_initializer(var_list=metrics_vars))
+      # Identical data to test_expected_calibration_error_all_bins_filled,
+      # except split over three batches.
+      sess.run(
+          update_op,
+          feed_dict={
+              y_pred: np.array([0., 0.2]),
+              y_true: np.array([0, 0])
+          })
+      sess.run(
+          update_op,
+          feed_dict={
+              y_pred: np.array([0.4, 0.5]),
+              y_true: np.array([1, 0])
+          })
+      sess.run(
+          update_op, feed_dict={
+              y_pred: np.array([1.0]),
+              y_true: np.array([1])
+          })
+    actual_ece = 0.08 + 0.1
+    expected_ece = sess.run(expected_ece_op)
+    self.assertAlmostEqual(actual_ece, expected_ece)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index ace75d99..98dad278 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -51,6 +51,7 @@ MODEL_BUILD_UTIL_MAP = {
         inputs.create_eval_input_fn,
     'create_predict_input_fn':
         inputs.create_predict_input_fn,
+    'detection_model_fn_base': model_builder.build,
 }
 
 
@@ -184,7 +185,8 @@ def unstack_batch(tensor_dict, unpad_groundtruth_tensors=True):
   return unbatched_tensor_dict
 
 
-def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
+def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
+                    postprocess_on_cpu=False):
   """Creates a model function for `Estimator`.
 
   Args:
@@ -193,6 +195,8 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
     hparams: `HParams` object.
     use_tpu: Boolean indicating whether model should be constructed for
         use on TPU.
+    postprocess_on_cpu: When use_tpu and postprocess_on_cpu is true, postprocess
+        is scheduled on the host cpu.
 
   Returns:
     `model_fn` for `Estimator`.
@@ -282,9 +286,20 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
       prediction_dict = detection_model.predict(
           preprocessed_images,
           features[fields.InputDataFields.true_image_shape])
+
+    def postprocess_wrapper(args):
+      return detection_model.postprocess(args[0], args[1])
+
     if mode in (tf.estimator.ModeKeys.EVAL, tf.estimator.ModeKeys.PREDICT):
-      detections = detection_model.postprocess(
-          prediction_dict, features[fields.InputDataFields.true_image_shape])
+      if use_tpu and postprocess_on_cpu:
+        detections = tf.contrib.tpu.outside_compilation(
+            postprocess_wrapper,
+            (prediction_dict,
+             features[fields.InputDataFields.true_image_shape]))
+      else:
+        detections = postprocess_wrapper((
+            prediction_dict,
+            features[fields.InputDataFields.true_image_shape]))
 
     if mode == tf.estimator.ModeKeys.TRAIN:
       if train_config.fine_tune_checkpoint and hparams.load_pretrained:
@@ -501,6 +516,8 @@ def create_estimator_and_inputs(run_config,
                                 params=None,
                                 override_eval_num_epochs=True,
                                 save_final_config=False,
+                                postprocess_on_cpu=False,
+                                export_to_tpu=None,
                                 **kwargs):
   """Creates `Estimator`, input functions, and steps.
 
@@ -535,10 +552,15 @@ def create_estimator_and_inputs(run_config,
       is True.
     params: Parameter dictionary passed from the estimator. Only used if
       `use_tpu_estimator` is True.
-    override_eval_num_epochs: Whether to overwrite the number of epochs to
-      1 for eval_input.
+    override_eval_num_epochs: Whether to overwrite the number of epochs to 1 for
+      eval_input.
     save_final_config: Whether to save final config (obtained after applying
       overrides) to `estimator.model_dir`.
+    postprocess_on_cpu: When use_tpu and postprocess_on_cpu are true,
+      postprocess is scheduled on the host cpu.
+    export_to_tpu: When use_tpu and export_to_tpu are true,
+      `export_savedmodel()` exports a metagraph for serving on TPU besides the
+      one on CPU.
     **kwargs: Additional keyword arguments for configuration override.
 
   Returns:
@@ -561,12 +583,14 @@ def create_estimator_and_inputs(run_config,
   create_train_input_fn = MODEL_BUILD_UTIL_MAP['create_train_input_fn']
   create_eval_input_fn = MODEL_BUILD_UTIL_MAP['create_eval_input_fn']
   create_predict_input_fn = MODEL_BUILD_UTIL_MAP['create_predict_input_fn']
+  detection_model_fn_base = MODEL_BUILD_UTIL_MAP['detection_model_fn_base']
 
-  configs = get_configs_from_pipeline_file(pipeline_config_path,
-                                           config_override=config_override)
+  configs = get_configs_from_pipeline_file(
+      pipeline_config_path, config_override=config_override)
   kwargs.update({
       'train_steps': train_steps,
-      'sample_1_of_n_eval_examples': sample_1_of_n_eval_examples
+      'sample_1_of_n_eval_examples': sample_1_of_n_eval_examples,
+      'use_bfloat16': configs['train_config'].use_bfloat16 and use_tpu
   })
   if override_eval_num_epochs:
     kwargs.update({'eval_num_epochs': 1})
@@ -595,7 +619,7 @@ def create_estimator_and_inputs(run_config,
     train_steps = train_config.num_steps
 
   detection_model_fn = functools.partial(
-      model_builder.build, model_config=model_config)
+      detection_model_fn_base, model_config=model_config)
 
   # Create the input functions for TRAIN/EVAL/PREDICT.
   train_input_fn = create_train_input_fn(
@@ -618,10 +642,13 @@ def create_estimator_and_inputs(run_config,
   predict_input_fn = create_predict_input_fn(
       model_config=model_config, predict_input_config=eval_input_configs[0])
 
-  export_to_tpu = hparams.get('export_to_tpu', False)
+  # Read export_to_tpu from hparams if not passed.
+  if export_to_tpu is None:
+    export_to_tpu = hparams.get('export_to_tpu', False)
   tf.logging.info('create_estimator_and_inputs: use_tpu %s, export_to_tpu %s',
                   use_tpu, export_to_tpu)
-  model_fn = model_fn_creator(detection_model_fn, configs, hparams, use_tpu)
+  model_fn = model_fn_creator(detection_model_fn, configs, hparams, use_tpu,
+                              postprocess_on_cpu)
   if use_tpu_estimator:
     estimator = tf.contrib.tpu.TPUEstimator(
         model_fn=model_fn,
@@ -630,7 +657,8 @@ def create_estimator_and_inputs(run_config,
         eval_batch_size=num_shards * 1 if use_tpu else 1,
         use_tpu=use_tpu,
         config=run_config,
-        # TODO(lzc): Remove conditional after CMLE moves to TF 1.9
+        export_to_tpu=export_to_tpu,
+        eval_on_tpu=False,  # Eval runs on CPU, so disable eval on TPU
         params=params if params else {})
   else:
     estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 4c2fe68d..f48a4df9 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -29,6 +29,11 @@ import tensorflow as tf
 from object_detection.utils import ops
 slim = tf.contrib.slim
 
+# Activation bound used for TPU v1. Activations will be clipped to
+# [-ACTIVATION_BOUND, ACTIVATION_BOUND] when training with
+# use_bounded_activations enabled.
+ACTIVATION_BOUND = 6.0
+
 
 def get_depth_fn(depth_multiplier, min_depth):
   """Builds a callable to compute depth (output channels) of conv filters.
@@ -418,7 +423,9 @@ def fpn_top_down_feature_maps(image_features,
                               depth,
                               use_depthwise=False,
                               use_explicit_padding=False,
-                              scope=None):
+                              use_bounded_activations=False,
+                              scope=None,
+                              use_native_resize_op=False):
   """Generates `top-down` feature maps for Feature Pyramid Networks.
 
   See https://arxiv.org/abs/1612.03144 for details.
@@ -431,7 +438,12 @@ def fpn_top_down_feature_maps(image_features,
     use_depthwise: whether to use depthwise separable conv instead of regular
       conv.
     use_explicit_padding: whether to use explicit padding.
+    use_bounded_activations: Whether or not to clip activations to range
+      [-ACTIVATION_BOUND, ACTIVATION_BOUND]. Bounded activations better lend
+      themselves to quantized inference.
     scope: A scope name to wrap this op under.
+    use_native_resize_op: If True, uses tf.image.resize_nearest_neighbor op for
+      the upsampling process instead of reshape and broadcasting implementation.
 
   Returns:
     feature_maps: an OrderedDict mapping keys (feature map names) to
@@ -449,21 +461,36 @@ def fpn_top_down_feature_maps(image_features,
           image_features[-1][1],
           depth, [1, 1], activation_fn=None, normalizer_fn=None,
           scope='projection_%d' % num_levels)
+      if use_bounded_activations:
+        top_down = tf.clip_by_value(top_down, -ACTIVATION_BOUND,
+                                    ACTIVATION_BOUND)
       output_feature_maps_list.append(top_down)
       output_feature_map_keys.append(
           'top_down_%s' % image_features[-1][0])
 
       for level in reversed(range(num_levels - 1)):
-        top_down = ops.nearest_neighbor_upsampling(top_down, 2)
+        if use_native_resize_op:
+          with tf.name_scope('nearest_neighbor_upsampling'):
+            top_down_shape = top_down.shape.as_list()
+            top_down = tf.image.resize_nearest_neighbor(
+                top_down, [top_down_shape[1] * 2, top_down_shape[2] * 2])
+        else:
+          top_down = ops.nearest_neighbor_upsampling(top_down, scale=2)
         residual = slim.conv2d(
             image_features[level][1], depth, [1, 1],
             activation_fn=None, normalizer_fn=None,
             scope='projection_%d' % (level + 1))
+        if use_bounded_activations:
+          residual = tf.clip_by_value(residual, -ACTIVATION_BOUND,
+                                      ACTIVATION_BOUND)
         if use_explicit_padding:
           # slice top_down to the same shape as residual
           residual_shape = tf.shape(residual)
           top_down = top_down[:, :residual_shape[1], :residual_shape[2], :]
         top_down += residual
+        if use_bounded_activations:
+          top_down = tf.clip_by_value(top_down, -ACTIVATION_BOUND,
+                                      ACTIVATION_BOUND)
         if use_depthwise:
           conv_op = functools.partial(slim.separable_conv2d, depth_multiplier=1)
         else:
diff --git a/research/object_detection/models/feature_map_generators_test.py b/research/object_detection/models/feature_map_generators_test.py
index f7ac0cc0..dabb918a 100644
--- a/research/object_detection/models/feature_map_generators_test.py
+++ b/research/object_detection/models/feature_map_generators_test.py
@@ -17,6 +17,7 @@
 
 from absl.testing import parameterized
 
+import numpy as np
 import tensorflow as tf
 
 from google.protobuf import text_format
@@ -124,7 +125,36 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
           (key, value.shape) for key, value in out_feature_maps.items())
       self.assertDictEqual(expected_feature_map_shapes, out_feature_map_shapes)
 
-  # TODO(kaftan): Remove conditional after CMLE moves to TF 1.10
+  def test_get_expected_feature_map_shapes_with_inception_v2_use_depthwise(
+      self, use_keras):
+    image_features = {
+        'Mixed_3c': tf.random_uniform([4, 28, 28, 256], dtype=tf.float32),
+        'Mixed_4c': tf.random_uniform([4, 14, 14, 576], dtype=tf.float32),
+        'Mixed_5c': tf.random_uniform([4, 7, 7, 1024], dtype=tf.float32)
+    }
+    layout_copy = INCEPTION_V2_LAYOUT.copy()
+    layout_copy['use_depthwise'] = True
+    feature_map_generator = self._build_feature_map_generator(
+        feature_map_layout=layout_copy,
+        use_keras=use_keras
+    )
+    feature_maps = feature_map_generator(image_features)
+
+    expected_feature_map_shapes = {
+        'Mixed_3c': (4, 28, 28, 256),
+        'Mixed_4c': (4, 14, 14, 576),
+        'Mixed_5c': (4, 7, 7, 1024),
+        'Mixed_5c_2_Conv2d_3_3x3_s2_512': (4, 4, 4, 512),
+        'Mixed_5c_2_Conv2d_4_3x3_s2_256': (4, 2, 2, 256),
+        'Mixed_5c_2_Conv2d_5_3x3_s2_256': (4, 1, 1, 256)}
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      out_feature_maps = sess.run(feature_maps)
+      out_feature_map_shapes = dict(
+          (key, value.shape) for key, value in out_feature_maps.items())
+      self.assertDictEqual(expected_feature_map_shapes, out_feature_map_shapes)
 
   def test_get_expected_feature_map_shapes_use_explicit_padding(
       self, use_keras):
@@ -297,12 +327,87 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
       else:
         self.assertSetEqual(expected_slim_variables, actual_variable_set)
 
-  # TODO(kaftan): Remove conditional after CMLE moves to TF 1.10
+  def test_get_expected_variable_names_with_inception_v2_use_depthwise(
+      self,
+      use_keras):
+    image_features = {
+        'Mixed_3c': tf.random_uniform([4, 28, 28, 256], dtype=tf.float32),
+        'Mixed_4c': tf.random_uniform([4, 14, 14, 576], dtype=tf.float32),
+        'Mixed_5c': tf.random_uniform([4, 7, 7, 1024], dtype=tf.float32)
+    }
+    layout_copy = INCEPTION_V2_LAYOUT.copy()
+    layout_copy['use_depthwise'] = True
+    feature_map_generator = self._build_feature_map_generator(
+        feature_map_layout=layout_copy,
+        use_keras=use_keras
+    )
+    feature_maps = feature_map_generator(image_features)
+
+    expected_slim_variables = set([
+        'Mixed_5c_1_Conv2d_3_1x1_256/weights',
+        'Mixed_5c_1_Conv2d_3_1x1_256/biases',
+        'Mixed_5c_2_Conv2d_3_3x3_s2_512_depthwise/depthwise_weights',
+        'Mixed_5c_2_Conv2d_3_3x3_s2_512_depthwise/biases',
+        'Mixed_5c_2_Conv2d_3_3x3_s2_512/weights',
+        'Mixed_5c_2_Conv2d_3_3x3_s2_512/biases',
+        'Mixed_5c_1_Conv2d_4_1x1_128/weights',
+        'Mixed_5c_1_Conv2d_4_1x1_128/biases',
+        'Mixed_5c_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights',
+        'Mixed_5c_2_Conv2d_4_3x3_s2_256_depthwise/biases',
+        'Mixed_5c_2_Conv2d_4_3x3_s2_256/weights',
+        'Mixed_5c_2_Conv2d_4_3x3_s2_256/biases',
+        'Mixed_5c_1_Conv2d_5_1x1_128/weights',
+        'Mixed_5c_1_Conv2d_5_1x1_128/biases',
+        'Mixed_5c_2_Conv2d_5_3x3_s2_256_depthwise/depthwise_weights',
+        'Mixed_5c_2_Conv2d_5_3x3_s2_256_depthwise/biases',
+        'Mixed_5c_2_Conv2d_5_3x3_s2_256/weights',
+        'Mixed_5c_2_Conv2d_5_3x3_s2_256/biases',
+    ])
+
+    expected_keras_variables = set([
+        'FeatureMaps/Mixed_5c_1_Conv2d_3_1x1_256_conv/kernel',
+        'FeatureMaps/Mixed_5c_1_Conv2d_3_1x1_256_conv/bias',
+        ('FeatureMaps/Mixed_5c_2_Conv2d_3_3x3_s2_512_depthwise_conv/'
+         'depthwise_kernel'),
+        ('FeatureMaps/Mixed_5c_2_Conv2d_3_3x3_s2_512_depthwise_conv/'
+         'bias'),
+        'FeatureMaps/Mixed_5c_2_Conv2d_3_3x3_s2_512_conv/kernel',
+        'FeatureMaps/Mixed_5c_2_Conv2d_3_3x3_s2_512_conv/bias',
+        'FeatureMaps/Mixed_5c_1_Conv2d_4_1x1_128_conv/kernel',
+        'FeatureMaps/Mixed_5c_1_Conv2d_4_1x1_128_conv/bias',
+        ('FeatureMaps/Mixed_5c_2_Conv2d_4_3x3_s2_256_depthwise_conv/'
+         'depthwise_kernel'),
+        ('FeatureMaps/Mixed_5c_2_Conv2d_4_3x3_s2_256_depthwise_conv/'
+         'bias'),
+        'FeatureMaps/Mixed_5c_2_Conv2d_4_3x3_s2_256_conv/kernel',
+        'FeatureMaps/Mixed_5c_2_Conv2d_4_3x3_s2_256_conv/bias',
+        'FeatureMaps/Mixed_5c_1_Conv2d_5_1x1_128_conv/kernel',
+        'FeatureMaps/Mixed_5c_1_Conv2d_5_1x1_128_conv/bias',
+        ('FeatureMaps/Mixed_5c_2_Conv2d_5_3x3_s2_256_depthwise_conv/'
+         'depthwise_kernel'),
+        ('FeatureMaps/Mixed_5c_2_Conv2d_5_3x3_s2_256_depthwise_conv/'
+         'bias'),
+        'FeatureMaps/Mixed_5c_2_Conv2d_5_3x3_s2_256_conv/kernel',
+        'FeatureMaps/Mixed_5c_2_Conv2d_5_3x3_s2_256_conv/bias',
+    ])
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      sess.run(feature_maps)
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+      if use_keras:
+        self.assertSetEqual(expected_keras_variables, actual_variable_set)
+      else:
+        self.assertSetEqual(expected_slim_variables, actual_variable_set)
 
 
-class FPNFeatureMapGeneratorTest(tf.test.TestCase):
+@parameterized.parameters({'use_native_resize_op': True},
+                          {'use_native_resize_op': False})
+class FPNFeatureMapGeneratorTest(tf.test.TestCase, parameterized.TestCase):
 
-  def test_get_expected_feature_map_shapes(self):
+  def test_get_expected_feature_map_shapes(self, use_native_resize_op):
     image_features = [
         ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
         ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
@@ -310,7 +415,9 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase):
         ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
     ]
     feature_maps = feature_map_generators.fpn_top_down_feature_maps(
-        image_features=image_features, depth=128)
+        image_features=image_features,
+        depth=128,
+        use_native_resize_op=use_native_resize_op)
 
     expected_feature_map_shapes = {
         'top_down_block2': (4, 8, 8, 128),
@@ -327,7 +434,95 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase):
                                 for key, value in out_feature_maps.items()}
       self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)
 
-  def test_get_expected_feature_map_shapes_with_depthwise(self):
+  def test_use_bounded_activations_add_operations(self, use_native_resize_op):
+    tf_graph = tf.Graph()
+    with tf_graph.as_default():
+      image_features = [('block2',
+                         tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
+                        ('block3',
+                         tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
+                        ('block4',
+                         tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
+                        ('block5',
+                         tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))]
+      feature_map_generators.fpn_top_down_feature_maps(
+          image_features=image_features,
+          depth=128,
+          use_bounded_activations=True,
+          use_native_resize_op=use_native_resize_op)
+
+      expected_added_operations = dict.fromkeys([
+          'top_down/clip_by_value', 'top_down/clip_by_value_1',
+          'top_down/clip_by_value_2', 'top_down/clip_by_value_3',
+          'top_down/clip_by_value_4', 'top_down/clip_by_value_5',
+          'top_down/clip_by_value_6'
+      ])
+      op_names = {op.name: None for op in tf_graph.get_operations()}
+      self.assertDictContainsSubset(expected_added_operations, op_names)
+
+  def test_use_bounded_activations_clip_value(self, use_native_resize_op):
+    tf_graph = tf.Graph()
+    with tf_graph.as_default():
+      image_features = [
+          ('block2', 255 * tf.ones([4, 8, 8, 256], dtype=tf.float32)),
+          ('block3', 255 * tf.ones([4, 4, 4, 256], dtype=tf.float32)),
+          ('block4', 255 * tf.ones([4, 2, 2, 256], dtype=tf.float32)),
+          ('block5', 255 * tf.ones([4, 1, 1, 256], dtype=tf.float32))
+      ]
+      feature_map_generators.fpn_top_down_feature_maps(
+          image_features=image_features,
+          depth=128,
+          use_bounded_activations=True,
+          use_native_resize_op=use_native_resize_op)
+
+      expected_clip_by_value_ops = [
+          'top_down/clip_by_value', 'top_down/clip_by_value_1',
+          'top_down/clip_by_value_2', 'top_down/clip_by_value_3',
+          'top_down/clip_by_value_4', 'top_down/clip_by_value_5',
+          'top_down/clip_by_value_6'
+      ]
+
+      # Gathers activation tensors before and after clip_by_value operations.
+      activations = {}
+      for clip_by_value_op in expected_clip_by_value_ops:
+        clip_input_tensor = tf_graph.get_operation_by_name(
+            '{}/Minimum'.format(clip_by_value_op)).inputs[0]
+        clip_output_tensor = tf_graph.get_tensor_by_name(
+            '{}:0'.format(clip_by_value_op))
+        activations.update({
+            'before_{}'.format(clip_by_value_op): clip_input_tensor,
+            'after_{}'.format(clip_by_value_op): clip_output_tensor,
+        })
+
+      expected_lower_bound = -feature_map_generators.ACTIVATION_BOUND
+      expected_upper_bound = feature_map_generators.ACTIVATION_BOUND
+      init_op = tf.global_variables_initializer()
+      with self.test_session() as session:
+        session.run(init_op)
+        activations_output = session.run(activations)
+        for clip_by_value_op in expected_clip_by_value_ops:
+          # Before clipping, activations are beyound the expected bound because
+          # of large input image_features values.
+          activations_before_clipping = (
+              activations_output['before_{}'.format(clip_by_value_op)])
+          before_clipping_lower_bound = np.amin(activations_before_clipping)
+          before_clipping_upper_bound = np.amax(activations_before_clipping)
+          self.assertLessEqual(before_clipping_lower_bound,
+                               expected_lower_bound)
+          self.assertGreaterEqual(before_clipping_upper_bound,
+                                  expected_upper_bound)
+
+          # After clipping, activations are bounded as expectation.
+          activations_after_clipping = (
+              activations_output['after_{}'.format(clip_by_value_op)])
+          after_clipping_lower_bound = np.amin(activations_after_clipping)
+          after_clipping_upper_bound = np.amax(activations_after_clipping)
+          self.assertGreaterEqual(after_clipping_lower_bound,
+                                  expected_lower_bound)
+          self.assertLessEqual(after_clipping_upper_bound, expected_upper_bound)
+
+  def test_get_expected_feature_map_shapes_with_depthwise(
+      self, use_native_resize_op):
     image_features = [
         ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
         ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
@@ -335,7 +530,10 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase):
         ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
     ]
     feature_maps = feature_map_generators.fpn_top_down_feature_maps(
-        image_features=image_features, depth=128, use_depthwise=True)
+        image_features=image_features,
+        depth=128,
+        use_depthwise=True,
+        use_native_resize_op=use_native_resize_op)
 
     expected_feature_map_shapes = {
         'top_down_block2': (4, 8, 8, 128),
diff --git a/research/object_detection/models/keras_applications/mobilenet_v2_test.py b/research/object_detection/models/keras_applications/mobilenet_v2_test.py
deleted file mode 100644
index dafa4bc4..00000000
--- a/research/object_detection/models/keras_applications/mobilenet_v2_test.py
+++ /dev/null
@@ -1,467 +0,0 @@
-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-"""Tests for mobilenet_v2."""
-import itertools
-import numpy as np
-import tensorflow as tf
-
-from google.protobuf import text_format
-
-from object_detection.builders import hyperparams_builder
-from object_detection.models.keras_applications import mobilenet_v2
-from object_detection.protos import hyperparams_pb2
-from object_detection.utils import test_case
-
-_layers_to_check = [
-    'Conv1_relu',
-    'block_1_expand_relu', 'block_1_depthwise_relu', 'block_1_project_BN',
-    'block_2_expand_relu', 'block_2_depthwise_relu', 'block_2_project_BN',
-    'block_3_expand_relu', 'block_3_depthwise_relu', 'block_3_project_BN',
-    'block_4_expand_relu', 'block_4_depthwise_relu', 'block_4_project_BN',
-    'block_5_expand_relu', 'block_5_depthwise_relu', 'block_5_project_BN',
-    'block_6_expand_relu', 'block_6_depthwise_relu', 'block_6_project_BN',
-    'block_7_expand_relu', 'block_7_depthwise_relu', 'block_7_project_BN',
-    'block_8_expand_relu', 'block_8_depthwise_relu', 'block_8_project_BN',
-    'block_9_expand_relu', 'block_9_depthwise_relu', 'block_9_project_BN',
-    'block_10_expand_relu', 'block_10_depthwise_relu', 'block_10_project_BN',
-    'block_11_expand_relu', 'block_11_depthwise_relu', 'block_11_project_BN',
-    'block_12_expand_relu', 'block_12_depthwise_relu', 'block_12_project_BN',
-    'block_13_expand_relu', 'block_13_depthwise_relu', 'block_13_project_BN',
-    'block_14_expand_relu', 'block_14_depthwise_relu', 'block_14_project_BN',
-    'block_15_expand_relu', 'block_15_depthwise_relu', 'block_15_project_BN',
-    'block_16_expand_relu', 'block_16_depthwise_relu', 'block_16_project_BN',
-    'out_relu']
-
-
-class MobilenetV2Test(test_case.TestCase):
-
-  def _build_conv_hyperparams(self):
-    conv_hyperparams = hyperparams_pb2.Hyperparams()
-    conv_hyperparams_text_proto = """
-      activation: RELU_6
-      regularizer {
-        l2_regularizer {
-        }
-      }
-      initializer {
-        truncated_normal_initializer {
-        }
-      }
-      batch_norm {
-        train: true,
-        scale: false,
-        center: true,
-        decay: 0.2,
-        epsilon: 0.1,
-      }
-    """
-    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
-    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
-
-  def _create_application_with_layer_outputs(
-      self, layer_names, batchnorm_training,
-      conv_hyperparams=None,
-      use_explicit_padding=False,
-      alpha=1.0,
-      min_depth=None):
-    """Constructs Keras mobilenetv2 that extracts intermediate layer outputs."""
-    if not layer_names:
-      layer_names = _layers_to_check
-    full_model = mobilenet_v2.mobilenet_v2(
-        batchnorm_training=batchnorm_training,
-        conv_hyperparams=conv_hyperparams,
-        weights=None,
-        use_explicit_padding=use_explicit_padding,
-        alpha=alpha,
-        min_depth=min_depth,
-        include_top=False)
-    layer_outputs = [full_model.get_layer(name=layer).output
-                     for layer in layer_names]
-    return tf.keras.Model(
-        inputs=full_model.inputs,
-        outputs=layer_outputs)
-
-  def _check_returns_correct_shape(
-      self, batch_size, image_height, image_width, depth_multiplier,
-      expected_feature_map_shapes, use_explicit_padding=False, min_depth=None,
-      layer_names=None):
-    def graph_fn(image_tensor):
-      model = self._create_application_with_layer_outputs(
-          layer_names=layer_names,
-          batchnorm_training=False, use_explicit_padding=use_explicit_padding,
-          min_depth=min_depth,
-          alpha=depth_multiplier)
-      return model(image_tensor)
-
-    image_tensor = np.random.rand(batch_size, image_height, image_width,
-                                  3).astype(np.float32)
-    feature_maps = self.execute(graph_fn, [image_tensor])
-
-    for feature_map, expected_shape in itertools.izip(
-        feature_maps, expected_feature_map_shapes):
-      self.assertAllEqual(feature_map.shape, expected_shape)
-
-  def _check_returns_correct_shapes_with_dynamic_inputs(
-      self, batch_size, image_height, image_width, depth_multiplier,
-      expected_feature_map_shapes, use_explicit_padding=False,
-      layer_names=None):
-    def graph_fn(image_height, image_width):
-      image_tensor = tf.random_uniform([batch_size, image_height, image_width,
-                                        3], dtype=tf.float32)
-      model = self._create_application_with_layer_outputs(
-          layer_names=layer_names,
-          batchnorm_training=False, use_explicit_padding=use_explicit_padding,
-          alpha=depth_multiplier)
-      return model(image_tensor)
-
-    feature_maps = self.execute_cpu(graph_fn, [
-        np.array(image_height, dtype=np.int32),
-        np.array(image_width, dtype=np.int32)
-    ])
-
-    for feature_map, expected_shape in itertools.izip(
-        feature_maps, expected_feature_map_shapes):
-      self.assertAllEqual(feature_map.shape, expected_shape)
-
-  def _get_variables(self, depth_multiplier, layer_names=None):
-    g = tf.Graph()
-    with g.as_default():
-      preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
-      model = self._create_application_with_layer_outputs(
-          layer_names=layer_names,
-          batchnorm_training=False, use_explicit_padding=False,
-          alpha=depth_multiplier)
-      model(preprocessed_inputs)
-      return g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
-
-  def test_returns_correct_shapes_128(self):
-    image_height = 128
-    image_width = 128
-    depth_multiplier = 1.0
-    expected_feature_map_shape = [(2, 64, 64, 32),
-                                  (2, 64, 64, 96),
-                                  (2, 32, 32, 96),
-                                  (2, 32, 32, 24),
-                                  (2, 32, 32, 144),
-                                  (2, 32, 32, 144),
-                                  (2, 32, 32, 24),
-                                  (2, 32, 32, 144),
-                                  (2, 16, 16, 144),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 8, 8, 192),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 4, 4, 576),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 320),
-                                  (2, 4, 4, 1280)]
-
-    self._check_returns_correct_shape(
-        2, image_height, image_width, depth_multiplier,
-        expected_feature_map_shape)
-
-  def test_returns_correct_shapes_128_explicit_padding(
-      self):
-    image_height = 128
-    image_width = 128
-    depth_multiplier = 1.0
-    expected_feature_map_shape = [(2, 64, 64, 32),
-                                  (2, 64, 64, 96),
-                                  (2, 32, 32, 96),
-                                  (2, 32, 32, 24),
-                                  (2, 32, 32, 144),
-                                  (2, 32, 32, 144),
-                                  (2, 32, 32, 24),
-                                  (2, 32, 32, 144),
-                                  (2, 16, 16, 144),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 8, 8, 192),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 4, 4, 576),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 320),
-                                  (2, 4, 4, 1280)]
-    self._check_returns_correct_shape(
-        2, image_height, image_width, depth_multiplier,
-        expected_feature_map_shape, use_explicit_padding=True)
-
-  def test_returns_correct_shapes_with_dynamic_inputs(
-      self):
-    image_height = 128
-    image_width = 128
-    depth_multiplier = 1.0
-    expected_feature_map_shape = [(2, 64, 64, 32),
-                                  (2, 64, 64, 96),
-                                  (2, 32, 32, 96),
-                                  (2, 32, 32, 24),
-                                  (2, 32, 32, 144),
-                                  (2, 32, 32, 144),
-                                  (2, 32, 32, 24),
-                                  (2, 32, 32, 144),
-                                  (2, 16, 16, 144),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 192),
-                                  (2, 16, 16, 32),
-                                  (2, 16, 16, 192),
-                                  (2, 8, 8, 192),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 64),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 384),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 576),
-                                  (2, 8, 8, 96),
-                                  (2, 8, 8, 576),
-                                  (2, 4, 4, 576),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 160),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 960),
-                                  (2, 4, 4, 320),
-                                  (2, 4, 4, 1280)]
-    self._check_returns_correct_shapes_with_dynamic_inputs(
-        2, image_height, image_width, depth_multiplier,
-        expected_feature_map_shape)
-
-  def test_returns_correct_shapes_299(self):
-    image_height = 299
-    image_width = 299
-    depth_multiplier = 1.0
-    expected_feature_map_shape = [(2, 150, 150, 32),
-                                  (2, 150, 150, 96),
-                                  (2, 75, 75, 96),
-                                  (2, 75, 75, 24),
-                                  (2, 75, 75, 144),
-                                  (2, 75, 75, 144),
-                                  (2, 75, 75, 24),
-                                  (2, 75, 75, 144),
-                                  (2, 38, 38, 144),
-                                  (2, 38, 38, 32),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 32),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 32),
-                                  (2, 38, 38, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 64),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 64),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 64),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 64),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 384),
-                                  (2, 19, 19, 96),
-                                  (2, 19, 19, 576),
-                                  (2, 19, 19, 576),
-                                  (2, 19, 19, 96),
-                                  (2, 19, 19, 576),
-                                  (2, 19, 19, 576),
-                                  (2, 19, 19, 96),
-                                  (2, 19, 19, 576),
-                                  (2, 10, 10, 576),
-                                  (2, 10, 10, 160),
-                                  (2, 10, 10, 960),
-                                  (2, 10, 10, 960),
-                                  (2, 10, 10, 160),
-                                  (2, 10, 10, 960),
-                                  (2, 10, 10, 960),
-                                  (2, 10, 10, 160),
-                                  (2, 10, 10, 960),
-                                  (2, 10, 10, 960),
-                                  (2, 10, 10, 320),
-                                  (2, 10, 10, 1280)]
-
-    self._check_returns_correct_shape(
-        2, image_height, image_width, depth_multiplier,
-        expected_feature_map_shape)
-
-  def test_returns_correct_shapes_enforcing_min_depth(
-      self):
-    image_height = 299
-    image_width = 299
-    depth_multiplier = 0.5**12
-    expected_feature_map_shape = [(2, 150, 150, 32),
-                                  (2, 150, 150, 192),
-                                  (2, 75, 75, 192),
-                                  (2, 75, 75, 32),
-                                  (2, 75, 75, 192),
-                                  (2, 75, 75, 192),
-                                  (2, 75, 75, 32),
-                                  (2, 75, 75, 192),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 32),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 32),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 192),
-                                  (2, 38, 38, 32),
-                                  (2, 38, 38, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 32),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 32),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 32),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 32),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 32),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 32),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 192),
-                                  (2, 19, 19, 32),
-                                  (2, 19, 19, 192),
-                                  (2, 10, 10, 192),
-                                  (2, 10, 10, 32),
-                                  (2, 10, 10, 192),
-                                  (2, 10, 10, 192),
-                                  (2, 10, 10, 32),
-                                  (2, 10, 10, 192),
-                                  (2, 10, 10, 192),
-                                  (2, 10, 10, 32),
-                                  (2, 10, 10, 192),
-                                  (2, 10, 10, 192),
-                                  (2, 10, 10, 32),
-                                  (2, 10, 10, 32)]
-    self._check_returns_correct_shape(
-        2, image_height, image_width, depth_multiplier,
-        expected_feature_map_shape, min_depth=32)
-
-  def test_hyperparam_override(self):
-    hyperparams = self._build_conv_hyperparams()
-    model = mobilenet_v2.mobilenet_v2(
-        batchnorm_training=True,
-        conv_hyperparams=hyperparams,
-        weights=None,
-        use_explicit_padding=False,
-        alpha=1.0,
-        min_depth=32,
-        include_top=False)
-    hyperparams.params()
-    bn_layer = model.get_layer(name='block_5_project_BN')
-    self.assertAllClose(bn_layer.momentum, 0.2)
-    self.assertAllClose(bn_layer.epsilon, 0.1)
-
-  def test_variable_count(self):
-    depth_multiplier = 1
-    variables = self._get_variables(depth_multiplier)
-    self.assertEqual(len(variables), 260)
-
-
-if __name__ == '__main__':
-  tf.test.main()
diff --git a/research/object_detection/models/keras_applications/__init__.py b/research/object_detection/models/keras_models/__init__.py
similarity index 100%
rename from research/object_detection/models/keras_applications/__init__.py
rename to research/object_detection/models/keras_models/__init__.py
diff --git a/research/object_detection/models/keras_models/mobilenet_v1.py b/research/object_detection/models/keras_models/mobilenet_v1.py
new file mode 100644
index 00000000..841cfe6e
--- /dev/null
+++ b/research/object_detection/models/keras_models/mobilenet_v1.py
@@ -0,0 +1,328 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""A wrapper around the Keras MobilenetV1 models for object detection."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from object_detection.core import freezable_batch_norm
+
+
+def _fixed_padding(inputs, kernel_size, rate=1):  # pylint: disable=invalid-name
+  """Pads the input along the spatial dimensions independently of input size.
+
+  Pads the input such that if it was used in a convolution with 'VALID' padding,
+  the output would have the same dimensions as if the unpadded input was used
+  in a convolution with 'SAME' padding.
+
+  Args:
+    inputs: A tensor of size [batch, height_in, width_in, channels].
+    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.
+    rate: An integer, rate for atrous convolution.
+
+  Returns:
+    output: A tensor of size [batch, height_out, width_out, channels] with the
+      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).
+  """
+  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),
+                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]
+  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]
+  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]
+  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]
+  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],
+                                  [pad_beg[1], pad_end[1]], [0, 0]])
+  return padded_inputs
+
+
+class _LayersOverride(object):
+  """Alternative Keras layers interface for the Keras MobileNetV1."""
+
+  def __init__(self,
+               batchnorm_training,
+               default_batchnorm_momentum=0.999,
+               conv_hyperparams=None,
+               use_explicit_padding=False,
+               alpha=1.0,
+               min_depth=None):
+    """Alternative tf.keras.layers interface, for use by the Keras MobileNetV1.
+
+    It is used by the Keras applications kwargs injection API to
+    modify the MobilenetV1 Keras application with changes required by
+    the Object Detection API.
+
+    These injected interfaces make the following changes to the network:
+
+    - Applies the Object Detection hyperparameter configuration
+    - Supports FreezableBatchNorms
+    - Adds support for a min number of filters for each layer
+    - Makes the `alpha` parameter affect the final convolution block even if it
+        is less than 1.0
+    - Adds support for explicit padding of convolutions
+
+    Args:
+      batchnorm_training: Bool. Assigned to Batch norm layer `training` param
+        when constructing `freezable_batch_norm.FreezableBatchNorm` layers.
+      default_batchnorm_momentum: Float. When 'conv_hyperparams' is None,
+        batch norm layers will be constructed using this value as the momentum.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops. Optionally set to `None`
+        to use default mobilenet_v1 layer builders.
+      use_explicit_padding: If True, use 'valid' padding for convolutions,
+        but explicitly pre-pads inputs so that the output dimensions are the
+        same as if 'same' padding were used. Off by default.
+      alpha: The width multiplier referenced in the MobileNetV1 paper. It
+        modifies the number of filters in each convolutional layer. It's called
+        depth multiplier in Keras application MobilenetV1.
+      min_depth: Minimum number of filters in the convolutional layers.
+    """
+    self._alpha = alpha
+    self._batchnorm_training = batchnorm_training
+    self._default_batchnorm_momentum = default_batchnorm_momentum
+    self._conv_hyperparams = conv_hyperparams
+    self._use_explicit_padding = use_explicit_padding
+    self._min_depth = min_depth
+    self.regularizer = tf.keras.regularizers.l2(0.00004 * 0.5)
+    self.initializer = tf.truncated_normal_initializer(stddev=0.09)
+
+  def _FixedPaddingLayer(self, kernel_size, rate=1):
+    return tf.keras.layers.Lambda(
+        lambda x: _fixed_padding(x, kernel_size, rate))
+
+  def Conv2D(self, filters, kernel_size, **kwargs):
+    """Builds a Conv2D layer according to the current Object Detection config.
+
+    Overrides the Keras MobileNetV1 application's convolutions with ones that
+    follow the spec specified by the Object Detection hyperparameters.
+
+    Args:
+      filters: The number of filters to use for the convolution.
+      kernel_size: The kernel size to specify the height and width of the 2D
+        convolution window.
+      **kwargs: Keyword args specified by the Keras application for
+        constructing the convolution.
+
+    Returns:
+      A one-arg callable that will either directly apply a Keras Conv2D layer to
+      the input argument, or that will first pad the input then apply a Conv2D
+      layer.
+    """
+    # Apply the width multiplier and the minimum depth to the convolution layers
+    filters = int(filters * self._alpha)
+    if self._min_depth and filters < self._min_depth:
+      filters = self._min_depth
+
+    if self._conv_hyperparams:
+      kwargs = self._conv_hyperparams.params(**kwargs)
+    else:
+      kwargs['kernel_regularizer'] = self.regularizer
+      kwargs['kernel_initializer'] = self.initializer
+
+    kwargs['padding'] = 'same'
+    if self._use_explicit_padding and kernel_size > 1:
+      kwargs['padding'] = 'valid'
+      def padded_conv(features):  # pylint: disable=invalid-name
+        padded_features = self._FixedPaddingLayer(kernel_size)(features)
+        return tf.keras.layers.Conv2D(
+            filters, kernel_size, **kwargs)(padded_features)
+      return padded_conv
+    else:
+      return tf.keras.layers.Conv2D(filters, kernel_size, **kwargs)
+
+  def DepthwiseConv2D(self, kernel_size, **kwargs):
+    """Builds a DepthwiseConv2D according to the Object Detection config.
+
+    Overrides the Keras MobileNetV2 application's convolutions with ones that
+    follow the spec specified by the Object Detection hyperparameters.
+
+    Args:
+      kernel_size: The kernel size to specify the height and width of the 2D
+        convolution window.
+      **kwargs: Keyword args specified by the Keras application for
+        constructing the convolution.
+
+    Returns:
+      A one-arg callable that will either directly apply a Keras DepthwiseConv2D
+      layer to the input argument, or that will first pad the input then apply
+      the depthwise convolution.
+    """
+    if self._conv_hyperparams:
+      kwargs = self._conv_hyperparams.params(**kwargs)
+    else:
+      kwargs['depthwise_initializer'] = self.initializer
+
+    kwargs['padding'] = 'same'
+    if self._use_explicit_padding:
+      kwargs['padding'] = 'valid'
+      def padded_depthwise_conv(features):  # pylint: disable=invalid-name
+        padded_features = self._FixedPaddingLayer(kernel_size)(features)
+        return tf.keras.layers.DepthwiseConv2D(
+            kernel_size, **kwargs)(padded_features)
+      return padded_depthwise_conv
+    else:
+      return tf.keras.layers.DepthwiseConv2D(kernel_size, **kwargs)
+
+  def BatchNormalization(self, **kwargs):
+    """Builds a normalization layer.
+
+    Overrides the Keras application batch norm with the norm specified by the
+    Object Detection configuration.
+
+    Args:
+      **kwargs: Only the name is used, all other params ignored.
+        Required for matching `layers.BatchNormalization` calls in the Keras
+        application.
+
+    Returns:
+      A normalization layer specified by the Object Detection hyperparameter
+      configurations.
+    """
+    name = kwargs.get('name')
+    if self._conv_hyperparams:
+      return self._conv_hyperparams.build_batch_norm(
+          training=self._batchnorm_training,
+          name=name)
+    else:
+      return freezable_batch_norm.FreezableBatchNorm(
+          training=self._batchnorm_training,
+          epsilon=1e-3,
+          momentum=self._default_batchnorm_momentum,
+          name=name)
+
+  def Input(self, shape):
+    """Builds an Input layer.
+
+    Overrides the Keras application Input layer with one that uses a
+    tf.placeholder_with_default instead of a tf.placeholder. This is necessary
+    to ensure the application works when run on a TPU.
+
+    Args:
+      shape: The shape for the input layer to use. (Does not include a dimension
+        for the batch size).
+    Returns:
+      An input layer for the specified shape that internally uses a
+      placeholder_with_default.
+    """
+    default_size = 224
+    default_batch_size = 1
+    shape = list(shape)
+    default_shape = [default_size if dim is None else dim for dim in shape]
+
+    input_tensor = tf.constant(0.0, shape=[default_batch_size] + default_shape)
+
+    placeholder_with_default = tf.placeholder_with_default(
+        input=input_tensor, shape=[None] + shape)
+    return tf.keras.layers.Input(tensor=placeholder_with_default)
+
+  # pylint: disable=unused-argument
+  def ReLU(self, *args, **kwargs):
+    """Builds an activation layer.
+
+    Overrides the Keras application ReLU with the activation specified by the
+    Object Detection configuration.
+
+    Args:
+      *args: Ignored, required to match the `tf.keras.ReLU` interface
+      **kwargs: Only the name is used,
+        required to match `tf.keras.ReLU` interface
+
+    Returns:
+      An activation layer specified by the Object Detection hyperparameter
+      configurations.
+    """
+    name = kwargs.get('name')
+    if self._conv_hyperparams:
+      return self._conv_hyperparams.build_activation_layer(name=name)
+    else:
+      return tf.keras.layers.Lambda(tf.nn.relu6, name=name)
+  # pylint: enable=unused-argument
+
+  # pylint: disable=unused-argument
+  def ZeroPadding2D(self, padding, **kwargs):
+    """Replaces explicit padding in the Keras application with a no-op.
+
+    Args:
+      padding: The padding values for image height and width.
+      **kwargs: Ignored, required to match the Keras applications usage.
+
+    Returns:
+      A no-op identity lambda.
+    """
+    return lambda x: x
+  # pylint: enable=unused-argument
+
+  # Forward all non-overridden methods to the keras layers
+  def __getattr__(self, item):
+    return getattr(tf.keras.layers, item)
+
+
+# pylint: disable=invalid-name
+def mobilenet_v1(batchnorm_training,
+                 default_batchnorm_momentum=0.9997,
+                 conv_hyperparams=None,
+                 use_explicit_padding=False,
+                 alpha=1.0,
+                 min_depth=None,
+                 **kwargs):
+  """Instantiates the MobileNetV1 architecture, modified for object detection.
+
+  This wraps the MobileNetV1 tensorflow Keras application, but uses the
+  Keras application's kwargs-based monkey-patching API to override the Keras
+  architecture with the following changes:
+
+  - Changes the default batchnorm momentum to 0.9997
+  - Applies the Object Detection hyperparameter configuration
+  - Supports FreezableBatchNorms
+  - Adds support for a min number of filters for each layer
+  - Makes the `alpha` parameter affect the final convolution block even if it
+      is less than 1.0
+  - Adds support for explicit padding of convolutions
+  - Makes the Input layer use a tf.placeholder_with_default instead of a
+      tf.placeholder, to work on TPUs.
+
+  Args:
+      batchnorm_training: Bool. Assigned to Batch norm layer `training` param
+        when constructing `freezable_batch_norm.FreezableBatchNorm` layers.
+      default_batchnorm_momentum: Float. When 'conv_hyperparams' is None,
+        batch norm layers will be constructed using this value as the momentum.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing hyperparameters for convolution ops. Optionally set to `None`
+        to use default mobilenet_v1 layer builders.
+      use_explicit_padding: If True, use 'valid' padding for convolutions,
+        but explicitly pre-pads inputs so that the output dimensions are the
+        same as if 'same' padding were used. Off by default.
+      alpha: The width multiplier referenced in the MobileNetV1 paper. It
+        modifies the number of filters in each convolutional layer.
+      min_depth: Minimum number of filters in the convolutional layers.
+      **kwargs: Keyword arguments forwarded directly to the
+        `tf.keras.applications.Mobilenet` method that constructs the Keras
+        model.
+
+  Returns:
+      A Keras model instance.
+  """
+  layers_override = _LayersOverride(
+      batchnorm_training,
+      default_batchnorm_momentum=default_batchnorm_momentum,
+      conv_hyperparams=conv_hyperparams,
+      use_explicit_padding=use_explicit_padding,
+      min_depth=min_depth,
+      alpha=alpha)
+  return tf.keras.applications.MobileNet(
+      alpha=alpha, layers=layers_override, **kwargs)
+# pylint: enable=invalid-name
diff --git a/research/object_detection/models/keras_models/mobilenet_v1_test.py b/research/object_detection/models/keras_models/mobilenet_v1_test.py
new file mode 100644
index 00000000..a326c46c
--- /dev/null
+++ b/research/object_detection/models/keras_models/mobilenet_v1_test.py
@@ -0,0 +1,237 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for mobilenet_v1.py.
+
+This test mainly focuses on comparing slim MobilenetV1 and Keras MobilenetV1 for
+object detection. To verify the consistency of the two models, we compare:
+  1. Output shape of each layer given different inputs
+  2. Number of global variables
+
+We also visualize the model structure via Tensorboard, and compare the model
+layout and the parameters of each Op to make sure the two implementations are
+consistent.
+"""
+
+import itertools
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+
+from object_detection.builders import hyperparams_builder
+from object_detection.models.keras_models import mobilenet_v1
+from object_detection.models.keras_models import test_utils
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+_KERAS_LAYERS_TO_CHECK = [
+    'conv1_relu',
+    'conv_dw_1_relu', 'conv_pw_1_relu',
+    'conv_dw_2_relu', 'conv_pw_2_relu',
+    'conv_dw_3_relu', 'conv_pw_3_relu',
+    'conv_dw_4_relu', 'conv_pw_4_relu',
+    'conv_dw_5_relu', 'conv_pw_5_relu',
+    'conv_dw_6_relu', 'conv_pw_6_relu',
+    'conv_dw_7_relu', 'conv_pw_7_relu',
+    'conv_dw_8_relu', 'conv_pw_8_relu',
+    'conv_dw_9_relu', 'conv_pw_9_relu',
+    'conv_dw_10_relu', 'conv_pw_10_relu',
+    'conv_dw_11_relu', 'conv_pw_11_relu',
+    'conv_dw_12_relu', 'conv_pw_12_relu',
+    'conv_dw_13_relu', 'conv_pw_13_relu',
+]
+
+_NUM_CHANNELS = 3
+_BATCH_SIZE = 2
+
+
+class MobilenetV1Test(test_case.TestCase):
+
+  def _build_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      activation: RELU_6
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      batch_norm {
+        train: true,
+        scale: false,
+        center: true,
+        decay: 0.2,
+        epsilon: 0.1,
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  def _create_application_with_layer_outputs(
+      self, layer_names, batchnorm_training,
+      conv_hyperparams=None,
+      use_explicit_padding=False,
+      alpha=1.0,
+      min_depth=None):
+    """Constructs Keras MobilenetV1 that extracts intermediate layer outputs."""
+    if not layer_names:
+      layer_names = _KERAS_LAYERS_TO_CHECK
+    full_model = mobilenet_v1.mobilenet_v1(
+        batchnorm_training=batchnorm_training,
+        conv_hyperparams=conv_hyperparams,
+        weights=None,
+        use_explicit_padding=use_explicit_padding,
+        alpha=alpha,
+        min_depth=min_depth,
+        include_top=False)
+    layer_outputs = [full_model.get_layer(name=layer).output
+                     for layer in layer_names]
+    return tf.keras.Model(
+        inputs=full_model.inputs,
+        outputs=layer_outputs)
+
+  def _check_returns_correct_shape(
+      self, image_height, image_width, depth_multiplier,
+      expected_feature_map_shape, use_explicit_padding=False, min_depth=8,
+      layer_names=None):
+    def graph_fn(image_tensor):
+      model = self._create_application_with_layer_outputs(
+          layer_names=layer_names,
+          batchnorm_training=False,
+          use_explicit_padding=use_explicit_padding,
+          min_depth=min_depth,
+          alpha=depth_multiplier)
+      return model(image_tensor)
+
+    image_tensor = np.random.rand(_BATCH_SIZE, image_height, image_width,
+                                  _NUM_CHANNELS).astype(np.float32)
+    feature_maps = self.execute(graph_fn, [image_tensor])
+
+    for feature_map, expected_shape in itertools.izip(
+        feature_maps, expected_feature_map_shape):
+      self.assertAllEqual(feature_map.shape, expected_shape)
+
+  def _check_returns_correct_shapes_with_dynamic_inputs(
+      self, image_height, image_width, depth_multiplier,
+      expected_feature_map_shape, use_explicit_padding=False, min_depth=8,
+      layer_names=None):
+    def graph_fn(image_height, image_width):
+      image_tensor = tf.random_uniform([_BATCH_SIZE, image_height, image_width,
+                                        _NUM_CHANNELS], dtype=tf.float32)
+      model = self._create_application_with_layer_outputs(
+          layer_names=layer_names,
+          batchnorm_training=False,
+          use_explicit_padding=use_explicit_padding,
+          alpha=depth_multiplier)
+      return model(image_tensor)
+
+    feature_maps = self.execute_cpu(graph_fn, [
+        np.array(image_height, dtype=np.int32),
+        np.array(image_width, dtype=np.int32)
+    ])
+
+    for feature_map, expected_shape in itertools.izip(
+        feature_maps, expected_feature_map_shape):
+      self.assertAllEqual(feature_map.shape, expected_shape)
+
+  def _get_variables(self, depth_multiplier, layer_names=None):
+    g = tf.Graph()
+    with g.as_default():
+      preprocessed_inputs = tf.placeholder(
+          tf.float32, (4, None, None, _NUM_CHANNELS))
+      model = self._create_application_with_layer_outputs(
+          layer_names=layer_names,
+          batchnorm_training=False, use_explicit_padding=False,
+          alpha=depth_multiplier)
+      model(preprocessed_inputs)
+      return g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+
+  def test_returns_correct_shapes_128(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.moblenet_v1_expected_feature_map_shape_128)
+    self._check_returns_correct_shape(
+        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+
+  def test_returns_correct_shapes_128_explicit_padding(
+      self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.moblenet_v1_expected_feature_map_shape_128_explicit_padding)
+    self._check_returns_correct_shape(
+        image_height, image_width, depth_multiplier, expected_feature_map_shape,
+        use_explicit_padding=True)
+
+  def test_returns_correct_shapes_with_dynamic_inputs(
+      self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.mobilenet_v1_expected_feature_map_shape_with_dynamic_inputs)
+    self._check_returns_correct_shapes_with_dynamic_inputs(
+        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+
+  def test_returns_correct_shapes_299(self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.moblenet_v1_expected_feature_map_shape_299)
+    self._check_returns_correct_shape(
+        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+
+  def test_returns_correct_shapes_enforcing_min_depth(
+      self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 0.5**12
+    expected_feature_map_shape = (
+        test_utils.moblenet_v1_expected_feature_map_shape_enforcing_min_depth)
+    self._check_returns_correct_shape(
+        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+
+  def test_hyperparam_override(self):
+    hyperparams = self._build_conv_hyperparams()
+    model = mobilenet_v1.mobilenet_v1(
+        batchnorm_training=True,
+        conv_hyperparams=hyperparams,
+        weights=None,
+        use_explicit_padding=False,
+        alpha=1.0,
+        min_depth=32,
+        include_top=False)
+    hyperparams.params()
+    bn_layer = model.get_layer(name='conv_pw_5_bn')
+    self.assertAllClose(bn_layer.momentum, 0.2)
+    self.assertAllClose(bn_layer.epsilon, 0.1)
+
+  def test_variable_count(self):
+    depth_multiplier = 1
+    variables = self._get_variables(depth_multiplier)
+    # 135 is the number of variables from slim MobilenetV1 model.
+    self.assertEqual(len(variables), 135)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/keras_applications/mobilenet_v2.py b/research/object_detection/models/keras_models/mobilenet_v2.py
similarity index 100%
rename from research/object_detection/models/keras_applications/mobilenet_v2.py
rename to research/object_detection/models/keras_models/mobilenet_v2.py
diff --git a/research/object_detection/models/keras_models/mobilenet_v2_test.py b/research/object_detection/models/keras_models/mobilenet_v2_test.py
new file mode 100644
index 00000000..74d26f71
--- /dev/null
+++ b/research/object_detection/models/keras_models/mobilenet_v2_test.py
@@ -0,0 +1,227 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for mobilenet_v2."""
+import itertools
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+
+from object_detection.builders import hyperparams_builder
+from object_detection.models.keras_models import mobilenet_v2
+from object_detection.models.keras_models import test_utils
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+_layers_to_check = [
+    'Conv1_relu',
+    'block_1_expand_relu', 'block_1_depthwise_relu', 'block_1_project_BN',
+    'block_2_expand_relu', 'block_2_depthwise_relu', 'block_2_project_BN',
+    'block_3_expand_relu', 'block_3_depthwise_relu', 'block_3_project_BN',
+    'block_4_expand_relu', 'block_4_depthwise_relu', 'block_4_project_BN',
+    'block_5_expand_relu', 'block_5_depthwise_relu', 'block_5_project_BN',
+    'block_6_expand_relu', 'block_6_depthwise_relu', 'block_6_project_BN',
+    'block_7_expand_relu', 'block_7_depthwise_relu', 'block_7_project_BN',
+    'block_8_expand_relu', 'block_8_depthwise_relu', 'block_8_project_BN',
+    'block_9_expand_relu', 'block_9_depthwise_relu', 'block_9_project_BN',
+    'block_10_expand_relu', 'block_10_depthwise_relu', 'block_10_project_BN',
+    'block_11_expand_relu', 'block_11_depthwise_relu', 'block_11_project_BN',
+    'block_12_expand_relu', 'block_12_depthwise_relu', 'block_12_project_BN',
+    'block_13_expand_relu', 'block_13_depthwise_relu', 'block_13_project_BN',
+    'block_14_expand_relu', 'block_14_depthwise_relu', 'block_14_project_BN',
+    'block_15_expand_relu', 'block_15_depthwise_relu', 'block_15_project_BN',
+    'block_16_expand_relu', 'block_16_depthwise_relu', 'block_16_project_BN',
+    'out_relu']
+
+
+class MobilenetV2Test(test_case.TestCase):
+
+  def _build_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      activation: RELU_6
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      batch_norm {
+        train: true,
+        scale: false,
+        center: true,
+        decay: 0.2,
+        epsilon: 0.1,
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
+
+  def _create_application_with_layer_outputs(
+      self, layer_names, batchnorm_training,
+      conv_hyperparams=None,
+      use_explicit_padding=False,
+      alpha=1.0,
+      min_depth=None):
+    """Constructs Keras mobilenetv2 that extracts intermediate layer outputs."""
+    if not layer_names:
+      layer_names = _layers_to_check
+    full_model = mobilenet_v2.mobilenet_v2(
+        batchnorm_training=batchnorm_training,
+        conv_hyperparams=conv_hyperparams,
+        weights=None,
+        use_explicit_padding=use_explicit_padding,
+        alpha=alpha,
+        min_depth=min_depth,
+        include_top=False)
+    layer_outputs = [full_model.get_layer(name=layer).output
+                     for layer in layer_names]
+    return tf.keras.Model(
+        inputs=full_model.inputs,
+        outputs=layer_outputs)
+
+  def _check_returns_correct_shape(
+      self, batch_size, image_height, image_width, depth_multiplier,
+      expected_feature_map_shapes, use_explicit_padding=False, min_depth=None,
+      layer_names=None):
+    def graph_fn(image_tensor):
+      model = self._create_application_with_layer_outputs(
+          layer_names=layer_names,
+          batchnorm_training=False, use_explicit_padding=use_explicit_padding,
+          min_depth=min_depth,
+          alpha=depth_multiplier)
+      return model(image_tensor)
+
+    image_tensor = np.random.rand(batch_size, image_height, image_width,
+                                  3).astype(np.float32)
+    feature_maps = self.execute(graph_fn, [image_tensor])
+
+    for feature_map, expected_shape in itertools.izip(
+        feature_maps, expected_feature_map_shapes):
+      self.assertAllEqual(feature_map.shape, expected_shape)
+
+  def _check_returns_correct_shapes_with_dynamic_inputs(
+      self, batch_size, image_height, image_width, depth_multiplier,
+      expected_feature_map_shapes, use_explicit_padding=False,
+      layer_names=None):
+    def graph_fn(image_height, image_width):
+      image_tensor = tf.random_uniform([batch_size, image_height, image_width,
+                                        3], dtype=tf.float32)
+      model = self._create_application_with_layer_outputs(
+          layer_names=layer_names,
+          batchnorm_training=False, use_explicit_padding=use_explicit_padding,
+          alpha=depth_multiplier)
+      return model(image_tensor)
+
+    feature_maps = self.execute_cpu(graph_fn, [
+        np.array(image_height, dtype=np.int32),
+        np.array(image_width, dtype=np.int32)
+    ])
+
+    for feature_map, expected_shape in itertools.izip(
+        feature_maps, expected_feature_map_shapes):
+      self.assertAllEqual(feature_map.shape, expected_shape)
+
+  def _get_variables(self, depth_multiplier, layer_names=None):
+    g = tf.Graph()
+    with g.as_default():
+      preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+      model = self._create_application_with_layer_outputs(
+          layer_names=layer_names,
+          batchnorm_training=False, use_explicit_padding=False,
+          alpha=depth_multiplier)
+      model(preprocessed_inputs)
+      return g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+
+  def test_returns_correct_shapes_128(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.moblenet_v2_expected_feature_map_shape_128)
+
+    self._check_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier,
+        expected_feature_map_shape)
+
+  def test_returns_correct_shapes_128_explicit_padding(
+      self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.moblenet_v2_expected_feature_map_shape_128_explicit_padding)
+    self._check_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_returns_correct_shapes_with_dynamic_inputs(
+      self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.mobilenet_v2_expected_feature_map_shape_with_dynamic_inputs)
+    self._check_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier,
+        expected_feature_map_shape)
+
+  def test_returns_correct_shapes_299(self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    expected_feature_map_shape = (
+        test_utils.moblenet_v2_expected_feature_map_shape_299)
+    self._check_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier,
+        expected_feature_map_shape)
+
+  def test_returns_correct_shapes_enforcing_min_depth(
+      self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 0.5**12
+    expected_feature_map_shape = (
+        test_utils.moblenet_v2_expected_feature_map_shape_enforcing_min_depth)
+    self._check_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier,
+        expected_feature_map_shape, min_depth=32)
+
+  def test_hyperparam_override(self):
+    hyperparams = self._build_conv_hyperparams()
+    model = mobilenet_v2.mobilenet_v2(
+        batchnorm_training=True,
+        conv_hyperparams=hyperparams,
+        weights=None,
+        use_explicit_padding=False,
+        alpha=1.0,
+        min_depth=32,
+        include_top=False)
+    hyperparams.params()
+    bn_layer = model.get_layer(name='block_5_project_BN')
+    self.assertAllClose(bn_layer.momentum, 0.2)
+    self.assertAllClose(bn_layer.epsilon, 0.1)
+
+  def test_variable_count(self):
+    depth_multiplier = 1
+    variables = self._get_variables(depth_multiplier)
+    self.assertEqual(len(variables), 260)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/keras_models/test_utils.py b/research/object_detection/models/keras_models/test_utils.py
new file mode 100644
index 00000000..b7aa503e
--- /dev/null
+++ b/research/object_detection/models/keras_models/test_utils.py
@@ -0,0 +1,189 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Test utils for other test files."""
+
+# import tensorflow as tf
+#
+# from nets import mobilenet_v1
+#
+# slim = tf.contrib.slim
+#
+# # Layer names of Slim to map Keras layer names in MobilenetV1
+# _MOBLIENET_V1_SLIM_ENDPOINTS = [
+#     'Conv2d_0',
+#     'Conv2d_1_depthwise', 'Conv2d_1_pointwise',
+#     'Conv2d_2_depthwise', 'Conv2d_2_pointwise',
+#     'Conv2d_3_depthwise', 'Conv2d_3_pointwise',
+#     'Conv2d_4_depthwise', 'Conv2d_4_pointwise',
+#     'Conv2d_5_depthwise', 'Conv2d_5_pointwise',
+#     'Conv2d_6_depthwise', 'Conv2d_6_pointwise',
+#     'Conv2d_7_depthwise', 'Conv2d_7_pointwise',
+#     'Conv2d_8_depthwise', 'Conv2d_8_pointwise',
+#     'Conv2d_9_depthwise', 'Conv2d_9_pointwise',
+#     'Conv2d_10_depthwise', 'Conv2d_10_pointwise',
+#     'Conv2d_11_depthwise', 'Conv2d_11_pointwise',
+#     'Conv2d_12_depthwise', 'Conv2d_12_pointwise',
+#     'Conv2d_13_depthwise', 'Conv2d_13_pointwise'
+# ]
+#
+#
+# # Function to get the output shape of each layer in Slim. It's used to
+# # generate the following constant expected_feature_map_shape for MobilenetV1.
+# # Similarly, this can also apply to MobilenetV2.
+# def _get_slim_endpoint_shapes(inputs, depth_multiplier=1.0, min_depth=8,
+#                               use_explicit_padding=False):
+#   with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
+#                       normalizer_fn=slim.batch_norm):
+#     _, end_points = mobilenet_v1.mobilenet_v1_base(
+#         inputs, final_endpoint='Conv2d_13_pointwise',
+#         depth_multiplier=depth_multiplier, min_depth=min_depth,
+#         use_explicit_padding=use_explicit_padding)
+#     return [end_points[endpoint_name].get_shape()
+#             for endpoint_name in _MOBLIENET_V1_SLIM_ENDPOINTS]
+
+
+# For Mobilenet V1
+moblenet_v1_expected_feature_map_shape_128 = [
+    (2, 64, 64, 32), (2, 64, 64, 32), (2, 64, 64, 64), (2, 32, 32, 64),
+    (2, 32, 32, 128), (2, 32, 32, 128), (2, 32, 32, 128), (2, 16, 16, 128),
+    (2, 16, 16, 256), (2, 16, 16, 256), (2, 16, 16, 256), (2, 8, 8, 256),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 4, 4, 512),
+    (2, 4, 4, 1024), (2, 4, 4, 1024), (2, 4, 4, 1024),
+]
+
+moblenet_v1_expected_feature_map_shape_128_explicit_padding = [
+    (2, 64, 64, 32), (2, 64, 64, 32), (2, 64, 64, 64), (2, 32, 32, 64),
+    (2, 32, 32, 128), (2, 32, 32, 128), (2, 32, 32, 128), (2, 16, 16, 128),
+    (2, 16, 16, 256), (2, 16, 16, 256), (2, 16, 16, 256), (2, 8, 8, 256),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 4, 4, 512),
+    (2, 4, 4, 1024), (2, 4, 4, 1024), (2, 4, 4, 1024),
+]
+
+mobilenet_v1_expected_feature_map_shape_with_dynamic_inputs = [
+    (2, 64, 64, 32), (2, 64, 64, 32), (2, 64, 64, 64), (2, 32, 32, 64),
+    (2, 32, 32, 128), (2, 32, 32, 128), (2, 32, 32, 128), (2, 16, 16, 128),
+    (2, 16, 16, 256), (2, 16, 16, 256), (2, 16, 16, 256), (2, 8, 8, 256),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512),
+    (2, 8, 8, 512), (2, 8, 8, 512), (2, 8, 8, 512), (2, 4, 4, 512),
+    (2, 4, 4, 1024), (2, 4, 4, 1024), (2, 4, 4, 1024),
+]
+
+moblenet_v1_expected_feature_map_shape_299 = [
+    (2, 150, 150, 32), (2, 150, 150, 32), (2, 150, 150, 64), (2, 75, 75, 64),
+    (2, 75, 75, 128), (2, 75, 75, 128), (2, 75, 75, 128), (2, 38, 38, 128),
+    (2, 38, 38, 256), (2, 38, 38, 256), (2, 38, 38, 256), (2, 19, 19, 256),
+    (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512),
+    (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512),
+    (2, 19, 19, 512), (2, 19, 19, 512), (2, 19, 19, 512), (2, 10, 10, 512),
+    (2, 10, 10, 1024), (2, 10, 10, 1024), (2, 10, 10, 1024),
+]
+
+moblenet_v1_expected_feature_map_shape_enforcing_min_depth = [
+    (2, 150, 150, 8), (2, 150, 150, 8), (2, 150, 150, 8), (2, 75, 75, 8),
+    (2, 75, 75, 8), (2, 75, 75, 8), (2, 75, 75, 8), (2, 38, 38, 8),
+    (2, 38, 38, 8), (2, 38, 38, 8), (2, 38, 38, 8), (2, 19, 19, 8),
+    (2, 19, 19, 8), (2, 19, 19, 8), (2, 19, 19, 8), (2, 19, 19, 8),
+    (2, 19, 19, 8), (2, 19, 19, 8), (2, 19, 19, 8), (2, 19, 19, 8),
+    (2, 19, 19, 8), (2, 19, 19, 8), (2, 19, 19, 8), (2, 10, 10, 8),
+    (2, 10, 10, 8), (2, 10, 10, 8), (2, 10, 10, 8),
+]
+
+# For Mobilenet V2
+moblenet_v2_expected_feature_map_shape_128 = [
+    (2, 64, 64, 32), (2, 64, 64, 96), (2, 32, 32, 96), (2, 32, 32, 24),
+    (2, 32, 32, 144), (2, 32, 32, 144), (2, 32, 32, 24), (2, 32, 32, 144),
+    (2, 16, 16, 144), (2, 16, 16, 32), (2, 16, 16, 192), (2, 16, 16, 192),
+    (2, 16, 16, 32), (2, 16, 16, 192), (2, 16, 16, 192), (2, 16, 16, 32),
+    (2, 16, 16, 192), (2, 8, 8, 192), (2, 8, 8, 64), (2, 8, 8, 384),
+    (2, 8, 8, 384), (2, 8, 8, 64), (2, 8, 8, 384), (2, 8, 8, 384),
+    (2, 8, 8, 64), (2, 8, 8, 384), (2, 8, 8, 384), (2, 8, 8, 64),
+    (2, 8, 8, 384), (2, 8, 8, 384), (2, 8, 8, 96), (2, 8, 8, 576),
+    (2, 8, 8, 576), (2, 8, 8, 96), (2, 8, 8, 576), (2, 8, 8, 576),
+    (2, 8, 8, 96), (2, 8, 8, 576), (2, 4, 4, 576), (2, 4, 4, 160),
+    (2, 4, 4, 960), (2, 4, 4, 960), (2, 4, 4, 160), (2, 4, 4, 960),
+    (2, 4, 4, 960), (2, 4, 4, 160), (2, 4, 4, 960), (2, 4, 4, 960),
+    (2, 4, 4, 320), (2, 4, 4, 1280)
+]
+
+moblenet_v2_expected_feature_map_shape_128_explicit_padding = [
+    (2, 64, 64, 32), (2, 64, 64, 96), (2, 32, 32, 96), (2, 32, 32, 24),
+    (2, 32, 32, 144), (2, 32, 32, 144), (2, 32, 32, 24), (2, 32, 32, 144),
+    (2, 16, 16, 144), (2, 16, 16, 32), (2, 16, 16, 192), (2, 16, 16, 192),
+    (2, 16, 16, 32), (2, 16, 16, 192), (2, 16, 16, 192), (2, 16, 16, 32),
+    (2, 16, 16, 192), (2, 8, 8, 192), (2, 8, 8, 64), (2, 8, 8, 384),
+    (2, 8, 8, 384), (2, 8, 8, 64), (2, 8, 8, 384), (2, 8, 8, 384),
+    (2, 8, 8, 64), (2, 8, 8, 384), (2, 8, 8, 384), (2, 8, 8, 64),
+    (2, 8, 8, 384), (2, 8, 8, 384), (2, 8, 8, 96), (2, 8, 8, 576),
+    (2, 8, 8, 576), (2, 8, 8, 96), (2, 8, 8, 576), (2, 8, 8, 576),
+    (2, 8, 8, 96), (2, 8, 8, 576), (2, 4, 4, 576), (2, 4, 4, 160),
+    (2, 4, 4, 960), (2, 4, 4, 960), (2, 4, 4, 160), (2, 4, 4, 960),
+    (2, 4, 4, 960), (2, 4, 4, 160), (2, 4, 4, 960), (2, 4, 4, 960),
+    (2, 4, 4, 320), (2, 4, 4, 1280)
+]
+
+mobilenet_v2_expected_feature_map_shape_with_dynamic_inputs = [
+    (2, 64, 64, 32), (2, 64, 64, 96), (2, 32, 32, 96), (2, 32, 32, 24),
+    (2, 32, 32, 144), (2, 32, 32, 144), (2, 32, 32, 24), (2, 32, 32, 144),
+    (2, 16, 16, 144), (2, 16, 16, 32), (2, 16, 16, 192), (2, 16, 16, 192),
+    (2, 16, 16, 32), (2, 16, 16, 192), (2, 16, 16, 192), (2, 16, 16, 32),
+    (2, 16, 16, 192), (2, 8, 8, 192), (2, 8, 8, 64), (2, 8, 8, 384),
+    (2, 8, 8, 384), (2, 8, 8, 64), (2, 8, 8, 384), (2, 8, 8, 384),
+    (2, 8, 8, 64), (2, 8, 8, 384), (2, 8, 8, 384), (2, 8, 8, 64),
+    (2, 8, 8, 384), (2, 8, 8, 384), (2, 8, 8, 96), (2, 8, 8, 576),
+    (2, 8, 8, 576), (2, 8, 8, 96), (2, 8, 8, 576), (2, 8, 8, 576),
+    (2, 8, 8, 96), (2, 8, 8, 576), (2, 4, 4, 576), (2, 4, 4, 160),
+    (2, 4, 4, 960), (2, 4, 4, 960), (2, 4, 4, 160), (2, 4, 4, 960),
+    (2, 4, 4, 960), (2, 4, 4, 160), (2, 4, 4, 960), (2, 4, 4, 960),
+    (2, 4, 4, 320), (2, 4, 4, 1280)
+]
+
+moblenet_v2_expected_feature_map_shape_299 = [
+    (2, 150, 150, 32), (2, 150, 150, 96), (2, 75, 75, 96), (2, 75, 75, 24),
+    (2, 75, 75, 144), (2, 75, 75, 144), (2, 75, 75, 24), (2, 75, 75, 144),
+    (2, 38, 38, 144), (2, 38, 38, 32), (2, 38, 38, 192), (2, 38, 38, 192),
+    (2, 38, 38, 32), (2, 38, 38, 192), (2, 38, 38, 192), (2, 38, 38, 32),
+    (2, 38, 38, 192), (2, 19, 19, 192), (2, 19, 19, 64), (2, 19, 19, 384),
+    (2, 19, 19, 384), (2, 19, 19, 64), (2, 19, 19, 384), (2, 19, 19, 384),
+    (2, 19, 19, 64), (2, 19, 19, 384), (2, 19, 19, 384), (2, 19, 19, 64),
+    (2, 19, 19, 384), (2, 19, 19, 384), (2, 19, 19, 96), (2, 19, 19, 576),
+    (2, 19, 19, 576), (2, 19, 19, 96), (2, 19, 19, 576), (2, 19, 19, 576),
+    (2, 19, 19, 96), (2, 19, 19, 576), (2, 10, 10, 576), (2, 10, 10, 160),
+    (2, 10, 10, 960), (2, 10, 10, 960), (2, 10, 10, 160), (2, 10, 10, 960),
+    (2, 10, 10, 960), (2, 10, 10, 160), (2, 10, 10, 960), (2, 10, 10, 960),
+    (2, 10, 10, 320), (2, 10, 10, 1280)
+]
+
+moblenet_v2_expected_feature_map_shape_enforcing_min_depth = [
+    (2, 150, 150, 32), (2, 150, 150, 192), (2, 75, 75, 192), (2, 75, 75, 32),
+    (2, 75, 75, 192), (2, 75, 75, 192), (2, 75, 75, 32), (2, 75, 75, 192),
+    (2, 38, 38, 192), (2, 38, 38, 32), (2, 38, 38, 192), (2, 38, 38, 192),
+    (2, 38, 38, 32), (2, 38, 38, 192), (2, 38, 38, 192), (2, 38, 38, 32),
+    (2, 38, 38, 192), (2, 19, 19, 192), (2, 19, 19, 32), (2, 19, 19, 192),
+    (2, 19, 19, 192), (2, 19, 19, 32), (2, 19, 19, 192), (2, 19, 19, 192),
+    (2, 19, 19, 32), (2, 19, 19, 192), (2, 19, 19, 192), (2, 19, 19, 32),
+    (2, 19, 19, 192), (2, 19, 19, 192), (2, 19, 19, 32), (2, 19, 19, 192),
+    (2, 19, 19, 192), (2, 19, 19, 32), (2, 19, 19, 192), (2, 19, 19, 192),
+    (2, 19, 19, 32), (2, 19, 19, 192), (2, 10, 10, 192), (2, 10, 10, 32),
+    (2, 10, 10, 192), (2, 10, 10, 192), (2, 10, 10, 32), (2, 10, 10, 192),
+    (2, 10, 10, 192), (2, 10, 10, 32), (2, 10, 10, 192), (2, 10, 10, 192),
+    (2, 10, 10, 32), (2, 10, 10, 32)
+]
+
diff --git a/research/object_detection/models/ssd_feature_extractor_test.py b/research/object_detection/models/ssd_feature_extractor_test.py
index 36699b63..33077022 100644
--- a/research/object_detection/models/ssd_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_feature_extractor_test.py
@@ -29,7 +29,7 @@ from object_detection.utils import test_case
 
 class SsdFeatureExtractorTestBase(test_case.TestCase):
 
-  def _build_conv_hyperparams(self):
+  def _build_conv_hyperparams(self, add_batch_norm=True):
     conv_hyperparams = hyperparams_pb2.Hyperparams()
     conv_hyperparams_text_proto = """
       activation: RELU_6
@@ -41,10 +41,14 @@ class SsdFeatureExtractorTestBase(test_case.TestCase):
         truncated_normal_initializer {
         }
       }
-      batch_norm {
-        scale: false
-      }
     """
+    if add_batch_norm:
+      batch_norm_proto = """
+        batch_norm {
+          scale: false
+        }
+      """
+      conv_hyperparams_text_proto += batch_norm_proto
     text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
     return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
 
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
index d3a9542b..ca59aba5 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
@@ -13,41 +13,69 @@
 # limitations under the License.
 # ==============================================================================
 
-"""Tests for ssd_mobilenet_v1_feature_extractor."""
+"""Tests for SSD Mobilenet V1 feature extractors.
+
+By using parameterized test decorator, this test serves for both Slim-based and
+Keras-based Mobilenet V1 feature extractors in SSD.
+"""
+from absl.testing import parameterized
+
 import numpy as np
 import tensorflow as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v1_feature_extractor
+from object_detection.models import ssd_mobilenet_v1_keras_feature_extractor
 
 slim = tf.contrib.slim
 
 
+@parameterized.parameters(
+    {'use_keras': False},
+    {'use_keras': True},
+)
 class SsdMobilenetV1FeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-                                is_training=True, use_explicit_padding=False):
+                                use_explicit_padding=False, is_training=False,
+                                use_keras=False):
     """Constructs a new feature extractor.
 
     Args:
       depth_multiplier: float depth multiplier for feature extractor
       pad_to_multiple: the nearest multiple to zero pad the input height and
         width dimensions to.
-      is_training: whether the network is in training mode.
       use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
         inputs so that the output dimensions are the same as if 'SAME' padding
         were used.
+      is_training: whether the network is in training mode.
+      use_keras: if True builds a keras-based feature extractor, if False builds
+        a slim-based one.
     Returns:
       an ssd_meta_arch.SSDFeatureExtractor object.
     """
     min_depth = 32
-    return ssd_mobilenet_v1_feature_extractor.SSDMobileNetV1FeatureExtractor(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        self.conv_hyperparams_fn,
-        use_explicit_padding=use_explicit_padding)
-
-  def test_extract_features_returns_correct_shapes_128(self):
+    if use_keras:
+      return (ssd_mobilenet_v1_keras_feature_extractor.
+              SSDMobileNetV1KerasFeatureExtractor(
+                  is_training=is_training,
+                  depth_multiplier=depth_multiplier,
+                  min_depth=min_depth,
+                  pad_to_multiple=pad_to_multiple,
+                  conv_hyperparams=self._build_conv_hyperparams(
+                      add_batch_norm=False),
+                  freeze_batchnorm=False,
+                  inplace_batchnorm_update=False,
+                  use_explicit_padding=use_explicit_padding,
+                  name='MobilenetV1'))
+    else:
+      return ssd_mobilenet_v1_feature_extractor.SSDMobileNetV1FeatureExtractor(
+          is_training, depth_multiplier, min_depth, pad_to_multiple,
+          self.conv_hyperparams_fn,
+          use_explicit_padding=use_explicit_padding)
+
+  def test_extract_features_returns_correct_shapes_128(self, use_keras):
     image_height = 128
     image_width = 128
     depth_multiplier = 1.0
@@ -57,12 +85,14 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 1, 1, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_299(self):
+  def test_extract_features_returns_correct_shapes_299(self, use_keras):
     image_height = 299
     image_width = 299
     depth_multiplier = 1.0
@@ -72,12 +102,14 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_with_dynamic_image_shape(self):
+  def test_extract_features_with_dynamic_image_shape(self, use_keras):
     image_height = 128
     image_width = 128
     depth_multiplier = 1.0
@@ -87,12 +119,15 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 1, 1, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
+  def test_extract_features_returns_correct_shapes_enforcing_min_depth(
+      self, use_keras):
     image_height = 299
     image_width = 299
     depth_multiplier = 0.5**12
@@ -102,12 +137,15 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 2, 2, 32), (2, 1, 1, 32)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
+  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(
+      self, use_keras):
     image_height = 299
     image_width = 299
     depth_multiplier = 1.0
@@ -117,48 +155,63 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=False)
+        expected_feature_map_shape, use_explicit_padding=False,
+        use_keras=use_keras)
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape, use_explicit_padding=True)
+        expected_feature_map_shape, use_explicit_padding=True,
+        use_keras=use_keras)
 
-  def test_extract_features_raises_error_with_invalid_image_size(self):
+  def test_extract_features_raises_error_with_invalid_image_size(
+      self, use_keras):
     image_height = 32
     image_width = 32
     depth_multiplier = 1.0
     pad_to_multiple = 1
     self.check_extract_features_raises_error_with_invalid_image_size(
-        image_height, image_width, depth_multiplier, pad_to_multiple)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
+        use_keras=use_keras)
 
-  def test_preprocess_returns_correct_value_range(self):
+  def test_preprocess_returns_correct_value_range(self, use_keras):
     image_height = 128
     image_width = 128
     depth_multiplier = 1
     pad_to_multiple = 1
     test_image = np.random.rand(2, image_height, image_width, 3)
     feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       pad_to_multiple)
+                                                       pad_to_multiple,
+                                                       use_keras=use_keras)
     preprocessed_image = feature_extractor.preprocess(test_image)
     self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))
 
-  def test_variables_only_created_in_scope(self):
+  def test_variables_only_created_in_scope(self, use_keras):
     depth_multiplier = 1
     pad_to_multiple = 1
     scope_name = 'MobilenetV1'
     self.check_feature_extractor_variables_under_scope(
-        depth_multiplier, pad_to_multiple, scope_name)
+        depth_multiplier, pad_to_multiple, scope_name, use_keras=use_keras)
 
-  def test_has_fused_batchnorm(self):
+  def test_variable_count(self, use_keras):
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    variables = self.get_feature_extractor_variables(
+        depth_multiplier, pad_to_multiple, use_keras=use_keras)
+    self.assertEqual(len(variables), 151)
+
+  def test_has_fused_batchnorm(self, use_keras):
     image_height = 40
     image_width = 40
     depth_multiplier = 1
     pad_to_multiple = 1
     image_placeholder = tf.placeholder(tf.float32,
                                        [1, image_height, image_width, 3])
-    feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       pad_to_multiple)
+    feature_extractor = self._create_feature_extractor(
+        depth_multiplier, pad_to_multiple, use_keras=use_keras)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
-    _ = feature_extractor.extract_features(preprocessed_image)
+    if use_keras:
+      _ = feature_extractor(preprocessed_image)
+    else:
+      _ = feature_extractor.extract_features(preprocessed_image)
     self.assertTrue(any(op.type == 'FusedBatchNorm'
                         for op in tf.get_default_graph().get_operations()))
 
diff --git a/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
new file mode 100644
index 00000000..f1d94f2e
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
@@ -0,0 +1,163 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""SSDFeatureExtractor for Keras MobilenetV1 features."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.models.keras_models import mobilenet_v1
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+
+slim = tf.contrib.slim
+
+
+class SSDMobileNetV1KerasFeatureExtractor(
+    ssd_meta_arch.SSDKerasFeatureExtractor):
+  """SSD Feature Extractor using Keras MobilenetV1 features."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams,
+               freeze_batchnorm,
+               inplace_batchnorm_update,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False,
+               name=None):
+    """Keras MobileNetV1 Feature Extractor for SSD Models.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+        containing convolution hyperparameters for the layers added on top of
+        the base feature extractor.
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      inplace_batchnorm_update: Whether to update batch norm moving average
+        values inplace. When this is false train op must add a control
+        dependency on tf.graphkeys.UPDATE_OPS collection in order to update
+        batch norm statistics.
+      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams`.
+      name: A string name scope to assign to the model. If 'None', Keras
+        will auto-generate one from the class name.
+    """
+    super(SSDMobileNetV1KerasFeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=freeze_batchnorm,
+        inplace_batchnorm_update=inplace_batchnorm_update,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams,
+        name=name)
+    self._feature_map_layout = {
+        'from_layer': ['Conv2d_11_pointwise', 'Conv2d_13_pointwise', '', '',
+                       '', ''],
+        'layer_depth': [-1, -1, 512, 256, 256, 128],
+        'use_explicit_padding': self._use_explicit_padding,
+        'use_depthwise': self._use_depthwise,
+    }
+    self._mobilenet_v1 = None
+    self._feature_map_generator = None
+
+  def build(self, input_shape):
+    full_mobilenet_v1 = mobilenet_v1.mobilenet_v1(
+        batchnorm_training=(self._is_training and not self._freeze_batchnorm),
+        conv_hyperparams=(self._conv_hyperparams
+                          if self._override_base_feature_extractor_hyperparams
+                          else None),
+        weights=None,
+        use_explicit_padding=self._use_explicit_padding,
+        alpha=self._depth_multiplier,
+        min_depth=self._min_depth,
+        include_top=False)
+    conv2d_11_pointwise = full_mobilenet_v1.get_layer(
+        name='conv_pw_11_relu').output
+    conv2d_13_pointwise = full_mobilenet_v1.get_layer(
+        name='conv_pw_13_relu').output
+    self._mobilenet_v1 = tf.keras.Model(
+        inputs=full_mobilenet_v1.inputs,
+        outputs=[conv2d_11_pointwise, conv2d_13_pointwise])
+    self._feature_map_generator = (
+        feature_map_generators.KerasMultiResolutionFeatureMaps(
+            feature_map_layout=self._feature_map_layout,
+            depth_multiplier=self._depth_multiplier,
+            min_depth=self._min_depth,
+            insert_1x1_conv=True,
+            is_training=self._is_training,
+            conv_hyperparams=self._conv_hyperparams,
+            freeze_batchnorm=self._freeze_batchnorm,
+            name='FeatureMaps'))
+    self.built = True
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def _extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+
+    image_features = self._mobilenet_v1(
+        ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple))
+
+    feature_maps = self._feature_map_generator({
+        'Conv2d_11_pointwise': image_features[0],
+        'Conv2d_13_pointwise': image_features[1]})
+
+    return feature_maps.values()
diff --git a/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
index 9bf560eb..de63b623 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
@@ -19,7 +19,7 @@ import tensorflow as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
-from object_detection.models.keras_applications import mobilenet_v2
+from object_detection.models.keras_models import mobilenet_v2
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
 
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index a7bc806a..33cedcce 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -53,8 +53,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     Args:
       is_training: whether the network is in training mode.
       depth_multiplier: float depth multiplier for feature extractor.
-        UNUSED currently.
-      min_depth: minimum feature extractor depth. UNUSED Currently.
+      min_depth: minimum feature extractor depth.
       pad_to_multiple: the nearest multiple to zero pad the input height and
         width dimensions to.
       conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
@@ -96,9 +95,6 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         use_depthwise=use_depthwise,
         override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
-    if self._depth_multiplier != 1.0:
-      raise ValueError('Only depth 1.0 is supported, found: {}'.
-                       format(self._depth_multiplier))
     if self._use_explicit_padding is True:
       raise ValueError('Explicit padding is not a valid option.')
     self._resnet_base_fn = resnet_base_fn
@@ -150,13 +146,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     Returns:
       feature_maps: a list of tensors where the ith tensor has shape
         [batch, height_i, width_i, depth_i]
-
-    Raises:
-      ValueError: depth multiplier is not supported.
     """
-    if self._depth_multiplier != 1.0:
-      raise ValueError('Depth multiplier not supported.')
-
     preprocessed_inputs = shape_utils.check_min_image_dim(
         129, preprocessed_inputs)
 
@@ -174,8 +164,11 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
               global_pool=False,
               output_stride=None,
               store_non_strided_activations=True,
+              min_base_depth=self._min_depth,
+              depth_multiplier=self._depth_multiplier,
               scope=scope)
           image_features = self._filter_features(image_features)
+      depth_fn = lambda d: max(int(d * self._depth_multiplier), self._min_depth)
       with slim.arg_scope(self._conv_hyperparams_fn()):
         with tf.variable_scope(self._fpn_scope_name,
                                reuse=self._reuse_weights):
@@ -185,7 +178,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             feature_block_list.append('block{}'.format(level - 1))
           fpn_features = feature_map_generators.fpn_top_down_feature_maps(
               [(key, image_features[key]) for key in feature_block_list],
-              depth=self._additional_layer_depth)
+              depth=depth_fn(self._additional_layer_depth))
           feature_maps = []
           for level in range(self._fpn_min_level, base_fpn_max_level + 1):
             feature_maps.append(
@@ -196,7 +189,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
           for i in range(base_fpn_max_level, self._fpn_max_level):
             last_feature_map = slim.conv2d(
                 last_feature_map,
-                num_outputs=self._additional_layer_depth,
+                num_outputs=depth_fn(self._additional_layer_depth),
                 kernel_size=[3, 3],
                 stride=2,
                 padding='SAME',
@@ -226,8 +219,7 @@ class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
     Args:
       is_training: whether the network is in training mode.
       depth_multiplier: float depth multiplier for feature extractor.
-        UNUSED currently.
-      min_depth: minimum feature extractor depth. UNUSED Currently.
+      min_depth: minimum feature extractor depth.
       pad_to_multiple: the nearest multiple to zero pad the input height and
         width dimensions to.
       conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
@@ -284,8 +276,7 @@ class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
     Args:
       is_training: whether the network is in training mode.
       depth_multiplier: float depth multiplier for feature extractor.
-        UNUSED currently.
-      min_depth: minimum feature extractor depth. UNUSED Currently.
+      min_depth: minimum feature extractor depth.
       pad_to_multiple: the nearest multiple to zero pad the input height and
         width dimensions to.
       conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
@@ -342,8 +333,7 @@ class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
     Args:
       is_training: whether the network is in training mode.
       depth_multiplier: float depth multiplier for feature extractor.
-        UNUSED currently.
-      min_depth: minimum feature extractor depth. UNUSED Currently.
+      min_depth: minimum feature extractor depth.
       pad_to_multiple: the nearest multiple to zero pad the input height and
         width dimensions to.
       conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
index 5f406359..31635893 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
@@ -25,8 +25,7 @@ class SSDResnet50V1FeatureExtractorTest(
   """SSDResnet50v1Fpn feature extractor test."""
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-                                use_explicit_padding=False):
-    min_depth = 32
+                                use_explicit_padding=False, min_depth=32):
     is_training = True
     return ssd_resnet_v1_fpn_feature_extractor.SSDResnet50V1FpnFeatureExtractor(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
@@ -42,8 +41,7 @@ class SSDResnet101V1FeatureExtractorTest(
   """SSDResnet101v1Fpn feature extractor test."""
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-                                use_explicit_padding=False):
-    min_depth = 32
+                                use_explicit_padding=False, min_depth=32):
     is_training = True
     return (
         ssd_resnet_v1_fpn_feature_extractor.SSDResnet101V1FpnFeatureExtractor(
@@ -64,8 +62,7 @@ class SSDResnet152V1FeatureExtractorTest(
   """SSDResnet152v1Fpn feature extractor test."""
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-                                use_explicit_padding=False):
-    min_depth = 32
+                                use_explicit_padding=False, min_depth=32):
     is_training = True
     return (
         ssd_resnet_v1_fpn_feature_extractor.SSDResnet152V1FpnFeatureExtractor(
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
index fd8d9f6d..e0aaf79f 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
@@ -14,6 +14,7 @@
 # ==============================================================================
 """Tests for ssd resnet v1 FPN feature extractors."""
 import abc
+import itertools
 import numpy as np
 import tensorflow as tf
 
@@ -32,6 +33,14 @@ class SSDResnetFPNFeatureExtractorTestBase(
   def _fpn_scope_name(self):
     return 'fpn'
 
+  @abc.abstractmethod
+  def _create_feature_extractor(self,
+                                depth_multiplier,
+                                pad_to_multiple,
+                                use_explicit_padding=False,
+                                min_depth=32):
+    pass
+
   def test_extract_features_returns_correct_shapes_256(self):
     image_height = 256
     image_width = 256
@@ -56,6 +65,45 @@ class SSDResnetFPNFeatureExtractorTestBase(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
+  def test_extract_features_returns_correct_shapes_with_depth_multiplier(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 0.5
+    expected_num_channels = int(256 * depth_multiplier)
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 32, 32, expected_num_channels),
+                                  (2, 16, 16, expected_num_channels),
+                                  (2, 8, 8, expected_num_channels),
+                                  (2, 4, 4, expected_num_channels),
+                                  (2, 2, 2, expected_num_channels)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_returns_correct_shapes_with_min_depth(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    min_depth = 320
+    expected_feature_map_shape = [(2, 32, 32, min_depth),
+                                  (2, 16, 16, min_depth),
+                                  (2, 8, 8, min_depth),
+                                  (2, 4, 4, min_depth),
+                                  (2, 2, 2, min_depth)]
+
+    def graph_fn(image_tensor):
+      feature_extractor = self._create_feature_extractor(
+          depth_multiplier, pad_to_multiple, min_depth=min_depth)
+      return feature_extractor.extract_features(image_tensor)
+
+    image_tensor = np.random.rand(2, image_height, image_width,
+                                  3).astype(np.float32)
+    feature_maps = self.execute(graph_fn, [image_tensor])
+    for feature_map, expected_shape in itertools.izip(
+        feature_maps, expected_feature_map_shape):
+      self.assertAllEqual(feature_map.shape, expected_shape)
+
   def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
     image_height = 254
     image_width = 254
diff --git a/research/object_detection/object_detection_tutorial.ipynb b/research/object_detection/object_detection_tutorial.ipynb
index ddbe0dca..f8dca2c7 100644
--- a/research/object_detection/object_detection_tutorial.ipynb
+++ b/research/object_detection/object_detection_tutorial.ipynb
@@ -54,8 +54,8 @@
         "sys.path.append(\"..\")\n",
         "from object_detection.utils import ops as utils_ops\n",
         "\n",
-        "if StrictVersion(tf.__version__) \u003c StrictVersion('1.9.0'):\n",
-        "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n"
+        "if StrictVersion(tf.__version__) \u003c StrictVersion('1.12.0'):\n",
+        "  raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')\n"
       ]
     },
     {
diff --git a/research/object_detection/predictors/convolutional_box_predictor.py b/research/object_detection/predictors/convolutional_box_predictor.py
index 628f7bc0..8762412a 100644
--- a/research/object_detection/predictors/convolutional_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_box_predictor.py
@@ -108,14 +108,16 @@ class ConvolutionalBoxPredictor(box_predictor.BoxPredictor):
         feature map.
 
     Returns:
-      box_encodings: A list of float tensors of shape
-        [batch_size, num_anchors_i, q, code_size] representing the location of
-        the objects, where q is 1 or the number of classes. Each entry in the
-        list corresponds to a feature map in the input `image_features` list.
-      class_predictions_with_background: A list of float tensors of shape
-        [batch_size, num_anchors_i, num_classes + 1] representing the class
-        predictions for the proposals. Each entry in the list corresponds to a
-        feature map in the input `image_features` list.
+      A dictionary containing:
+        box_encodings: A list of float tensors of shape
+          [batch_size, num_anchors_i, q, code_size] representing the location of
+          the objects, where q is 1 or the number of classes. Each entry in the
+          list corresponds to a feature map in the input `image_features` list.
+        class_predictions_with_background: A list of float tensors of shape
+          [batch_size, num_anchors_i, num_classes + 1] representing the class
+          predictions for the proposals. Each entry in the list corresponds to a
+          feature map in the input `image_features` list.
+        (optional) Predictions from other heads.
     """
     predictions = {
         BOX_ENCODINGS: [],
@@ -226,8 +228,8 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
       kernel_size: Size of final convolution kernel.
       apply_batch_norm: Whether to apply batch normalization to conv layers in
         this predictor.
-      share_prediction_tower: Whether to share the multi-layer tower between box
-        prediction and class prediction heads.
+      share_prediction_tower: Whether to share the multi-layer tower among box
+        prediction head, class prediction head and other heads.
       use_depthwise: Whether to use depthwise separable conv2d instead of
        regular conv2d.
     """
@@ -270,9 +272,7 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
     inserted_layer_counter += 1
     return image_feature, inserted_layer_counter
 
-  def _compute_base_tower(self, tower_name_scope, image_feature, feature_index,
-                          has_different_feature_channels, target_channel,
-                          inserted_layer_counter):
+  def _compute_base_tower(self, tower_name_scope, image_feature, feature_index):
     net = image_feature
     for i in range(self._num_layers_before_predictor):
       if self._use_depthwise:
@@ -296,23 +296,18 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
     return net
 
   def _predict_head(self, head_name, head_obj, image_feature, box_tower_feature,
-                    feature_index, has_different_feature_channels,
-                    target_channel, inserted_layer_counter,
-                    num_predictions_per_location):
+                    feature_index, num_predictions_per_location):
     if head_name == CLASS_PREDICTIONS_WITH_BACKGROUND:
       tower_name_scope = 'ClassPredictionTower'
     else:
-      raise ValueError('Unknown head')
+      tower_name_scope = head_name + 'PredictionTower'
     if self._share_prediction_tower:
       head_tower_feature = box_tower_feature
     else:
       head_tower_feature = self._compute_base_tower(
           tower_name_scope=tower_name_scope,
           image_feature=image_feature,
-          feature_index=feature_index,
-          has_different_feature_channels=has_different_feature_channels,
-          target_channel=target_channel,
-          inserted_layer_counter=inserted_layer_counter)
+          feature_index=feature_index)
     return head_obj.predict(
         features=head_tower_feature,
         num_predictions_per_location=num_predictions_per_location)
@@ -341,13 +336,13 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
           [batch_size, num_anchors_i, num_classes + 1] representing the class
           predictions for the proposals. Each entry in the list corresponds to a
           feature map in the input `image_features` list.
-        (optional) mask_predictions: A list of float tensors of shape
+        (optional) Predictions from other heads.
+          E.g., mask_predictions: A list of float tensors of shape
           [batch_size, num_anchord_i, num_classes, mask_height, mask_width].
 
 
     Raises:
-      ValueError: If the image feature maps do not have the same number of
-        channels or if the num predictions per locations is differs between the
+      ValueError: If the num predictions per locations differs between the
         feature maps.
     """
     if len(set(num_predictions_per_location_list)) > 1:
@@ -392,10 +387,7 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
           box_tower_feature = self._compute_base_tower(
               tower_name_scope=box_tower_scope,
               image_feature=image_feature,
-              feature_index=feature_index,
-              has_different_feature_channels=has_different_feature_channels,
-              target_channel=target_channel,
-              inserted_layer_counter=inserted_layer_counter)
+              feature_index=feature_index)
           box_encodings = self._box_prediction_head.predict(
               features=box_tower_feature,
               num_predictions_per_location=num_predictions_per_location)
@@ -413,9 +405,8 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
                 image_feature=image_feature,
                 box_tower_feature=box_tower_feature,
                 feature_index=feature_index,
-                has_different_feature_channels=has_different_feature_channels,
-                target_channel=target_channel,
-                inserted_layer_counter=inserted_layer_counter,
                 num_predictions_per_location=num_predictions_per_location)
             predictions[head_name].append(prediction)
     return predictions
+
+
diff --git a/research/object_detection/predictors/convolutional_box_predictor_test.py b/research/object_detection/predictors/convolutional_box_predictor_test.py
index 3ae1dbf2..e4c4fe99 100644
--- a/research/object_detection/predictors/convolutional_box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_box_predictor_test.py
@@ -14,6 +14,8 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.convolutional_box_predictor."""
+
+from absl.testing import parameterized
 import numpy as np
 import tensorflow as tf
 
@@ -21,6 +23,9 @@ from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
 from object_detection.builders import hyperparams_builder
 from object_detection.predictors import convolutional_box_predictor as box_predictor
+from object_detection.predictors.heads import box_head
+from object_detection.predictors.heads import class_head
+from object_detection.predictors.heads import mask_head
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import test_case
 
@@ -852,5 +857,66 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       self.assertAllEqual(objectness_predictions_shape,
                           [4, expected_num_anchors, 1])
 
+  def test_other_heads_predictions(self):
+    box_code_size = 4
+    num_classes_without_background = 3
+    other_head_name = 'Mask'
+    mask_height = 5
+    mask_width = 5
+    num_predictions_per_location = 5
+
+    def graph_fn(image_features):
+      box_prediction_head = box_head.WeightSharedConvolutionalBoxHead(
+          box_code_size)
+      class_prediction_head = class_head.WeightSharedConvolutionalClassHead(
+          num_classes_without_background + 1)
+      other_heads = {
+          other_head_name:
+              mask_head.WeightSharedConvolutionalMaskHead(
+                  num_classes_without_background,
+                  mask_height=mask_height,
+                  mask_width=mask_width)
+      }
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          box_prediction_head=box_prediction_head,
+          class_prediction_head=class_prediction_head,
+          other_heads=other_heads,
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=2)
+      box_predictions = conv_box_predictor.predict(
+          [image_features],
+          num_predictions_per_location=[num_predictions_per_location],
+          scope='BoxPredictor')
+      for key, value in box_predictions.items():
+        box_predictions[key] = tf.concat(value, axis=1)
+      assert len(box_predictions) == 3
+      return (box_predictions[box_predictor.BOX_ENCODINGS],
+              box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+              box_predictions[other_head_name])
+
+    batch_size = 4
+    feature_ht = 8
+    feature_wt = 8
+    image_features = np.random.rand(batch_size, feature_ht, feature_wt,
+                                    64).astype(np.float32)
+    (box_encodings, class_predictions, other_head_predictions) = self.execute(
+        graph_fn, [image_features])
+    num_anchors = feature_ht * feature_wt * num_predictions_per_location
+    self.assertAllEqual(box_encodings.shape,
+                        [batch_size, num_anchors, box_code_size])
+    self.assertAllEqual(
+        class_predictions.shape,
+        [batch_size, num_anchors, num_classes_without_background + 1])
+    self.assertAllEqual(other_head_predictions.shape, [
+        batch_size, num_anchors, num_classes_without_background, mask_height,
+        mask_width
+    ])
+
+
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/convolutional_keras_box_predictor_test.py b/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
index dff2a36c..48f0009e 100644
--- a/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
@@ -191,7 +191,69 @@ class ConvolutionalKerasBoxPredictorTest(test_case.TestCase):
     self.assertEqual(conv_box_predictor._sorted_head_names,
                      ['box_encodings', 'class_predictions_with_background'])
 
-  # TODO(kaftan): Remove conditional after CMLE moves to TF 1.10
+  def test_use_depthwise_convolution(self):
+    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
+    conv_box_predictor = (
+        box_predictor_builder.build_convolutional_keras_box_predictor(
+            is_training=False,
+            num_classes=0,
+            conv_hyperparams=self._build_conv_hyperparams(),
+            freeze_batchnorm=False,
+            inplace_batchnorm_update=False,
+            num_predictions_per_location_list=[5],
+            min_depth=0,
+            max_depth=32,
+            num_layers_before_predictor=1,
+            use_dropout=True,
+            dropout_keep_prob=0.8,
+            kernel_size=1,
+            box_code_size=4,
+            use_depthwise=True
+        ))
+    box_predictions = conv_box_predictor([image_features])
+    box_encodings = tf.concat(
+        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+    objectness_predictions = tf.concat(
+        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+        axis=1)
+    init_op = tf.global_variables_initializer()
+
+    resolution = 32
+    expected_num_anchors = resolution*resolution*5
+    with self.test_session() as sess:
+      sess.run(init_op)
+      (box_encodings_shape,
+       objectness_predictions_shape) = sess.run(
+           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
+           feed_dict={image_features:
+                      np.random.rand(4, resolution, resolution, 64)})
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])
+    self.assertAllEqual(objectness_predictions_shape,
+                        [4, expected_num_anchors, 1])
+    expected_variable_set = set([
+        'BoxPredictor/SharedConvolutions_0/Conv2d_0_1x1_32/bias',
+        'BoxPredictor/SharedConvolutions_0/Conv2d_0_1x1_32/kernel',
+
+        'BoxPredictor/ConvolutionalBoxHead_0/BoxEncodingPredictor_depthwise/'
+        'bias',
+
+        'BoxPredictor/ConvolutionalBoxHead_0/BoxEncodingPredictor_depthwise/'
+        'depthwise_kernel',
+
+        'BoxPredictor/ConvolutionalBoxHead_0/BoxEncodingPredictor/bias',
+        'BoxPredictor/ConvolutionalBoxHead_0/BoxEncodingPredictor/kernel',
+        'BoxPredictor/ConvolutionalClassHead_0/ClassPredictor_depthwise/bias',
+
+        'BoxPredictor/ConvolutionalClassHead_0/ClassPredictor_depthwise/'
+        'depthwise_kernel',
+
+        'BoxPredictor/ConvolutionalClassHead_0/ClassPredictor/bias',
+        'BoxPredictor/ConvolutionalClassHead_0/ClassPredictor/kernel'])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+    self.assertEqual(conv_box_predictor._sorted_head_names,
+                     ['box_encodings', 'class_predictions_with_background'])
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/heads/keras_box_head_test.py b/research/object_detection/predictors/heads/keras_box_head_test.py
index b4057378..c03d7313 100644
--- a/research/object_detection/predictors/heads/keras_box_head_test.py
+++ b/research/object_detection/predictors/heads/keras_box_head_test.py
@@ -56,7 +56,20 @@ class ConvolutionalKerasBoxHeadTest(test_case.TestCase):
     box_encodings = box_prediction_head(image_feature)
     self.assertAllEqual([64, 323, 1, 4], box_encodings.get_shape().as_list())
 
-  # TODO(kaftan): Remove conditional after CMLE moves to TF 1.10
+  def test_prediction_size_depthwise_true(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    box_prediction_head = keras_box_head.ConvolutionalBoxHead(
+        is_training=True,
+        box_code_size=4,
+        kernel_size=3,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=False,
+        num_predictions_per_location=1,
+        use_depthwise=True)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    box_encodings = box_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 1, 4], box_encodings.get_shape().as_list())
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/heads/keras_class_head_test.py b/research/object_detection/predictors/heads/keras_class_head_test.py
index 125fcdf8..8a49b7ab 100644
--- a/research/object_detection/predictors/heads/keras_class_head_test.py
+++ b/research/object_detection/predictors/heads/keras_class_head_test.py
@@ -59,7 +59,23 @@ class ConvolutionalKerasClassPredictorTest(test_case.TestCase):
     self.assertAllEqual([64, 323, 20],
                         class_predictions.get_shape().as_list())
 
-  # TODO(kaftan): Remove conditional after CMLE moves to TF 1.10
+  def test_prediction_size_depthwise_true(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    class_prediction_head = keras_class_head.ConvolutionalClassHead(
+        is_training=True,
+        num_class_slots=20,
+        use_dropout=True,
+        dropout_keep_prob=0.5,
+        kernel_size=3,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=False,
+        num_predictions_per_location=1,
+        use_depthwise=True)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    class_predictions = class_prediction_head(image_feature,)
+    self.assertAllEqual([64, 323, 20],
+                        class_predictions.get_shape().as_list())
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/predictors/heads/keras_mask_head_test.py b/research/object_detection/predictors/heads/keras_mask_head_test.py
index 67274d50..2177c334 100644
--- a/research/object_detection/predictors/heads/keras_mask_head_test.py
+++ b/research/object_detection/predictors/heads/keras_mask_head_test.py
@@ -61,7 +61,25 @@ class ConvolutionalMaskPredictorTest(test_case.TestCase):
     self.assertAllEqual([64, 323, 20, 7, 7],
                         mask_predictions.get_shape().as_list())
 
-  # TODO(kaftan): Remove conditional after CMLE moves to TF 1.10
+  def test_prediction_size_use_depthwise_true(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    mask_prediction_head = keras_mask_head.ConvolutionalMaskHead(
+        is_training=True,
+        num_classes=20,
+        use_dropout=True,
+        dropout_keep_prob=0.5,
+        kernel_size=3,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=False,
+        num_predictions_per_location=1,
+        use_depthwise=True,
+        mask_height=7,
+        mask_width=7)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    mask_predictions = mask_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 20, 7, 7],
+                        mask_predictions.get_shape().as_list())
 
   def test_class_agnostic_prediction_size_use_depthwise_false(self):
     conv_hyperparams = self._build_conv_hyperparams()
@@ -84,7 +102,26 @@ class ConvolutionalMaskPredictorTest(test_case.TestCase):
     self.assertAllEqual([64, 323, 1, 7, 7],
                         mask_predictions.get_shape().as_list())
 
-  # TODO(kaftan): Remove conditional after CMLE moves to TF 1.10
+  def test_class_agnostic_prediction_size_use_depthwise_true(self):
+    conv_hyperparams = self._build_conv_hyperparams()
+    mask_prediction_head = keras_mask_head.ConvolutionalMaskHead(
+        is_training=True,
+        num_classes=20,
+        use_dropout=True,
+        dropout_keep_prob=0.5,
+        kernel_size=3,
+        conv_hyperparams=conv_hyperparams,
+        freeze_batchnorm=False,
+        num_predictions_per_location=1,
+        use_depthwise=True,
+        mask_height=7,
+        mask_width=7,
+        masks_are_class_agnostic=True)
+    image_feature = tf.random_uniform(
+        [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    mask_predictions = mask_prediction_head(image_feature)
+    self.assertAllEqual([64, 323, 1, 7, 7],
+                        mask_predictions.get_shape().as_list())
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/protos/calibration.proto b/research/object_detection/protos/calibration.proto
new file mode 100644
index 00000000..3d34d47a
--- /dev/null
+++ b/research/object_detection/protos/calibration.proto
@@ -0,0 +1,68 @@
+syntax = "proto2";
+
+package object_detection.protos;
+
+// Message wrapper for various calibration configurations
+message CalibrationConfig {
+  oneof calibrator {
+    // Class-agnostic calibration via linear interpolation (usually output from
+    // isotonic regression)
+    FunctionApproximation function_approximation = 1;
+
+    // Per-class calibration via linear interpolation
+    LabelFunctionApproximations label_function_approximations = 2;
+
+    // Class-agnostic sigmoid calibration
+    SigmoidCalibration sigmoid_calibration = 3;
+
+    // Per-class sigmoid calibration
+    LabelSigmoidCalibrations label_sigmoid_calibrations = 4;
+  }
+}
+
+// Message for class-agnostic domain/range mapping for function
+// approximations
+message FunctionApproximation {
+  // Message mapping class labels to indices
+  optional XYPairs x_y_pairs = 1;
+}
+
+// Message for class-specific domain/range mapping for function
+// approximations
+message LabelFunctionApproximations {
+  // Message mapping class labels to indices
+  map<string, XYPairs> label_xy_pairs_map = 1;
+  // Label map to map label names from to class ids.
+  optional string label_map_path = 2;
+}
+
+// Message for class-agnostic Sigmoid Calibration
+message SigmoidCalibration {
+  // Message mapping class index to Sigmoid Parameters
+  optional SigmoidParameters sigmoid_parameters = 1;
+}
+
+// Message for class-specific Sigmoid Calibration
+message LabelSigmoidCalibrations {
+  // Message mapping class index to Sigmoid Parameters
+  map<string, SigmoidParameters> label_sigmoid_parameters_map = 1;
+  // Label map to map label names from to class ids.
+  optional string label_map_path = 2;
+}
+
+// Message to store a domain/range pair for function to be approximated
+message XYPairs {
+  message XYPair {
+    optional float x = 1;
+    optional float y = 2;
+  }
+
+  // Sequence of x/y pairs for function approximation
+  repeated XYPair x_y_pair = 1;
+}
+
+// Message defining parameters for sigmoid calibration.
+message SigmoidParameters {
+  optional float a = 1 [default = -1.0];
+  optional float b = 2 [default = 0.0];
+}
diff --git a/research/object_detection/protos/image_resizer.proto b/research/object_detection/protos/image_resizer.proto
index f8b3f660..031aeae0 100644
--- a/research/object_detection/protos/image_resizer.proto
+++ b/research/object_detection/protos/image_resizer.proto
@@ -8,6 +8,7 @@ message ImageResizer {
   oneof image_resizer_oneof {
     KeepAspectRatioResizer keep_aspect_ratio_resizer = 1;
     FixedShapeResizer fixed_shape_resizer = 2;
+    IdentityResizer identity_resizer = 3;
   }
 }
 
@@ -19,6 +20,9 @@ enum ResizeType {
   AREA = 3; // Corresponds to tf.image.ResizeMethod.AREA
 }
 
+message IdentityResizer {
+}
+
 // Configuration proto for image resizer that keeps aspect ratio.
 message KeepAspectRatioResizer {
   // Desired size of the smaller image dimension in pixels.
diff --git a/research/object_detection/protos/input_reader.proto b/research/object_detection/protos/input_reader.proto
index 38848f6b..701dacae 100644
--- a/research/object_detection/protos/input_reader.proto
+++ b/research/object_detection/protos/input_reader.proto
@@ -22,7 +22,7 @@ enum InstanceMaskType {
   PNG_MASKS = 2;        // Encoded PNG masks.
 }
 
-// Next id: 24
+// Next id: 25
 message InputReader {
   // Name of input reader. Typically used to describe the dataset that is read
   // by this input reader.
@@ -94,6 +94,9 @@ message InputReader {
   // otherwise some groundtruth boxes may be clipped.
   optional int32 max_number_of_boxes = 21 [default=100];
 
+  // Whether to load multiclass scores from the dataset.
+  optional bool load_multiclass_scores = 24 [default = false];
+
   // Whether to load groundtruth instance masks.
   optional bool load_instance_masks = 7 [default = false];
 
diff --git a/research/object_detection/protos/optimizer.proto b/research/object_detection/protos/optimizer.proto
index 8bbebe45..006648c9 100644
--- a/research/object_detection/protos/optimizer.proto
+++ b/research/object_detection/protos/optimizer.proto
@@ -38,6 +38,7 @@ message AdamOptimizer {
   optional LearningRate learning_rate = 1;
 }
 
+
 // Configuration message for optimizer learning rate.
 message LearningRate {
   oneof learning_rate {
diff --git a/research/object_detection/protos/post_processing.proto b/research/object_detection/protos/post_processing.proto
index 3d501b6d..e9610a5e 100644
--- a/research/object_detection/protos/post_processing.proto
+++ b/research/object_detection/protos/post_processing.proto
@@ -2,6 +2,8 @@ syntax = "proto2";
 
 package object_detection.protos;
 
+import "object_detection/protos/calibration.proto";
+
 // Configuration proto for non-max-suppression operation on a batch of
 // detections.
 message BatchNonMaxSuppression {
@@ -46,4 +48,7 @@ message PostProcessing {
   // Typically used for softmax distillation, though can be used to scale for
   // other reasons.
   optional float logit_scale = 3 [default = 1.0];
+  // Calibrate score outputs. Calibration is applied after score converter
+  // and before non max suppression.
+  optional CalibrationConfig calibration_config = 4;
 }
diff --git a/research/object_detection/protos/preprocessor.proto b/research/object_detection/protos/preprocessor.proto
index 4905bf4b..0c8c6526 100644
--- a/research/object_detection/protos/preprocessor.proto
+++ b/research/object_detection/protos/preprocessor.proto
@@ -34,6 +34,8 @@ message PreprocessingStep {
     RandomRotation90 random_rotation90 = 26;
     RGBtoGray rgb_to_gray = 27;
     ConvertClassLogitsToSoftmax convert_class_logits_to_softmax = 28;
+    RandomAbsolutePadImage random_absolute_pad_image = 29;
+    RandomSelfConcatImage random_self_concat_image = 30;
   }
 }
 
@@ -179,6 +181,18 @@ message RandomPadImage {
   repeated float pad_color = 5;
 }
 
+// Randomly adds a padding of size [0, max_height_padding), [0, max_width_padding).
+message RandomAbsolutePadImage {
+  // Height will be padded uniformly at random from [0, max_height_padding).
+  optional int32 max_height_padding = 1;
+  // Width will be padded uniformly at random from [0, max_width_padding).
+  optional int32 max_width_padding = 2;
+
+  // Color of the padding. If unset, will pad using average color of the input
+  // image.
+  repeated float pad_color = 3;
+}
+
 // Randomly crops an image followed by a random pad.
 message RandomCropPadImage {
   // Cropping operation must cover at least one box by this fraction.
@@ -243,8 +257,8 @@ message RandomBlackPatches {
 
 // Randomly resizes the image up to [target_height, target_width].
 message RandomResizeMethod {
-  optional float target_height = 1;
-  optional float target_width = 2;
+  optional int32 target_height = 1;
+  optional int32 target_width = 2;
 }
 
 // Converts the RGB image to a grayscale image. This also converts the image
@@ -439,3 +453,11 @@ message ConvertClassLogitsToSoftmax {
   // Scale to use on logits before applying softmax.
   optional float temperature = 1 [default=1.0];
 }
+
+// Randomly concatenates the image with itself horizontally and/or vertically.
+message RandomSelfConcatImage {
+  // Probability of concatenating the image vertically.
+  optional float concat_vertical_probability = 1 [default = 0.1];
+  // Probability of concatenating the image horizontally.
+  optional float concat_horizontal_probability = 2 [default = 0.1];
+}
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index 21313836..77f6118f 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -184,6 +184,11 @@ message SsdFeatureExtractor {
 
   // Feature Pyramid Networks config.
   optional FeaturePyramidNetworks fpn = 10;
+
+  // If true, replace preprocess function of feature extractor with a
+  // placeholder. This should only be used if all the image preprocessing steps
+  // happen outside the graph.
+  optional bool replace_preprocessor_with_placeholder = 11 [default = false];
 }
 
 // Configuration for Feature Pyramid Networks.
@@ -210,4 +215,5 @@ message FeaturePyramidNetworks {
 
   // channel depth for additional coarse feature layers.
   optional int32 additional_layer_depth = 3 [default = 256];
+
 }
diff --git a/research/object_detection/protos/train.proto b/research/object_detection/protos/train.proto
index 0d2a7e62..009a3c10 100644
--- a/research/object_detection/protos/train.proto
+++ b/research/object_detection/protos/train.proto
@@ -74,13 +74,13 @@ message TrainConfig {
   optional int32 replicas_to_aggregate = 13 [default=1];
 
   // Maximum number of elements to store within a queue.
-  optional int32 batch_queue_capacity = 14 [default=150];
+  optional int32 batch_queue_capacity = 14 [default=150, deprecated=true];
 
   // Number of threads to use for batching.
-  optional int32 num_batch_queue_threads = 15 [default=8];
+  optional int32 num_batch_queue_threads = 15 [default=8, deprecated=true];
 
   // Maximum capacity of the queue used to prefetch assembled batches.
-  optional int32 prefetch_queue_capacity = 16 [default=5];
+  optional int32 prefetch_queue_capacity = 16 [default=5, deprecated=true];
 
   // If true, boxes with the same coordinates will be merged together.
   // This is useful when each box can have multiple labels.
@@ -113,7 +113,8 @@ message TrainConfig {
   // will lead to a larger memory footprint.
   optional bool retain_original_images = 23 [default=false];
 
-  // Whether to use bfloat16 for training.
+  // Whether to use bfloat16 for training. This is currently only supported for
+  // TPUs.
   optional bool use_bfloat16 = 26 [default=false];
 
   // Whether to summarize gradients.
diff --git a/research/object_detection/samples/cloud/cloud.yml b/research/object_detection/samples/cloud/cloud.yml
index 3dde6bd1..56e74af1 100644
--- a/research/object_detection/samples/cloud/cloud.yml
+++ b/research/object_detection/samples/cloud/cloud.yml
@@ -1,5 +1,5 @@
 trainingInput:
-  runtimeVersion: "1.9"
+  runtimeVersion: "1.12"
   scaleTier: CUSTOM
   masterType: standard_gpu
   workerCount: 5
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config
new file mode 100644
index 00000000..d3654c9e
--- /dev/null
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config
@@ -0,0 +1,140 @@
+# Faster R-CNN with Inception Resnet v2, Atrous version;
+# Configured for Open Images V4 Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 601
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 600
+        max_dimension: 1024
+      }
+    }
+    feature_extractor {
+      type: "faster_rcnn_inception_resnet_v2"
+      first_stage_features_stride: 8
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: 0.25
+        scales: 0.5
+        scales: 1.0
+        scales: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        height_stride: 8
+        width_stride: 8
+      }
+    }
+    first_stage_atrous_rate: 2
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 17
+    maxpool_kernel_size: 1
+    maxpool_stride: 1
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.00006
+          schedule {
+            step: 6000000
+            learning_rate: .000006
+          }
+          schedule {
+            step: 7000000
+            learning_rate: .0000006
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  num_steps: 10000000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/oid_v4_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "open_images_V2_detection_metrics"
+}
+
+eval_input_reader: {
+  sample_1_of_n_examples: 10
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/oid_v4_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config b/research/object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config
new file mode 100644
index 00000000..d78a592c
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config
@@ -0,0 +1,190 @@
+# SSD with Mobilenet v2 configuration for OpenImages V4 Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  ssd {
+    num_classes: 601
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.9997,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v2'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.9997,
+          epsilon: 0.001,
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid {
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      hard_example_miner {
+        num_hard_examples: 3000
+        iou_threshold: 0.99
+        loss_type: CLASSIFICATION
+        max_negatives_per_positive: 3
+        min_negatives_per_image: 3
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 24
+  optimizer {
+    rms_prop_optimizer: {
+      learning_rate: {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.0008
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+  gradient_clipping_by_norm: 10.0
+  keep_checkpoint_every_n_hours: 24
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  num_steps: 10000000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/oid_v4_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "open_images_V2_detection_metrics"
+}
+
+eval_input_reader: {
+  sample_1_of_n_examples: 10
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/oid_v4_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
+
diff --git a/research/object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config b/research/object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config
new file mode 100644
index 00000000..4ec990c4
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config
@@ -0,0 +1,194 @@
+# SSD with Resnet 101 v1 FPN feature extractor, shared box predictor and focal
+# loss (a.k.a Retinanet).
+# See Lin et al, https://arxiv.org/abs/1708.02002
+# Trained on open image dataset v4, initialized from scratch.
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 601
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      multiscale_anchor_generator {
+        min_level: 2
+        max_level: 7
+        anchor_scale: 4.0
+        aspect_ratios: [1.0, 2.0, 0.5]
+        scales_per_octave: 2
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 512
+        width: 512
+      }
+    }
+    box_predictor {
+      weight_shared_convolutional_box_predictor {
+        depth: 256
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.0004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            scale: true,
+            decay: 0.997,
+            epsilon: 0.001,
+          }
+        }
+        num_layers_before_predictor: 2
+        kernel_size: 3
+      }
+    }
+    feature_extractor {
+      type: 'ssd_resnet101_v1_fpn'
+      fpn {
+        min_level: 2
+        max_level: 7
+      }
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.0004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true,
+          decay: 0.997,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.25
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+        use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 64
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 400000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    random_crop_image {
+      min_object_covered: 0.0
+      min_aspect_ratio: 0.75
+      max_aspect_ratio: 3.0
+      min_area: 0.75
+      max_area: 1.0
+      overlap_thresh: 0.0
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: .04
+          total_steps: 400000
+          warmup_learning_rate: .013333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "oid_V2_detection_metrics"
+}
+
+eval_input_reader: {
+  sample_1_of_n_examples: 10
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/oid_bbox_trainable_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
+
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index 69da3bf9..a88782ab 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -73,6 +73,8 @@ def get_spatial_image_size(image_resizer_config):
       return [image_resizer_config.keep_aspect_ratio_resizer.max_dimension] * 2
     else:
       return [-1, -1]
+  if image_resizer_config.HasField("identity_resizer"):
+    return [-1, -1]
   raise ValueError("Unknown image resizer type.")
 
 
diff --git a/research/object_detection/utils/label_map_util.py b/research/object_detection/utils/label_map_util.py
index c395a716..f6ecbf1d 100644
--- a/research/object_detection/utils/label_map_util.py
+++ b/research/object_detection/utils/label_map_util.py
@@ -184,7 +184,9 @@ def get_label_map_dict(label_map_path,
       # there are gaps in the labels, fill in gaps.
       for value in range(1, max(values)):
         if value not in values:
-          label_map_dict['class_' + str(value)] = value
+          # TODO(rathodv): Add a prefix 'class_' here once the tool to generate
+          # teacher annotation adds this prefix in the data.
+          label_map_dict[str(value)] = value
 
   return label_map_dict
 
diff --git a/research/object_detection/utils/label_map_util_test.py b/research/object_detection/utils/label_map_util_test.py
index e5f69f91..529baee6 100644
--- a/research/object_detection/utils/label_map_util_test.py
+++ b/research/object_detection/utils/label_map_util_test.py
@@ -138,7 +138,7 @@ class LabelMapUtilTest(tf.test.TestCase):
 
     self.assertEqual(label_map_dict['background'], 0)
     self.assertEqual(label_map_dict['dog'], 1)
-    self.assertEqual(label_map_dict['class_2'], 2)
+    self.assertEqual(label_map_dict['2'], 2)
     self.assertEqual(label_map_dict['cat'], 3)
     self.assertEqual(len(label_map_dict), max(label_map_dict.values()) + 1)
 
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 15c2a0c7..32609afa 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -328,6 +328,7 @@ def retain_groundtruth(tensor_dict, valid_indices):
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_confidences
       fields.InputDataFields.groundtruth_keypoints
       fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
@@ -357,7 +358,9 @@ def retain_groundtruth(tensor_dict, valid_indices):
     for key in tensor_dict:
       if key in [fields.InputDataFields.groundtruth_boxes,
                  fields.InputDataFields.groundtruth_classes,
+                 fields.InputDataFields.groundtruth_confidences,
                  fields.InputDataFields.groundtruth_keypoints,
+                 fields.InputDataFields.groundtruth_keypoint_visibilities,
                  fields.InputDataFields.groundtruth_instance_masks]:
         valid_dict[key] = tf.gather(tensor_dict[key], valid_indices)
       # Input decoder returns empty tensor when these fields are not provided.
@@ -385,6 +388,7 @@ def retain_groundtruth_with_positive_classes(tensor_dict):
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_confidences
       fields.InputDataFields.groundtruth_keypoints
       fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
@@ -426,6 +430,7 @@ def filter_groundtruth_with_crowd_boxes(tensor_dict):
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_confidences
       fields.InputDataFields.groundtruth_keypoints
       fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
@@ -451,6 +456,7 @@ def filter_groundtruth_with_nan_box_coordinates(tensor_dict):
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_confidences
       fields.InputDataFields.groundtruth_keypoints
       fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
@@ -470,6 +476,36 @@ def filter_groundtruth_with_nan_box_coordinates(tensor_dict):
   return retain_groundtruth(tensor_dict, valid_indices)
 
 
+def filter_unrecognized_classes(tensor_dict):
+  """Filters out class labels that are not unrecognized by the labelmap.
+
+  Decoder would parse unrecognized classes (not included in the labelmap) to
+  a label of value -1. Such targets are unecessary for training, and causes
+  issue for evaluation, due to labeling mapping logic. This function filters
+  those labels out for both training and evaluation.
+
+  Args:
+    tensor_dict: dictionary containing input tensors keyed by
+      fields.InputDataFields.
+
+  Returns:
+    A dictionary keyed by fields.InputDataFields containing the tensors
+    obtained after applying the filtering.
+
+  Raises:
+    ValueError: If groundtruth_classes tensor is not in tensor_dict.
+  """
+  if fields.InputDataFields.groundtruth_classes not in tensor_dict:
+    raise ValueError('`groundtruth classes` not in tensor_dict.')
+  # Refer to tf_example_decoder for how unrecognized labels are handled.
+  unrecognized_label = -1
+  recognized_indices = tf.where(
+      tf.greater(tensor_dict[fields.InputDataFields.groundtruth_classes],
+                 unrecognized_label))
+
+  return retain_groundtruth(tensor_dict, recognized_indices)
+
+
 def normalize_to_target(inputs,
                         target_norm_value,
                         dim,
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index 5c03b619..67b5a37d 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -401,6 +401,7 @@ class GroundtruthFilterTest(tf.test.TestCase):
     input_area = tf.placeholder(tf.float32, shape=(None,))
     input_difficult = tf.placeholder(tf.float32, shape=(None,))
     input_label_types = tf.placeholder(tf.string, shape=(None,))
+    input_confidences = tf.placeholder(tf.float32, shape=(None,))
     valid_indices = tf.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.image: input_image,
@@ -409,7 +410,8 @@ class GroundtruthFilterTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
         fields.InputDataFields.groundtruth_area: input_area,
         fields.InputDataFields.groundtruth_difficult: input_difficult,
-        fields.InputDataFields.groundtruth_label_types: input_label_types
+        fields.InputDataFields.groundtruth_label_types: input_label_types,
+        fields.InputDataFields.groundtruth_confidences: input_confidences,
     }
     output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
 
@@ -418,40 +420,31 @@ class GroundtruthFilterTest(tf.test.TestCase):
         input_image: image_tensor,
         input_boxes:
         np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),
-        input_classes:
-        np.array([1, 2], dtype=np.int32),
-        input_is_crowd:
-        np.array([False, True], dtype=np.bool),
-        input_area:
-        np.array([32, 48], dtype=np.float32),
-        input_difficult:
-        np.array([True, False], dtype=np.bool),
+        input_classes: np.array([1, 2], dtype=np.int32),
+        input_is_crowd: np.array([False, True], dtype=np.bool),
+        input_area: np.array([32, 48], dtype=np.float32),
+        input_difficult: np.array([True, False], dtype=np.bool),
         input_label_types:
         np.array(['APPROPRIATE', 'INCORRECT'], dtype=np.string_),
-        valid_indices:
-        np.array([0], dtype=np.int32)
+        input_confidences: np.array([0.99, 0.5], dtype=np.float32),
+        valid_indices: np.array([0], dtype=np.int32),
     }
     expected_tensors = {
-        fields.InputDataFields.image:
-        image_tensor,
-        fields.InputDataFields.groundtruth_boxes:
-        [[0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes:
-        [1],
-        fields.InputDataFields.groundtruth_is_crowd:
-        [False],
-        fields.InputDataFields.groundtruth_area:
-        [32],
-        fields.InputDataFields.groundtruth_difficult:
-        [True],
-        fields.InputDataFields.groundtruth_label_types:
-        ['APPROPRIATE']
+        fields.InputDataFields.image: image_tensor,
+        fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
+        fields.InputDataFields.groundtruth_classes: [1],
+        fields.InputDataFields.groundtruth_is_crowd: [False],
+        fields.InputDataFields.groundtruth_area: [32],
+        fields.InputDataFields.groundtruth_difficult: [True],
+        fields.InputDataFields.groundtruth_label_types: ['APPROPRIATE'],
+        fields.InputDataFields.groundtruth_confidences: [0.99],
     }
     with self.test_session() as sess:
       output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
       for key in [fields.InputDataFields.image,
                   fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area]:
+                  fields.InputDataFields.groundtruth_area,
+                  fields.InputDataFields.groundtruth_confidences]:
         self.assertAllClose(expected_tensors[key], output_tensors[key])
       for key in [fields.InputDataFields.groundtruth_classes,
                   fields.InputDataFields.groundtruth_is_crowd,
@@ -496,46 +489,41 @@ class GroundtruthFilterTest(tf.test.TestCase):
     input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
     input_area = tf.placeholder(tf.float32, shape=(None,))
     input_difficult = tf.placeholder(tf.float32, shape=(None,))
+    input_confidences = tf.placeholder(tf.float32, shape=(None,))
     valid_indices = tf.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.groundtruth_boxes: input_boxes,
         fields.InputDataFields.groundtruth_classes: input_classes,
         fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
         fields.InputDataFields.groundtruth_area: input_area,
-        fields.InputDataFields.groundtruth_difficult: input_difficult
+        fields.InputDataFields.groundtruth_difficult: input_difficult,
+        fields.InputDataFields.groundtruth_confidences: input_confidences,
     }
     output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
 
     feed_dict = {
         input_boxes:
         np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),
-        input_classes:
-        np.array([1, 2], dtype=np.int32),
-        input_is_crowd:
-        np.array([False, True], dtype=np.bool),
-        input_area:
-        np.array([], dtype=np.float32),
-        input_difficult:
-        np.array([], dtype=np.float32),
-        valid_indices:
-        np.array([0], dtype=np.int32)
+        input_classes: np.array([1, 2], dtype=np.int32),
+        input_is_crowd: np.array([False, True], dtype=np.bool),
+        input_area: np.array([], dtype=np.float32),
+        input_difficult: np.array([], dtype=np.float32),
+        input_confidences: np.array([0.99, 0.5], dtype=np.float32),
+        valid_indices: np.array([0], dtype=np.int32)
     }
     expected_tensors = {
-        fields.InputDataFields.groundtruth_boxes:
-        [[0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes:
-        [1],
-        fields.InputDataFields.groundtruth_is_crowd:
-        [False],
-        fields.InputDataFields.groundtruth_area:
-        [],
-        fields.InputDataFields.groundtruth_difficult:
-        []
+        fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
+        fields.InputDataFields.groundtruth_classes: [1],
+        fields.InputDataFields.groundtruth_is_crowd: [False],
+        fields.InputDataFields.groundtruth_area: [],
+        fields.InputDataFields.groundtruth_difficult: [],
+        fields.InputDataFields.groundtruth_confidences: [0.99],
     }
     with self.test_session() as sess:
       output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
       for key in [fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area]:
+                  fields.InputDataFields.groundtruth_area,
+                  fields.InputDataFields.groundtruth_confidences]:
         self.assertAllClose(expected_tensors[key], output_tensors[key])
       for key in [fields.InputDataFields.groundtruth_classes,
                   fields.InputDataFields.groundtruth_is_crowd]:
@@ -547,29 +535,26 @@ class GroundtruthFilterTest(tf.test.TestCase):
     input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
     input_area = tf.placeholder(tf.float32, shape=(None,))
     input_difficult = tf.placeholder(tf.float32, shape=(None,))
+    input_confidences = tf.placeholder(tf.float32, shape=(None,))
     valid_indices = tf.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.groundtruth_boxes: input_boxes,
         fields.InputDataFields.groundtruth_classes: input_classes,
         fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
         fields.InputDataFields.groundtruth_area: input_area,
-        fields.InputDataFields.groundtruth_difficult: input_difficult
+        fields.InputDataFields.groundtruth_difficult: input_difficult,
+        fields.InputDataFields.groundtruth_confidences: input_confidences,
     }
     output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
 
     feed_dict = {
-        input_boxes:
-        np.array([], dtype=np.float).reshape(0, 4),
-        input_classes:
-        np.array([], dtype=np.int32),
-        input_is_crowd:
-        np.array([], dtype=np.bool),
-        input_area:
-        np.array([], dtype=np.float32),
-        input_difficult:
-        np.array([], dtype=np.float32),
-        valid_indices:
-        np.array([], dtype=np.int32)
+        input_boxes: np.array([], dtype=np.float).reshape(0, 4),
+        input_classes: np.array([], dtype=np.int32),
+        input_is_crowd: np.array([], dtype=np.bool),
+        input_area: np.array([], dtype=np.float32),
+        input_difficult: np.array([], dtype=np.float32),
+        input_confidences: np.array([], dtype=np.float32),
+        valid_indices: np.array([], dtype=np.int32),
     }
     with self.test_session() as sess:
       output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
@@ -590,6 +575,7 @@ class RetainGroundTruthWithPositiveClasses(tf.test.TestCase):
     input_area = tf.placeholder(tf.float32, shape=(None,))
     input_difficult = tf.placeholder(tf.float32, shape=(None,))
     input_label_types = tf.placeholder(tf.string, shape=(None,))
+    input_confidences = tf.placeholder(tf.float32, shape=(None,))
     valid_indices = tf.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.image: input_image,
@@ -598,7 +584,8 @@ class RetainGroundTruthWithPositiveClasses(tf.test.TestCase):
         fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
         fields.InputDataFields.groundtruth_area: input_area,
         fields.InputDataFields.groundtruth_difficult: input_difficult,
-        fields.InputDataFields.groundtruth_label_types: input_label_types
+        fields.InputDataFields.groundtruth_label_types: input_label_types,
+        fields.InputDataFields.groundtruth_confidences: input_confidences,
     }
     output_tensors = ops.retain_groundtruth_with_positive_classes(input_tensors)
 
@@ -607,40 +594,31 @@ class RetainGroundTruthWithPositiveClasses(tf.test.TestCase):
         input_image: image_tensor,
         input_boxes:
         np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),
-        input_classes:
-        np.array([1, 0], dtype=np.int32),
-        input_is_crowd:
-        np.array([False, True], dtype=np.bool),
-        input_area:
-        np.array([32, 48], dtype=np.float32),
-        input_difficult:
-        np.array([True, False], dtype=np.bool),
+        input_classes: np.array([1, 0], dtype=np.int32),
+        input_is_crowd: np.array([False, True], dtype=np.bool),
+        input_area: np.array([32, 48], dtype=np.float32),
+        input_difficult: np.array([True, False], dtype=np.bool),
         input_label_types:
         np.array(['APPROPRIATE', 'INCORRECT'], dtype=np.string_),
-        valid_indices:
-        np.array([0], dtype=np.int32)
+        input_confidences: np.array([0.99, 0.5], dtype=np.float32),
+        valid_indices: np.array([0], dtype=np.int32),
     }
     expected_tensors = {
-        fields.InputDataFields.image:
-        image_tensor,
-        fields.InputDataFields.groundtruth_boxes:
-        [[0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes:
-        [1],
-        fields.InputDataFields.groundtruth_is_crowd:
-        [False],
-        fields.InputDataFields.groundtruth_area:
-        [32],
-        fields.InputDataFields.groundtruth_difficult:
-        [True],
-        fields.InputDataFields.groundtruth_label_types:
-        ['APPROPRIATE']
+        fields.InputDataFields.image: image_tensor,
+        fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
+        fields.InputDataFields.groundtruth_classes: [1],
+        fields.InputDataFields.groundtruth_is_crowd: [False],
+        fields.InputDataFields.groundtruth_area: [32],
+        fields.InputDataFields.groundtruth_difficult: [True],
+        fields.InputDataFields.groundtruth_label_types: ['APPROPRIATE'],
+        fields.InputDataFields.groundtruth_confidences: [0.99],
     }
     with self.test_session() as sess:
       output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
       for key in [fields.InputDataFields.image,
                   fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area]:
+                  fields.InputDataFields.groundtruth_area,
+                  fields.InputDataFields.groundtruth_confidences]:
         self.assertAllClose(expected_tensors[key], output_tensors[key])
       for key in [fields.InputDataFields.groundtruth_classes,
                   fields.InputDataFields.groundtruth_is_crowd,
@@ -675,23 +653,18 @@ class GroundtruthFilterWithCrowdBoxesTest(tf.test.TestCase):
     input_tensors = {
         fields.InputDataFields.groundtruth_boxes:
         [[0.1, 0.2, 0.6, 0.8], [0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes:
-        [1, 2],
-        fields.InputDataFields.groundtruth_is_crowd:
-        [True, False],
-        fields.InputDataFields.groundtruth_area:
-        [100.0, 238.7]
+        fields.InputDataFields.groundtruth_classes: [1, 2],
+        fields.InputDataFields.groundtruth_is_crowd: [True, False],
+        fields.InputDataFields.groundtruth_area: [100.0, 238.7],
+        fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
     }
 
     expected_tensors = {
-        fields.InputDataFields.groundtruth_boxes:
-        [[0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes:
-        [2],
-        fields.InputDataFields.groundtruth_is_crowd:
-        [False],
-        fields.InputDataFields.groundtruth_area:
-        [238.7]
+        fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
+        fields.InputDataFields.groundtruth_classes: [2],
+        fields.InputDataFields.groundtruth_is_crowd: [False],
+        fields.InputDataFields.groundtruth_area: [238.7],
+        fields.InputDataFields.groundtruth_confidences: [0.99],
     }
 
     output_tensors = ops.filter_groundtruth_with_crowd_boxes(
@@ -699,7 +672,8 @@ class GroundtruthFilterWithCrowdBoxesTest(tf.test.TestCase):
     with self.test_session() as sess:
       output_tensors = sess.run(output_tensors)
       for key in [fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area]:
+                  fields.InputDataFields.groundtruth_area,
+                  fields.InputDataFields.groundtruth_confidences]:
         self.assertAllClose(expected_tensors[key], output_tensors[key])
       for key in [fields.InputDataFields.groundtruth_classes,
                   fields.InputDataFields.groundtruth_is_crowd]:
@@ -712,23 +686,18 @@ class GroundtruthFilterWithNanBoxTest(tf.test.TestCase):
     input_tensors = {
         fields.InputDataFields.groundtruth_boxes:
         [[np.nan, np.nan, np.nan, np.nan], [0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes:
-        [1, 2],
-        fields.InputDataFields.groundtruth_is_crowd:
-        [False, True],
-        fields.InputDataFields.groundtruth_area:
-        [100.0, 238.7]
+        fields.InputDataFields.groundtruth_classes: [1, 2],
+        fields.InputDataFields.groundtruth_is_crowd: [False, True],
+        fields.InputDataFields.groundtruth_area: [100.0, 238.7],
+        fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
     }
 
     expected_tensors = {
-        fields.InputDataFields.groundtruth_boxes:
-        [[0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes:
-        [2],
-        fields.InputDataFields.groundtruth_is_crowd:
-        [True],
-        fields.InputDataFields.groundtruth_area:
-        [238.7]
+        fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
+        fields.InputDataFields.groundtruth_classes: [2],
+        fields.InputDataFields.groundtruth_is_crowd: [True],
+        fields.InputDataFields.groundtruth_area: [238.7],
+        fields.InputDataFields.groundtruth_confidences: [0.99],
     }
 
     output_tensors = ops.filter_groundtruth_with_nan_box_coordinates(
@@ -736,7 +705,40 @@ class GroundtruthFilterWithNanBoxTest(tf.test.TestCase):
     with self.test_session() as sess:
       output_tensors = sess.run(output_tensors)
       for key in [fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area]:
+                  fields.InputDataFields.groundtruth_area,
+                  fields.InputDataFields.groundtruth_confidences]:
+        self.assertAllClose(expected_tensors[key], output_tensors[key])
+      for key in [fields.InputDataFields.groundtruth_classes,
+                  fields.InputDataFields.groundtruth_is_crowd]:
+        self.assertAllEqual(expected_tensors[key], output_tensors[key])
+
+
+class GroundtruthFilterWithUnrecognizedClassesTest(tf.test.TestCase):
+
+  def test_filter_unrecognized_classes(self):
+    input_tensors = {
+        fields.InputDataFields.groundtruth_boxes:
+        [[.3, .3, .5, .7], [0.2, 0.4, 0.1, 0.8]],
+        fields.InputDataFields.groundtruth_classes: [-1, 2],
+        fields.InputDataFields.groundtruth_is_crowd: [False, True],
+        fields.InputDataFields.groundtruth_area: [100.0, 238.7],
+        fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
+    }
+
+    expected_tensors = {
+        fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
+        fields.InputDataFields.groundtruth_classes: [2],
+        fields.InputDataFields.groundtruth_is_crowd: [True],
+        fields.InputDataFields.groundtruth_area: [238.7],
+        fields.InputDataFields.groundtruth_confidences: [0.99],
+    }
+
+    output_tensors = ops.filter_unrecognized_classes(input_tensors)
+    with self.test_session() as sess:
+      output_tensors = sess.run(output_tensors)
+      for key in [fields.InputDataFields.groundtruth_boxes,
+                  fields.InputDataFields.groundtruth_area,
+                  fields.InputDataFields.groundtruth_confidences]:
         self.assertAllClose(expected_tensors[key], output_tensors[key])
       for key in [fields.InputDataFields.groundtruth_classes,
                   fields.InputDataFields.groundtruth_is_crowd]:
diff --git a/research/object_detection/utils/variables_helper.py b/research/object_detection/utils/variables_helper.py
index 141228cd..82d2892a 100644
--- a/research/object_detection/utils/variables_helper.py
+++ b/research/object_detection/utils/variables_helper.py
@@ -20,6 +20,8 @@ import re
 
 import tensorflow as tf
 
+from tensorflow.python.ops import variables as tf_variables
+
 slim = tf.contrib.slim
 
 
@@ -118,7 +120,13 @@ def get_variables_available_in_checkpoint(variables,
     ValueError: if `variables` is not a list or dict.
   """
   if isinstance(variables, list):
-    variable_names_map = {variable.op.name: variable for variable in variables}
+    variable_names_map = {}
+    for variable in variables:
+      if isinstance(variable, tf_variables.PartitionedVariable):
+        name = variable.name
+      else:
+        name = variable.op.name
+      variable_names_map[name] = variable
   elif isinstance(variables, dict):
     variable_names_map = variables
   else:
diff --git a/research/object_detection/utils/variables_helper_test.py b/research/object_detection/utils/variables_helper_test.py
index 94585d71..efbe7e3d 100644
--- a/research/object_detection/utils/variables_helper_test.py
+++ b/research/object_detection/utils/variables_helper_test.py
@@ -144,6 +144,24 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
           variables, checkpoint_path)
     self.assertItemsEqual(out_variables, variables)
 
+  def test_return_all_variables_from_checkpoint_with_partition(self):
+    with tf.Graph().as_default():
+      partitioner = tf.fixed_size_partitioner(2)
+      variables = [
+          tf.get_variable(
+              name='weights', shape=(2, 2), partitioner=partitioner),
+          tf.Variable([1.0, 2.0], name='biases')
+      ]
+      checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
+      init_op = tf.global_variables_initializer()
+      saver = tf.train.Saver(variables)
+      with self.test_session() as sess:
+        sess.run(init_op)
+        saver.save(sess, checkpoint_path)
+      out_variables = variables_helper.get_variables_available_in_checkpoint(
+          variables, checkpoint_path)
+    self.assertItemsEqual(out_variables, variables)
+
   def test_return_variables_available_in_checkpoint(self):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
     with tf.Graph().as_default():
