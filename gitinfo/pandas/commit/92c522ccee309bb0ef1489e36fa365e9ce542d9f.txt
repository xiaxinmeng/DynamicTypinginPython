commit 92c522ccee309bb0ef1489e36fa365e9ce542d9f
Author: Adam Klein <adamklein@gmail.com>
Date:   Thu Feb 16 18:06:18 2012 -0500

    added scikits.timeseries python code to codebase

diff --git a/pandas/timeseries/__init__.py b/pandas/timeseries/__init__.py
new file mode 100644
index 000000000..b5c468034
--- /dev/null
+++ b/pandas/timeseries/__init__.py
@@ -0,0 +1,32 @@
+"""TimeSeries
+
+:author: Pierre GF Gerard-Marchant & Matt Knox
+:contact: pierregm_at_uga_dot_edu - mattknox_ca_at_hotmail_dot_com
+:version: $Id$
+"""
+
+
+__author__ = "Pierre GF Gerard-Marchant  & Matt Knox ($Author$)"
+__revision__ = "$Revision$"
+__date__     = '$Date$'
+
+import const
+import tdates
+from tdates import *
+import tseries
+from tseries import *
+import trecords
+from trecords import *
+_c = const
+from extras import tsfromtxt, guess_freq
+
+from scikits.timeseries.version import __version__
+
+__all__ = [
+    '_c', 'const', 'tdates','tseries','trecords', 'tsfromtxt', 'guess_freq']
+__all__.extend(tdates.__all__)
+__all__.extend(tseries.__all__)
+__all__.extend(trecords.__all__)
+
+from numpy.testing import Tester
+test = Tester().test
diff --git a/pandas/timeseries/_preview.py b/pandas/timeseries/_preview.py
new file mode 100644
index 000000000..7647b7146
--- /dev/null
+++ b/pandas/timeseries/_preview.py
@@ -0,0 +1,1387 @@
+"""
+This file contains local copies of functions to be released in numpy 1.4.0.
+Once we drop support for numpy 1.3.x we can eliminate this file.
+"""
+
+__all__ = ['genfromtxt']
+
+import itertools
+import warnings
+from operator import itemgetter
+from __builtin__ import bool, int, long, float, complex, object, unicode, str
+
+import numpy as np
+import numpy.core.numeric as nx
+
+#####--------------------------------------------------------------------------
+#---- numpy.lib._iotools functions ---
+#####--------------------------------------------------------------------------
+
+def _is_string_like(obj):
+    """
+    Check whether obj behaves like a string.
+    """
+    try:
+        obj + ''
+    except (TypeError, ValueError):
+        return False
+    return True
+
+
+def _to_filehandle(fname, flag='r', return_opened=False):
+    """
+    Returns the filehandle corresponding to a string or a file.
+    If the string ends in '.gz', the file is automatically unzipped.
+
+    Parameters
+    ----------
+    fname : string, filehandle
+        Name of the file whose filehandle must be returned.
+    flag : string, optional
+        Flag indicating the status of the file ('r' for read, 'w' for write).
+    return_opened : boolean, optional
+        Whether to return the opening status of the file.
+    """
+    if _is_string_like(fname):
+        if fname.endswith('.gz'):
+            import gzip
+            fhd = gzip.open(fname, flag)
+        elif fname.endswith('.bz2'):
+            import bz2
+            fhd = bz2.BZ2File(fname)
+        else:
+            fhd = file(fname, flag)
+        opened = True
+    elif hasattr(fname, 'seek'):
+        fhd = fname
+        opened = False
+    else:
+        raise ValueError('fname must be a string or file handle')
+    if return_opened:
+        return fhd, opened
+    return fhd
+
+
+def has_nested_fields(ndtype):
+    """
+    Returns whether one or several fields of a dtype are nested.
+
+    Parameters
+    ----------
+    ndtype : dtype
+        Data-type of a structured array.
+
+    Raises
+    ------
+    AttributeError : If `ndtype` does not have a `names` attribute.
+
+    Examples
+    --------
+    >>> dt = np.dtype([('name', 'S4'), ('x', float), ('y', float)])
+    >>> np.lib._iotools.has_nested_fields(dt)
+    False
+
+    """
+    for name in ndtype.names or ():
+        if ndtype[name].names:
+            return True
+    return False
+
+
+def flatten_dtype(ndtype, flatten_base=False):
+    """
+    Unpack a structured data-type by collapsing nested fields and/or fields
+    with a shape.
+
+    Note that the field names are lost.
+
+    Parameters
+    ----------
+    ndtype : dtype
+        The datatype to collapse
+    flatten_base : {False, True}, optional
+        Whether to transform a field with a shape into several fields or not.
+
+    Examples
+    --------
+    >>> dt = np.dtype([('name', 'S4'), ('x', float), ('y', float),
+    ...                ('block', int, (2, 3))])
+    >>> np.lib._iotools.flatten_dtype(dt)
+    [dtype('|S4'), dtype('float64'), dtype('float64'), dtype('int32')]
+    >>> np.lib._iotools.flatten_dtype(dt, flatten_base=True)
+    [dtype('|S4'), dtype('float64'), dtype('float64'), dtype('int32'),
+     dtype('int32'), dtype('int32'), dtype('int32'), dtype('int32'),
+     dtype('int32')]
+
+    """
+    names = ndtype.names
+    if names is None:
+        if flatten_base:
+            return [ndtype.base] * int(np.prod(ndtype.shape))
+        return [ndtype.base]
+    else:
+        types = []
+        for field in names:
+            (typ, _) = ndtype.fields[field]
+            flat_dt = flatten_dtype(typ, flatten_base)
+            types.extend(flat_dt)
+        return types
+
+
+
+
+
+
+class LineSplitter:
+    """
+    Object to split a string at a given delimiter or at given places.
+
+    Parameters
+    ----------
+    delimiter : str, int, or sequence of ints, optional
+        If a string, character used to delimit consecutive fields.
+        If an integer or a sequence of integers, width(s) of each field.
+    comment : str, optional
+        Character used to mark the beginning of a comment. Default is '#'.
+    autostrip : bool, optional
+        Whether to strip each individual field. Default is True.
+
+    """
+
+    def autostrip(self, method):
+        """
+        Wrapper to strip each member of the output of `method`.
+
+        Parameters
+        ----------
+        method : function
+            Function that takes a single argument and returns a sequence of
+            strings.
+
+        Returns
+        -------
+        wrapped : function
+            The result of wrapping `method`. `wrapped` takes a single input
+            argument and returns a list of strings that are stripped of
+            white-space.
+
+        """
+        return lambda input: [_.strip() for _ in method(input)]
+    #
+    def __init__(self, delimiter=None, comments='#', autostrip=True):
+        self.comments = comments
+        # Delimiter is a character
+        if (delimiter is None) or _is_string_like(delimiter):
+            delimiter = delimiter or None
+            _handyman = self._delimited_splitter
+        # Delimiter is a list of field widths
+        elif hasattr(delimiter, '__iter__'):
+            _handyman = self._variablewidth_splitter
+            idx = np.cumsum([0] + list(delimiter))
+            delimiter = [slice(i, j) for (i, j) in zip(idx[:-1], idx[1:])]
+        # Delimiter is a single integer
+        elif int(delimiter):
+            (_handyman, delimiter) = (self._fixedwidth_splitter, int(delimiter))
+        else:
+            (_handyman, delimiter) = (self._delimited_splitter, None)
+        self.delimiter = delimiter
+        if autostrip:
+            self._handyman = self.autostrip(_handyman)
+        else:
+            self._handyman = _handyman
+    #
+    def _delimited_splitter(self, line):
+        line = line.split(self.comments)[0].strip()
+        if not line:
+            return []
+        return line.split(self.delimiter)
+    #
+    def _fixedwidth_splitter(self, line):
+        line = line.split(self.comments)[0]
+        if not line:
+            return []
+        fixed = self.delimiter
+        slices = [slice(i, i + fixed) for i in range(len(line))[::fixed]]
+        return [line[s] for s in slices]
+    #
+    def _variablewidth_splitter(self, line):
+        line = line.split(self.comments)[0]
+        if not line:
+            return []
+        slices = self.delimiter
+        return [line[s] for s in slices]
+    #
+    def __call__(self, line):
+        return self._handyman(line)
+
+
+
+class NameValidator:
+    """
+    Object to validate a list of strings to use as field names.
+
+    The strings are stripped of any non alphanumeric character, and spaces
+    are replaced by '_'. During instantiation, the user can define a list of
+    names to exclude, as well as a list of invalid characters. Names in the
+    exclusion list are appended a '_' character.
+
+    Once an instance has been created, it can be called with a list of names,
+    and a list of valid names will be created.
+    The `__call__` method accepts an optional keyword "default" that sets
+    the default name in case of ambiguity. By default this is 'f', so
+    that names will default to `f0`, `f1`, etc.
+
+    Parameters
+    ----------
+    excludelist : sequence, optional
+        A list of names to exclude. This list is appended to the default list
+        ['return', 'file', 'print']. Excluded names are appended an underscore:
+        for example, `file` becomes `file_` if supplied.
+    deletechars : str, optional
+        A string combining invalid characters that must be deleted from the
+        names.
+    casesensitive : {True, False, 'upper', 'lower'}, optional
+        * If True, field names are case-sensitive.
+        * If False or 'upper', field names are converted to upper case.
+        * If 'lower', field names are converted to lower case.
+
+        The default value is True.
+
+    Notes
+    -----
+    Calling an instance of `NameValidator` is the same as calling its method
+    `validate`.
+
+    Examples
+    --------
+    >>> validator = np.lib._iotools.NameValidator()
+    >>> validator(['file', 'field2', 'with space', 'CaSe'])
+    ['file_', 'field2', 'with_space', 'CaSe']
+
+    >>> validator = np.lib._iotools.NameValidator(excludelist=['excl'],
+                                                  deletechars='q',
+                                                  case_sensitive='False')
+    >>> validator(['excl', 'field2', 'no_q', 'with space', 'CaSe'])
+    ['excl_', 'field2', 'no_', 'with_space', 'case']
+
+    """
+    #
+    defaultexcludelist = ['return', 'file', 'print']
+    defaultdeletechars = set("""~!@#$%^&*()-=+~\|]}[{';: /?.>,<""")
+    #
+    def __init__(self, excludelist=None, deletechars=None, case_sensitive=None):
+        # Process the exclusion list ..
+        if excludelist is None:
+            excludelist = []
+        excludelist.extend(self.defaultexcludelist)
+        self.excludelist = excludelist
+        # Process the list of characters to delete
+        if deletechars is None:
+            delete = self.defaultdeletechars
+        else:
+            delete = set(deletechars)
+        delete.add('"')
+        self.deletechars = delete
+        # Process the case option .....
+        if (case_sensitive is None) or (case_sensitive is True):
+            self.case_converter = lambda x: x
+        elif (case_sensitive is False) or ('u' in case_sensitive):
+            self.case_converter = lambda x: x.upper()
+        elif 'l' in case_sensitive:
+            self.case_converter = lambda x: x.lower()
+        else:
+            self.case_converter = lambda x: x
+
+    def validate(self, names, defaultfmt="f%i", nbfields=None):
+        """
+        Validate a list of strings to use as field names for a structured array.
+
+        Parameters
+        ----------
+        names : sequence of str
+            Strings to be validated.
+        defaultfmt : str, optional
+            Default format string, used if validating a given string reduces its
+            length to zero.
+        nboutput : integer, optional
+            Final number of validated names, used to expand or shrink the initial
+            list of names.
+
+        Returns
+        -------
+        validatednames : list of str
+            The list of validated field names.
+
+        Notes
+        -----
+        A `NameValidator` instance can be called directly, which is the same as
+        calling `validate`. For examples, see `NameValidator`.
+
+        """
+        # Initial checks ..............
+        if (names is None):
+            if (nbfields is None):
+                return None
+            names = []
+        if isinstance(names, basestring):
+            names = [names, ]
+        if nbfields is not None:
+            nbnames = len(names)
+            if (nbnames < nbfields):
+                names = list(names) + [''] * (nbfields - nbnames)
+            elif (nbnames > nbfields):
+                names = names[:nbfields]
+        # Set some shortcuts ...........
+        deletechars = self.deletechars
+        excludelist = self.excludelist
+        case_converter = self.case_converter
+        # Initializes some variables ...
+        validatednames = []
+        seen = dict()
+        nbempty = 0
+        #
+        for item in names:
+            item = case_converter(item)
+            item = item.strip().replace(' ', '_')
+            item = ''.join([c for c in item if c not in deletechars])
+            if item == '':
+                item = defaultfmt % nbempty
+                while item in names:
+                    nbempty += 1
+                    item = defaultfmt % nbempty
+                nbempty += 1
+            elif item in excludelist:
+                item += '_'
+            cnt = seen.get(item, 0)
+            if cnt > 0:
+                validatednames.append(item + '_%d' % cnt)
+            else:
+                validatednames.append(item)
+            seen[item] = cnt + 1
+        return tuple(validatednames)
+    #
+    def __call__(self, names, defaultfmt="f%i", nbfields=None):
+        return self.validate(names, defaultfmt=defaultfmt, nbfields=nbfields)
+
+
+
+def str2bool(value):
+    """
+    Tries to transform a string supposed to represent a boolean to a boolean.
+
+    Parameters
+    ----------
+    value : str
+        The string that is transformed to a boolean.
+
+    Returns
+    -------
+    boolval : bool
+        The boolean representation of `value`.
+
+    Raises
+    ------
+    ValueError
+        If the string is not 'True' or 'False' (case independent)
+
+    Examples
+    --------
+    >>> np.lib._iotools.str2bool('TRUE')
+    True
+    >>> np.lib._iotools.str2bool('false')
+    False
+
+    """
+    value = value.upper()
+    if value == 'TRUE':
+        return True
+    elif value == 'FALSE':
+        return False
+    else:
+        raise ValueError("Invalid boolean")
+
+
+class ConverterError(Exception):
+    """
+    Exception raised when an error occurs in a converter for string values.
+
+    """
+    pass
+
+class ConverterLockError(ConverterError):
+    """
+    Exception raised when an attempt is made to upgrade a locked converter.
+
+    """
+    pass
+
+class ConversionWarning(UserWarning):
+    """
+    Warning issued when a string converter has a problem.
+
+    Notes
+    -----
+    In `genfromtxt` a `ConversionWarning` is issued if raising exceptions
+    is explicitly suppressed with the "invalid_raise" keyword.
+
+    """
+    pass
+
+
+
+class StringConverter:
+    """
+    Factory class for function transforming a string into another object (int,
+    float).
+
+    After initialization, an instance can be called to transform a string
+    into another object. If the string is recognized as representing a missing
+    value, a default value is returned.
+
+    Attributes
+    ----------
+    func : function
+        Function used for the conversion.
+    default : any
+        Default value to return when the input corresponds to a missing value.
+    type : type
+        Type of the output.
+    _status : int
+        Integer representing the order of the conversion.
+    _mapper : sequence of tuples
+        Sequence of tuples (dtype, function, default value) to evaluate in
+        order.
+    _locked : bool
+        Holds `locked` parameter.
+
+    Parameters
+    ----------
+    dtype_or_func : {None, dtype, function}, optional
+        If a `dtype`, specifies the input data type, used to define a basic
+        function and a default value for missing data. For example, when
+        `dtype` is float, the `func` attribute is set to `float` and the
+        default value to `np.nan`.
+        If a function, this function is used to convert a string to another
+        object. In this case, it is recommended to give an associated default
+        value as input.
+    default : any, optional
+        Value to return by default, that is, when the string to be converted
+        is flagged as missing. If not given, `StringConverter` tries to supply
+        a reasonable default value.
+    missing_values : sequence of str, optional
+        Sequence of strings indicating a missing value.
+    locked : bool, optional
+        Whether the StringConverter should be locked to prevent automatic
+        upgrade or not. Default is False.
+
+    """
+    #
+    _mapper = [(nx.bool_, str2bool, False),
+               (nx.integer, int, -1),
+               (nx.floating, float, nx.nan),
+               (complex, complex, nx.nan + 0j),
+               (nx.string_, str, '???')]
+    (_defaulttype, _defaultfunc, _defaultfill) = zip(*_mapper)
+    #
+    @classmethod
+    def _getsubdtype(cls, val):
+        """Returns the type of the dtype of the input variable."""
+        return np.array(val).dtype.type
+    #
+    @classmethod
+    def upgrade_mapper(cls, func, default=None):
+        """
+    Upgrade the mapper of a StringConverter by adding a new function and its
+    corresponding default.
+
+    The input function (or sequence of functions) and its associated default
+    value (if any) is inserted in penultimate position of the mapper.
+    The corresponding type is estimated from the dtype of the default value.
+
+    Parameters
+    ----------
+    func : var
+        Function, or sequence of functions
+
+    Examples
+    --------
+    >>> import dateutil.parser
+    >>> import datetime
+    >>> dateparser = datetustil.parser.parse
+    >>> defaultdate = datetime.date(2000, 1, 1)
+    >>> StringConverter.upgrade_mapper(dateparser, default=defaultdate)
+        """
+        # Func is a single functions
+        if hasattr(func, '__call__'):
+            cls._mapper.insert(-1, (cls._getsubdtype(default), func, default))
+            return
+        elif hasattr(func, '__iter__'):
+            if isinstance(func[0], (tuple, list)):
+                for _ in func:
+                    cls._mapper.insert(-1, _)
+                return
+            if default is None:
+                default = [None] * len(func)
+            else:
+                default = list(default)
+                default.append([None] * (len(func) - len(default)))
+            for (fct, dft) in zip(func, default):
+                cls._mapper.insert(-1, (cls._getsubdtype(dft), fct, dft))
+    #
+    def __init__(self, dtype_or_func=None, default=None, missing_values=None,
+                 locked=False):
+        # Defines a lock for upgrade
+        self._locked = bool(locked)
+        # No input dtype: minimal initialization
+        if dtype_or_func is None:
+            self.func = str2bool
+            self._status = 0
+            self.default = default or False
+            ttype = np.bool
+        else:
+            # Is the input a np.dtype ?
+            try:
+                self.func = None
+                ttype = np.dtype(dtype_or_func).type
+            except TypeError:
+                # dtype_or_func must be a function, then
+                if not hasattr(dtype_or_func, '__call__'):
+                    errmsg = "The input argument `dtype` is neither a function"\
+                             " or a dtype (got '%s' instead)"
+                    raise TypeError(errmsg % type(dtype_or_func))
+                # Set the function
+                self.func = dtype_or_func
+                # If we don't have a default, try to guess it or set it to None
+                if default is None:
+                    try:
+                        default = self.func('0')
+                    except ValueError:
+                        default = None
+                ttype = self._getsubdtype(default)
+            # Set the status according to the dtype
+            _status = -1
+            for (i, (deftype, func, default_def)) in enumerate(self._mapper):
+                if np.issubdtype(ttype, deftype):
+                    _status = i
+                    if default is None:
+                        self.default = default_def
+                    else:
+                        self.default = default
+                    break
+            if _status == -1:
+                # We never found a match in the _mapper...
+                _status = 0
+                self.default = default
+            self._status = _status
+            # If the input was a dtype, set the function to the last we saw
+            if self.func is None:
+                self.func = func
+            # If the status is 1 (int), change the function to smthg more robust
+            if self.func == self._mapper[1][1]:
+                self.func = lambda x : int(float(x))
+        # Store the list of strings corresponding to missing values.
+        if missing_values is None:
+            self.missing_values = set([''])
+        else:
+            if isinstance(missing_values, basestring):
+                missing_values = missing_values.split(",")
+            self.missing_values = set(list(missing_values) + [''])
+        #
+        self._callingfunction = self._strict_call
+        self.type = ttype
+        self._checked = False
+        self._initial_default = default
+    #
+    def _loose_call(self, value):
+        try:
+            return self.func(value)
+        except ValueError:
+            return self.default
+    #
+    def _strict_call(self, value):
+        try:
+            return self.func(value)
+        except ValueError:
+            if value.strip() in self.missing_values:
+                if not self._status:
+                    self._checked = False
+                return self.default
+            raise ValueError("Cannot convert string '%s'" % value)
+    #
+    def __call__(self, value):
+        return self._callingfunction(value)
+    #
+    def upgrade(self, value):
+        """
+        Try to find the best converter for a given string, and return the result.
+
+        The supplied string `value` is converted by testing different
+        converters in order. First the `func` method of the `StringConverter`
+        instance is tried, if this fails other available converters are tried.
+        The order in which these other converters are tried is determined by the
+        `_status` attribute of the instance.
+
+        Parameters
+        ----------
+        value : str
+            The string to convert.
+
+        Returns
+        -------
+        out : any
+            The result of converting `value` with the appropriate converter.
+
+        """
+        self._checked = True
+        try:
+            self._strict_call(value)
+        except ValueError:
+            # Raise an exception if we locked the converter...
+            if self._locked:
+                errmsg = "Converter is locked and cannot be upgraded"
+                raise ConverterLockError(errmsg)
+            _statusmax = len(self._mapper)
+            # Complains if we try to upgrade by the maximum
+            _status = self._status
+            if _status == _statusmax:
+                errmsg = "Could not find a valid conversion function"
+                raise ConverterError(errmsg)
+            elif _status < _statusmax - 1:
+                _status += 1
+            (self.type, self.func, default) = self._mapper[_status]
+            self._status = _status
+            if self._initial_default is not None:
+                self.default = self._initial_default
+            else:
+                self.default = default
+            self.upgrade(value)
+
+    def iterupgrade(self, value):
+        self._checked = True
+        if not hasattr(value, '__iter__'):
+            value = (value,)
+        _strict_call = self._strict_call
+        try:
+            map(_strict_call, value)
+        except ValueError:
+            # Raise an exception if we locked the converter...
+            if self._locked:
+                errmsg = "Converter is locked and cannot be upgraded"
+                raise ConverterLockError(errmsg)
+            _statusmax = len(self._mapper)
+            # Complains if we try to upgrade by the maximum
+            _status = self._status
+            if _status == _statusmax:
+                raise ConverterError("Could not find a valid conversion function")
+            elif _status < _statusmax - 1:
+                _status += 1
+            (self.type, self.func, default) = self._mapper[_status]
+            if self._initial_default is not None:
+                self.default = self._initial_default
+            else:
+                self.default = default
+            self._status = _status
+            self.iterupgrade(value)
+
+    def update(self, func, default=None, missing_values='', locked=False):
+        """
+        Set StringConverter attributes directly.
+
+        Parameters
+        ----------
+        func : function
+            Conversion function.
+        default : any, optional
+            Value to return by default, that is, when the string to be converted
+            is flagged as missing. If not given, `StringConverter` tries to supply
+            a reasonable default value.
+        missing_values : sequence of str, optional
+            Sequence of strings indicating a missing value.
+        locked : bool, optional
+            Whether the StringConverter should be locked to prevent automatic
+            upgrade or not. Default is False.
+
+        Notes
+        -----
+        `update` takes the same parameters as the constructor of `StringConverter`,
+        except that `func` does not accept a `dtype` whereas `dtype_or_func` in
+        the constructor does.
+
+        """
+        self.func = func
+        self._locked = locked
+        # Don't reset the default to None if we can avoid it
+        if default is not None:
+            self.default = default
+            self.type = self._getsubdtype(default)
+        else:
+            try:
+                tester = func('1')
+            except (TypeError, ValueError):
+                tester = None
+            self.type = self._getsubdtype(tester)
+        # Add the missing values to the existing set
+        if missing_values is not None:
+            if _is_string_like(missing_values):
+                self.missing_values.add(missing_values)
+            elif hasattr(missing_values, '__iter__'):
+                for val in missing_values:
+                    self.missing_values.add(val)
+        else:
+            self.missing_values = []
+
+
+
+def easy_dtype(ndtype, names=None, defaultfmt="f%i", **validationargs):
+    """
+    Convenience function to create a `np.dtype` object.
+
+    The function processes the input `dtype` and matches it with the given
+    names.
+
+    Parameters
+    ----------
+    ndtype : var
+        Definition of the dtype. Can be any string or dictionary
+        recognized by the `np.dtype` function, or a sequence of types.
+    names : str or sequence, optional
+        Sequence of strings to use as field names for a structured dtype.
+        For convenience, `names` can be a string of a comma-separated list of
+        names.
+    defaultfmt : str, optional
+        Format string used to define missing names, such as ``"f%i"``
+        (default) or ``"fields_%02i"``.
+    validationargs : optional
+        A series of optional arguments used to initialize a `NameValidator`.
+
+    Examples
+    --------
+    >>> np.lib._iotools.easy_dtype(float)
+    dtype('float64')
+    >>> np.lib._iotools.easy_dtype("i4, f8")
+    dtype([('f0', '<i4'), ('f1', '<f8')])
+    >>> np.lib._iotools.easy_dtype("i4, f8", defaultfmt="field_%03i")
+    dtype([('field_000', '<i4'), ('field_001', '<f8')])
+
+    >>> np.lib._iotools.easy_dtype((int, float, float), names="a,b,c")
+    dtype([('a', '<i8'), ('b', '<f8'), ('c', '<f8')])
+    >>> np.lib._iotools.easy_dtype(float, names="a,b,c")
+    dtype([('a', '<f8'), ('b', '<f8'), ('c', '<f8')])
+
+    """
+    try:
+        ndtype = np.dtype(ndtype)
+    except TypeError:
+        validate = NameValidator(**validationargs)
+        nbfields = len(ndtype)
+        if names is None:
+            names = [''] * len(ndtype)
+        elif isinstance(names, basestring):
+            names = names.split(",")
+        names = validate(names, nbfields=nbfields, defaultfmt=defaultfmt)
+        ndtype = np.dtype(dict(formats=ndtype, names=names))
+    else:
+        nbtypes = len(ndtype)
+        # Explicit names
+        if names is not None:
+            validate = NameValidator(**validationargs)
+            if isinstance(names, basestring):
+                names = names.split(",")
+            # Simple dtype: repeat to match the nb of names
+            if nbtypes == 0:
+                formats = tuple([ndtype.type] * len(names))
+                names = validate(names, defaultfmt=defaultfmt)
+                ndtype = np.dtype(zip(names, formats))
+            # Structured dtype: just validate the names as needed
+            else:
+                ndtype.names = validate(names, nbfields=nbtypes,
+                                        defaultfmt=defaultfmt)
+        # No implicit names
+        elif (nbtypes > 0):
+            validate = NameValidator(**validationargs)
+            # Default initial names : should we change the format ?
+            if (ndtype.names == tuple("f%i" % i for i in range(nbtypes))) and \
+               (defaultfmt != "f%i"):
+                ndtype.names = validate([''] * nbtypes, defaultfmt=defaultfmt)
+            # Explicit initial names : just validate
+            else:
+                ndtype.names = validate(ndtype.names, defaultfmt=defaultfmt)
+    return ndtype
+
+
+#####--------------------------------------------------------------------------
+#---- numpy.lib.io functions ---
+#####--------------------------------------------------------------------------
+
+_string_like = _is_string_like
+
+def genfromtxt(fname, dtype=float, comments='#', delimiter=None,
+               skiprows=0, skip_header=0, skip_footer=0, converters=None,
+               missing='', missing_values=None, filling_values=None,
+               usecols=None, names=None, excludelist=None, deletechars=None,
+               autostrip=False, case_sensitive=True, defaultfmt="f%i",
+               unpack=None, usemask=False, loose=True, invalid_raise=True):
+    """
+    Load data from a text file, with missing values handled as specified.
+
+    Each line past the first `skiprows` lines is split at the `delimiter`
+    character, and characters following the `comments` character are discarded.
+
+    Parameters
+    ----------
+    fname : file or str
+        File or filename to read.  If the filename extension is `.gz` or
+        `.bz2`, the file is first decompressed.
+    dtype : dtype, optional
+        Data type of the resulting array.
+        If None, the dtypes will be determined by the contents of each
+        column, individually.
+    comments : str, optional
+        The character used to indicate the start of a comment.
+        All the characters occurring on a line after a comment are discarded
+    delimiter : str, int, or sequence, optional
+        The string used to separate values.  By default, any consecutive
+        whitespaces act as delimiter.  An integer or sequence of integers
+        can also be provided as width(s) of each field.
+    skip_header : int, optional
+        The numbers of lines to skip at the beginning of the file.
+    skip_footer : int, optional
+        The numbers of lines to skip at the end of the file
+    converters : variable or None, optional
+        The set of functions that convert the data of a column to a value.
+        The converters can also be used to provide a default value
+        for missing data: ``converters = {3: lambda s: float(s or 0)}``.
+    missing_values : variable or None, optional
+        The set of strings corresponding to missing data.
+    filling_values : variable or None, optional
+        The set of values to be used as default when the data are missing.
+    usecols : sequence or None, optional
+        Which columns to read, with 0 being the first.  For example,
+        ``usecols = (1, 4, 5)`` will extract the 2nd, 5th and 6th columns.
+    names : {None, True, str, sequence}, optional
+        If `names` is True, the field names are read from the first valid line
+        after the first `skiprows` lines.
+        If `names` is a sequence or a single-string of comma-separated names,
+        the names will be used to define the field names in a structured dtype.
+        If `names` is None, the names of the dtype fields will be used, if any.
+    excludelist : sequence, optional
+        A list of names to exclude. This list is appended to the default list
+        ['return','file','print']. Excluded names are appended an underscore:
+        for example, `file` would become `file_`.
+    deletechars : str, optional
+        A string combining invalid characters that must be deleted from the
+        names.
+    defaultfmt : str, optional
+        A format used to define default field names, such as "f%i" or "f_%02i".
+    autostrip : bool, optional
+        Whether to automatically strip white spaces from the variables.
+    case_sensitive : {True, False, 'upper', 'lower'}, optional
+        If True, field names are case sensitive.
+        If False or 'upper', field names are converted to upper case.
+        If 'lower', field names are converted to lower case.
+    unpack : bool, optional
+        If True, the returned array is transposed, so that arguments may be
+        unpacked using ``x, y, z = loadtxt(...)``
+    usemask : bool, optional
+        If True, return a masked array.
+        If False, return a regular array.
+    invalid_raise : bool, optional
+        If True, an exception is raised if an inconsistency is detected in the
+        number of columns.
+        If False, a warning is emitted and the offending lines are skipped.
+
+    Returns
+    -------
+    out : ndarray
+        Data read from the text file. If `usemask` is True, this is a
+        masked array.
+
+    See Also
+    --------
+    numpy.loadtxt : equivalent function when no data is missing.
+
+    Notes
+    -----
+    * When spaces are used as delimiters, or when no delimiter has been given
+      as input, there should not be any missing data between two fields.
+    * When the variables are named (either by a flexible dtype or with `names`,
+      there must not be any header in the file (else a ValueError
+      exception is raised).
+    * Individual values are not stripped of spaces by default.
+      When using a custom converter, make sure the function does remove spaces.
+
+    Examples
+    ---------
+    >>> from StringIO import StringIO
+    >>> import numpy as np
+
+    Comma delimited file with mixed dtype
+
+    >>> s = StringIO("1,1.3,abcde")
+    >>> data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),
+        ('mystring','S5')], delimiter=",")
+    >>> data
+    array((1, 1.3, 'abcde'),
+          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', '|S5')])
+
+    Using dtype = None
+
+    >>> s.seek(0) # needed for StringIO example only
+    >>> data = np.genfromtxt(s, dtype=None,
+        names = ['myint','myfloat','mystring'], delimiter=",")
+    >>> data
+    array((1, 1.3, 'abcde'),
+          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', '|S5')])
+
+    Specifying dtype and names
+
+    >>> s.seek(0)
+    >>> data = np.genfromtxt(s, dtype="i8,f8,S5",
+        names=['myint','myfloat','mystring'], delimiter=",")
+    >>> data
+    array((1, 1.3, 'abcde'),
+          dtype=[('myint', '<i8'), ('myfloat', '<f8'), ('mystring', '|S5')])
+
+    An example with fixed-width columns
+
+    >>> s = StringIO("11.3abcde")
+    >>> data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],
+            delimiter=[1,3,5])
+    >>> data
+    array((1, 1.3, 'abcde'),
+          dtype=[('intvar', '<i8'), ('fltvar', '<f8'), ('strvar', '|S5')])
+
+    """
+    #
+    if usemask:
+        from numpy.ma import MaskedArray, make_mask_descr
+    # Check the input dictionary of converters
+    user_converters = converters or {}
+    if not isinstance(user_converters, dict):
+        errmsg = "The input argument 'converter' should be a valid dictionary "\
+            "(got '%s' instead)"
+        raise TypeError(errmsg % type(user_converters))
+
+    # Initialize the filehandle, the LineSplitter and the NameValidator
+    if isinstance(fname, basestring):
+        fhd = np.lib._datasource.open(fname)
+    elif not hasattr(fname, 'read'):
+        raise TypeError("The input should be a string or a filehandle. "\
+                        "(got %s instead)" % type(fname))
+    else:
+        fhd = fname
+    split_line = LineSplitter(delimiter=delimiter, comments=comments,
+                              autostrip=autostrip)._handyman
+    validate_names = NameValidator(excludelist=excludelist,
+                                   deletechars=deletechars,
+                                   case_sensitive=case_sensitive)
+
+    # Get the first valid lines after the first skiprows ones ..
+    if skiprows:
+        warnings.warn("The use of `skiprows` is deprecated.\n"\
+                      "Please use `skip_header` instead.",
+                      DeprecationWarning)
+        skip_header = skiprows
+    # Skip the first `skip_header` rows
+    for i in xrange(skip_header):
+        fhd.readline()
+    # Keep on until we find the first valid values
+    first_values = None
+    while not first_values:
+        first_line = fhd.readline()
+        if first_line == '':
+            raise IOError('End-of-file reached before encountering data.')
+        if names is True:
+            if comments in first_line:
+                first_line = ''.join(first_line.split(comments)[1])
+        first_values = split_line(first_line)
+    # Should we take the first values as names ?
+    if names is True:
+        fval = first_values[0].strip()
+        if fval in comments:
+            del first_values[0]
+
+    # Check the columns to use
+    if usecols is not None:
+        try:
+            usecols = [_.strip() for _ in usecols.split(",")]
+        except AttributeError:
+            try:
+                usecols = list(usecols)
+            except TypeError:
+                usecols = [usecols, ]
+    nbcols = len(usecols or first_values)
+
+    # Check the names and overwrite the dtype.names if needed
+    if names is True:
+        names = validate_names([_.strip() for _ in first_values])
+        first_line = ''
+    elif _is_string_like(names):
+        names = validate_names([_.strip() for _ in names.split(',')])
+    elif names:
+        names = validate_names(names)
+    # Get the dtype
+    if dtype is not None:
+        dtype = easy_dtype(dtype, defaultfmt=defaultfmt, names=names)
+        names = dtype.names
+    # Make sure the names is a list (for 2.5)
+    if names is not None:
+        names = list(names)
+
+
+    if usecols:
+        for (i, current) in enumerate(usecols):
+            # if usecols is a list of names, convert to a list of indices
+            if _is_string_like(current):
+                usecols[i] = names.index(current)
+            elif current < 0:
+                usecols[i] = current + len(first_values)
+        # If the dtype is not None, make sure we update it
+        if (dtype is not None) and (len(dtype) > nbcols):
+            descr = dtype.descr
+            dtype = np.dtype([descr[_] for _ in usecols])
+            names = list(dtype.names)
+        # If `names` is not None, update the names
+        elif (names is not None) and (len(names) > nbcols):
+            names = [names[_] for _ in usecols]
+
+
+    # Process the missing values ...............................
+    # Rename missing_values for convenience
+    user_missing_values = missing_values or ()
+
+    # Define the list of missing_values (one column: one list)
+    missing_values = [list(['']) for _ in range(nbcols)]
+
+    # We have a dictionary: process it field by field
+    if isinstance(user_missing_values, dict):
+        # Loop on the items
+        for (key, val) in user_missing_values.items():
+            # Is the key a string ?
+            if _is_string_like(key):
+                try:
+                    # Transform it into an integer
+                    key = names.index(key)
+                except ValueError:
+                    # We couldn't find it: the name must have been dropped, then
+                    continue
+            # Redefine the key as needed if it's a column number
+            if usecols:
+                try:
+                    key = usecols.index(key)
+                except ValueError:
+                    pass
+            # Transform the value as a list of string
+            if isinstance(val, (list, tuple)):
+                val = [str(_) for _ in val]
+            else:
+                val = [str(val), ]
+            # Add the value(s) to the current list of missing
+            if key is None:
+                # None acts as default
+                for miss in missing_values:
+                    miss.extend(val)
+            else:
+                missing_values[key].extend(val)
+    # We have a sequence : each item matches a column
+    elif isinstance(user_missing_values, (list, tuple)):
+        for (value, entry) in zip(user_missing_values, missing_values):
+            value = str(value)
+            if value not in entry:
+                entry.append(value)
+    # We have a string : apply it to all entries
+    elif isinstance(user_missing_values, basestring):
+        user_value = user_missing_values.split(",")
+        for entry in missing_values:
+            entry.extend(user_value)
+    # We have something else: apply it to all entries
+    else:
+        for entry in missing_values:
+            entry.extend([str(user_missing_values)])
+
+    # Process the deprecated `missing`
+    if missing != '':
+        warnings.warn("The use of `missing` is deprecated.\n"\
+                      "Please use `missing_values` instead.",
+                      DeprecationWarning)
+        values = [str(_) for _ in missing.split(",")]
+        for entry in missing_values:
+            entry.extend(values)
+
+    # Process the filling_values ...............................
+    # Rename the input for convenience
+    user_filling_values = filling_values or []
+    # Define the default
+    filling_values = [None] * nbcols
+    # We have a dictionary : update each entry individually
+    if isinstance(user_filling_values, dict):
+        for (key, val) in user_filling_values.items():
+            if _is_string_like(key):
+                try:
+                    # Transform it into an integer
+                    key = names.index(key)
+                except ValueError:
+                    # We couldn't find it: the name must have been dropped, then
+                    continue
+            # Redefine the key if it's a column number and usecols is defined
+            if usecols:
+                try:
+                    key = usecols.index(key)
+                except ValueError:
+                    pass
+            # Add the value to the list
+            filling_values[key] = val
+    # We have a sequence : update on a one-to-one basis
+    elif isinstance(user_filling_values, (list, tuple)):
+        n = len(user_filling_values)
+        if (n <= nbcols):
+            filling_values[:n] = user_filling_values
+        else:
+            filling_values = user_filling_values[:nbcols]
+    # We have something else : use it for all entries
+    else:
+        filling_values = [user_filling_values] * nbcols
+
+    # Initialize the converters ................................
+    if dtype is None:
+        # Note: we can't use a [...]*nbcols, as we would have 3 times the same
+        # ... converter, instead of 3 different converters.
+        converters = [StringConverter(None, missing_values=miss, default=fill)
+                      for (miss, fill) in zip(missing_values, filling_values)]
+    else:
+        dtype_flat = flatten_dtype(dtype, flatten_base=True)
+        # Initialize the converters
+        if len(dtype_flat) > 1:
+            # Flexible type : get a converter from each dtype
+            zipit = zip(dtype_flat, missing_values, filling_values)
+            converters = [StringConverter(dt, locked=True,
+                                          missing_values=miss, default=fill)
+                           for (dt, miss, fill) in zipit]
+        else:
+            # Set to a default converter (but w/ different missing values)
+            zipit = zip(missing_values, filling_values)
+            converters = [StringConverter(dtype, locked=True,
+                                          missing_values=miss, default=fill)
+                          for (miss, fill) in zipit]
+    # Update the converters to use the user-defined ones
+    uc_update = []
+    for (i, conv) in user_converters.items():
+        # If the converter is specified by column names, use the index instead
+        if _is_string_like(i):
+            try:
+                i = names.index(i)
+            except ValueError:
+                continue
+        elif usecols:
+            try:
+                i = usecols.index(i)
+            except ValueError:
+                # Unused converter specified
+                continue
+        converters[i].update(conv, locked=True,
+                             default=filling_values[i],
+                             missing_values=missing_values[i],)
+        uc_update.append((i, conv))
+    # Make sure we have the corrected keys in user_converters...
+    user_converters.update(uc_update)
+
+    miss_chars = [_.missing_values for _ in converters]
+
+
+    # Initialize the output lists ...
+    # ... rows
+    rows = []
+    append_to_rows = rows.append
+    # ... masks
+    if usemask:
+        masks = []
+        append_to_masks = masks.append
+    # ... invalid
+    invalid = []
+    append_to_invalid = invalid.append
+
+    # Parse each line
+    for (i, line) in enumerate(itertools.chain([first_line, ], fhd)):
+        values = split_line(line)
+        nbvalues = len(values)
+        # Skip an empty line
+        if nbvalues == 0:
+            continue
+        # Select only the columns we need
+        if usecols:
+            try:
+                values = [values[_] for _ in usecols]
+            except IndexError:
+                append_to_invalid((i, nbvalues))
+                continue
+        elif nbvalues != nbcols:
+            append_to_invalid((i, nbvalues))
+            continue
+        # Store the values
+        append_to_rows(tuple(values))
+        if usemask:
+            append_to_masks(tuple([v.strip() in m
+                                   for (v, m) in zip(values, missing_values)]))
+
+    # Strip the last skip_footer data
+    if skip_footer > 0:
+        rows = rows[:-skip_footer]
+        if usemask:
+            masks = masks[:-skip_footer]
+
+    # Upgrade the converters (if needed)
+    if dtype is None:
+        for (i, converter) in enumerate(converters):
+            current_column = map(itemgetter(i), rows)
+            try:
+                converter.iterupgrade(current_column)
+            except ConverterLockError:
+                errmsg = "Converter #%i is locked and cannot be upgraded: " % i
+                current_column = itertools.imap(itemgetter(i), rows)
+                for (j, value) in enumerate(current_column):
+                    try:
+                        converter.upgrade(value)
+                    except (ConverterError, ValueError):
+                        errmsg += "(occurred line #%i for value '%s')"
+                        errmsg %= (j + 1 + skip_header, value)
+                        raise ConverterError(errmsg)
+
+    # Check that we don't have invalid values
+    if len(invalid) > 0:
+        nbrows = len(rows)
+        # Construct the error message
+        template = "    Line #%%i (got %%i columns instead of %i)" % nbcols
+        if skip_footer > 0:
+            nbrows -= skip_footer
+            errmsg = [template % (i + skip_header + 1, nb)
+                      for (i, nb) in invalid if i < nbrows]
+        else:
+            errmsg = [template % (i + skip_header + 1, nb)
+                      for (i, nb) in invalid]
+        if len(errmsg):
+            errmsg.insert(0, "Some errors were detected !")
+            errmsg = "\n".join(errmsg)
+            # Raise an exception ?
+            if invalid_raise:
+                raise ValueError(errmsg)
+            # Issue a warning ?
+            else:
+                warnings.warn(errmsg, ConversionWarning)
+
+    # Convert each value according to the converter:
+    # We want to modify the list in place to avoid creating a new one...
+#    if loose:
+#        conversionfuncs = [conv._loose_call for conv in converters]
+#    else:
+#        conversionfuncs = [conv._strict_call for conv in converters]
+#    for (i, vals) in enumerate(rows):
+#        rows[i] = tuple([convert(val)
+#                         for (convert, val) in zip(conversionfuncs, vals)])
+    if loose:
+        rows = zip(*(map(converter._loose_call, map(itemgetter(i), rows))
+                     for (i, converter) in enumerate(converters)))
+    else:
+        rows = zip(*(map(converter._strict_call, map(itemgetter(i), rows))
+                     for (i, converter) in enumerate(converters)))
+    # Reset the dtype
+    data = rows
+    if dtype is None:
+        # Get the dtypes from the types of the converters
+        column_types = [conv.type for conv in converters]
+        # Find the columns with strings...
+        strcolidx = [i for (i, v) in enumerate(column_types)
+                     if v in (type('S'), np.string_)]
+        # ... and take the largest number of chars.
+        for i in strcolidx:
+            column_types[i] = "|S%i" % max(len(row[i]) for row in data)
+        #
+        if names is None:
+            # If the dtype is uniform, don't define names, else use ''
+            base = set([c.type for c in converters if c._checked])
+            if len(base) == 1:
+                (ddtype, mdtype) = (list(base)[0], np.bool)
+            else:
+                ddtype = [(defaultfmt % i, dt)
+                          for (i, dt) in enumerate(column_types)]
+                if usemask:
+                    mdtype = [(defaultfmt % i, np.bool)
+                              for (i, dt) in enumerate(column_types)]
+        else:
+            ddtype = zip(names, column_types)
+            mdtype = zip(names, [np.bool] * len(column_types))
+        output = np.array(data, dtype=ddtype)
+        if usemask:
+            outputmask = np.array(masks, dtype=mdtype)
+    else:
+        # Overwrite the initial dtype names if needed
+        if names and dtype.names:
+            dtype.names = names
+        # Case 1. We have a structured type
+        if len(dtype_flat) > 1:
+            # Nested dtype, eg  [('a', int), ('b', [('b0', int), ('b1', 'f4')])]
+            # First, create the array using a flattened dtype:
+            # [('a', int), ('b1', int), ('b2', float)]
+            # Then, view the array using the specified dtype.
+            if 'O' in (_.char for _ in dtype_flat):
+                if has_nested_fields(dtype):
+                    errmsg = "Nested fields involving objects "\
+                             "are not supported..."
+                    raise NotImplementedError(errmsg)
+                else:
+                    output = np.array(data, dtype=dtype)
+            else:
+                rows = np.array(data, dtype=[('', _) for _ in dtype_flat])
+                output = rows.view(dtype)
+            # Now, process the rowmasks the same way
+            if usemask:
+                rowmasks = np.array(masks,
+                                    dtype=np.dtype([('', np.bool)
+                                    for t in dtype_flat]))
+                # Construct the new dtype
+                mdtype = make_mask_descr(dtype)
+                outputmask = rowmasks.view(mdtype)
+        # Case #2. We have a basic dtype
+        else:
+            # We used some user-defined converters
+            if user_converters:
+                ishomogeneous = True
+                descr = []
+                for (i, ttype) in enumerate([conv.type for conv in converters]):
+                    # Keep the dtype of the current converter
+                    if i in user_converters:
+                        ishomogeneous &= (ttype == dtype.type)
+                        if ttype == np.string_:
+                            ttype = "|S%i" % max(len(row[i]) for row in data)
+                        descr.append(('', ttype))
+                    else:
+                        descr.append(('', dtype))
+                # So we changed the dtype ?
+                if not ishomogeneous:
+                    # We have more than one field
+                    if len(descr) > 1:
+                        dtype = np.dtype(descr)
+                    # We have only one field: drop the name if not needed.
+                    else:
+                        dtype = np.dtype(ttype)
+            #
+            output = np.array(data, dtype)
+            if usemask:
+                if dtype.names:
+                    mdtype = [(_, np.bool) for _ in dtype.names]
+                else:
+                    mdtype = np.bool
+                outputmask = np.array(masks, dtype=mdtype)
+    # Try to take care of the missing data we missed
+    names = output.dtype.names
+    if usemask and names:
+        for (name, conv) in zip(names or (), converters):
+            missing_values = [conv(_) for _ in conv.missing_values if _ != '']
+            for mval in missing_values:
+                outputmask[name] |= (output[name] == mval)
+    # Construct the final array
+    if usemask:
+        output = output.view(MaskedArray)
+        output._mask = outputmask
+    if unpack:
+        return output.squeeze().T
+    return output.squeeze()
diff --git a/pandas/timeseries/_tools.py b/pandas/timeseries/_tools.py
new file mode 100644
index 000000000..ce5520f74
--- /dev/null
+++ b/pandas/timeseries/_tools.py
@@ -0,0 +1,95 @@
+"""
+A collection of helper tools
+
+
+Decorators
+==========
+
+.. autoclass:: docwrapper
+
+.. autoclass:: deprecated_for
+
+"""
+
+import functools
+import warnings
+
+
+
+class docwrapper(object):
+    """
+    Decorator that updates the docstring of a function with a dictionary
+    of templates.
+
+    Parameters
+    ----------
+    template : dictionary
+        A dictionary giving for each key the docstring to extend.
+    """
+    #
+    def __init__(self, template):
+        self.template = template
+    #
+    def __call__(self, func):
+        def wrapped(*args, **kwargs):
+            "Just call the function w/ the proper arguments"
+            return func(*args, **kwargs)
+        wrapped.__name__ = func.__name__
+        wrapped.__dict__ = func.__dict__
+        wrapped.__doc__ = ((func.__doc__ or "") % self.template) or None
+        return wrapped
+
+
+
+
+class deprecated_for:
+    """
+    Decorator marking a function as deprecated.
+
+    When a decorated function is called, a warning is emitted.
+
+    Parameters
+    ----------
+    newfunc : function, optional
+        Function that should replace the deprecated one.
+
+    Examples
+    --------
+    >>> def new_function(*args, **kwargs):
+    ...     do_something
+    ...
+    >>> @deprecated_for(new_function)
+    >>> def old_function(*args, **kwargs):
+    ...    do_something
+    ...
+    >>> old_function(*args)
+    DeprecationWarning: The function `old_function` is deprecated.
+    Please use the `new_function` instead.
+    """
+    #
+    def __init__(self, newfunc=None):
+        self.replacement = newfunc
+        msg = "The function `%(oldname)s` is deprecated.\n"
+        doctemplate = "(Deprecated function)\n%(olddoc)s"
+        if newfunc is not None:
+            msg += "Please use the `%s` function instead." % newfunc.__name__
+            newdoc = newfunc.__doc__
+            if newdoc is not None:
+                doctemplate += "\n(New usage)\n%s" % newdoc
+        self.msg = msg
+        self.doctemplate = doctemplate
+    #
+    def __call__(self, func):
+        oldinfo = dict(oldname=func.__name__, olddoc=func.__doc__ or "")
+        msg = self.msg % oldinfo
+        def wrapped(*args, **kwargs):
+            warnings.warn_explicit(msg, category=DeprecationWarning,
+                                   filename=func.func_code.co_filename,
+                                   lineno=func.func_code.co_firstlineno + 1)
+            return func(*args, **kwargs)
+        wrapped.__name__ = func.__name__
+        wrapped.__dict__ = func.__dict__
+        wrapped.__doc__ = (self.doctemplate % oldinfo) or None
+        return wrapped
+
+deprecated = deprecated_for()
diff --git a/pandas/timeseries/const.py b/pandas/timeseries/const.py
new file mode 100644
index 000000000..7013bcae5
--- /dev/null
+++ b/pandas/timeseries/const.py
@@ -0,0 +1,87 @@
+"""
+This module contains all the integer frequency constants. Below is a detailed
+description of the constants, as well as a listing of the corresponding string
+aliases.
+
+All functions in the timeseries module that accept a frequency parameter can
+accept either the integer constant, or a valid string alias.
+
+|----------------------------------------------------------------------------|
+|CONSTANT         | String aliases (case insensitive) and comments           |
+|----------------------------------------------------------------------------|
+| Note: For annual frequencies, "Year" is determined by where the last month |
+| of the year falls.                                                         |
+|----------------------------------------------------------------------------|
+| FR_ANN          | 'A', 'Y', 'ANNUAL', 'ANNUALLY', 'YEAR', 'YEARLY'         |
+|----------------------------------------------------------------------------|
+| FR_ANNDEC       | 'A-DEC', 'A-December', 'Y-DEC', 'ANNUAL-DEC', etc...     |
+|                 | (annual frequency with December year end, equivalent to  |
+|                 | FR_ANN)                                                  |
+|----------------------------------------------------------------------------|
+| FR_ANNNOV       | 'A-NOV', 'A-NOVEMBER', 'Y-NOVEMBER', 'ANNUAL-NOV', etc...|
+|                   (annual frequency with November year end)                |
+| ...etc for the rest of the months                                          |
+|----------------------------------------------------------------------------|
+| Note: For the following quarterly frequencies, "Year" is determined by     |
+| where the last quarter of the current group of quarters ENDS               |
+|----------------------------------------------------------------------------|
+| FR_QTR          | 'Q', 'QUARTER', 'QUARTERLY'                              |
+|----------------------------------------------------------------------------|
+| FR_QTREDEC      | 'Q-DEC', 'QTR-December', 'QUARTERLY-DEC', etc...         |
+|                 | (quarterly frequency with December year end, equivalent  |
+|                 | to FR_QTR)                                               |
+|----------------------------------------------------------------------------|
+| FR_QTRENOV      | 'Q-NOV', 'QTR-NOVEMBER', 'QUARTERLY-NOV', etc...         |
+|                 | (quarterly frequency with November year end)             |
+| ...etc for the rest of the months                                          |
+|----------------------------------------------------------------------------|
+| Note: For the following quarterly frequencies, "Year" is determined by     |
+| where the first quarter of the current group of quarters STARTS            |
+|----------------------------------------------------------------------------|
+| FR_QTRSDEC      | 'Q-S-DEC', 'QTR-S-December', etc... (quarterly frequency |
+|                 | with December year end)                                  |
+| ...etc for the rest of the months                                          |
+|----------------------------------------------------------------------------|
+| FR_MTH          | 'M', 'MONTH', 'MONTHLY'                                  |
+|----------------------------------------------------------------------------|
+| FR_WK           | 'W', 'WEEK', 'WEEKLY'                                    |
+|----------------------------------------------------------------------------|
+| FR_WKSUN        | 'W-SUN', 'WEEK-SUNDAY', 'WEEKLY-SUN', etc... (weekly     |
+|                 | frequency with Sunday being the last day of the week,    |
+|                 | equivalent to FR_WK)                                     |
+|----------------------------------------------------------------------------|
+| FR_WKSAT        | 'W-SAT', 'WEEK-SATURDAY', 'WEEKLY-SAT', etc... (weekly   |
+|                 | frequency with Saturday being the last day of the week)  |
+| ...etc for the rest of the days of the week                                |
+|----------------------------------------------------------------------------|
+| FR_DAY          | 'D', 'DAY', 'DAILY'                                      |
+|----------------------------------------------------------------------------|
+| FR_BUS          | 'B', 'BUSINESS', 'BUSINESSLY' (this is a daily frequency |
+|                 | excluding Saturdays and Sundays)                         |
+|----------------------------------------------------------------------------|
+| FR_HR           | 'H', 'HOUR', 'HOURLY'                                    |
+|----------------------------------------------------------------------------|
+| FR_MIN          | 'T', 'MINUTE', 'MINUTELY'                                |
+|----------------------------------------------------------------------------|
+| FR_SEC          | 'S', 'SECOND', 'SECONDLY'                                |
+|----------------------------------------------------------------------------|
+| FR_UND          | 'U', 'UNDEF', 'UNDEFINED'                                |
+|----------------------------------------------------------------------------|
+
+:author: Pierre GF Gerard-Marchant & Matt Knox
+:contact: pierregm_at_uga_dot_edu - mattknox_ca_at_hotmail_dot_com
+:version: $Id$
+"""
+__author__ = "Pierre GF Gerard-Marchant & Matt Knox ($Author$)"
+__revision__ = "$Revision$"
+__date__     = '$Date$'
+
+from pandas._skts import freq_constants
+
+"""add constants in pandas._skts.freq_constants dictionary to global namespace
+for this module"""
+
+__all__ = [list(freq_constants)]
+
+_g = globals()
+_g.update(freq_constants)
diff --git a/pandas/timeseries/extras.py b/pandas/timeseries/extras.py
new file mode 100644
index 000000000..714e32fcd
--- /dev/null
+++ b/pandas/timeseries/extras.py
@@ -0,0 +1,575 @@
+"""
+Extras functions for time series.
+
+:author: Pierre GF Gerard-Marchant & Matt Knox
+:contact: pierregm_at_uga_dot_edu - mattknox_ca_at_hotmail_dot_com
+:version: $Id$
+"""
+__author__ = "Pierre GF Gerard-Marchant & Matt Knox ($Author$)"
+__revision__ = "$Revision$"
+__date__ = '$Date$'
+
+
+import numpy as np
+import numpy.ma as ma
+from numpy.ma import masked
+
+import const as _c
+from tdates import Date, date_array, DateArray
+from tseries import TimeSeries, time_series
+from pandas._skts import DateCalc_Error
+
+from _preview import genfromtxt, easy_dtype
+
+__all__ = ['accept_atmost_missing',
+           'convert_to_annual', 'count_missing',
+           'guess_freq',
+           'isleapyear',
+           'tsfromtxt']
+
+#..............................................................................
+def isleapyear(year):
+    """
+    Returns true if year is a leap year.
+
+    Parameters
+    ----------
+    year : integer / sequence
+        A given (list of) year(s).
+    """
+    year = np.asarray(year)
+    return np.logical_or(year % 400 == 0,
+                         np.logical_and(year % 4 == 0, year % 100 > 0))
+
+#..............................................................................
+def count_missing(series):
+    """
+    Returns the number of missing data per period.
+
+    Notes
+    -----
+    This function is designed to return the actual number of missing values when
+    a series has been converted from one frequency to a smaller frequency.
+
+    For example, converting a 12-month-long daily series to months will yield
+    a (12x31) array, with missing values in February, April, June...
+    count_missing will discard these extra missing values.
+    """
+    if not isinstance(series, TimeSeries):
+        raise TypeError, "The input data should be a valid TimeSeries object! "\
+                         "(got %s instead)" % type(series)
+    if series.ndim == 1:
+        return len(series) - series.count()
+    elif series.ndim != 2:
+        raise NotImplementedError
+    #
+    missing = series.shape[-1] - series.count(axis= -1)
+    period = series.shape[-1]
+    freq = series.freq
+    if (period == 366) and (freq // _c.FR_ANN == 1):
+        # row: years, cols: days
+        missing -= ~isleapyear(series.year)
+    elif period == 31 and (freq // _c.FR_MTH == 1):
+        months = series.months
+        # row: months, cols: days
+        missing[np.array([m in [4, 6, 9, 11] for m in months])] -= 1
+        isfeb = (months == 2)
+        missing[isfeb] -= 2
+        missing[isfeb & ~isleapyear(series.year)] -= 1
+    elif period == 92 and (freq // _c.FR_QTR == 1):
+        # row: quarters, cold:days
+        months = series.months
+        if freq in (_c.FR_QTREJAN, _c.FR_QTRSJAN, _c.FR_QTREAPR, _c.FR_QTRSAPR,
+                    _c.FR_QTREOCT, _c.FR_QTRSOCT, _c.FR_QTREOCT, _c.FR_QTRSOCT):
+            isfeb = (months == 4)
+            missing[isfeb] -= 2
+        elif freq in (_c.FR_QTREFEB, _c.FR_QTRSFEB, _c.FR_QTREMAY, _c.FR_QTRSMAY,
+                      _c.FR_QTREAUG, _c.FR_QTRSAUG, _c.FR_QTRENOV, _c.FR_QTRSNOV):
+            missing[np.array([m in [2, 11] for m in months])] -= 1
+            isfeb = (months == 2)
+        elif freq in (_c.FR_QTREMAR, _c.FR_QTRSMAR, _c.FR_QTREJUN, _c.FR_QTRSJUN,
+                      _c.FR_QTRESEP, _c.FR_QTRSSEP, _c.FR_QTREDEC, _c.FR_QTRSDEC):
+            missing[np.array([m in [3, 6] for m in months])] -= 1
+            isfeb = (months == 3)
+        missing[isfeb & ~isleapyear(series.year)] -= 1
+    elif period not in (12, 7):
+        raise NotImplementedError, "Not yet implemented for that frequency..."
+    return missing
+
+
+
+def convert_to_annual(series):
+    """
+    Group a series by years, taking leap years into account.
+
+    The output has as many rows as distinct years in the original series,
+    and as many columns as the length of a leap year in the units corresponding
+    to the original frequency (366 for daily frequency, 366*24 for hourly...).
+    The fist column of the output corresponds to Jan. 1st, 00:00:00,
+    while the last column corresponds to Dec, 31st, 23:59:59.
+    Entries corresponding to Feb. 29th are masked for non-leap years.
+
+    For example, if the initial series has a daily frequency, the 59th column
+    of the output always corresponds to Feb. 28th, the 61st column to Mar. 1st,
+    and the 60th column is masked for non-leap years.
+    With a hourly initial frequency, the (59*24)th column of the output always
+    correspond to Feb. 28th 23:00, the (61*24)th column to Mar. 1st, 00:00, and
+    the 24 columns between (59*24) and (61*24) are masked.
+
+    If the original frequency is less than daily, the output is equivalent to
+    ``series.convert('A', func=None)``.
+
+
+    Parameters
+    ----------
+    series : TimeSeries
+        A valid :class:`~scikits.timeseries.TimeSeries` object.
+
+    Returns
+    -------
+    aseries : TimeSeries
+        A 2D  :class:`~scikits.timeseries.TimeSeries` object with annual ('A')
+        frequency.
+
+    """
+    freq = series._dates.freq
+    if freq < _c.FR_DAY:
+        return series.convert('A')
+    baseidx = np.array((59, 60), dtype=int)
+    if (freq == _c.FR_DAY):
+        (idx0228, idx0301) = baseidx
+    elif (freq == _c.FR_HR):
+        (idx0228, idx0301) = baseidx * 24
+    elif (freq == _c.FR_MIN):
+        (idx0228, idx0301) = baseidx * 24 * 60
+    elif (freq == _c.FR_SEC):
+        (idx0228, idx0301) = baseidx * 24 * 3600
+    aseries = series.convert('A')
+    leapcondition = isleapyear(aseries.dates.years)
+    leapidx = np.arange(len(aseries), dtype=int)[~leapcondition]
+    aseries[leapidx, idx0301:] = aseries[leapidx, idx0228:idx0228 - idx0301]
+    aseries[leapidx, idx0228:idx0301] = ma.masked
+    return aseries
+
+
+
+#.............................................................................
+def accept_atmost_missing(series, max_missing, strict=False):
+    """
+    Masks the rows of `series` that contain more than `max_missing` missing data.
+    Returns a new masked series.
+
+    Parameters
+    ----------
+    series : TimeSeries
+        Input time series.
+    max_missing : float
+        Number of maximum acceptable missing values per row (if larger than 1),
+        or maximum acceptable percentage of missing values (if lower than 1).
+    strict : boolean *[False]*
+        Whether the number of missing values should be strictly greater than
+        `max_missing` or not.
+
+    Returns
+    -------
+    output : TimeSeries
+        A new TimeSeries object
+    """
+    series = np.array(series, copy=True, subok=True)
+    if not isinstance(series, TimeSeries):
+        raise TypeError, "The input data should be a valid TimeSeries object! "\
+                         "(got %s instead)" % type(series)
+    # Find the number of missing values ....
+    missing = count_missing(series)
+    # Transform an acceptable percentage in a number
+    if max_missing < 1:
+        max_missing = np.round(max_missing * series.shape[-1], 0)
+    #
+    series.unshare_mask()
+    if strict:
+        series[missing > max_missing] = masked
+    else:
+        series[missing >= max_missing] = masked
+    return series
+
+
+def guess_freq(dates):
+    """
+    Return an estimate of the frequency from a list of dates.
+
+    The frequency is estimated from the difference of dates (in days or seconds)
+    after chronological sorting.
+
+
+    Parameters
+    ----------
+    dates : var
+        Sequence of dates
+
+    Notes
+    -----
+    * In practice, the list of dates is first transformed into a list of
+      :class:`datetime.datetime` objects.
+    """
+    if isinstance(dates, DateArray):
+        dates = dates.copy()
+
+    try:
+        dates = date_array(dates, freq='S', autosort=True)
+    except c_dates.DateCalc_Error:
+        # contains dates prior to 1979, assume lower frequency
+        dates = date_array(dates, freq='D', autosort=True)
+
+    ddif = np.diff(dates)
+    mind = np.min(ddif)
+
+    if dates.freq == _c.FR_SEC and mind < 86400:
+
+        # hourly, minutely, or secondly frequency
+        if (mind > 3599) and not np.all(ddif % 3600 > 0):
+            freq = _c.FR_HR
+        elif mind < 59:
+            freq = _c.FR_SEC
+        else:
+            freq = _c.FR_MIN
+        return freq
+
+    # daily or lower frequency
+    if dates.freq == _c.FR_SEC:
+        dates = dates.asfreq('D')
+        ddif = np.diff(dates)
+        mind = np.min(ddif)
+
+    if mind > 360:
+        return _c.FR_ANN
+
+    if mind > 88:
+        qincs = [89, 90, 91, 92, 273, 274, 275, 276, 277]
+        if np.all([i in qincs for i in (ddif % 365)]):
+            freq = _c.FR_QTR
+        else:
+            freq = _c.FR_MTH
+        return freq
+
+    if (mind > 27):
+        return _c.FR_MTH
+
+    dow = dates.day_of_week
+    if (mind % 7 == 0) and np.all((ddif % 7) == 0):
+
+        mdow = np.min(dow)
+        freq = _c.FR_WKSUN + ((mdow + 1) % 7)
+        return freq
+    else:
+        if np.any((dow == 5) | (dow == 6)):
+            freq = _c.FR_DAY
+        else:
+            # no weekends, assume business frequency
+            freq = _c.FR_BUS
+        return freq
+
+
+
+
+def tsfromtxt(fname, dtype=None, freq='U', comments='#', delimiter=None,
+              skip_header=0, skip_footer=0, skiprows=0,
+              converters=None, dateconverter=None,
+              missing='', missing_values=None, filling_values=None,
+              usecols=None, datecols=None,
+              names=None, excludelist=None, deletechars=None, autostrip=True,
+              case_sensitive=True, defaultfmt="f%i", unpack=None, loose=True,
+              asrecarray=False, invalid_raise=True):
+    """
+    Load a TimeSeries from a text file.
+
+    Each line of the input after the first `skiprows` ones is split at
+    `delimiter`. Characters occuring after `comments` are discarded.
+
+    If a column is named ``'dates'`` (case insensitive), it is used to define
+    the dates. The ``freq`` parameter should be set to the expected frequency of
+    the output series.
+    If the date information spans several columns (for example, year in col #1,
+    month in col #2...), a specific conversion function must be defined with
+    the ``dateconverter`` parameter. This function should accept as many inputs
+    as date columns, and return a valid :class:`Date` object.
+
+    Parameters
+    ----------
+    fname : file or string
+        File or filename to read.
+        If the file extension is ``.gz`` or ``.bz2``, the file is first
+        decompressed.
+    dtype : data-type, optional
+        Data type of the resulting array.
+        If it is a structured data-type, the resulting array is 1-dimensional,
+        and each row is interpreted as an element of the array. In this case,
+        the number of columns used must match the number of fields in the dtype
+        and the names of each field are set by the corresponding name of the dtype.
+        If None, the dtypes will be determined by the contents of each
+        column, individually.
+    comments : {string}, optional
+        The character used to indicate the start of a comment.
+        All the characters occurring on a line after a comment are discarded.
+    delimiter : {string}, optional
+        The string used to separate values.  By default, any consecutive
+        whitespace act as delimiter.
+    skip_header : int, optional
+        The numbers of lines to skip at the beginning of the file.
+    skip_footer : int, optional
+        The numbers of lines to skip at the end of the file
+    converters : variable or None, optional
+        The set of functions that convert the data of a column to a value.
+        The converters can also be used to provide a default value
+        for missing data: ``converters = {3: lambda s: float(s or 0)}``.
+    dateconverter : {function}, optional
+        The function to convert the date information to a :class:`Date` object.
+        This function requires as many parameters as number of ``datecols``.
+        This parameter is mandatory if ``dtype=None``.
+    missing_values : variable or None, optional
+        The set of strings corresponding to missing data.
+    filling_values : variable or None, optional
+        The set of values to be used as default when the data are missing.
+    usecols : sequence or None, optional
+        Which columns to read, with 0 being the first.  For example,
+        ``usecols = (1, 4, 5)`` will extract the 2nd, 5th and 6th columns.
+    datecols : {None, int, sequence}, optional
+        Which columns store the date information.
+    names : {None, True, str, sequence}, optional
+        If `names` is True, the field names are read from the first valid line
+        after the first `skiprows` lines.
+        If `names` is a sequence or a single-string of comma-separated names,
+        the names will be used to define the field names in a structured dtype.
+        If `names` is None, the names of the dtype fields will be used, if any.
+    excludelist : sequence, optional
+        A list of names to exclude. This list is appended to the default list
+        ['return','file','print']. Excluded names are appended an underscore:
+        for example, `file` would become `file_`.
+    deletechars : str, optional
+        A string combining invalid characters that must be deleted from the
+        names.
+    defaultfmt : str, optional
+        A format used to define default field names, such as "f%i" or "f_%02i".
+    autostrip : bool, optional
+        Whether to automatically strip white spaces from the variables.
+    case_sensitive : {True, False, 'upper', 'lower'}, optional
+        If True, field names are case sensitive.
+        If False or 'upper', field names are converted to upper case.
+        If 'lower', field names are converted to lower case.
+    unpack : bool, optional
+        If True, the returned array is transposed, so that arguments may be
+        unpacked using ``x, y, z = loadtxt(...)``
+    usemask : bool, optional
+        If True, return a masked array.
+        If False, return a regular array.
+    asrecarray : {False, True}, optional
+        Whether to return a TimeSeriesRecords or a series with a structured
+        dtype.
+    invalid_raise : bool, optional
+        If True, an exception is raised if an inconsistency is detected in the
+        number of columns.
+        If False, a warning is emitted and the offending lines are skipped.
+
+
+    Returns
+    -------
+    out : MaskedArray
+        Data read from the text file.
+
+    See Also
+    --------
+    numpy.lib.io.genfromtxt
+        Equivalent function for standard arrays
+
+    Notes
+    -----
+    * When spaces are used as delimiters, or when no delimiter has been given
+      as input, there should not be any missing data between two fields.
+    * When the variable are named (either by a flexible dtype or with `names`,
+      there must not be any header in the file (else a :exc:`ValueError`
+      exception is raised).
+    * If ``names`` is True or a sequence of strings, these names overwrite
+      the fields names of a structured array.
+    * The sequence of names must NOT take the date columns into account.
+    * If the datatype is not given explicitly (``dtype=None``),
+      a :keyword:`dateconverter` must be given explicitly.
+    * If the ``dtype`` is given explicitly,
+      it must NOT refer to the date columns.
+    * By default, the types of variables is defined from the values encountered
+      in the file (``dtype=None``). This is *NOT* the default for np.genfromtxt.
+
+    Examples
+    --------
+    >>> data = "year, month, a, b\\n 2001, 01, 0.0, 10.\\n 2001, 02, 1.1, 11."
+    >>> dateconverter = lambda y, m: Date('M', year=int(y), month=int(m))
+    >>> series = tsfromtxt(StringIO.StringIO(data), delimiter=',', names=True,
+    ...                    datecols=(0,1), dateconverter=dateconverter,)
+    >>> series
+    timeseries([(0.0, 10.0) (1.1, 11.0)],
+       dtype = [('a', '<f8'), ('b', '<f8')],
+       dates = [Jan-2001 Feb-2001],
+       freq  = M)
+    >>> series = tsfromtxt(StringIO.StringIO(data), delimiter=",",
+    ...                    datecols=(0, 1), dateconverter=dateconverter,
+    ...                    names="A, B", skip_header=1)
+    timeseries([(0.0, 10.0) (1.1000000000000001, 11.0)],
+       dtype = [('A', '<f8'), ('B', '<f8')],
+       dates = [Jan-2001 Feb-2001],
+       freq  = M)
+
+    """
+    # Update the date converter ...........................
+    converters = converters or {}
+    dateconv = dateconverter or None
+    if dateconv is None:
+        dateconv = lambda s: Date(freq, string=s)
+    if 'dates' in converters:
+        dateconv = converters['dates']
+        del(converters['dates'])
+
+    # Make sure `datecols` is a sequence ..................
+    if datecols is not None:
+        try:
+            datecols = [_.strip() for _ in datecols.split(",")]
+        except AttributeError:
+            try:
+                datecols = list(datecols)
+            except TypeError:
+                datecols = [datecols, ]
+        # ... and update the converters
+        converters.update((i, str) for i in datecols)
+
+    # Save the initial names and dtypes ...................
+    idtype = dtype
+    if isinstance(names, basestring):
+        names = names.split(",")
+    inames = names
+
+    # Update the dtype (if needed) ........................
+    if (dtype is not None):
+        # Crash if we can't find the datecols
+        if datecols is None:
+            raise TypeError("No column selected for the dates!")
+        # Make sure dtype is a valid np.dtype and make a copy
+        dtype = easy_dtype(dtype, names=names)
+        idtype = dtype
+        inames = dtype.names
+        if inames is not None:
+            nbfields = len(inames) + len(datecols)
+            # Create a new dtype description and a set of names
+            dtype = [''] * nbfields
+            names = [''] * nbfields
+            idx = range(nbfields)
+            for i in datecols:
+                if i < 0:
+                    i += nbfields
+                del idx[idx.index(i)]
+                # Set the default dtype for date columns, as np.object
+                # (we can't use string as we don't know the final size)
+                dtype[i] = ('', np.object)
+            convdict = {'b': bool, 'i': int, 'l':int, 'u': int,
+                        'f': float, 'd': float, 'g': float,
+                        'c': complex, 'D': complex,
+                        'S': str, 'U': str, 'a': str}
+            converter_update = []
+            for (i, name) in zip(idx, inames):
+                field = idtype[name]
+                dtype[i] = (name, field)
+                converter_update.append((i, convdict[field.char]))
+                names[i] = name
+            converters.update(converter_update)
+    elif names not in (True, None):
+        # Make sure that we saved the names as a list
+        inames = list(inames)
+        # Get the list of columns to use
+        if usecols is None:
+            nbcols = len(datecols) + len(inames)
+            names = [''] * nbcols
+            ucols = range(nbcols)
+        else:
+            names = [''] * (max(usecols) + 1)
+            ucols = usecols
+        # Fill the list of names:
+        for i in ucols:
+            if i in datecols:
+                names[i] = "__%i" % i
+            else:
+                names[i] = inames.pop(0)
+    #
+    # Update the optional arguments ...
+    kwargs = dict(dtype=dtype, comments=comments, delimiter=delimiter,
+                  skiprows=skiprows, converters=converters,
+                  skip_header=skip_header, skip_footer=skip_footer,
+                  missing=missing, missing_values=missing_values,
+                  filling_values=filling_values,
+                  usecols=usecols, unpack=unpack, names=names,
+                  excludelist=excludelist, deletechars=deletechars,
+                  case_sensitive=case_sensitive, defaultfmt=defaultfmt,
+                  autostrip=autostrip, loose=loose, invalid_raise=invalid_raise,
+                  usemask=True)
+    # Get the raw data ................
+    mrec = genfromtxt(fname, **kwargs)
+    if not mrec.shape:
+        mrec.shape = -1
+    names = mrec.dtype.names
+    # Revert to the original dtype.....
+    dtype = idtype
+    # Get the date columns ................................
+    if datecols is None:
+        import re
+        datespattern = re.compile("'?_?dates?'?", re.IGNORECASE)
+        datecols = [i for (i, name) in enumerate(names or ())
+                     if datespattern.search(name)]
+        if not datecols:
+            raise TypeError("No column selected for the dates!")
+    else:
+        # We have `datecols` already, make sure the indices are positive
+        # (the nb of fields might still be undefined)
+        nbfields = len(names)
+        for (i, v) in enumerate(datecols):
+            if (v < 0):
+                datecols[i] = v + nbfields
+    # Fix the date columns if usecols was given
+    if usecols is not None:
+        datecols = tuple([list(usecols).index(d) for d in datecols])
+    # Get the date info ...............
+    if names:
+        _dates = [mrec[names[i]] for i in datecols]
+    else:
+        _dates = [mrec[:, i] for i in datecols]
+    # Convert the date columns to a date_array
+    if len(_dates) == 1:
+        _dates = np.array(_dates[0], copy=False, ndmin=1)
+        dates = date_array([dateconv(args) for args in _dates],
+                           freq=freq, autosort=False)
+    else:
+        dates = date_array([dateconv(*args) for args in zip(*_dates)],
+                           freq=freq, autosort=False)
+    # Resort the array according to the dates
+    sortidx = dates.argsort()
+    dates = dates[sortidx]
+    mrec = mrec[sortidx]
+    # Get the dtype from the named columns (if any), or just use the initial one
+    mdtype = mrec.dtype
+    if mdtype.names:
+        newdescr = [descr for (i, descr) in enumerate(mdtype.descr)
+                    if i not in datecols]
+        output = time_series(ma.empty((len(mrec),), dtype=newdescr),
+                             dates=dates)
+        for name in output.dtype.names:
+            output[name] = mrec[name]
+        if (idtype is not None):
+            if (idtype.names is None):
+                dtype = (idtype, len(output.dtype.names))
+            else:
+                dtype = idtype
+            output = output.view(dtype)
+    else:
+        dataidx = [i for i in range(mrec.shape[-1]) if i not in datecols]
+        if len(dataidx) == 1:
+            dataidx = dataidx[0]
+        output = time_series(mrec[:, dataidx], dates=dates)
+    #
+    if asrecarray:
+        from trecords import TimeSeriesRecords
+        return output.view(TimeSeriesRecords)
+    return output
diff --git a/pandas/timeseries/parser.py b/pandas/timeseries/parser.py
new file mode 100644
index 000000000..a3be9a5d9
--- /dev/null
+++ b/pandas/timeseries/parser.py
@@ -0,0 +1,963 @@
+#-*- coding: latin-1 -*-
+""" 
+Date/Time string parsing module.
+
+This code is a slightly modified version of Parser.py found in mx.DateTime
+version 3.0.0
+
+As such, it is subject to the terms of the eGenix public license version 1.1.0.
+Please see license.txt for more details.
+"""
+
+__all__ = [
+'DateFromString', 'DateTimeFromString'
+           ]
+
+import types
+import re
+import datetime as dt
+
+class RangeError(Exception): pass
+
+# Enable to produce debugging output
+_debug = 0
+
+# REs for matching date and time parts in a string; These REs
+# parse a superset of ARPA, ISO, American and European style dates.
+# Timezones are supported via the Timezone submodule.
+
+_year = '(?P<year>-?\d+\d(?!:))'
+_fullyear = '(?P<year>-?\d+\d\d(?!:))'
+_year_epoch = '(?:' + _year + '(?P<epoch> *[ABCDE\.]+)?)'
+_fullyear_epoch = '(?:' + _fullyear + '(?P<epoch> *[ABCDE\.]+)?)'
+_relyear = '(?:\((?P<relyear>[-+]?\d+)\))'
+
+_month = '(?P<month>\d?\d(?!:))'
+_fullmonth = '(?P<month>\d\d(?!:))'
+_litmonth = ('(?P<litmonth>'
+             'jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|'
+             'mr|mae|mrz|mai|okt|dez|'
+             'fev|avr|juin|juil|aou|ao|dc|'
+             'ene|abr|ago|dic|'
+             'out'
+             ')[a-z,\.;]*')
+litmonthtable = {
+    # English
+    'jan':1, 'feb':2, 'mar':3, 'apr':4, 'may':5, 'jun':6,
+    'jul':7, 'aug':8, 'sep':9, 'oct':10, 'nov':11, 'dec':12,
+    # German
+    'mr':3, 'mae':3, 'mrz':3, 'mai':5, 'okt':10, 'dez':12,
+    # French
+    'fev':2, 'avr':4, 'juin':6, 'juil':7, 'aou':8, 'ao':8,
+    'dc':12,
+    # Spanish
+    'ene':1, 'abr':4, 'ago':8, 'dic':12,
+    # Portuguese
+    'out':10,
+    }
+_relmonth = '(?:\((?P<relmonth>[-+]?\d+)\))'
+
+_day = '(?P<day>\d?\d(?!:))'
+_usday = '(?P<day>\d?\d(?!:))(?:st|nd|rd|th|[,\.;])?'
+_fullday = '(?P<day>\d\d(?!:))'
+_litday = ('(?P<litday>'
+           'mon|tue|wed|thu|fri|sat|sun|'
+           'die|mit|don|fre|sam|son|'
+           'lun|mar|mer|jeu|ven|sam|dim|'
+           'mie|jue|vie|sab|dom|'
+           'pri|seg|ter|cua|qui'
+           ')[a-z]*')
+litdaytable = {
+    # English
+    'mon':0, 'tue':1, 'wed':2, 'thu':3, 'fri':4, 'sat':5, 'sun':6,
+    # German
+    'die':1, 'mit':2, 'don':3, 'fre':4, 'sam':5, 'son':6,
+    # French
+    'lun':0, 'mar':1, 'mer':2, 'jeu':3, 'ven':4, 'sam':5, 'dim':6,
+    # Spanish
+    'mie':2, 'jue':3, 'vie':4, 'sab':5, 'dom':6,
+    # Portuguese
+    'pri':0, 'seg':1, 'ter':2, 'cua':3, 'qui':4,
+    }
+_relday = '(?:\((?P<relday>[-+]?\d+)\))'
+
+_hour = '(?P<hour>[012]?\d)'
+_minute = '(?P<minute>[0-6]\d)'
+_second = '(?P<second>[0-6]\d(?:[.,]\d+)?)'
+
+_days = '(?P<days>\d*\d(?:[.,]\d+)?)'
+_hours = '(?P<hours>\d*\d(?:[.,]\d+)?)'
+_minutes = '(?P<minutes>\d*\d(?:[.,]\d+)?)'
+_seconds = '(?P<seconds>\d*\d(?:[.,]\d+)?)'
+
+_reldays = '(?:\((?P<reldays>[-+]?\d+(?:[.,]\d+)?)\))'
+_relhours = '(?:\((?P<relhours>[-+]?\d+(?:[.,]\d+)?)\))'
+_relminutes = '(?:\((?P<relminutes>[-+]?\d+(?:[.,]\d+)?)\))'
+_relseconds = '(?:\((?P<relseconds>[-+]?\d+(?:[.,]\d+)?)\))'
+
+_sign = '(?:(?P<sign>[-+]) *)'
+_week = 'W(?P<week>\d?\d)'
+_zone = '(?P<zone>[A-Z]+|[+-]\d\d?:?(?:\d\d)?)'
+_ampm = '(?P<ampm>[ap][m.]+)'
+
+_time = (_hour + ':' + _minute + '(?::' + _second + '|[^:]|$) *'
+         + _ampm + '? *' + _zone + '?')
+_isotime = _hour + ':?' + _minute + ':?' + _second + '? *' + _zone + '?'
+
+_yeardate = _year
+_weekdate = _year + '-?(?:' + _week + '-?' + _day + '?)?'
+_eurodate = _day + '\.' + _month + '\.' + _year_epoch + '?'
+_usdate = _month + '/' + _day + '(?:/' + _year_epoch + '|[^/]|$)'
+_altusdate = _month + '-' + _day + '-' + _fullyear_epoch
+_isodate = _year + '-' + _month + '-?' + _day + '?(?!:)'
+_altisodate = _year + _fullmonth + _fullday + '(?!:)'
+_usisodate = _fullyear + '/' + _fullmonth + '/' + _fullday
+_litdate = ('(?:'+ _litday + ',? )? *' +
+            _usday + ' *' +
+            '[- ] *(?:' + _litmonth + '|'+ _month +') *[- ] *' +
+            _year_epoch + '?')
+_altlitdate = ('(?:'+ _litday + ',? )? *' +
+               _litmonth + '[ ,.a-z]+' +
+               _usday +
+               '(?:[ a-z]+' + _year_epoch + ')?')
+_eurlitdate = ('(?:'+ _litday + ',?[ a-z]+)? *' +
+               '(?:'+ _usday + '[ a-z]+)? *' +
+               _litmonth +
+               '(?:[ ,.a-z]+' + _year_epoch + ')?')
+
+_relany = '[*%?a-zA-Z]+'
+
+_relisodate = ('(?:(?:' + _relany + '|' + _year + '|' + _relyear + ')-' +
+               '(?:' + _relany + '|' + _month + '|' + _relmonth + ')-' +
+               '(?:' + _relany + '|' + _day + '|' + _relday + '))')
+
+_asctime = ('(?:'+ _litday + ',? )? *' +
+                _usday + ' *' +
+                '[- ] *(?:' + _litmonth + '|'+ _month +') *[- ]' +
+                '(?:[0-9: ]+)' +
+                _year_epoch + '?')
+
+_relisotime = ('(?:(?:' + _relany + '|' + _hour + '|' + _relhours + '):' +
+               '(?:' + _relany + '|' + _minute + '|' + _relminutes + ')' +
+               '(?::(?:' + _relany + '|' + _second + '|' + _relseconds + '))?)')
+
+_isodelta1 = (_sign + '?' +
+              _days + ':' + _hours + ':' + _minutes + ':' + _seconds)
+_isodelta2 = (_sign + '?' +
+              _hours + ':' + _minutes + ':' + _seconds)
+_isodelta3 = (_sign + '?' +
+              _hours + ':' + _minutes)
+_litdelta = (_sign + '?' +
+             '(?:' + _days + ' *d[a-z]*[,; ]*)?' +
+             '(?:' + _hours + ' *h[a-z]*[,; ]*)?' +
+             '(?:' + _minutes + ' *m[a-z]*[,; ]*)?' +
+             '(?:' + _seconds + ' *s[a-z]*[,; ]*)?')
+_litdelta2 = (_sign + '?' +
+             '(?:' + _days + ' *d[a-z]*[,; ]*)?' +
+              _hours + ':' + _minutes + '(?::' + _seconds + ')?')
+
+_timeRE = re.compile(_time, re.I)
+_isotimeRE = re.compile(_isotime, re.I)
+_isodateRE = re.compile(_isodate, re.I)
+_altisodateRE = re.compile(_altisodate, re.I)
+_usisodateRE = re.compile(_usisodate, re.I)
+_yeardateRE = re.compile(_yeardate, re.I)
+_eurodateRE = re.compile(_eurodate, re.I)
+_usdateRE = re.compile(_usdate, re.I)
+_altusdateRE = re.compile(_altusdate, re.I)
+_litdateRE = re.compile(_litdate, re.I)
+_altlitdateRE = re.compile(_altlitdate, re.I)
+_eurlitdateRE = re.compile(_eurlitdate, re.I)
+_relisodateRE = re.compile(_relisodate, re.I)
+_asctimeRE = re.compile(_asctime, re.I)
+_isodelta1RE = re.compile(_isodelta1)
+_isodelta2RE = re.compile(_isodelta2)
+_isodelta3RE = re.compile(_isodelta3)
+_litdeltaRE = re.compile(_litdelta)
+_litdelta2RE = re.compile(_litdelta2)
+_relisotimeRE = re.compile(_relisotime, re.I)
+
+# Available date parsers
+_date_formats = ('euro',
+                 'usiso', 'us', 'altus',
+                 'iso', 'altiso',
+                 'lit', 'altlit', 'eurlit',
+                 'year', 'unknown')
+
+# Available time parsers
+_time_formats = ('standard',
+                 'iso',
+                 'unknown')
+
+_zoneoffset = ('(?:'
+              '(?P<zonesign>[+-])?'
+              '(?P<hours>\d\d?)'
+              ':?'
+              '(?P<minutes>\d\d)?'
+              '(?P<extra>\d+)?'
+              ')'
+              )
+
+_zoneoffsetRE = re.compile(_zoneoffset)
+
+_zonetable = {
+              # Timezone abbreviations
+              # Std     Summer
+
+              # Standards
+              'UT':0,
+              'UTC':0,
+              'GMT':0,
+
+              # A few common timezone abbreviations
+              'CET':1,  'CEST':2, 'CETDST':2, # Central European
+              'MET':1,  'MEST':2, 'METDST':2, # Mean European
+              'MEZ':1,  'MESZ':2,             # Mitteleuropische Zeit
+              'EET':2,  'EEST':3, 'EETDST':3, # Eastern Europe
+              'WET':0,  'WEST':1, 'WETDST':1, # Western Europe
+              'MSK':3,  'MSD':4,  # Moscow
+              'IST':5.5,          # India
+              'JST':9,            # Japan
+              'KST':9,            # Korea
+              'HKT':8,            # Hong Kong
+
+              # US time zones
+              'AST':-4, 'ADT':-3, # Atlantic
+              'EST':-5, 'EDT':-4, # Eastern
+              'CST':-6, 'CDT':-5, # Central
+              'MST':-7, 'MDT':-6, # Midwestern
+              'PST':-8, 'PDT':-7, # Pacific
+
+              # Australian time zones
+              'CAST':9.5, 'CADT':10.5, # Central
+              'EAST':10,  'EADT':11,   # Eastern
+              'WAST':8,   'WADT':9,    # Western
+              'SAST':9.5, 'SADT':10.5, # Southern
+
+              # US military time zones
+              'Z': 0,
+              'A': 1,
+              'B': 2,
+              'C': 3,
+              'D': 4,
+              'E': 5,
+              'F': 6,
+              'G': 7,
+              'H': 8,
+              'I': 9,
+              'K': 10,
+              'L': 11,
+              'M': 12,
+              'N':-1,
+              'O':-2,
+              'P':-3,
+              'Q':-4,
+              'R':-5,
+              'S':-6,
+              'T':-7,
+              'U':-8,
+              'V':-9,
+              'W':-10,
+              'X':-11,
+              'Y':-12
+              }
+
+
+def utc_offset(zone):
+    """ utc_offset(zonestring)
+
+        Return the UTC time zone offset in minutes.
+
+        zone must be string and can either be given as +-HH:MM,
+        +-HHMM, +-HH numeric offset or as time zone
+        abbreviation. Daylight saving time must be encoded into the
+        zone offset.
+
+        Timezone abbreviations are treated case-insensitive.
+
+    """
+    if not zone:
+        return 0
+    uzone = zone.upper()
+    if uzone in _zonetable:
+        return _zonetable[uzone]*60
+    offset = _zoneoffsetRE.match(zone)
+    if not offset:
+        raise ValueError,'wrong format or unkown time zone: "%s"' % zone
+    zonesign,hours,minutes,extra = offset.groups()
+    if extra:
+        raise ValueError,'illegal time zone offset: "%s"' % zone
+    offset = int(hours or 0) * 60 + int(minutes or 0)
+    if zonesign == '-':
+        offset = -offset
+    return offset
+
+def add_century(year):
+
+    """ Sliding window approach to the Y2K problem: adds a suitable
+        century to the given year and returns it as integer.
+
+        The window used depends on the current year. If adding the current
+        century to the given year gives a year within the range
+        current_year-70...current_year+30 [both inclusive], then the
+        current century is added. Otherwise the century (current + 1 or
+        - 1) producing the least difference is chosen.
+
+    """
+
+    current_year=dt.datetime.now().year
+    current_century=(dt.datetime.now().year / 100) * 100
+
+    if year > 99:
+        # Take it as-is
+        return year
+    year = year + current_century
+    diff = year - current_year
+    if diff >= -70 and diff <= 30:
+        return year
+    elif diff < -70:
+        return year + 100
+    else:
+        return year - 100
+
+
+def _parse_date(text):
+    """
+    Parses the date part given in text and returns a tuple
+    (text,day,month,year,style) with the following meanings:
+
+    * text gives the original text without the date part
+
+    * day,month,year give the parsed date
+
+    * style gives information about which parser was successful:
+      'euro' - the European date parser
+      'us' - the US date parser
+      'altus' - the alternative US date parser (with '-' instead of '/')
+      'iso' - the ISO date parser
+      'altiso' - the alternative ISO date parser (without '-')
+      'usiso' - US style ISO date parser (yyyy/mm/dd)
+      'lit' - the US literal date parser
+      'altlit' - the alternative US literal date parser
+      'eurlit' - the Eurpean literal date parser
+      'unknown' - no date part was found, defaultdate was used
+
+    Formats may be set to a tuple of style strings specifying which of the above
+    parsers to use and in which order to try them.
+    Default is to try all of them in the above order.
+
+    ``defaultdate`` provides the defaults to use in case no date part is found.
+    Most other parsers default to the current year January 1 if some of these
+    date parts are missing.
+
+    If ``'unknown'`` is not given in formats and the date cannot be parsed,
+    a :exc:`ValueError` is raised.
+
+    """
+    match = None
+    style = ''
+
+    formats = _date_formats
+
+    us_formats=('us', 'altus')
+    iso_formats=('iso', 'altiso', 'usiso')
+
+    now=dt.datetime.now
+
+    # Apply parsers in the order given in formats
+    for format in formats:
+
+        if format == 'euro':
+            # European style date
+            match = _eurodateRE.search(text)
+            if match is not None:
+                day,month,year,epoch = match.groups()
+                if year:
+                    if len(year) == 2:
+                        # Y2K problem:
+                        year = add_century(int(year))
+                    else:
+                        year = int(year)
+                else:
+                    defaultdate = now()
+                    year = defaultdate.year
+                if epoch and 'B' in epoch:
+                    year = -year + 1
+                month = int(month)
+                day = int(day)
+                # Could have mistaken euro format for us style date
+                # which uses month, day order
+                if month > 12 or month == 0:
+                    match = None
+                    continue
+                break
+
+        elif format == 'year':
+            # just a year specified
+            match = _yeardateRE.match(text)
+            if match is not None:
+                year = match.groups()[0]
+                if year:
+                    if len(year) == 2:
+                        # Y2K problem:
+                        year = add_century(int(year))
+                    else:
+                        year = int(year)
+                else:
+                    defaultdate = now()
+                    year = defaultdate.year
+                day = 1
+                month = 1
+                break
+
+        elif format in iso_formats:
+            # ISO style date
+            if format == 'iso':
+                match = _isodateRE.search(text)
+            elif format == 'altiso':
+                match = _altisodateRE.search(text)
+                # Avoid mistaking ISO time parts ('Thhmmss') for dates
+                if match is not None:
+                    left, right = match.span()
+                    if left > 0 and \
+                       text[left - 1:left] == 'T':
+                        match = None
+                        continue
+            else:
+                match = _usisodateRE.search(text)
+            if match is not None:
+                year,month,day = match.groups()
+                if len(year) == 2:
+                    # Y2K problem:
+                    year = add_century(int(year))
+                else:
+                    year = int(year)
+                # Default to January 1st
+                if not month:
+                    month = 1
+                else:
+                    month = int(month)
+                if not day:
+                    day = 1
+                else:
+                    day = int(day)
+                break
+
+        elif format in us_formats:
+            # US style date
+            if format == 'us':
+                match = _usdateRE.search(text)
+            else:
+                match = _altusdateRE.search(text)
+            if match is not None:
+                month,day,year,epoch = match.groups()
+                if year:
+                    if len(year) == 2:
+                        # Y2K problem:
+                        year = add_century(int(year))
+                    else:
+                        year = int(year)
+                else:
+                    defaultdate = now()
+                    year = defaultdate.year
+                if epoch and 'B' in epoch:
+                    year = -year + 1
+                # Default to 1 if no day is given
+                if day:
+                    day = int(day)
+                else:
+                    day = 1
+                month = int(month)
+                # Could have mistaken us format for euro style date
+                # which uses day, month order
+                if month > 12 or month == 0:
+                    match = None
+                    continue
+                break
+
+        elif format == 'lit':
+            # US style literal date
+            match = _litdateRE.search(text)
+            if match is not None:
+                litday,day,litmonth,month,year,epoch = match.groups()
+                break
+
+        elif format == 'altlit':
+            # Alternative US style literal date
+            match = _altlitdateRE.search(text)
+            if match is not None:
+                litday,litmonth,day,year,epoch = match.groups()
+                month = '<missing>'
+                break
+
+        elif format == 'eurlit':
+            # European style literal date
+            match = _eurlitdateRE.search(text)
+            if match is not None:
+                litday,day,litmonth,year,epoch = match.groups()
+                month = '<missing>'
+                break
+
+        elif format == 'unknown':
+            # No date part: use defaultdate
+            defaultdate = now()
+            year = defaultdate.year
+            month = defaultdate.month
+            day = defaultdate.day
+            style = format
+            break
+
+    # Check success
+    if match is not None:
+        # Remove date from text
+        left, right = match.span()
+        if 0 and _debug:
+            print 'parsed date:',repr(text[left:right]),\
+                  'giving:',year,month,day
+        text = text[:left] + text[right:]
+        style = format
+
+    elif not style:
+        # Not recognized: raise an error
+        raise ValueError, 'unknown date format: "%s"' % text
+
+    # Literal date post-processing
+    if style in ('lit', 'altlit', 'eurlit'):
+        if 0 and _debug: print match.groups()
+        # Default to current year, January 1st
+        if not year:
+            defaultdate = now()
+            year = defaultdate.year
+        else:
+            if len(year) == 2:
+                # Y2K problem:
+                year = add_century(int(year))
+            else:
+                year = int(year)
+        if epoch and 'B' in epoch:
+            year = -year + 1
+        if litmonth:
+            litmonth = litmonth.lower()
+            try:
+                month = litmonthtable[litmonth]
+            except KeyError:
+                raise ValueError,\
+                      'wrong month name: "%s"' % litmonth
+        elif month:
+            month = int(month)
+        else:
+            month = 1
+        if day:
+            day = int(day)
+        else:
+            day = 1
+
+    #print '_parse_date:',text,day,month,year,style
+    return text,day,month,year,style
+
+def _parse_time(text):
+
+    """ Parses a time part given in text and returns a tuple
+        (text,hour,minute,second,offset,style) with the following
+        meanings:
+
+        * text gives the original text without the time part
+        * hour,minute,second give the parsed time
+        * offset gives the time zone UTC offset
+        * style gives information about which parser was successful:
+          'standard' - the standard parser
+          'iso' - the ISO time format parser
+          'unknown' - no time part was found
+
+        formats may be set to a tuple specifying the parsers to use:
+          'standard' - standard time format with ':' delimiter
+          'iso' - ISO time format (superset of 'standard')
+          'unknown' - default to 0:00:00, 0 zone offset
+
+        If 'unknown' is not given in formats and the time cannot be
+        parsed, a ValueError is raised.
+
+    """
+    match = None
+    style = ''
+
+    formats=_time_formats
+
+    # Apply parsers in the order given in formats
+    for format in formats:
+
+        # Standard format
+        if format == 'standard':
+            match = _timeRE.search(text)
+            if match is not None:
+                hour,minute,second,ampm,zone = match.groups()
+                style = 'standard'
+                break
+
+        # ISO format
+        if format == 'iso':
+            match =  _isotimeRE.search(text)
+            if match is not None:
+                hour,minute,second,zone = match.groups()
+                ampm = None
+                style = 'iso'
+                break
+
+        # Default handling
+        elif format == 'unknown':
+            hour,minute,second,offset = 0,0,0.0,0
+            style = 'unknown'
+            break
+
+    if not style:
+        # If no default handling should be applied, raise an error
+        raise ValueError, 'unknown time format: "%s"' % text
+
+    # Post-processing
+    if match is not None:
+
+        if zone:
+            # Convert to UTC offset
+            offset = utc_offset(zone)
+        else:
+            offset = 0
+
+        hour = int(hour)
+        if ampm:
+            if ampm[0] in ('p', 'P'):
+                # 12pm = midday
+                if hour < 12:
+                    hour = hour + 12
+            else:
+                # 12am = midnight
+                if hour >= 12:
+                    hour = hour - 12
+        if minute:
+            minute = int(minute)
+        else:
+            minute = 0
+        if not second:
+            second = 0.0
+        else:
+            if ',' in second:
+                second = second.replace(',', '.')
+            second = float(second)
+
+        # Remove time from text
+        left,right = match.span()
+        if 0 and _debug:
+            print 'parsed time:',repr(text[left:right]),\
+                  'giving:',hour,minute,second,offset
+        text = text[:left] + text[right:]
+
+    #print '_parse_time:',text,hour,minute,second,offset,style
+    return text,hour,minute,second,offset,style
+
+###
+
+def DateTimeFromString(text):
+
+    """ DateTimeFromString(text, [formats, defaultdate])
+
+        Returns a datetime instance reflecting the date and time given
+        in text. In case a timezone is given, the returned instance
+        will point to the corresponding UTC time value. Otherwise, the
+        value is set as given in the string.
+
+        formats may be set to a tuple of strings specifying which of
+        the following parsers to use and in which order to try
+        them. Default is to try all of them in the order given below:
+
+          'euro' - the European date parser
+          'us' - the US date parser
+          'altus' - the alternative US date parser (with '-' instead of '/')
+          'iso' - the ISO date parser
+          'altiso' - the alternative ISO date parser (without '-')
+          'usiso' - US style ISO date parser (yyyy/mm/dd)
+          'lit' - the US literal date parser
+          'altlit' - the alternative US literal date parser
+          'eurlit' - the Eurpean literal date parser
+          'unknown' - if no date part is found, use defaultdate
+
+        defaultdate provides the defaults to use in case no date part
+        is found. Most of the parsers default to the current year
+        January 1 if some of these date parts are missing.
+
+        If 'unknown' is not given in formats and the date cannot
+        be parsed, a ValueError is raised.
+
+        time_formats may be set to a tuple of strings specifying which
+        of the following parsers to use and in which order to try
+        them. Default is to try all of them in the order given below:
+
+          'standard' - standard time format HH:MM:SS (with ':' delimiter)
+          'iso' - ISO time format (superset of 'standard')
+          'unknown' - default to 00:00:00 in case the time format
+                      cannot be parsed
+
+        Defaults to 00:00:00.00 for time parts that are not included
+        in the textual representation.
+
+        If 'unknown' is not given in time_formats and the time cannot
+        be parsed, a ValueError is raised.
+
+    """
+    origtext = text
+
+    text,hour,minute,second,offset,timestyle = _parse_time(origtext)
+    text,day,month,year,datestyle = _parse_date(text)
+
+    if 0 and _debug:
+        print 'tried time/date on %s, date=%s, time=%s' % (origtext,
+                                                           datestyle,
+                                                           timestyle)
+
+    # If this fails, try the ISO order (date, then time)
+    if timestyle in ('iso', 'unknown'):
+        text,day,month,year,datestyle = _parse_date(origtext)
+        text,hour,minute,second,offset,timestyle = _parse_time(text)
+        if 0 and _debug:
+            print 'tried ISO on %s, date=%s, time=%s' % (origtext,
+                                                         datestyle,
+                                                         timestyle)
+
+    try:
+        microsecond = int(1000000 * (second % 1))
+        second = int(second)
+        return dt.datetime(year,month,day,hour,minute,second, microsecond) - \
+                                        dt.timedelta(minutes=offset)
+    except ValueError, why:
+        raise RangeError,\
+              'Failed to parse "%s": %s' % (origtext, why)
+
+def DateFromString(text):
+
+    """ DateFromString(text, [formats, defaultdate])
+
+        Returns a datetime instance reflecting the date given in
+        text. A possibly included time part is ignored.
+
+        formats and defaultdate work just like for
+        DateTimeFromString().
+
+    """
+    _text,day,month,year,datestyle = _parse_date(text)
+
+    try:
+        return dt.datetime(year,month,day)
+    except ValueError, why:
+        raise RangeError,\
+              'Failed to parse "%s": %s' % (text, why)
+
+def validateDateTimeString(text):
+
+    """ validateDateTimeString(text, [formats, defaultdate])
+
+        Validates the given text and returns 1/0 depending on whether
+        text includes parseable date and time values or not.
+
+        formats works just like for DateTimeFromString() and defines
+        the order of date/time parsers to apply. It defaults to the
+        same list of parsers as for DateTimeFromString().
+
+        XXX Undocumented !
+
+    """
+    try:
+        DateTimeFromString(text)
+    except ValueError, why:
+        return 0
+    return 1
+
+
+def validateDateString(text):
+
+    """ validateDateString(text, [formats, defaultdate])
+
+        Validates the given text and returns 1/0 depending on whether
+        text includes a parseable date value or not.
+
+        formats works just like for DateTimeFromString() and defines
+        the order of date/time parsers to apply. It defaults to the
+        same list of parsers as for DateTimeFromString().
+
+        XXX Undocumented !
+
+    """
+    try:
+        DateFromString(text)
+    except ValueError, why:
+        return 0
+    return 1
+
+### Tests
+
+def _test():
+
+    import sys
+
+    t = dt.datetime.now()
+    _date = t.strftime('%Y-%m-%d')
+
+    print 'Testing DateTime Parser...'
+
+    l = [
+
+        # Literal formats
+        ('Sun Nov  6 08:49:37 1994', '1994-11-06 08:49:37.00'),
+        ('sun nov  6 08:49:37 1994', '1994-11-06 08:49:37.00'),
+        ('sUN NOV  6 08:49:37 1994', '1994-11-06 08:49:37.00'),
+        ('Sunday, 06-Nov-94 08:49:37 GMT', '1994-11-06 08:49:37.00'),
+        ('Sun, 06 Nov 1994 08:49:37 GMT', '1994-11-06 08:49:37.00'),
+        ('06-Nov-94 08:49:37', '1994-11-06 08:49:37.00'),
+        ('06-Nov-94', '1994-11-06 00:00:00.00'),
+        ('06-NOV-94', '1994-11-06 00:00:00.00'),
+        ('November 19 08:49:37', '%s-11-19 08:49:37.00' % t.year),
+        ('Nov. 9', '%s-11-09 00:00:00.00' % t.year),
+        ('Sonntag, der 6. November 1994, 08:49:37 GMT', '1994-11-06 08:49:37.00'),
+        ('6. November 2001, 08:49:37', '2001-11-06 08:49:37.00'),
+        ('sep 6', '%s-09-06 00:00:00.00' % t.year),
+        ('sep 6 2000', '2000-09-06 00:00:00.00'),
+        ('September 29', '%s-09-29 00:00:00.00' % t.year),
+        ('Sep. 29', '%s-09-29 00:00:00.00' % t.year),
+        ('6 sep', '%s-09-06 00:00:00.00' % t.year),
+        ('29 September', '%s-09-29 00:00:00.00' % t.year),
+        ('29 Sep.', '%s-09-29 00:00:00.00' % t.year),
+        ('sep 6 2001', '2001-09-06 00:00:00.00'),
+        ('Sep 6, 2001', '2001-09-06 00:00:00.00'),
+        ('September 6, 2001', '2001-09-06 00:00:00.00'),
+        ('sep 6 01', '2001-09-06 00:00:00.00'),
+        ('Sep 6, 01', '2001-09-06 00:00:00.00'),
+        ('September 6, 01', '2001-09-06 00:00:00.00'),
+        ('30 Apr 2006 20:19:00', '2006-04-30 20:19:00.00'),
+
+        # ISO formats
+        ('1994-11-06 08:49:37', '1994-11-06 08:49:37.00'),
+        ('010203', '2001-02-03 00:00:00.00'),
+        ('2001-02-03 00:00:00.00', '2001-02-03 00:00:00.00'),
+        ('2001-02 00:00:00.00', '2001-02-01 00:00:00.00'),
+        ('2001-02-03', '2001-02-03 00:00:00.00'),
+        ('2001-02', '2001-02-01 00:00:00.00'),
+        ('20000824/2300', '2000-08-24 23:00:00.00'),
+        ('20000824/0102', '2000-08-24 01:02:00.00'),
+        ('20000824', '2000-08-24 00:00:00.00'),
+        ('20000824/020301', '2000-08-24 02:03:01.00'),
+        ('20000824 020301', '2000-08-24 02:03:01.00'),
+        ('20000824T020301', '2000-08-24 02:03:01.00'),
+        ('20000824 020301', '2000-08-24 02:03:01.00'),
+        ('2000-08-24 02:03:01.00', '2000-08-24 02:03:01.00'),
+        ('T020311', '%s 02:03:11.00' % _date),
+        ('2003-12-9', '2003-12-09 00:00:00.00'),
+        ('03-12-9', '2003-12-09 00:00:00.00'),
+        ('003-12-9', '0003-12-09 00:00:00.00'),
+        ('0003-12-9', '0003-12-09 00:00:00.00'),
+        ('2003-1-9', '2003-01-09 00:00:00.00'),
+        ('03-1-9', '2003-01-09 00:00:00.00'),
+        ('003-1-9', '0003-01-09 00:00:00.00'),
+        ('0003-1-9', '0003-01-09 00:00:00.00'),
+
+        # US formats
+        ('06/11/94 08:49:37', '1994-06-11 08:49:37.00'),
+        ('11/06/94 08:49:37', '1994-11-06 08:49:37.00'),
+        ('9/23/2001', '2001-09-23 00:00:00.00'),
+        ('9-23-2001', '2001-09-23 00:00:00.00'),
+        ('9/6', '%s-09-06 00:00:00.00' % t.year),
+        ('09/6', '%s-09-06 00:00:00.00' % t.year),
+        ('9/06', '%s-09-06 00:00:00.00' % t.year),
+        ('09/06', '%s-09-06 00:00:00.00' % t.year),
+        ('9/6/2001', '2001-09-06 00:00:00.00'),
+        ('09/6/2001', '2001-09-06 00:00:00.00'),
+        ('9/06/2001', '2001-09-06 00:00:00.00'),
+        ('09/06/2001', '2001-09-06 00:00:00.00'),
+        ('9-6-2001', '2001-09-06 00:00:00.00'),
+        ('09-6-2001', '2001-09-06 00:00:00.00'),
+        ('9-06-2001', '2001-09-06 00:00:00.00'),
+        ('09-06-2001', '2001-09-06 00:00:00.00'),
+        ('2002/05/28 13:10:56.1147 GMT+2', '2002-05-28 13:10:56.114699'),
+        ('1970/01/01', '1970-01-01 00:00:00.00'),
+        ('20021025 12:00 PM', '2002-10-25 12:00:00.00'),
+        ('20021025 12:30 PM', '2002-10-25 12:30:00.00'),
+        ('20021025 12:00 AM', '2002-10-25 00:00:00.00'),
+        ('20021025 12:30 AM', '2002-10-25 00:30:00.00'),
+        ('20021025 1:00 PM', '2002-10-25 13:00:00.00'),
+        ('20021025 2:00 AM', '2002-10-25 02:00:00.00'),
+        ('Thursday, February 06, 2003 12:40 PM', '2003-02-06 12:40:00.00'),
+        ('Mon, 18 Sep 2006 23:03:00', '2006-09-18 23:03:00.00'),
+
+        # European formats
+        ('6.11.2001, 08:49:37', '2001-11-06 08:49:37.00'),
+        ('06.11.2001, 08:49:37', '2001-11-06 08:49:37.00'),
+        ('06.11. 08:49:37', '%s-11-06 08:49:37.00' % t.year),
+        #('21/12/2002', '2002-12-21 00:00:00.00'),
+        #('21/08/2002', '2002-08-21 00:00:00.00'),
+        #('21-08-2002', '2002-08-21 00:00:00.00'),
+        #('13/01/03', '2003-01-13 00:00:00.00'),
+        #('13/1/03', '2003-01-13 00:00:00.00'),
+        #('13/1/3', '2003-01-13 00:00:00.00'),
+        #('13/01/3', '2003-01-13 00:00:00.00'),
+
+        # Time only formats
+        ('01:03', '%s 01:03:00.00' % _date),
+        ('01:03:11', '%s 01:03:11.00' % _date),
+        ('01:03:11.50', '%s 01:03:11.500000' % _date),
+        ('01:03:11.50 AM', '%s 01:03:11.500000' % _date),
+        ('01:03:11.50 PM', '%s 13:03:11.500000' % _date),
+        ('01:03:11.50 a.m.', '%s 01:03:11.500000' % _date),
+        ('01:03:11.50 p.m.', '%s 13:03:11.500000' % _date),
+
+        # Invalid formats
+        ('6..2001, 08:49:37', '%s 08:49:37.00' % _date),
+        ('9//2001', 'ignore'),
+        ('06--94 08:49:37', 'ignore'),
+        ('20-03 00:00:00.00', 'ignore'),
+        ('9/2001', 'ignore'),
+        ('9-6', 'ignore'),
+        ('09-6', 'ignore'),
+        ('9-06', 'ignore'),
+        ('09-06', 'ignore'),
+        ('20000824/23', 'ignore'),
+        ('November 1994 08:49:37', 'ignore'),
+        ]
+
+    # Add Unicode versions
+    try:
+        unicode
+    except NameError:
+        pass
+    else:
+        k = []
+        for text, result in l:
+            k.append((unicode(text), result))
+        l.extend(k)
+
+    for text, reference in l:
+        try:
+            value = DateTimeFromString(text)
+        except:
+            if reference is None:
+                continue
+            else:
+                value = str(sys.exc_info()[1])
+        valid_datetime = validateDateTimeString(text)
+        valid_date = validateDateString(text)
+
+        if reference[-3:] == '.00': reference = reference[:-3]
+
+        if str(value) != reference and \
+           not reference == 'ignore':
+            print 'Failed to parse "%s"' % text
+            print '  expected: %s' % (reference or '<exception>')
+            print '  parsed:   %s' % value
+        elif _debug:
+            print 'Parsed "%s" successfully' % text
+        if _debug:
+            if not valid_datetime:
+                print '  "%s" failed date/time validation' % text
+            if not valid_date:
+                print '  "%s" failed date validation' % text
+
+    et = dt.datetime.now()
+    print 'done. (after %f seconds)' % ((et-t).seconds)
+
+if __name__ == '__main__':
+    _test()
diff --git a/pandas/timeseries/setup.py b/pandas/timeseries/setup.py
new file mode 100644
index 000000000..e0f5128ad
--- /dev/null
+++ b/pandas/timeseries/setup.py
@@ -0,0 +1,28 @@
+
+__revision__ = "$Revision$"
+__date__     = '$Date$'
+
+import os
+from os.path import join
+
+def configuration(parent_package='',top_path=None):
+    from numpy.distutils.misc_util import Configuration, get_numpy_include_dirs
+    nxheader = join(get_numpy_include_dirs()[0],'numpy',)
+    confgr = Configuration('timeseries',parent_package,top_path)
+    sources = [join('src', x) for x in ('c_lib.c',
+                                        'c_dates.c',
+                                        'c_tseries.c',
+                                        'pandas._skts.c')]
+    confgr.add_extension('pandas._skts',
+                         sources=sources,
+                         include_dirs=[nxheader, 'include'])
+
+    confgr.add_subpackage('lib')
+    confgr.add_subpackage('tests')
+    return confgr
+
+if __name__ == "__main__":
+    from numpy.distutils.core import setup
+    #setup.update(nmasetup)
+    config = configuration(top_path='').todict()
+    setup(**config)
diff --git a/pandas/timeseries/tdates.py b/pandas/timeseries/tdates.py
new file mode 100644
index 000000000..510fe10e8
--- /dev/null
+++ b/pandas/timeseries/tdates.py
@@ -0,0 +1,1155 @@
+"""
+Classes definition for the support of individual dates and array of dates.
+
+:author: Pierre GF Gerard-Marchant & Matt Knox
+:contact: pierregm_at_uga_dot_edu - mattknox_ca_at_hotmail_dot_com
+
+"""
+
+# TODO: Implement DateArray in C (Cython ?)
+# TODO: Optimize the cache of DateArray (steps computation, sorting...)
+# TODO: Optimize date_array / _listparser : sort only at the end ?
+# TODO: __getitem__ w/ slice and ischrono : get the proper steps...
+
+__author__ = "Pierre GF Gerard-Marchant & Matt Knox"
+__revision__ = "$Revision$"
+__date__ = '$Date$'
+
+import datetime as dt
+
+import warnings
+
+import numpy as np
+from numpy import ndarray
+import numpy.core.numerictypes as ntypes
+from numpy.core.numerictypes import generic
+
+from parser import DateFromString, DateTimeFromString
+
+import const as _c
+import pandas._skts
+# initialize python callbacks for C code
+pandas._skts.set_callback_DateFromString(DateFromString)
+pandas._skts.set_callback_DateTimeFromString(DateTimeFromString)
+from pandas._skts import Date, now, check_freq, check_freq_str, get_freq_group, \
+                    DateCalc_Error, DateCalc_RangeError
+
+__all__ = ['ArithmeticDateError',
+           'Date', 'DateArray', 'DateCalc_Error', 'DateCalc_RangeError',
+           'DateError',
+           'FrequencyDateError',
+           'InsufficientDateError',
+           'check_freq', 'check_freq_str', 'convert_to_float',
+           'date_array', 'day', 'day_of_year',
+           'get_freq_group',
+           'hour',
+           'minute', 'month',
+           'nodates', 'now',
+           'period_break', 'prevbusday',
+           'quarter',
+           'second',
+           'weekday',
+           'week',
+           'year',
+          ]
+
+#####---------------------------------------------------------------------------
+#---- --- Date Exceptions ---
+#####---------------------------------------------------------------------------
+class DateError(Exception):
+    """
+    Defines a generic DateArrayError.
+    """
+    def __init__ (self, value=None):
+        "Creates an exception."
+        self.value = value
+    def __str__(self):
+        "Calculates the string representation."
+        return str(self.value)
+    __repr__ = __str__
+
+class InsufficientDateError(DateError):
+    """
+    Defines the exception raised when there is not enough information
+    to create a Date object.
+    """
+    def __init__(self, msg=None):
+        if msg is None:
+            msg = "Insufficient parameters to create a date "\
+                  "at the given frequency."
+        DateError.__init__(self, msg)
+
+class FrequencyDateError(DateError):
+    """
+    Defines the exception raised when the frequencies are incompatible.
+    """
+    def __init__(self, msg, freql=None, freqr=None):
+        msg += " : Incompatible frequencies!"
+        if not (freql is None or freqr is None):
+            msg += " (%s<>%s)" % (freql, freqr)
+        DateError.__init__(self, msg)
+
+class ArithmeticDateError(DateError):
+    """
+    Defines the exception raised when dates are used in arithmetic expressions.
+    """
+    def __init__(self, msg=''):
+        msg += " Cannot use dates for arithmetics!"
+        DateError.__init__(self, msg)
+
+
+#####---------------------------------------------------------------------------
+#---- --- Functions ---
+#####---------------------------------------------------------------------------
+
+def prevbusday(day_end_hour=18, day_end_min=0):
+    """
+    Returns the previous business day (Monday-Friday) at business frequency.
+
+    Parameters
+    ----------
+    day_end_hour : {18, int}, optional
+        Hour of the end of a business day.
+    day_end_min : {0, int}, optional
+        Minutes of the end of a business day.
+
+    Notes
+    -----
+    If it is currently Saturday or Sunday, then the preceding Friday will be
+    returned.
+    If it is later than the specified ``day_end_hour`` and ``day_end_min``,
+    ``now('Business')`` will be returned.
+    Otherwise, ``now('Business')-1`` will be returned.
+
+    """
+    tempDate = dt.datetime.now()
+    dateNum = tempDate.hour + float(tempDate.minute) / 60
+    checkNum = day_end_hour + float(day_end_min) / 60
+    if dateNum < checkNum and tempDate.weekday() < 5:
+        return now(_c.FR_BUS) - 1
+    else:
+        return now(_c.FR_BUS)
+
+#####---------------------------------------------------------------------------
+#---- --- DateArray ---
+#####---------------------------------------------------------------------------
+def _check_chronological_order(dates):
+    """
+    Checks whether dates are in the chronological order.
+    If not, output the indices corresponding to the chronological order.
+    Otherwise, output None.
+    """
+    idx = dates.argsort()
+    if (idx[1:] - idx[:-1] < 0).any():
+        return idx.view(ndarray)
+    return
+
+
+ufunc_dateOK = ['add', 'subtract',
+                'equal', 'not_equal', 'less', 'less_equal',
+                'greater', 'greater_equal',
+                'isnan']
+
+class _datearithmetics(object):
+    """
+    Defines a wrapper for arithmetic methods.
+    Instead of directly calling a ufunc, the corresponding method of the `._data`
+    object is called instead.
+    If `asdates` is True, a DateArray object is returned , else a regular ndarray
+    is returned.
+    """
+    def __init__ (self, methodname, asdates=True):
+        """
+    Parameters
+    ----------
+    methodname : string
+        Method name.
+    asdates : {True, False}
+        Whether to return a DateArray object (True) or a regular ndarray.
+        """
+        self.methodname = methodname
+        self._asdates = asdates
+        self.__doc__ = getattr(methodname, '__doc__')
+        self.obj = None
+    #
+    def __get__(self, obj, objtype=None):
+        self.obj = obj
+        return self
+    #
+    def __call__ (self, other, *args, **kwargs):
+        "Execute the call behavior."
+        instance = self.obj
+        freq = instance.freq
+        if 'context' not in kwargs:
+            kwargs['context'] = 'DateOK'
+        method = getattr(super(DateArray, instance), self.methodname)
+        other_val = other
+        if isinstance(other, DateArray):
+            if other.freq != freq:
+                raise FrequencyDateError("Cannot operate on dates", \
+                                         freq, other.freq)
+        elif isinstance(other, Date):
+            if other.freq != freq:
+                raise FrequencyDateError("Cannot operate on dates", \
+                                         freq, other.freq)
+            other_val = other.value
+        elif isinstance(other, ndarray):
+            if other.dtype.kind not in ['i', 'f']:
+                raise ArithmeticDateError
+        if self._asdates and not isinstance(other, (DateArray, Date)):
+            return instance.__class__(method(other_val, *args),
+                                      freq=freq)
+        else:
+            return method(other_val, *args).view(np.ndarray)
+
+
+
+class DateArray(ndarray):
+    """
+    Defines a ndarray of dates, as ordinals.
+
+    When viewed globally (array-wise), ``DateArray`` is an array of integers.
+    When viewed element-wise, ``DateArray`` is a sequence of dates.
+    For example, a test such as :
+
+    >>> DateArray(...) == value
+
+    will be valid only if value is an integer, not a :class:`Date` object.
+    However, a loop such as :
+
+    >>> for d in DateArray(...):
+
+    accesses the array element by element. Therefore, `d` is a :class:`Date` object.
+    """
+
+    def __new__(cls, dates=None, freq=None, copy=False):
+        # Get the frequency ......
+        if freq is None:
+            _freq = getattr(dates, 'freq', _c.FR_UND)
+        else:
+            _freq = check_freq(freq)
+        # Get the dates ..........
+        _dates = np.array(dates, copy=copy, dtype=int, subok=1)
+        if _dates.ndim == 0:
+            _dates.shape = (1,)
+        _dates = _dates.view(cls)
+        _dates.freq = _freq
+        #
+        _cached = _dates._cachedinfo
+        if _cached['ischrono'] is None:
+            sortidx = _dates.__array__().ravel().argsort()
+            sortflag = (sortidx == np.arange(_dates.size)).all()
+            if sortflag:
+                _cached['chronidx'] = None
+            else:
+                _cached['chronidx'] = sortidx
+            _cached['ischrono'] = sortflag
+        return _dates
+
+    def _reset_cachedinfo(self):
+        "Reset the internal cache information"
+        self._cachedinfo = dict(toobj=None, tostr=None, toord=None,
+                                steps=None, full=None, hasdups=None,
+                                chronidx=None, ischrono=None)
+
+    def __array_wrap__(self, obj, context=None):
+        if context is None:
+            return self
+        elif context[0].__name__ not in ufunc_dateOK:
+            raise ArithmeticDateError, "(function %s)" % context[0].__name__
+
+    def __array_finalize__(self, obj):
+        self.freq = getattr(obj, 'freq', _c.FR_UND)
+        self._reset_cachedinfo()
+        self._cachedinfo.update(getattr(obj, '_cachedinfo', {}))
+        return
+
+    def _get_unsorted(self):
+        "Returns the indices of the dates in chronological order"
+        chronidx = self._cachedinfo['chronidx']
+        if chronidx is None:
+            flag = self.is_chronological()
+            chronidx = self._cachedinfo['chronidx']
+        if np.size(chronidx) < 1:
+            return None
+        return chronidx
+    def _set_unsorted(self, value):
+        "Sets the indices of the dates in chronological order"
+        self._cachedinfo.update({'chronidx': value})
+    _unsorted = property(fget=_get_unsorted, fset=_set_unsorted)
+
+    def __getitem__(self, indx):
+        reset_full = True
+        keep_chrono = False
+        # Determine what kind of index is used
+        if isinstance(indx, Date):
+            # indx = self.find_dates(indx)
+            # indx = int(self.find_dates(indx)[0])
+            indx = self.date_to_index(indx)
+            reset_full = False
+        elif isinstance(indx, slice):
+            keep_chrono = True
+        elif np.asarray(indx).dtype.kind == 'O':
+            try:
+                indx = self.find_dates(indx)
+            except AttributeError:
+                pass
+
+        # Select the data
+        r = ndarray.__getitem__(self, indx)
+        # Case 1. A simple integer
+        if isinstance(r, (generic, int)):
+            return Date(self.freq, value=r)
+        elif not getattr(r, 'ndim', 1):
+            # need to check if it has a ndim attribute for situations
+            # like when the datearray is the data for a maskedarray
+            # or some other subclass of ndarray with wierd getitem
+            # behaviour
+            return Date(self.freq, value=r.item())
+        else:
+            if hasattr(r, '_cachedinfo'):
+                _cache = r._cachedinfo
+                # Select the appropriate cached representations
+                _cache.update(dict([(k, _cache[k][indx])
+                                    for k in ('toobj', 'tostr', 'toord')
+                                    if _cache[k] is not None]))
+                # Reset the ischrono flag if needed
+                if not (keep_chrono and _cache['ischrono']):
+                    _cache['ischrono'] = None
+                # Reset the sorting indices
+                _cache['chronidx'] = None
+                # Reset the steps
+                _cache['steps'] = None
+                if reset_full:
+                    _cache['full'] = None
+                    _cache['hasdups'] = None
+            return r
+
+    def __getslice__(self, i, j):
+        """
+    Returns a slice of the date_array
+        """
+        return self.__getitem__(slice(i, j))
+
+
+    def __repr__(self):
+        return ndarray.__repr__(self)[:-1] + \
+               ",\n          freq='%s')" % self.freqstr
+
+
+    def __contains__(self, date):
+        """
+    Used to check whether a single date (or equivalent integer value) is
+    contained in the DateArray.
+        """
+        if isinstance(date, Date) and date.freq != self.freq:
+            raise ValueError(
+                "expected date of frequency '%s' but got date of frequency "\
+                "'%s'" % (self.freqstr, date.freqstr))
+        datenum = np.array(date, dtype=self.dtype)
+        if datenum.ndim != 0:
+            raise ValueError("Cannot check containment of multiple dates")
+        return datenum in self.view(np.ndarray)
+
+    #......................................................
+    __add__ = _datearithmetics('__add__', asdates=True)
+    __radd__ = _datearithmetics('__add__', asdates=True)
+    __sub__ = _datearithmetics('__sub__', asdates=True)
+    __rsub__ = _datearithmetics('__rsub__', asdates=True)
+    __le__ = _datearithmetics('__le__', asdates=False)
+    __lt__ = _datearithmetics('__lt__', asdates=False)
+    __ge__ = _datearithmetics('__ge__', asdates=False)
+    __gt__ = _datearithmetics('__gt__', asdates=False)
+    __eq__ = _datearithmetics('__eq__', asdates=False)
+    __ne__ = _datearithmetics('__ne__', asdates=False)
+    #......................................................
+
+    def min(self, *args, **kwargs):
+        """
+    Returns the minimum Date.
+
+    For a description of the input parameters, please refer to numpy.min.
+        """
+        obj = ndarray.min(self, *args, **kwargs)
+        if not obj.shape:
+            return Date(self.freq, obj)
+        return obj
+
+    def max(self, *args, **kwargs):
+        """
+    Returns the maximum Date.
+
+    For a description of the input parameters, please refer to numpy.max.
+        """
+        obj = ndarray.max(self, *args, **kwargs)
+        if not obj.shape:
+            return Date(self.freq, obj)
+        return obj
+
+    @property
+    def freqstr(self):
+        "Returns the frequency string code."
+        return check_freq_str(self.freq)
+
+    @property
+    def year(self):
+        "Returns the corresponding year for each date of the instance."
+        return self.__getdateinfo__('Y')
+    years = year
+    @property
+    def qyear(self):
+        """
+    For quarterly frequency dates, returns the year corresponding to the
+    year end (start) month. When using QTR or QTR-E based quarterly
+    frequencies, this is the fiscal year in a financial context.
+
+    For non-quarterly dates, this simply returns the year of the date.
+
+    """
+        return self.__getdateinfo__('F')
+    qyears = qyear
+    @property
+    def quarter(self):
+        "Returns the corresponding quarter for each date of the instance."
+        return self.__getdateinfo__('Q')
+    quarters = quarter
+    @property
+    def month(self):
+        "Returns the corresponding month for each month of the instance."
+        return self.__getdateinfo__('M')
+    months = month
+    @property
+    def week(self):
+        "Returns the corresponding week for each week of the instance."
+        return self.__getdateinfo__('I')
+    weeks = week
+    @property
+    def day(self):
+        "Returns the corresponding day of month for each date of the instance."
+        return self.__getdateinfo__('D')
+    days = day
+    @property
+    def day_of_week(self):
+        "Returns the corresponding day of week for each date of the instance."
+        return self.__getdateinfo__('W')
+    weekdays = weekday = day_of_week
+    @property
+    def day_of_year(self):
+        "Returns the corresponding day of year for each date of the instance."
+        return self.__getdateinfo__('R')
+    yeardays = day_of_year
+    @property
+    def hour(self):
+        "Returns the corresponding hour for each date of the instance."
+        return self.__getdateinfo__('H')
+    hours = hour
+    @property
+    def minute(self):
+        "Returns the corresponding minute for each date of the instance."
+        return self.__getdateinfo__('T')
+    minutes = minute
+    @property
+    def second(self):
+        "Returns the corresponding second for each date of the instance."
+        return self.__getdateinfo__('S')
+    seconds = second
+
+    def __getdateinfo__(self, info):
+        return np.asarray(pandas._skts.DA_getDateInfo(np.asarray(self),
+                                                 self.freq, info,
+                                                 int(self.is_full())),
+                          dtype=int)
+    __getDateInfo = __getdateinfo__
+
+    #.... Conversion methods ....................
+
+    def tovalues(self):
+        """
+    Converts the instance to a :class:`~numpy.ndarray` of integers.
+
+    The values correspond to the integer representation of the underlying
+    :class:`Date` objects, as controlled by the frequency attribute.
+
+    Examples
+    --------
+    >>> d = ts.date_array(start_date=ts.Date('M', '2001-01'), length=5)
+    >>> d.tovalues()
+    array([24001, 24002, 24003, 24004, 24005])
+
+        """
+        return np.asarray(self)
+    tovalue = values = tovalues
+    #
+    def toordinals(self):
+        """
+    Converts the dates to the corresponding proleptic Gregorian ordinals,
+    and returns a :class:`~numpy.ndarray` of integers.
+
+    Examples
+    --------
+    >>> d = ts.date_array(start_date=ts.Date('M', '2001-01'), length=5)
+    >>> d.toordinals()
+    array([ 730516.,  730544.,  730575.,  730605.,  730636.])
+
+        """
+        # TODO: Why do we need floats ?
+        # Note: we better try to cache the result
+        if self._cachedinfo['toord'] is None:
+            if self.freq == _c.FR_UND:
+                diter = (d.value for d in self)
+            else:
+                diter = (d.toordinal() for d in self)
+            toord = np.fromiter(diter, dtype=float)
+            self._cachedinfo['toord'] = toord
+        return self._cachedinfo['toord']
+    toordinal = toordinals
+    #
+    def tolist(self):
+        """
+    Returns a hierarchical Python list of standard :class:`datetime.datetime`
+    objects corresponding to the dates of the instance.
+
+    If the input is nD, the list will be nested in a such way that
+    ``np.array(self.tolist())`` is also nD and corresponding to the
+    values of the instance.
+
+    Examples
+    --------
+    >>> d = ts.date_array(start_date=ts.Date('M', '2001-01'), length=5)
+    >>> d.tolist()
+    [datetime.datetime(2001, 1, 31, 0, 0),
+     datetime.datetime(2001, 2, 28, 0, 0),
+     datetime.datetime(2001, 3, 31, 0, 0),
+     datetime.datetime(2001, 4, 30, 0, 0),
+     datetime.datetime(2001, 5, 31, 0, 0)]
+    >>> d[:4].reshape(2,2).tolist()
+    [[datetime.datetime(2001, 1, 31, 0, 0), datetime.datetime(2001, 2, 28, 0, 0)],
+     [datetime.datetime(2001, 3, 31, 0, 0), datetime.datetime(2001, 4, 30, 0, 0)]]
+
+        """
+        # We need an array to 
+        _result = np.empty(self.shape, dtype=np.object_)
+        _result.flat = [d.datetime for d in self.ravel()]
+        return _result.tolist()
+    #
+    def tostring(self):
+        """
+    Converts the dates to a :class:`~numpy.ndarray` of strings.
+
+    The format of the strings depends of the frequency of the 
+    instance.
+
+    Examples
+    --------
+    >>> d = ts.date_array(start_date=ts.Date('M', '2001-01'), length=5)
+    >>> d.tostrings()
+    array(['Jan-2001', 'Feb-2001', 'Mar-2001', 'Apr-2001', 'May-2001'], 
+          dtype='|S8')
+
+        """
+        # Note: we better cache the result
+        if self._cachedinfo['tostr'] is None:
+            firststr = str(self[0])
+            if self.size:
+                ncharsize = len(firststr)
+                tostr = np.fromiter((str(d) for d in self),
+                                    dtype='|S%i' % ncharsize)
+            else:
+                tostr = firststr
+            self._cachedinfo['tostr'] = tostr
+        return self._cachedinfo['tostr']
+    #
+    def asfreq(self, freq=None, relation="END"):
+        """
+
+    Converts the dates to another frequency.
+
+    Parameters
+    ----------
+    freq : {freq_spec}
+        Frequency into which :class:`DateArray` must be converted.
+        Accepts any valid frequency specification (string or integer)
+    relation : {"END", "START"} (optional)
+        Applies only when converting a lower frequency :class:`Date` to a higher
+        frequency :class:`Date`, or when converting a weekend :class:`Date` to a business
+        frequency :class:`Date`. Valid values are 'START' and 'END' (or just 'S' and
+        'E' for brevity if you wish).
+
+        For example, if converting a monthly date to a daily date, specifying
+        'START' ('END') would result in the first (last) day in the month.
+
+        """
+        # Note: As we define a new object, we don't need caching
+        if (freq is None) or (freq == _c.FR_UND):
+            return self
+        tofreq = check_freq(freq)
+        if tofreq == self.freq:
+            return self
+
+        relation = relation.upper()
+        if relation not in ('START', 'END', 'S', 'E'):
+            errmsg = "Invalid specification for the 'relation' parameter: %s"
+            raise ValueError(errmsg % relation)
+
+        fromfreq = self.freq
+        if fromfreq == _c.FR_UND:
+            new = self.__array__()
+        else:
+            new = pandas._skts.DA_asfreq(self.__array__(),
+                                    fromfreq, tofreq, relation[0])
+        return DateArray(new, freq=freq)
+
+
+    #......................................................
+    # Pickling
+    def __getstate__(self):
+        """
+    Returns the internal state of the TimeSeries, for pickling purposes.
+        """
+        state = (1,
+                 self.shape,
+                 self.dtype,
+                 self.flags.fnc,
+                 self.view(ndarray).tostring(),
+                 self.freq,
+                 )
+        return state
+    #
+    def __setstate__(self, state):
+        """
+    Restores the internal state of the TimeSeries, for pickling purposes.
+    `state` is typically the output of the ``__getstate__`` output, and is a 5-tuple:
+
+        - class name
+        - a tuple giving the shape of the data
+        - a typecode for the data
+        - a binary string for the data
+        - a binary string for the mask.
+        """
+        (ver, shp, typ, isf, raw, frq) = state
+        ndarray.__setstate__(self, (shp, typ, isf, raw))
+        self.freq = frq
+
+    def __reduce__(self):
+        """Returns a 3-tuple for pickling a DateArray."""
+        return (self.__class__,
+                (self.__array__(), self.freq),
+                self.__getstate__())
+
+
+    def find_dates(self, *dates):
+        """
+    Returns the indices corresponding to given dates, as an array.
+
+        """
+        #http://aspn.activestate.com/ASPN/Mail/Message/python-tutor/2302348
+        def flatten_sequence(iterable):
+            """Flattens a compound of nested iterables."""
+            itm = iter(iterable)
+            for elm in itm:
+                if hasattr(elm, '__iter__') and not isinstance(elm, basestring):
+                    for f in flatten_sequence(elm):
+                        yield f
+                else:
+                    yield elm
+        #
+        def flatargs(*args):
+            "Flattens the arguments."
+            if not hasattr(args, '__iter__'):
+                return args
+            else:
+                return flatten_sequence(args)
+
+        ifreq = self.freq
+        c = np.zeros(self.shape, dtype=bool)
+        for d in flatargs(*dates):
+            if d.freq != ifreq:
+                d = d.asfreq(ifreq)
+            c += (self == d.value)
+        c = c.nonzero()
+        if np.size(c) == 0:
+            raise IndexError("Date out of bounds!")
+        return c
+
+    def date_to_index(self, dates):
+        """
+   Returns the index corresponding to one given date, as an integer.
+        """
+        vals = self.view(ndarray)
+        if isinstance(dates, Date):
+            _val = dates.value
+            if _val not in vals:
+                raise IndexError("Date '%s' is out of bounds" % dates)
+            if self.is_valid():
+                return _val - vals[0]
+            else:
+                return np.where(vals == _val)[0][0]
+        #
+        _dates = DateArray(dates, freq=self.freq)
+        if self.is_valid():
+            indx = (_dates.view(ndarray) - vals[0])
+            err_cond = (indx < 0) | (indx > self.size)
+            if err_cond.any():
+                err_indx = np.compress(err_cond, _dates)[0]
+                err_msg = "Date '%s' is out of bounds '%s' <= date <= '%s'"
+                raise IndexError(err_msg % (err_indx, self[0], self[-1]))
+            return indx
+        vals = vals.tolist()
+        indx = np.array([vals.index(d) for d in _dates.view(ndarray)])
+        #
+        return indx
+
+
+    def is_chronological(self):
+        """
+    Returns whether the dates are sorted in chronological order
+        """
+        _cached = self._cachedinfo
+        chronoflag = _cached['ischrono']
+        if chronoflag is None:
+            sortidx = ndarray.argsort(self.__array__(), axis=None)
+            chronoflag = (sortidx == np.arange(self.size)).all()
+            _cached['ischrono'] = chronoflag
+            if chronoflag:
+                _cached['chronidx'] = np.array([], dtype=int)
+            else:
+                _cached['chronidx'] = sortidx
+        return chronoflag
+
+    def sort_chronologically(self):
+        """
+    Forces the instance to be sorted in chronological order.
+        """
+        _cached = self._cachedinfo
+        if not self.is_chronological():
+            self.sort()
+            _cached['chronidx'] = np.array([], dtype=int)
+            _cached['ischrono'] = True
+
+
+    def get_steps(self):
+        """
+    Returns the time steps between consecutive dates, in the same unit as
+    the frequency of the instance.
+        """
+        _cached = self._cachedinfo
+        if _cached['steps'] is None:
+            if self.size > 1:
+                val = self.__array__().ravel()
+                if not self.is_chronological():
+                    val = val[_cached['chronidx']]
+                steps = val[1:] - val[:-1]
+                if _cached['full'] is None:
+                    _cached['full'] = (steps.max() == 1)
+                if _cached['hasdups'] is None:
+                    _cached['hasdups'] = (steps.min() == 0)
+            else:
+                _cached.update(ischrono=True,
+                               chronidx=np.array([], dtype=int),
+                               full=True,
+                               hasdups=False)
+                steps = np.array([], dtype=int)
+            _cached['steps'] = steps
+        return _cached['steps']
+
+    def has_missing_dates(self):
+        "Returns whether the instance has missing dates."
+        if self._cachedinfo['full'] is None:
+            steps = self.get_steps()
+        return not(self._cachedinfo['full'])
+
+    def is_full(self):
+        "Returns whether the instance has no missing dates."
+        if self._cachedinfo['full'] is None:
+            steps = self.get_steps()
+        return self._cachedinfo['full']
+
+    def isfull(self):
+        "Deprecated version of :meth:`DateArray.is_full"
+        errmsg = "Deprecated name: use 'is_full' instead."
+        warnings.warn(errmsg, DeprecationWarning,)
+        return self.is_full()
+
+    def has_duplicated_dates(self):
+        "Returns whether the instance has duplicated dates."
+        if self._cachedinfo['hasdups'] is None:
+            steps = self.get_steps()
+        return self._cachedinfo['hasdups']
+
+    def is_valid(self):
+        "Returns whether the instance is valid: no missing/duplicated dates."
+        return  (self.is_full() and not self.has_duplicated_dates())
+
+    def isvalid(self):
+        "Deprecated version of :meth:`DateArray.is_valid"
+        errmsg = "Deprecated name: use 'is_valid' instead."
+        warnings.warn(errmsg, DeprecationWarning,)
+        return  (self.is_full() and not self.has_duplicated_dates())
+    #......................................................
+    @property
+    def start_date(self):
+        "Returns the first date of the array (in chronological order)."
+        if self.size:
+            if self.is_chronological():
+                return self[0]
+            _sortidx = self._cachedinfo['chronidx']
+            return self[_sortidx[0]]
+        return None
+
+    @property
+    def end_date(self):
+        "Returns the last date of the array (in chronological order)."
+        if self.size:
+            if self.is_chronological():
+                return self[-1]
+            _sortidx = self._cachedinfo['chronidx']
+            return self[_sortidx[-1]]
+        return None
+
+    #-----------------------------
+
+    def argsort(self, axis= -1, kind='quicksort', order=None):
+        """
+        Returns the indices that would sort the DateArray.
+        Refer to `numpy.argsort` for full documentation
+        
+        See Also
+        --------
+        numpy.argsort : equivalent function
+        """
+        return self.__array__().argsort(axis=axis, kind=kind, order=order)
+
+    def sort(self, axis= -1, kind='quicksort', order=None):
+        "(This docstring should be overwritten)"
+        ndarray.sort(self, axis=axis, kind=kind, order=order)
+        _cached = self._cachedinfo
+        kwargs = dict(toobj=None, toord=None, tostr=None)
+        if self.ndim == 1:
+            kwargs.update(ischrono=True, chronidx=np.array([], dtype=int))
+        _cached.update(**kwargs)
+        return None
+    sort.__doc__ = ndarray.sort.__doc__
+
+
+def fill_missing_dates(dates, freq=None):
+    """
+    Finds and fills the missing dates in a :class:`DateArray`.
+
+    Parameters
+    ----------
+    dates : {DateArray}
+        Initial array of dates.
+    freq : {freq_spec}, optional
+        Frequency of result. 
+        If not specified, the frequency of the input is used.
+    """
+    # Check the frequency ........
+    orig_freq = freq
+    freq = check_freq(freq)
+    if (orig_freq is not None) and (freq == _c.FR_UND):
+        freqstr = check_freq_str(freq)
+        errmsg = "Unable to define a proper date resolution (found %s)."
+        raise ValueError(errmsg % freqstr)
+    # Check the dates .............
+    if not isinstance(dates, DateArray):
+        errmsg = "A DateArray was expected, got %s instead."
+        raise ValueError(errmsg % type(dates))
+    # Convert the dates to the new frequency (if needed)
+    if freq != dates.freq:
+        dates = dates.asfreq(freq)
+    # Flatten the array
+    if dates.ndim != 1:
+        dates = dates.ravel()
+    # Skip if there's no need to fill
+    if not dates.has_missing_dates():
+        return dates
+    # ...and now, fill it ! ......
+    (tstart, tend) = dates[[0, -1]]
+    return date_array(start_date=tstart, end_date=tend)
+DateArray.fill_missing_dates = fill_missing_dates
+
+nodates = DateArray([])
+
+
+#####---------------------------------------------------------------------------
+#---- --- DateArray functions ---
+#####---------------------------------------------------------------------------
+def _listparser(dlist, freq=None):
+    "Constructs a DateArray from a list."
+    dlist = np.array(dlist, copy=False, ndmin=1)
+    # Case #1: dates as strings .................
+    if dlist.dtype.kind in 'SU':
+        #...construct a list of dates
+        dlist = np.fromiter((Date(freq, string=s).value for s in dlist),
+                            dtype=int)
+    # Case #2: dates as numbers .................
+    elif dlist.dtype.kind in 'if':
+        #...hopefully, they are values
+        dlist = dlist.astype(int)
+    # Case #3: dates as objects .................
+    elif dlist.dtype.kind == 'O':
+        template = dlist[0]
+        #...as Date objects
+        if isinstance(template, Date):
+            dlist = np.fromiter((d.value for d in dlist), dtype=int)
+            if freq in (_c.FR_UND, None):
+                freq = template.freq
+        #...as mx.DateTime objects
+        elif hasattr(template, 'absdays'):
+            dlist = np.fromiter((Date(freq, datetime=m) for m in dlist),
+                                dtype=int)
+        #...as datetime objects
+        elif hasattr(template, 'toordinal'):
+            dlist = np.fromiter((Date(freq, datetime=d) for d in dlist),
+                                dtype=int)
+    #
+    result = dlist.view(DateArray)
+    result.freq = freq
+    return result
+
+
+def date_array(dlist=None, start_date=None, end_date=None, length=None,
+               freq=None, autosort=False):
+    """
+    Factory function for constructing a :class:`DateArray`.
+
+    Parameters
+    ----------
+    dlist : {sequence, DateArray}, optional
+        If not None, :keyword:`dlist` must be any of these possibilities:
+
+        * an existing :class:`DateArray` object;
+        * a sequence of :class:`Date` objects with the same frequency;
+        * a sequence of :class:`datetime.datetime` objects;
+        * a sequence of dates in string format;
+        * a sequence of integers corresponding to the representation of 
+          :class:`Date` objects.
+
+        In any of the last four possibilities, the :keyword:`freq` parameter
+        must also be given.
+    start_date : {var}, optional
+        First date of a continuous :class:`DateArray`.
+        This parameter is used only if :keyword:`dlist` is None.
+        In that case, one of the :keyword:`end_date` or the :keyword:`length`
+        parameters must be given.
+    end_date : {var}, optional
+        Last date of the output. 
+        Use this parameter in combination with :keyword:`start_date` to create
+        a continuous :class:`DateArray`.
+    length : {int}, optional
+        Length of the output.
+        Use this parameter in combination with :keyword:`start_date` to create
+        a continuous :class:`DateArray`.
+    autosort : {True, False}, optional
+        Whether the input dates must be sorted in chronological order.
+
+    Notes
+    -----
+    * When the input is a list of dates, the dates are **not** sorted.
+      Use ``autosort = True`` to sort the dates by chronological order.
+    * If `start_date` is a :class:`Date` object and `freq` is None,
+      the frequency of the output is ``start_date.freq``.
+
+
+    Returns
+    -------
+    output : :class:`DateArray` object.
+
+    """
+    freq = check_freq(freq)
+    # Case #1: we have a list ...................
+    if dlist is not None:
+        # Already a DateArray....................
+        if isinstance(dlist, DateArray):
+            if (freq != _c.FR_UND) and (dlist.freq != check_freq(freq)):
+                # Convert to the new frequency
+                dlist = dlist.asfreq()
+            else:
+                # Take a view so that we don't propagate modifications....
+                # ... in _cachedinfo.
+                dlist = dlist.view()
+            if autosort:
+                dlist.sort_chronologically()
+            return dlist
+        # Make sure it's a sequence, else that's a start_date
+        if hasattr(dlist, '__len__') and not isinstance(dlist, basestring):
+            dlist = _listparser(dlist, freq=freq)
+            if autosort:
+                dlist.sort_chronologically()
+            return dlist
+        elif start_date is not None:
+            if end_date is not None:
+                dmsg = "What starting date should be used ? '%s' or '%s' ?"
+                raise DateError, dmsg % (dlist, start_date)
+            else:
+                (start_date, end_date) = (dlist, start_date)
+        else:
+            start_date = dlist
+    # Case #2: we have a starting date ..........
+    if start_date is None:
+        if length == 0:
+            return DateArray([], freq=freq)
+        raise InsufficientDateError
+    if not isinstance(start_date, Date):
+        try:
+            start_date = Date(freq, start_date)
+        except:
+            dmsg = "Starting date should be a valid Date instance! "
+            dmsg += "(got '%s' instead)" % type(start_date)
+            raise DateError, dmsg
+    # Check if we have an end_date
+    if end_date is None:
+        if length is None:
+            length = 1
+    else:
+        try:
+            end_date = Date(start_date.freq, end_date)
+        except:
+            raise DateError, "Ending date should be a valid Date instance!"
+        # Make sure end_date is after start_date
+        if (end_date < start_date):
+            (start_date, end_date) = (end_date, start_date)
+        length = int(end_date - start_date) + 1
+    #
+    dlist = np.arange(length, dtype=np.int)
+    dlist += start_date.value
+    if freq == _c.FR_UND:
+        freq = start_date.freq
+    # Transform the dates and set the cache
+    dates = dlist.view(DateArray)
+    dates.freq = freq
+    dates._cachedinfo.update(ischrono=True, chronidx=np.array([], dtype=int))
+    return dates
+
+
+
+#####---------------------------------------------------------------------------
+#---- --- Definition of functions from the corresponding methods ---
+#####---------------------------------------------------------------------------
+class _frommethod(object):
+    """
+    Defines functions from existing MaskedArray methods.
+    :ivar _methodname (String): Name of the method to transform.
+    """
+    def __init__(self, methodname):
+        self._methodname = methodname
+        self.__doc__ = self.getdoc()
+    #
+    def getdoc(self):
+        "Returns the doc of the function (from the doc of the method)."
+        try:
+            return getattr(DateArray, self._methodname).__doc__
+        except AttributeError:
+            return "???"
+    #
+    def __call__(self, caller, *args, **params):
+        if hasattr(caller, self._methodname):
+            method = getattr(caller, self._methodname)
+            # If method is not callable, it's a property, and don't call it
+            if hasattr(method, '__call__'):
+                return method.__call__(*args, **params)
+            return method
+        method = getattr(np.asarray(caller), self._methodname)
+        try:
+            return method(*args, **params)
+        except SystemError:
+            method = getattr(np, self._methodname)
+            return method(caller, *args, **params)
+
+#............................
+weekday = _frommethod('weekday')
+week = _frommethod('week')
+day_of_year = _frommethod('day_of_year')
+year = _frommethod('year')
+quarter = _frommethod('quarter')
+month = _frommethod('month')
+day = _frommethod('day')
+hour = _frommethod('hour')
+minute = _frommethod('minute')
+second = _frommethod('second')
+
+
+def period_break(dates, period):
+    """
+    Returns the indices where the given period changes.
+
+    Parameters
+    ----------
+    dates : DateArray
+        Array of dates to monitor.
+    period : string
+        Name of the period to monitor.
+    """
+    current = getattr(dates, period)
+    previous = getattr(dates - 1, period)
+    return (current - previous).nonzero()[0]
+
+
+def convert_to_float(datearray, ofreq):
+    """
+    Convert a :class:`~scikits.timeseries.DateArray` object from a ndarray
+    of integers to a ndarray of float at a lower frequency.
+
+    Parameters
+    ----------
+    datearray : DateArray
+        Input :class:`~scikits.timeseries.DateArray` to convert.
+    ofreq : var
+        Valid frequency specifier.
+
+    Notes
+    -----
+    This function is currently restricted to conversion between annual (``'A'``),
+    quarterly (``'Q'``), monthly (``'M'``) and daily (``'D'``) frequencies only.
+    """
+    if not isinstance(datearray, DateArray):
+        raise TypeError("The input should be a valid DateArray instance !"\
+                        " (got '%s' instead)" % type(datearray))
+    errmsg = "Not implemented for the frequencies ('%s', '%s')"
+    #
+    freqdict = dict([(f, check_freq(f)) for f in ('A', 'Q', 'M', 'D')])
+    ifreq = datearray.freq
+    ofreq = check_freq(ofreq)
+    errmsg = "Not implemented for the frequencies ('%s', '%s')" % \
+             (check_freq_str(ifreq), check_freq_str(ofreq))
+    if ifreq < ofreq:
+        output = datearray.asfreq(ofreq).tovalue().astype(float)
+    elif ifreq == ofreq:
+        output = datearray.tovalue().astype(float)
+    # Quarterly.........
+    elif (ifreq >= freqdict['Q']) and (ifreq < freqdict['M']):
+        if (ofreq >= freqdict['A']) and (ofreq < freqdict['Q']):
+            output = datearray.years.astype(float) + (datearray.quarters - 1.) / 4.
+    # Monthly...........
+    elif ifreq == freqdict['M']:
+        #... to annual
+        if (ofreq >= freqdict['A']) and (ofreq < freqdict['Q']):
+            output = datearray.years.astype(float) + (datearray.months - 1) / 12.
+        else:
+            raise NotImplementedError(errmsg)
+    # Daily ............
+    elif ifreq == freqdict['D']:
+        # ... to annual
+        if (ofreq >= freqdict['A']) and (ofreq < freqdict['Q']):
+            output = datearray.asfreq('A')
+            output = output.tovalue().astype(float) + \
+                     (datearray.yeardays - 1.) / output.yeardays.astype(float)
+        # ... to quarterly
+        elif (ofreq >= freqdict['Q']) and (ofreq < freqdict['M']):
+            raise NotImplementedError
+        # ... to monthly
+        elif ofreq == freqdict['M']:
+            output = datearray.asfreq('M')
+            output = output.tovalue().astype(float) + \
+                     (datearray.days - 1.) / output.days.astype(float)
+        # ... to other
+        else:
+            raise NotImplementedError(errmsg)
+    # Less than daily
+    elif ifreq > freqdict['D']:
+        raise NotImplementedError(errmsg)
+    else:
+        raise NotImplementedError(errmsg)
+    return output
+DateArray.tofloat = convert_to_float
+
diff --git a/pandas/timeseries/trecords.py b/pandas/timeseries/trecords.py
new file mode 100644
index 000000000..02a9b9e49
--- /dev/null
+++ b/pandas/timeseries/trecords.py
@@ -0,0 +1,501 @@
+# pylint: disable-msg=W0201, W0212
+"""
+Support for multi-variable time series, through masked record arrays.
+
+Individual fields can be accessed as keys or attributes.
+
+:author: Pierre GF Gerard-Marchant & Matt Knox
+:contact: pierregm_at_uga_dot_edu - mattknox_ca_at_hotmail_dot_com
+:version: $Id$
+"""
+__author__ = "Pierre GF Gerard-Marchant & Matt Knox ($Author$)"
+__revision__ = "$Revision$"
+__date__ = '$Date$'
+
+
+import sys
+
+import numpy as np
+from numpy import bool_, complex_, float_, int_, str_, object_, \
+    ndarray, chararray, recarray
+import numpy.core.numerictypes as ntypes
+import numpy.core.umath as umath
+from numpy.core.records import find_duplicate, format_parser, record, \
+    fromarrays as recfromarrays
+
+import numpy.ma as ma
+from numpy.ma import MaskedArray, MAError, \
+     default_fill_value, masked_print_option, masked, nomask, \
+     getmask, getmaskarray, make_mask, make_mask_none, mask_or, filled
+
+import numpy.ma.mrecords
+from numpy.ma.mrecords import _checknames, \
+     _guessvartypes, openfile, MaskedRecords, mrecarray, addfield, \
+     fromrecords as mrecfromrecords, fromarrays as mrecfromarrays
+
+from tseries import TimeSeries, TimeSeriesCompatibilityError, \
+    time_series, _getdatalength, nodates, get_varshape
+from tdates import Date, DateArray, date_array
+from extras import tsfromtxt
+
+_byteorderconv = numpy.core.records._byteorderconv
+_typestr = ntypes._typestr
+
+reserved_fields = numpy.ma.mrecords.reserved_fields + ['_dates']
+
+import warnings
+
+__all__ = [
+'TimeSeriesRecords', 'time_records',
+'fromarrays', 'fromrecords', 'fromtextfile',
+]
+
+def _getformats(data):
+    """
+    Returns the formats of each array of arraylist as a comma-separated string.
+    """
+    if isinstance(data, record):
+        return ",".join([desc[1] for desc in data.dtype.descr])
+
+    formats = ''
+    for obj in data:
+        obj = np.asarray(obj)
+        formats += _typestr[obj.dtype.type]
+        if issubclass(obj.dtype.type, ntypes.flexible):
+            formats += `obj.itemsize`
+        formats += ','
+    return formats[:-1]
+
+
+def _getdates(dates=None, newdates=None, length=None, freq=None,
+              start_date=None):
+    """
+    Determines new dates (private function not meant to be used).
+    """
+    if dates is None:
+        if newdates is not None:
+            if not hasattr(newdates, 'freq'):
+                newdates = date_array(dlist=newdates, freq=freq)
+        else:
+            newdates = date_array(start_date=start_date, length=length,
+                                  freq=freq)
+    elif not hasattr(dates, 'freq'):
+        newdates = date_array(dlist=dates, freq=freq)
+    else:
+        newdates = dates
+    return newdates
+
+
+class TimeSeriesRecords(TimeSeries, MaskedRecords, object):
+    """
+    MaskedRecords with support for time-indexing.
+
+    Fields can be retrieved either as indices (using the indexing scheme
+    based on `__getitem__`) or as attributes (using `__getattribute__`).
+
+    The type of the output of `__getitem__` is variable:
+    
+    field
+       returns a :class:`TimeSeries` object.
+    single record, no masked fields
+        returns a ``numpy.void`` object
+    single record with at least one masked field
+        returns a :class:`MaskedRecords` object.
+    slice
+        return a :class:`TimeSeriesRecords`.
+    """
+    def __new__(cls, shape, dtype=None, buf=None, offset=0, strides=None,
+                formats=None, names=None, titles=None,
+                byteorder=None, aligned=False,
+                mask=nomask, hard_mask=False, fill_value=None, keep_mask=True,
+                copy=False,
+                dates=None, freq='U', start_date=None, observed=None,
+                **options):
+        _data = mrecarray.__new__(cls, shape, dtype=dtype, buf=buf, offset=offset,
+                                  strides=strides, formats=formats,
+                                  byteorder=byteorder, aligned=aligned,
+                                  mask=mask, hard_mask=hard_mask, copy=copy,
+                                  keep_mask=keep_mask, fill_value=fill_value,
+                                  )
+        #
+        newdates = _getdates(dates, length=len(_data),
+                             start_date=start_date, freq=freq)
+        _data._dates = newdates
+        _data._observed = observed
+        #
+        return _data
+
+    def __array_finalize__(self, obj):
+        self.__dict__.update(_varshape=getattr(obj, '_varshape', ()),
+                             _dates=getattr(obj, '_dates', DateArray([])),
+                             _observed=getattr(obj, '_observed', None),
+                             _optinfo=getattr(obj, '_optinfo', {}))
+        MaskedRecords.__array_finalize__(self, obj)
+        return
+
+
+    def _getdata(self):
+        "Returns the data as a recarray."
+        return ndarray.view(self, recarray)
+    _data = property(fget=_getdata)
+
+    def _getseries(self):
+        "Returns the data as a MaskedRecord array."
+        return MaskedArray.view(self, mrecarray)
+    _series = property(fget=_getseries)
+
+
+    def __getattribute__(self, attr):
+        getattribute = MaskedRecords.__getattribute__
+        _dict = getattribute(self, '__dict__')
+        if attr == '_dict':
+            return _dict
+        _names = ndarray.__getattribute__(self, 'dtype').names
+        if attr in (_names or []):
+            obj = getattribute(self, attr).view(TimeSeries)
+            obj._dates = _dict['_dates']
+            return obj
+        return getattribute(self, attr)
+
+
+    def __setattr__(self, attr, value):
+        if attr in ['_dates', 'dates']:
+            self.__setdates__(value)
+        elif attr == 'shape':
+            if self._varshape:
+                err_msg = "Reshaping a nV/nD series is not implemented yet !"
+                raise NotImplementedError(err_msg)
+        return MaskedRecords.__setattr__(self, attr, value)
+
+    #......................................................
+    def __getitem__(self, indx):
+        """Returns all the fields sharing the same fieldname base.
+    The fieldname base is either `_data` or `_mask`."""
+        _localdict = self.__dict__
+        # We want a field ........
+        if indx in ndarray.__getattribute__(self, 'dtype').names:
+            obj = self._data[indx].view(TimeSeries)
+            obj._dates = _localdict['_dates']
+            obj._mask = make_mask(_localdict['_mask'][indx])
+            return obj
+        # We want some elements ..
+        obj = TimeSeries.__getitem__(self, indx)
+        if isinstance(obj, MaskedArray) and not isinstance(obj, TimeSeries):
+            obj = ndarray.view(obj, MaskedRecords)
+        return obj
+
+
+    def __setslice__(self, i, j, value):
+        """Sets the slice described by [i,j] to `value`."""
+        MaskedRecords.__setitem__(self, slice(i, j), value)
+        return
+
+    #......................................................
+    def __str__(self):
+        """x.__str__() <==> str(x)
+    Calculates the string representation, using masked for fill if it is enabled.
+    Otherwise, fills with fill value.
+        """
+        if self.size > 1:
+            mstr = ["(%s)" % ",".join([str(i) for i in s])
+                    for s in zip(*[getattr(self, f)._series
+                                   for f in self.dtype.names])]
+            return "[%s]" % ", ".join(mstr)
+        else:
+            mstr = ["%s" % ",".join([str(i) for i in s])
+                    for s in zip([getattr(self, f)._series
+                                  for f in self.dtype.names])]
+            return "(%s)" % ", ".join(mstr)
+
+    def __repr__(self):
+        """x.__repr__() <==> repr(x)
+    Calculates the repr representation, using masked for fill if it is enabled.
+    Otherwise fill with fill value.
+        """
+        _names = self.dtype.names
+        _dates = self._dates
+        if np.size(_dates) > 2 and self._dates.is_valid():
+            timestr = "[%s ... %s]" % (str(_dates[0]), str(_dates[-1]))
+        else:
+            timestr = str(_dates)
+        fmt = "%%%is : %%s" % (max([len(n) for n in _names]) + 4,)
+        reprstr = [fmt % (f, getattr(self, f)) for f in self.dtype.names]
+        reprstr.insert(0, 'TimeSeriesRecords(')
+        reprstr.extend([fmt % ('dates', timestr),
+                        fmt % ('    fill_value', self.fill_value),
+                         '               )'])
+        return str("\n".join(reprstr))
+
+
+    def copy(self):
+        "Returns a copy of the argument."
+        copied = MaskedRecords.copy(self)
+        copied._dates = self._dates.copy()
+        return copied
+
+
+    def convert(self, freq, func=None, position='END', *args, **kwargs):
+        """
+    Converts a series to another frequency.
+
+    Parameters
+    ----------
+    series : TimeSeries
+        the series to convert. Skip this parameter if you are calling this as
+        a method of the TimeSeries object instead of the module function.
+    freq : freq_spec
+        Frequency to convert the TimeSeries to. Accepts any valid frequency
+        specification (string or integer)
+    func : {None,function}, optional
+        When converting to a lower frequency, `func` is a function that acts on
+        one date's worth of data. `func` should handle masked values appropriately.
+        If `func` is None, then each entry of the resulting series is the group
+        of data points that fall into the date at the lower frequency.
+        For example, if converting from monthly to daily and you wanted each
+        data point in the resulting series to be the average value for each
+        month, you could specify numpy.ma.average for the 'func' parameter.
+    position : {'END', 'START'}, optional
+        When converting to a higher frequency, position is 'START' or 'END'
+        and determines where the data point is in each period. For example, if
+        going from monthly to daily, and position is 'END', then each data
+        point is placed at the end of the month.
+    *args : {extra arguments for func parameter}, optional
+        if a func is specified that requires additional parameters, specify
+        them here.
+    **kwargs : {extra keyword arguments for func parameter}, optional
+        if a func is specified that requires additional keyword parameters,
+        specify them here.
+
+        """
+        kwargs.update(func=func, position=position)
+        field_names = self.dtype.names
+        by_field = [self[f].convert(freq, **kwargs) for f in field_names]
+        output = fromarrays(by_field,
+                            dates=by_field[0].dates,
+                            names=field_names)
+        output.fill_value = self._fill_value
+        return output
+trecarray = TimeSeriesRecords
+
+
+#####---------------------------------------------------------------------------
+#---- --- Constructors ---
+#####---------------------------------------------------------------------------
+
+def time_records(data, dates=None, start_date=None, freq=None, mask=nomask,
+                dtype=None, copy=False, fill_value=None, keep_mask=True,
+                hard_mask=False):
+    """
+    Creates a TimeSeriesRecords object.
+
+    Parameters
+    ----------
+    data : array_like
+        Data portion of the array. Any data that is valid for constructing a
+        MaskedArray can be used here. May also be a TimeSeries object.
+    dates : {None, DateArray}, optional
+        A sequence of dates corresponding to each entry.
+        If None, the dates will be constructed as a DateArray with the same
+        length as ``data``, starting at ``start_date`` with frequency ``freq``.
+    start_date : {Date}, optional
+        Date corresponding to the first entry of the data (index 0).
+        This parameter must be a valid Date object, and is mandatory if ``dates``
+        is None and if ``data`` has a length greater or equal to 1.
+    freq : {freq_spec}, optional
+        A valid frequency specification, as a string or an integer.
+        This parameter is mandatory if ``dates`` is None.
+    mask : {nomask, sequence}, optional
+        Mask.  Must be convertible to an array of booleans with
+        the same shape as data: True indicates a masked (eg.,
+        invalid) data.
+    dtype : {dtype}, optional
+        Data type of the output.
+        If dtype is None, the type of the data argument (`data.dtype`) is used.
+        If dtype is not None and different from `data.dtype`, a copy is performed.
+    copy : {False, True}, optional
+        Whether to copy the input data (True), or to use a reference instead.
+        Note: data are NOT copied by default.
+    fill_value : {var}, optional
+        Value used to fill in the masked values when necessary.
+        If None, a default based on the datatype is used.
+    keep_mask : {True, boolean}, optional
+        Whether to combine mask with the mask of the input data,
+        if any (True), or to use only mask for the output (False).
+    hard_mask : {False, boolean}, optional
+        Whether to use a hard mask or not.
+        With a hard mask, masked values cannot be unmasked.
+
+    Notes
+    -----
+    * All other parameters that are accepted by the :func:`numpy.ma.array`
+      function in the :mod:`numpy.ma` module are also accepted by this function.
+    * The date portion of the time series must be specified in one of the
+      following ways:
+
+       * specify a TimeSeries object for the ``data`` parameter.
+       * pass a DateArray for the ``dates`` parameter.
+       * specify a start_date (a continuous DateArray will be automatically
+         constructed for the dates portion).
+       * specify just a frequency (for TimeSeries of size zero).
+
+    """
+    series = time_series(data, dates=dates, start_date=start_date, freq=freq,
+                          mask=mask, dtype=dtype, copy=copy,
+                          fill_value=fill_value, keep_mask=keep_mask,
+                          hard_mask=hard_mask)
+    return series.view(TimeSeriesRecords)
+
+#!!!: * The docstrings of the following functions need some serious work ;)
+#!!!: * We should try to have a list of TimeSeries sufficient to build a record...
+#!!!:   without having to precise a list of dates...
+#!!!:   > check the compatibility of dates
+#!!!:   > try to adjust endpoints if needed
+#!!!:   > if one of the series is not a TimeSeries, keep going.
+
+def fromarrays(arraylist, dates=None, start_date=None, freq='U',
+               fill_value=None, autosort=True,
+               dtype=None, shape=None, formats=None,
+               names=None, titles=None, aligned=False, byteorder=None,):
+    """
+    Creates a mrecarray from a (flat) list of masked arrays.
+
+    Parameters
+    ----------
+    arraylist : array_like
+        A list of (masked) arrays. Each element of the sequence is first converted
+        to a masked array if needed. If a 2D array is passed as argument, it is
+        processed line by line
+    dates : {DateArray}, optional
+        Array of dates corresponding to each entry.
+        If None, a DateArray is constructed from `start_date` and the length
+        of the arrays in the input list.
+    start_date : {Date}, optional
+        First date of the output.
+        This parameter is inly needed if `dates` is None.
+    freq : {var}, optional
+        Frequency of the DateArray
+    fill_value : {var}, optional
+        Value used to fill in the masked values when necessary.
+        If None, a default based on the datatype is used.
+    autosort : {True, False}, optional
+        Whether the records should be sorted chronologically.
+
+    See Also
+    --------
+    numpy.core.records.fromarrays : equivalent function for ndarrays
+        The docstring of this function describes the additional optional
+        input parameters.
+    
+
+    Notes
+    -----
+    * Lists of tuples should be preferred over lists of lists as inputs 
+      for faster processing.
+    """
+    _array = mrecfromarrays(arraylist, dtype=dtype, shape=shape, formats=formats,
+                            names=names, titles=titles, aligned=aligned,
+                            byteorder=byteorder, fill_value=fill_value)
+    _dates = _getdates(dates, length=len(_array), start_date=start_date,
+                       freq=freq)
+#    if _dates._unsorted is not None:
+#        idx = _dates._unsorted
+#        _array = _array[idx]
+#        _dates._unsorted = None
+    result = _array.view(trecarray)
+    result._dates = _dates
+    if autosort:
+        result.sort_chronologically()
+    return result
+
+
+#..............................................................................
+def fromrecords(reclist, dates=None, freq=None, start_date=None,
+                fill_value=None, mask=nomask, autosort=True,
+                dtype=None, shape=None, formats=None, names=None,
+                titles=None, aligned=False, byteorder=None):
+    """
+    Creates a TimeSeriesRecords from a list of records.
+
+    The data in the same field can be heterogeneous, they will be promoted
+    to the highest data type.  This method is intended for creating
+    smaller record arrays.  If used to create large array without formats
+    defined, it can be slow.
+
+    If formats is None, then this will auto-detect formats. Use a list of
+    tuples rather than a list of lists for faster processing.
+
+
+    Parameters
+    ----------
+    reclist : array_like
+        A list of records. Each element of the sequence is first converted
+        to a masked array if needed. If a 2D array is passed as argument, it is
+        processed line by line
+    dates : {DateArray}, optional
+        Array of dates corresponding to each entry.
+        If None, a DateArray is constructed from `start_date` and the length
+        of the arrays in the input list.
+    freq : {var}, optional
+        Frequency of the DateArray
+    start_date : {Date}, optional
+        First date of the output.
+        This parameter is inly needed if `dates` is None.
+    fill_value : {var}, optional
+        Value used to fill in the masked values when necessary.
+        If None, a default based on the datatype is used.
+    autosort : {True, False}, optional
+        Whether the records should be sorted chronologically.
+        
+
+    See Also
+    --------
+    numpy.core.records.fromrecords : equivalent function for ndarrays
+
+
+    """
+    _data = mrecfromrecords(reclist, dtype=dtype, shape=shape, formats=formats,
+                            names=names, titles=titles, aligned=aligned,
+                            byteorder=byteorder, mask=mask)
+    _dtype = _data.dtype
+    # Check the names for a '_dates' .................
+    newdates = None
+    _names = list(_dtype.names)
+    reserved = [n for n in _names if n.lower() in ['dates', '_dates']]
+    if len(reserved) > 0:
+        newdates = _data[reserved[-1]]
+        [_names.remove(n) for n in reserved]
+        _dtype = np.dtype([t for t in _dtype.descr \
+                                    if t[0] not in reserved ])
+        _data = mrecfromarrays([_data[n] for n in _names], dtype=_dtype)
+    #
+    if dates is None:
+        dates = getattr(reclist, '_dates', None)
+    _dates = _getdates(dates=dates, newdates=newdates, length=len(_data),
+                       freq=freq, start_date=start_date)
+    #
+    result = _data.view(trecarray)
+    result._dates = _dates
+    if autosort:
+        result.sort_chronologically()
+    return result
+
+
+
+def fromtextfile(fname, delimitor=None, commentchar='#', missingchar='',
+                 dates_column=None, varnames=None, vartypes=None,
+                 dates=None, freq=None, skiprows=0):
+    """
+    Deprecated function: please use tsfromtxt instead.
+    """
+    msg = "This function is deprecated.\nPlease use `tsfromtxt` instead."
+    warnings.warn(msg, DeprecationWarning)
+    # Split the names by comma first, then per character
+    if isinstance(varnames, basestring):
+        varnames = varnames.split(",")
+        if len(varnames) == 1:
+            varnames = zip(*varnames[0])[0]
+    return tsfromtxt(fname, dtype=vartypes, names=(varnames or True), freq=freq,
+                     datecols=dates_column, skiprows=skiprows,
+                     delimiter=delimitor, comments=commentchar,
+                     missing=missingchar, asrecarray=True)
+
diff --git a/pandas/timeseries/tseries.py b/pandas/timeseries/tseries.py
new file mode 100644
index 000000000..ce26509a1
--- /dev/null
+++ b/pandas/timeseries/tseries.py
@@ -0,0 +1,2458 @@
+"""
+The :class:`TimeSeries` class provides a base for the definition of time series.
+A time series is defined here as the combination of two arrays:
+
+- an array storing the time information
+  (as a :class:`~scikits.timeseries.tdates.DateArray` instance);
+- an array storing the data (as a :class:`MaskedArray` instance.)
+
+These two classes were liberally adapted from :class:`MaskedArray` class.
+
+
+:author: Pierre GF Gerard-Marchant & Matt Knox
+:contact: pierregm_at_uga_dot_edu - mattknox_ca_at_hotmail_dot_com
+"""
+
+#!!!: * Allow different lengths for data and dates to handle 2D data more easily
+#!!!:    In that case, just make sure that the data is (n,rows,cols) where n is the nb of dates
+#!!!: * Add some kind of marker telling whether we are 1D or nD:
+#!!!:    That could be done by checking the ratio series.size/series._dates.size
+#!!!: * Disable some of the tests on date compatibility if we are nD
+#!!!: * Adapt reshaping to preserve the first dimension: that goes for squeeze
+
+__author__ = "Pierre GF Gerard-Marchant & Matt Knox"
+__revision__ = "$Revision$"
+__date__ = '$Date$'
+
+import sys
+import warnings
+
+import numpy as np
+from numpy import bool_, complex_, float_, int_, object_, dtype, \
+    ndarray, recarray
+import numpy.core.umath as umath
+from numpy.core.records import fromarrays as recfromarrays
+
+from numpy import ma
+from numpy.ma import MaskedArray, MAError, masked, nomask, \
+    filled, getmask, getmaskarray, hsplit, make_mask_none, mask_or, make_mask, \
+    masked_array
+
+import tdates
+from tdates import \
+    DateError, FrequencyDateError, InsufficientDateError, Date, DateArray, \
+    date_array, now, check_freq, check_freq_str, nodates
+
+import const as _c
+import pandas._skts
+
+__all__ = ['TimeSeries', 'TimeSeriesCompatibilityError', 'TimeSeriesError',
+           'adjust_endpoints', 'align_series', 'align_with', 'aligned',
+           'asrecords',
+           'compressed', 'concatenate', 'convert',
+           'day', 'day_of_year',
+           'empty_like',
+           'fill_missing_dates', 'find_duplicated_dates', 'first_unmasked_val',
+           'flatten',
+           'hour',
+           'last_unmasked_val',
+           'minute', 'month',
+           'pct', 'pct_log', 'pct_symmetric',
+           'quarter',
+           'remove_duplicated_dates',
+           'second', 'split', 'stack',
+           'time_series', 'tofile', 'tshift', 'masked', 'nomask',
+           'week', 'weekday',
+           'year',
+           ]
+
+
+def _unmasked_val(a, kind, axis=None):
+    "helper function for first_unmasked_val and last_unmasked_val"
+    if axis is None or a.ndim == 1:
+        a = a.ravel()
+        m = getmask(a)
+        if m is nomask or not np.any(m):
+            if kind == 0:
+                indx = 0
+            else:
+                indx = -1
+        else:
+            indx = np.flatnonzero(~m)[[0, -1]][kind]
+    else:
+        m = ma.getmaskarray(a)
+        indx = ma.array(np.indices(a.shape), mask=np.asarray([m] * a.ndim))
+        if kind == 0:
+            indx = tuple([indx[i].min(axis=axis).filled(0)
+                          for i in range(a.ndim)])
+        else:
+            indx = tuple([indx[i].max(axis=axis).filled(0)
+                          for i in range(a.ndim)])
+    return a[indx]
+
+def first_unmasked_val(a, axis=None):
+    """
+    Retrieve the first unmasked value along the given axis in a MaskedArray.
+
+    Parameters
+    ----------
+    a : MaskedArray
+        Input MaskedArray (or a subclass of).
+    axis : int, optional
+        Axis along which to perform the operation.
+        If None, applies to a flattened version of the array.
+
+    Returns
+    -------
+    val : {singleton of type marray.dtype}
+        First unmasked value in a.
+        If all values in a are masked, returns the numpy.ma.masked constant.
+    """
+    return _unmasked_val(a, 0, axis=axis)
+
+
+def last_unmasked_val(a, axis=None):
+    """
+    Retrieve the last unmasked value along the given axis in a MaskedArray.
+
+    Parameters
+    ----------
+    a : MaskedArray
+        Input MaskedArray (or a subclass of).
+    axis : int, optional
+        Axis along which to perform the operation.
+        If None, applies to a flattened version of the array.
+
+    Returns
+    -------
+    val : {singleton of type marray.dtype}
+        Last unmasked value in a.
+        If all values in a are masked, returns the numpy.ma.masked constant.
+    """
+    return _unmasked_val(a, 1, axis=axis)
+
+#### -------------------------------------------------------------------------
+#--- ... TimeSeriesError class ...
+#### -------------------------------------------------------------------------
+class TimeSeriesError(Exception):
+    "Class for TS related errors."
+    def __init__ (self, value=None):
+        "Creates an exception."
+        self.value = value
+    def __str__(self):
+        "Calculates the string representation."
+        return str(self.value)
+    __repr__ = __str__
+
+
+class TimeSeriesCompatibilityError(TimeSeriesError):
+    """
+    Defines the exception raised when series are incompatible.
+    Incompatibility can arise from:
+
+    * Inconsistent frequency;
+    * Inconsistent starting dates;
+    * Inconsistent size and/or shape.
+
+    """
+    def __init__(self, mode, first, second):
+        if mode == 'freq':
+            msg = "Incompatible time steps! (%s <> %s)"
+        elif mode == 'start_date':
+            msg = "Incompatible starting dates! (%s <> %s)"
+        elif mode in ('size', 'shape'):
+            msg = "Incompatible sizes! (%s <> %s)"
+        elif mode == 'order':
+            msg = "The series must be sorted in chronological order !"
+        else:
+            msg = "Incompatibility !  (%s <> %s)"
+        msg = msg % (first, second)
+        TimeSeriesError.__init__(self, msg)
+
+#???: Should we go crazy and add some new exceptions ?
+#???: TimeSeriesShapeCompatibilityError
+#???: TimeSeriesStepCompatibilityError
+
+
+def _timeseriescompat(a, b, raise_error=True):
+    """
+    Checks the date compatibility of two TimeSeries object.
+    Returns True if everything's fine, or raises an exception.
+    """
+    #!!!: We need to use _varshape to simplify the analysis
+    # Check the frequency ..............
+    (afreq, bfreq) = (getattr(a, 'freq', None), getattr(b, 'freq', None))
+    if afreq != bfreq:
+        if raise_error:
+            raise TimeSeriesCompatibilityError('freq', afreq, bfreq)
+        return False
+    # Make sure a.freq is not None
+    if afreq is None:
+        return True
+    # Make sure that the series are sorted in chronological order
+    if (not a.is_chronological()) or (not b.is_chronological()):
+        if raise_error:
+            raise TimeSeriesCompatiblityError('sort', None, None)
+        return False
+    # Check the starting dates ..........
+    (astart, bstart) = (getattr(a, 'start_date'), getattr(b, 'start_date'))
+    if astart != bstart:
+        if raise_error:
+            raise TimeSeriesCompatibilityError('start_date', astart, bstart)
+        return False
+    # Check the time steps ..............
+    asteps = getattr(a, '_dates', a).get_steps()
+    bsteps = getattr(b, '_dates', b).get_steps()
+    step_diff = (asteps != bsteps)
+    if (step_diff is True) or \
+       (hasattr(step_diff, "any") and step_diff.any()):
+        if raise_error:
+            raise TimeSeriesCompatibilityError('time_steps', asteps, bsteps)
+        return False
+    elif a.shape != b.shape:
+        if raise_error:
+            raise TimeSeriesCompatibilityError('size', "1: %s" % str(a.shape),
+                                                       "2: %s" % str(b.shape))
+        return False
+    return True
+
+def _timeseriescompat_multiple(*series):
+    """
+    Checks the date compatibility of multiple TimeSeries objects.
+    Returns True if everything's fine, or raises an exception. Unlike
+    the binary version, all items must be TimeSeries objects.
+    """
+
+    defsteps = series[0]._dates.get_steps()
+
+    def _check_steps(ser):
+        _defsteps = s._dates.get_steps()
+        _ds_comp = (_defsteps != defsteps)
+        if not hasattr(_ds_comp, "any") or _ds_comp.any():
+            return True
+        else:
+            return False
+
+    (freqs, start_dates, steps, shapes) = \
+                                zip(*[(s.freq,
+                                       s.start_date,
+                                       _check_steps(s),
+                                       s.shape) for s in series])
+    # Check the frequencies ................
+    freqset = set(freqs)
+    if len(set(freqs)) > 1:
+        err_items = tuple(freqset)
+        raise TimeSeriesCompatibilityError('freq', err_items[0], err_items[1])
+    # Check the strting dates ..............
+    startset = set(start_dates)
+    if len(startset) > 1:
+        err_items = tuple(startset)
+        raise TimeSeriesCompatibilityError('start_dates',
+                                           err_items[0], err_items[1])
+    # Check the shapes .....................
+    shapeset = set(shapes)
+    if len(shapeset) > 1:
+        err_items = tuple(shapeset)
+        raise TimeSeriesCompatibilityError('size',
+                                           "1: %s" % str(err_items[0]),
+                                           "2: %s" % str(err_items[1]))
+    # Check the steps ......................
+    if max(steps) == True:
+        bad_index = [x for (x, val) in enumerate(steps) if val][0]
+        raise TimeSeriesCompatibilityError('time_steps',
+                                           defsteps,
+                                           series[bad_index]._dates.get_steps())
+    return True
+
+
+def get_varshape(data, dates):
+    """
+    Checks the compatibility of dates and data.
+
+    Parameters
+    ----------
+    data : array-like
+        Array of data
+    dates : Date, DateArray
+        Sequence of dates
+
+    Returns
+    -------
+    varshape : tuple
+        A tuple indicating the shape of the data at any date.
+
+    Raises
+    ------
+    A :exc:`TimeSeriesCompatibilityError` exception is raised if something goes
+    wrong.
+
+    """
+
+    dshape = data.shape
+    dates = np.array(dates, copy=False, ndmin=1)
+    tshape = dates.shape
+    err_args = ('shape', "data: %s" % str(dshape), "dates: %s" % str(tshape))
+    # Same size: all is well
+    #???: The (not dates.size) is introduced to deal with masked
+    if (not dates.size):
+        return ()
+    if (dates.size == data.size):
+        if (dates.ndim > 1) or (data.ndim < 2):
+            return ()
+#    if (dates.size == data.size):
+#        if (dates.ndim > 1) or (data.ndim == 1):
+#            return ()
+    # More dates than data: not good
+    if (dates.size > data.size) or (data.ndim == 1):
+        raise TimeSeriesCompatibilityError(*err_args)
+    #....................
+    dcumulshape = np.cumprod(dshape).tolist()
+    try:
+        k = dcumulshape.index(dates.size)
+    except ValueError:
+        raise TimeSeriesCompatibilityError(*err_args)
+    else:
+        return dshape[k + 1:]
+
+
+def _getdatalength(data):
+    "Estimates the length of a series (size/nb of variables)."
+    if np.ndim(data) >= 2:
+        return np.asarray(np.shape(data))[:-1].prod()
+    else:
+        return np.size(data)
+
+def _compare_frequencies(*series):
+    """Compares the frequencies of a sequence of series.
+
+Returns the common frequency, or raises an exception if series have different
+frequencies.
+"""
+    unique_freqs = np.unique([x.freqstr for x in series])
+    try:
+        common_freq = unique_freqs.item()
+    except ValueError:
+        raise TimeSeriesError, \
+            "All series must have same frequency! (got %s instead)" % \
+            unique_freqs
+    return common_freq
+
+
+
+##### ------------------------------------------------------------------------
+##--- ... Time Series ...
+##### ------------------------------------------------------------------------
+_print_templates = dict(desc="""\
+timeseries(
+ %(data)s,
+    dates =
+ %(time)s,
+    freq  = %(freq)s)
+""",
+                        desc_short="""\
+timeseries(%(data)s,
+   dates = %(time)s,
+   freq  = %(freq)s)
+""",
+                        desc_flx="""\
+timeseries(
+ %(data)s,
+   dtype = %(dtype)s,
+   dates =
+ %(time)s,
+   freq  = %(freq)s)
+""",
+                        desc_flx_short="""\
+timeseries(%(data)s,
+   dtype = %(dtype)s,
+   dates = %(time)s,
+   freq  = %(freq)s)
+"""
+)
+
+
+
+class _tsmathmethod(object):
+    """
+    Defines a wrapper for arithmetic array methods (add, mul...).
+    When called, returns a new TimeSeries object, with the new series the result
+    of the method applied on the original series. The `_dates` part remains
+    unchanged.
+    """
+    def __init__ (self, methodname):
+        self.__name__ = methodname
+        self.__doc__ = getattr(MaskedArray, methodname).__doc__
+        self.obj = None
+
+    def __get__(self, obj, objtype=None):
+        "Gets the calling object."
+        self.obj = obj
+        return self
+
+    def __call__ (self, other, *args):
+        "Execute the call behavior."
+        instance = self.obj
+        if isinstance(other, TimeSeries):
+            compat = _timeseriescompat(instance, other, raise_error=False)
+        else:
+            compat = True
+        func = getattr(super(TimeSeries, instance), self.__name__)
+        if compat:
+            result = np.array(func(other, *args), subok=True).view(type(instance))
+            result._dates = instance._dates
+        else:
+            other_ = getattr(other, '_series', other)
+            result_ = func(other_, *args)
+            result = getattr(result_, '_series', result_)
+        return result
+
+
+class _tsarraymethod(object):
+    """
+    Defines a wrapper for basic array methods.
+    When called, returns a new TimeSeries object, with the new series the result
+    of the method applied on the original series.
+    If `ondates` is True, the same operation is performed on the `_dates`.
+    If `ondates` is False, the `_dates` part remains unchanged.
+    """
+    def __init__ (self, methodname, ondates=False):
+        self.__name__ = methodname
+        self.__doc__ = getattr(MaskedArray, methodname).__doc__
+        self._ondates = ondates
+        self.obj = None
+
+    def __get__(self, obj, objtype=None):
+        self.obj = obj
+        return self
+
+    def __call__ (self, *args, **kwargs):
+        "Execute the call behavior."
+        _name = self.__name__
+        instance = self.obj
+        # Fallback: if the instance has not been set, use the first argument
+        if instance is None:
+            args = list(args)
+            instance = args.pop(0)
+        _series = ndarray.__getattribute__(instance, '_series')
+        _dates = ndarray.__getattribute__(instance, '_dates')
+        func_series = getattr(_series, _name)
+        result = func_series(*args, **kwargs).view(type(instance))
+        if self._ondates:
+            result._dates = getattr(_dates, _name)(*args, **kwargs)
+        else:
+            result._dates = _dates
+        return result
+
+
+class _tsaxismethod(object):
+    """
+    Defines a wrapper for array methods working on an axis (mean...).
+
+    When called, returns a ndarray, as the result of the method applied on the
+    series.
+    """
+    def __init__ (self, methodname):
+        """abfunc(fillx, filly) must be defined.
+           abinop(x, filly) = x for all x to enable reduce.
+        """
+        self.__name__ = methodname
+        self.__doc__ = getattr(MaskedArray, methodname).__doc__
+        self.obj = None
+
+    def __get__(self, obj, objtype=None):
+        self.obj = obj
+        return self
+
+    def __call__ (self, *args, **params):
+        "Execute the call behavior."
+        instance = self.obj
+        if instance is None:
+            args = list(args)
+            instance = args.pop(0)
+        (_dates, _series) = (instance._dates, instance._series)
+        func = getattr(_series, self.__name__)
+        result = func(*args, **params)
+        if _dates.size != _series.size:
+            axis = params.get('axis', None)
+            if axis is None and len(args):
+                axis = args[0]
+            if axis in [-1, _series.ndim - 1]:
+                result = result.view(type(instance))
+                result._dates = _dates
+        return result
+
+
+
+
+
+
+class TimeSeries(MaskedArray, object):
+    """
+    Base class for the definition of time series.
+
+    Parameters
+    ----------
+    data : {array_like}
+        Data portion of the array.
+        Any data that is valid for constructing a MaskedArray can be used here.
+    dates : {DateArray}
+        A `DateArray` instance.
+    **optional_parameters:
+        All the parameters recognized by `MaskedArray` are also recognized by
+        TimeSeries.
+
+    Notes
+    -----
+    It is recommended to use the :func:`time_series` function for construction,
+    as it is more flexible and convenient.
+
+    See Also
+    --------
+    numpy.ma.MaskedArray
+        ndarray with support for missing data.
+    scikits.timeseries.DateArray
+    """
+
+    __array_priority__ = 20
+
+    def __new__(cls, data, dates, mask=nomask, dtype=None, copy=False,
+                fill_value=None, subok=True, keep_mask=True, hard_mask=False,
+                autosort=True, **options):
+
+        maparms = dict(copy=copy, dtype=dtype, fill_value=fill_value,
+                       subok=subok, keep_mask=keep_mask, hard_mask=hard_mask)
+        _data = MaskedArray.__new__(cls, data, mask=mask, **maparms)
+
+        # Get the data .......................................................
+        if not subok or not isinstance(_data, TimeSeries):
+            _data = _data.view(cls)
+        if _data is masked:
+            assert(np.size(dates) == 1)
+            return _data.view(cls)
+        # Check that the dates and data are compatible in shape.
+        _data._varshape = get_varshape(_data, dates)
+        # Set the dates
+        _data._dates = dates
+        if autosort:
+            _data.sort_chronologically()
+        return _data
+
+    def __array_finalize__(self, obj):
+        self._varshape = getattr(obj, '_varshape', ())
+        MaskedArray.__array_finalize__(self, obj)
+
+    def _update_from(self, obj):
+        _dates = getattr(self, '_dates', nodates)
+        newdates = getattr(obj, '_dates', nodates)
+        # Only update the dates if we don't have any
+        if not getattr(_dates, 'size', 0):
+            self.__setdates__(newdates)
+        MaskedArray._update_from(self, obj)
+
+
+    def view(self, dtype=None, type=None):
+        try:
+            output = super(TimeSeries, self).view(dtype=dtype, type=type)
+        except ValueError:
+            output = super(TimeSeries, self).view(dtype)
+        if isinstance(output, TimeSeries):
+            if output.dtype.fields:
+                output._varshape = ()
+            else:
+                fields = self.dtype.fields
+                if fields:
+                    output._varshape = (len(fields),)
+        return output
+
+
+    def _get_series(self):
+        """
+    Returns a view of the instance as a regular masked array.
+
+    This attribute is read-only.
+        """
+        _mask = self._mask
+        if _mask.ndim == 0 and _mask:
+            return masked
+        return self.view(MaskedArray)
+    series = _series = property(fget=_get_series)
+
+    @property
+    def varshape(self):
+        """
+    Returns the shape of the underlying variables.
+        """
+        return self._varshape
+
+
+    def _index_checker(self, indx):
+        """
+    Private function to process the index.
+        """
+        # Basic index ............
+        if isinstance(indx, int):
+            return (indx, indx, False)
+        _dates = self._dates
+        # String index : field name or date ?
+        if isinstance(indx, basestring):
+            if indx in (self.dtype.names or ()):
+                return (indx, slice(None, None, None), False)
+            try:
+                indx = _dates.date_to_index(Date(_dates.freq, string=indx))
+            except IndexError:
+                # Trap the exception: we need the traceback
+                exc_info = sys.exc_info()
+                msg = "Invalid field or date '%s'" % indx
+                raise IndexError(msg), None, exc_info[2]
+            return (indx, indx, False)
+        # Date or DateArray index ..........
+        if isinstance(indx, (Date, DateArray)):
+            indx = _dates.date_to_index(indx)
+            return (indx, indx, False)
+        # Slice ............................
+        if isinstance(indx, slice):
+            indx = slice(self._slicebound_checker(indx.start),
+                         self._slicebound_checker(indx.stop),
+                         indx.step)
+            return (indx, indx, False)
+        # Tuple index ......................
+        if isinstance(indx, tuple):
+            if not self._varshape:
+                return (indx, indx, False)
+            else:
+                return (indx, indx[0], False)
+        return (indx, indx, True)
+
+
+    def _slicebound_checker(self, bound):
+        "Private functions to check the bounds of a slice"
+        # Integer bound (or None) ..........
+        if bound is None or isinstance(bound, int):
+            return bound
+        # The bound is a date (string or Date)
+        _dates = self._dates
+        if isinstance(bound, str):
+            bound = Date(_dates.freq, string=bound)
+        if not isinstance(bound, Date):
+            raise ValueError(
+                "invalid object used in slice: %s" % repr(bound))
+        if bound.freq != _dates.freq:
+            raise TimeSeriesCompatibilityError('freq',
+                                               _dates.freq, bound.freq)
+        # this allows for slicing with dates outside the end points of the
+        # series and slicing on series with missing dates
+        return np.sum(self._dates < bound)
+
+
+    def __getitem__(self, indx):
+        """x.__getitem__(y) <==> x[y]
+
+    Returns the item described by i. Not a copy.
+        """
+        (sindx, dindx, recheck) = self._index_checker(indx)
+        _data = ndarray.__getattribute__(self, '_data')
+        _mask = ndarray.__getattribute__(self, '_mask')
+        _dates = ndarray.__getattribute__(self, '_dates')
+        try:
+            output = _data.__getitem__(sindx)
+        except IndexError:
+            # We don't need to recheck the index: just raise an exception
+            if not recheck:
+                raise
+            # Maybe the index is a list of Dates ?
+            try:
+                indx = _dates.date_to_index(indx)
+            except (IndexError, ValueError):
+                # Mmh, is it a list of dates as strings ?
+                try:
+                    indx = _dates.date_to_index(date_array(indx,
+                                                           freq=_dates.freq))
+                except (IndexError, ValueError, DateError):
+                    exc_info = sys.exc_info()
+                    msg = "Invalid index or date '%s'" % indx
+                    raise IndexError(msg), None, exc_info[2]
+                else:
+                    output = _data.__getitem__(indx)
+                    sindx = dindx = indx
+            else:
+                output = _data.__getitem__(indx)
+                sindx = dindx = indx
+        # Don't find the date if it's not needed......
+        if not getattr(output, 'ndim', False):
+            # A record ................
+            if isinstance(output, np.void):
+                mask = _mask[sindx]
+                if mask.view((bool, len(mask.dtype))).any():
+                    output = masked_array(output, mask=mask)
+                else:
+                    return output
+            elif _mask is not nomask and _mask[sindx]:
+                return masked
+            return output
+        # Get the date................................
+        newdates = _dates.__getitem__(dindx)
+        if not getattr(newdates, 'shape', 0):
+            # No dates ? Output a MaskedArray
+            newseries = output.view(MaskedArray)
+        else:
+            # Some dates: output a TimeSeries
+            newseries = output.view(type(self))
+            newseries._dates = newdates
+        # Update some info from self (fill_value, _basedict...)
+        MaskedArray._update_from(newseries, self)
+        # Fix the fill_value if we were accessing a field of a flexible array
+        if isinstance(sindx, basestring):
+            _fv = self._fill_value
+            if _fv is not None and not np.isscalar(_fv):
+                 newseries._fill_value = _fv[sindx]
+            newseries._isfield = True
+        # Fix the mask
+        if _mask is not nomask:
+            newseries._mask = _mask[sindx]
+            newseries._sharedmask = True
+        return newseries
+
+
+    def __setitem__(self, indx, value):
+        """x.__setitem__(i, y) <==> x[i]=y
+
+    Sets item described by index. If value is masked, masks those locations.
+        """
+        (sindx, dindx, recheck) = self._index_checker(indx)
+        _dates = ndarray.__getattribute__(self, '_dates')
+        try:
+            MaskedArray.__setitem__(self, sindx, value)
+        except IndexError:
+            # We don't need to recheck the index: just raise an exception
+            if not recheck:
+                raise
+            # Maybe the index is a list of Dates ?
+            try:
+                indx = _dates.date_to_index(indx)
+            except (IndexError, ValueError):
+                # Mmh, is it a list of dates as strings ?
+                try:
+                    indx = _dates.date_to_index(date_array(indx,
+                                                           freq=_dates.freq))
+                except (IndexError, ValueError, DateError):
+                    exc_info = sys.exc_info()
+                    msg = "Invalid index or date '%s'" % indx
+                    raise IndexError(msg), None, exc_info[2]
+                else:
+                    MaskedArray.__setitem__(self, indx, value)
+            else:
+                MaskedArray.__setitem__(self, indx, value)
+
+    def __setattr__(self, attr, value):
+        if attr in ['_dates', 'dates']:
+            return self.__setdates__(value)
+        elif attr == 'shape':
+            if self._varshape:
+                err_msg = "Reshaping a nV/nD series is not implemented yet !"
+                raise NotImplementedError(err_msg)
+            else:
+                self._dates.shape = value
+        return ndarray.__setattr__(self, attr, value)
+
+
+    def __setdates__(self, value):
+        """
+    Sets the dates to `value`.
+        """
+        # Make sure it's a DateArray
+        if not isinstance(value, DateArray):
+            err_msg = "The input dates should be a valid "\
+                      "DateArray object (got %s instead)" % type(value)
+            raise TypeError(err_msg)
+        # Skip if dates is nodates (or empty)\
+        if value is nodates or not getattr(value, 'size', 0):
+            return super(TimeSeries, self).__setattr__('_dates', value)
+        # Make sure it has the proper size
+        tsize = getattr(value, 'size', 1)
+        # Check the _varshape
+        varshape = self._varshape
+        if not varshape:
+            # We may be using the default: retry
+            varshape = self._varshape = get_varshape(self, value)
+        # Get the data length (independently of the nb of variables)
+        dsize = self.size // int(np.prod(varshape))
+        if tsize != dsize:
+            raise TimeSeriesCompatibilityError("size",
+                                               "data: %s" % dsize,
+                                               "dates: %s" % tsize)
+#        # Check whether the dates are already sorted
+#        if not value.is_chronological():
+#            _cached = value._cachedinfo
+#            idx = _cached['chronidx']
+#            _series = self._series
+#            if not varshape:
+#                if self.ndim > 1:
+#                    flatseries = _series.flat
+#                    flatseries[:] = flatseries[idx]
+#                else:
+#                    _series[:] = _series[idx]
+#            else:
+#                inishape = self.shape
+#                _series.shape = tuple([-1,]+list(varshape))
+#                _series[:] = _series[idx]
+#                _series.shape = inishape
+#            _cached['chronidx'] = np.array([], dtype=int)
+#            _cached['ischrono'] = True
+        #
+        if not varshape and (value.shape != self.shape):
+            # The data is 1D
+            value = value.reshape(self.shape)
+        super(TimeSeries, self).__setattr__('_dates', value)
+        return
+
+    dates = property(fget=lambda self:self._dates,
+                     fset=__setdates__)
+
+
+    #
+    def sort_chronologically(self):
+        """
+    Sort the series by chronological order (in place).
+
+    Notes
+    -----
+    This method sorts the series **in place**.
+    To sort the a copy of the series, use the :func:`sort_chronologically`
+    function.
+
+    See Also
+    --------
+    sort_chronologically
+        Equivalent function.
+        """
+        _dates = self._dates
+        _series = self._series
+        if not _dates.is_chronological():
+            _cached = _dates._cachedinfo
+            idx = _cached['chronidx']
+            if not self._varshape:
+                flatseries = _series.flat
+                flatseries[:] = flatseries[idx]
+            else:
+                inishape = self.shape
+                _series.shape = tuple([-1, ] + list(self._varshape))
+                _series[:] = _series[idx]
+                _series.shape = inishape
+            # Sort the dates and reset the cache
+            flatdates = _dates.ravel()
+            flatdates[:] = flatdates[idx]
+            _cached['chronidx'] = np.array([], dtype=int)
+            _cached['ischrono'] = True
+
+
+
+    def __str__(self):
+        """Returns a string representation of self (w/o the dates...)"""
+        return str(self._series)
+
+
+    def __repr__(self):
+        """
+    Calculates the repr representation, using masked for fill if it is
+    enabled. Otherwise fill with fill value.
+    """
+        _dates = self._dates
+        if np.size(self._dates) > 2 and self.is_valid():
+            timestr = "[%s ... %s]" % (str(_dates[0]), str(_dates[-1]))
+        else:
+            timestr = str(_dates)
+        kwargs = {'data': str(self._series), 'time': timestr,
+                  'freq': self.freqstr, 'dtype': self.dtype}
+        names = kwargs['dtype'].names
+        if self.ndim <= 1:
+            if names:
+                return _print_templates['desc_flx_short'] % kwargs
+            return _print_templates['desc_short'] % kwargs
+        if names:
+            return _print_templates['desc_flx'] % kwargs
+        return _print_templates['desc'] % kwargs
+
+    #............................................
+    __add__ = _tsmathmethod('__add__')
+    __radd__ = _tsmathmethod('__add__')
+    __sub__ = _tsmathmethod('__sub__')
+    __rsub__ = _tsmathmethod('__rsub__')
+    __pow__ = _tsmathmethod('__pow__')
+    __mul__ = _tsmathmethod('__mul__')
+    __rmul__ = _tsmathmethod('__mul__')
+    __div__ = _tsmathmethod('__div__')
+    __rdiv__ = _tsmathmethod('__rdiv__')
+    __truediv__ = _tsmathmethod('__truediv__')
+    __rtruediv__ = _tsmathmethod('__rtruediv__')
+    __floordiv__ = _tsmathmethod('__floordiv__')
+    __rfloordiv__ = _tsmathmethod('__rfloordiv__')
+    __eq__ = _tsmathmethod('__eq__')
+    __ne__ = _tsmathmethod('__ne__')
+    __lt__ = _tsmathmethod('__lt__')
+    __le__ = _tsmathmethod('__le__')
+    __gt__ = _tsmathmethod('__gt__')
+    __ge__ = _tsmathmethod('__ge__')
+
+    copy = _tsarraymethod('copy', ondates=True)
+    compress = _tsarraymethod('compress', ondates=True)
+    cumsum = _tsarraymethod('cumsum', ondates=False)
+    cumprod = _tsarraymethod('cumprod', ondates=False)
+    anom = _tsarraymethod('anom', ondates=False)
+
+    sum = _tsaxismethod('sum')
+    prod = _tsaxismethod('prod')
+    mean = _tsaxismethod('mean')
+    var = _tsaxismethod('var')
+    std = _tsaxismethod('std')
+    all = _tsaxismethod('all')
+    any = _tsaxismethod('any')
+
+
+    def ravel(self):
+        """
+    Returns a ravelled view of the instance.
+
+    If the instance corresponds to one variable (e.g., ``self.varshape == ()``),
+    the result is the ravelled view of the input.
+    Otherwise, the result is actually a TimeSeries where the `series` attribute
+    is a reshaped view of the input and where the `dates` attribute is
+    a ravelled view of the input `dates` attribute.
+
+    Examples
+    --------
+    >>> start_date = ts.Date('M', '2001-01')
+    >>> dates = ts.date_array(start_date=start_date, length=4)
+    >>> series = ts.time_series([[1, 2], [3, 4]], dates=dates)
+    >>> series
+    timeseries(
+     [[1 2]
+     [3 4]],
+        dates =
+     [[Jan-2001 Feb-2001] ... [Mar-2001 Apr-2001]],
+        freq  = M)
+    >>> series.ravel()
+    timeseries([1 2 3 4],
+       dates = [Jan-2001 ... Apr-2001],
+       freq  = M)
+    >>> series = ts.time_series([[1, 2], [3, 4]], start_date=start_date)
+    >>> series
+    timeseries(
+     [[1 2]
+     [3 4]],
+        dates =
+     [Jan-2001 Feb-2001],
+        freq  = M)
+    >>> series.ravel()
+    timeseries(
+     [[1 2]
+     [3 4]],
+        dates =
+     [Jan-2001 Feb-2001],
+        freq  = M)
+        """
+        _varshape = self._varshape
+        if _varshape:
+            newshape = tuple([-1] + [np.prod(_varshape), ])
+            result = MaskedArray.reshape(self, *newshape)
+        else:
+            result = MaskedArray.ravel(self)
+        result._dates = self._dates.ravel()
+        return result
+
+
+    def reshape(self, *newshape, **kwargs):
+        """
+    Returns a time series containing the data of a, but with a new shape.
+
+    The result is a view to the original array; if this is not possible,
+    a ValueError is raised.
+
+    Parameters
+    ----------
+    shape : shape tuple or int
+       The new shape should be compatible with the original shape. If an
+       integer, then the result will be a 1D array of that length.
+    order : {'C', 'F'}, optional
+        Determines whether the array data should be viewed as in C
+        (row-major) order or FORTRAN (column-major) order.
+
+    Returns
+    -------
+    reshaped_array : array
+        A new view to the timeseries.
+
+    Warnings
+    --------
+    The `._dates` part is reshaped, but the order is NOT ensured.
+
+        """
+        kwargs.update(order=kwargs.get('order', 'C'))
+        if self._varshape:
+            try:
+                bkdtype = (self.dtype, self._varshape)
+                _series = self._series.view([('', bkdtype)])
+                result = _series.reshape(*newshape, **kwargs)
+                result = result.view(dtype=bkdtype, type=type(self))
+                result._dates = ndarray.reshape(self._dates, *newshape, **kwargs)
+            except:
+                err_msg = "Reshaping a nV/nD series is not implemented yet !"
+                raise NotImplementedError(err_msg)
+        # 1D series : reshape the dates as well
+        else:
+            result = MaskedArray.reshape(self, *newshape, **kwargs)
+            result._dates = ndarray.reshape(self._dates, *newshape, **kwargs)
+            result._varshape = ()
+        return result
+
+    #.........................................................................
+    def ids (self):
+        """Return the ids of the data, dates and mask areas"""
+        return (id(self._series), id(self.dates),)
+
+    #.........................................................................
+    @property
+    def freq(self):
+        """Returns the corresponding frequency (as an integer)."""
+        return self._dates.freq
+    @property
+    def freqstr(self):
+        """Returns the corresponding frequency (as a string)."""
+        return self._dates.freqstr
+
+    @property
+    def year(self):
+        """Returns the year for each date of the instance."""
+        return self._dates.year
+    years = year
+    @property
+    def qyear(self):
+        """
+    For quarterly frequencies, returns the equivalent of the 'fiscal' year
+    for each date of the instance.
+    For non-quarterly frequencies, returns the year.
+        """
+        return self._dates.qyear
+    @property
+    def quarter(self):
+        """Returns the quarter for each date of the instance."""
+        return self._dates.quarter
+    quarters = quarter
+    @property
+    def month(self):
+        """Returns the month for each date of the instance."""
+        return self._dates.month
+    months = month
+    @property
+    def week(self):
+        """Returns the week for each date in self._dates."""
+        return self._dates.week
+    weeks = week
+    @property
+    def day(self):
+        """Returns the day of month for each date of the instance."""
+        return self._dates.day
+    days = day
+    @property
+    def day_of_week(self):
+        """Returns the day of week for each date of the instance."""
+        return self._dates.weekday
+    weekdays = weekday = day_of_week
+    @property
+    def day_of_year(self):
+        """Returns the day of year for each date of the instance."""
+        return self._dates.day_of_year
+    yeardays = day_of_year
+    @property
+    def hour(self):
+        """Returns the hour for each date in self._dates."""
+        return self._dates.hour
+    hours = hour
+    @property
+    def minute(self):
+        """Returns the minute for each date in self._dates."""
+        return self._dates.minute
+    minutes = minute
+    @property
+    def second(self):
+        """Returns the second for each date in self._dates."""
+        return self._dates.second
+    seconds = second
+
+    @property
+    def start_date(self):
+        """Returns the first date of the series."""
+        _dates = self._dates
+        dsize = _dates.size
+        if dsize == 0:
+            return None
+        elif dsize == 1:
+            return _dates[0]
+        else:
+            return Date(self.freq, _dates.flat[0])
+
+    @property
+    def end_date(self):
+        """Returns the last date of the series."""
+        _dates = self._dates
+        dsize = _dates.size
+        if dsize == 0:
+            return None
+        elif dsize == 1:
+            return _dates[-1]
+        else:
+            return Date(self.freq, _dates.flat[-1])
+
+    def is_valid(self):
+        """Returns whether the series has no duplicate/missing dates."""
+        return self._dates.is_valid()
+
+    def isvalid(self):
+        """Deprecated name: use '.is_valid' instead."""
+        return self._dates.isvalid()
+
+    def has_missing_dates(self):
+        """Returns whether there's a date gap in the series."""
+        return self._dates.has_missing_dates()
+
+    def is_full(self):
+        """Returns whether there's no date gap in the series."""
+        return self._dates.is_full()
+
+    def isfull(self):
+        """Deprecated name: use '.is_full' instead."""
+        return self._dates.isfull()
+
+    def has_duplicated_dates(self):
+        """Returns whether there are duplicated dates in the series."""
+        return self._dates.has_duplicated_dates()
+
+    def get_steps(self):
+        """
+    Returns the time steps between consecutive dates, in the same unit as
+    the frequency of the instance.
+        """
+        return self._dates.get_steps()
+
+    def is_chronological(self):
+        """Returns whether the series is in chronological order."""
+        return self._dates.is_chronological()
+
+    def date_to_index(self, date):
+        """Returns the index corresponding to a given date, as an integer."""
+        return self._dates.date_to_index(date)
+
+    #.....................................................
+    def asfreq(self, freq, relation="END"):
+        """
+    Converts the dates portion of the TimeSeries to another frequency.
+
+    The resulting TimeSeries will have the same shape and dimensions
+    as the original series (unlike the :meth:`convert` method).
+
+    Parameters
+    ----------
+    freq : {freq_spec}
+    relation : {'END', 'START'} (optional)
+
+    Returns
+    -------
+    A new TimeSeries with the :attr:`.dates` :class:`DateArray` at the
+    specified frequency (the :meth`.asfreq` method of the :attr:`.dates`
+    property will be called).
+    The data in the resulting series will be a VIEW of the original series.
+
+    Notes
+    -----
+    The parameters are the exact same as for
+    :meth:`~scikit.timeseries.DateArray.asfreq`. Please see the docstring for
+    that method for details on the parameters and how the actual conversion is
+    performed.
+
+    """
+        if freq is None: return self
+
+        return TimeSeries(self._series,
+                          dates=self._dates.asfreq(freq, relation=relation))
+    #.....................................................
+    def transpose(self, *axes):
+        if self._dates.size == self.size:
+            result = MaskedArray.transpose(self, *axes)
+            result._dates = self._dates.transpose(*axes)
+        else:
+            errmsg = "Operation not permitted on multi-variable series"
+            if (len(axes) == 0) or axes[0] != 0:
+                raise TimeSeriesError, errmsg
+            else:
+                result = MaskedArray.transpose(self, *axes)
+                result._dates = self._dates
+        return result
+    transpose.__doc__ = np.transpose.__doc__
+
+    def split(self):
+        """Split a multi-dimensional series into individual columns."""
+        if self.ndim == 1:
+            return [self]
+        else:
+            n = self.shape[1]
+            arr = hsplit(self, n)[0]
+            return [self.__class__(np.squeeze(a),
+                                   self._dates,
+                                   **_attrib_dict(self)) for a in arr]
+
+    def filled(self, fill_value=None):
+        """
+    Returns an array of the same class as `_data`,  with masked values
+    filled with `fill_value`. Subclassing is preserved.
+
+    Parameters
+    ----------
+    fill_value : {None, singleton of type self.dtype}, optional
+        The value to fill in masked values with.
+        If `fill_value` is None, uses ``self.fill_value``.
+
+    """
+        result = self._series.filled(fill_value=fill_value).view(type(self))
+        result._dates = self._dates
+        return result
+
+
+    def tolist(self):
+        """
+    Returns the dates and data portion of the TimeSeries "zipped" up in
+    a list of standard python objects (eg. datetime, int, etc...).
+
+        """
+        if self.ndim > 0:
+            return zip(self.dates.tolist(), self.series.tolist())
+        else:
+            return self.series.tolist()
+
+    def torecords(self):
+        """
+    Transforms a TimeSeries into a structured array with three fields:.
+
+    * the ``_dates`` field stores the date information;
+    * the ``_data`` field stores the ``_data`` part of the series;
+    * the ``_mask`` field stores the mask.
+
+
+    Returns
+    -------
+    record : ndarray
+        A new flexible-type ndarray with three fields: the first element
+        contains the date (as an integer), the second element contains the
+        corresponding value and the third the corresponding mask boolean.
+        The returned record shape matches the shape of the instance.
+
+        """
+        _varshape = self._varshape
+        if not _varshape:
+            desctype = [('_dates', int),
+                        ('_data', self.dtype),
+                        ('_mask', self.mask.dtype)]
+        else:
+            desctype = [('_dates', int),
+                        ('_data', (self.dtype, _varshape)),
+                        ('_mask', (self.mask.dtype, _varshape))]
+        flat = self.ravel()
+        _dates = np.asarray(flat._dates)
+        if flat.size > 0:
+            result = np.empty(len(_dates), dtype=desctype)
+            result['_dates'] = _dates
+            result['_data'] = flat._data
+            result['_mask'] = flat._mask
+            return result
+        else:
+            return np.array([[], [], []], dtype=desctype,)
+
+    # for backwards compatibility
+    toflex = torecords
+    #......................................................
+    # Pickling
+    def __getstate__(self):
+        """
+
+    Returns the internal state of the TimeSeries, for pickling purposes.
+        """
+    #    raise NotImplementedError,"Please use timeseries.archive/unarchive instead."""
+        state = (1,
+                 self.shape,
+                 self.dtype,
+                 self.flags.fnc,
+                 self._data.tostring(),
+                 getmaskarray(self).tostring(),
+                 self._fill_value,
+                 self._dates.shape,
+                 self._dates.__array__().tostring(),
+                 self.freq,
+                 self._optinfo,
+                 )
+        return state
+    #
+    def __setstate__(self, state):
+        """
+
+    Restores the internal state of the TimeSeries, for pickling purposes.
+    `state` is typically the output of the ``__getstate__`` output, and is a 5-tuple:
+
+        - class name
+        - a tuple giving the shape of the data
+        - a typecode for the data
+        - a binary string for the data
+        - a binary string for the mask.
+        """
+        (ver, shp, typ, isf, raw, msk, flv, dsh, dtm, frq, infodict) = state
+        MaskedArray.__setstate__(self, (ver, shp, typ, isf, raw, msk, flv))
+        _dates = self._dates
+        _dates.__setstate__((ver, dsh, dtype(int_), isf, dtm, frq))
+        _dates.freq = frq
+        _dates._cachedinfo.update(dict(full=None, hasdups=None, steps=None,
+                                       toobj=None, toord=None, tostr=None))
+        # Update the _optinfo dictionary
+        self._optinfo.update(infodict)
+#
+    def __reduce__(self):
+        """Returns a 3-tuple for pickling a MaskedArray."""
+        return (_tsreconstruct,
+                (self.__class__, self._baseclass,
+                 self.shape, self._dates.shape, self.dtype, self._fill_value),
+                self.__getstate__())
+
+def _tsreconstruct(genclass, baseclass, baseshape, dateshape, basetype, fill_value):
+    """Internal function that builds a new TimeSeries from the information stored
+    in a pickle."""
+    #    raise NotImplementedError,"Please use timeseries.archive/unarchive instead."""
+    _series = ndarray.__new__(baseclass, baseshape, basetype)
+    _dates = ndarray.__new__(DateArray, dateshape, int_)
+    _mask = ndarray.__new__(ndarray, baseshape, bool_)
+    return genclass.__new__(genclass, _series, dates=_dates, mask=_mask,
+                            dtype=basetype, fill_value=fill_value)
+
+
+def _attrib_dict(series, exclude=[]):
+    """this function is used for passing through attributes of one
+time series to a new one being created"""
+    result = {'fill_value':series.fill_value}
+    return dict(filter(lambda x: x[0] not in exclude, result.iteritems()))
+
+
+##### --------------------------------------------------------------------------
+##--- ... Additional methods ...
+##### --------------------------------------------------------------------------
+
+def _extrema(self, method, axis=None, fill_value=None):
+    "Private function used by max/min"
+    (_series, _dates) = (self._series, self._dates)
+    func = getattr(_series, method)
+    idx = func(axis, fill_value)
+
+    # 1D series .......................
+    if (_dates.size == _series.size):
+        if axis is None:
+            return self.ravel()[idx]
+        else:
+            return self[idx]
+    # nD series .......................
+    else:
+        if axis is None:
+            idces = np.unravel_index(idx, _series.shape)
+            result = time_series(_series[idces], dates=_dates[idces[0]])
+        else:
+            _shape = _series.shape
+            _dates = np.repeat(_dates, np.prod(_shape[1:])).reshape(_shape)
+            _s = ma.choose(idx, np.rollaxis(_series, axis, 0))
+            _d = ma.choose(idx, np.rollaxis(_dates, axis, 0))
+            result = time_series(_s, dates=_d, freq=_dates.freq)
+        return result
+
+def _max(self, axis=None, fill_value=None):
+    """Return the maximum of self along the given axis.
+    Masked values are filled with fill_value.
+
+    Parameters
+    ----------
+    axis : int, optional
+        Axis along which to perform the operation.
+        If None, applies to a flattened view of the array.
+    fill_value : {var}, optional
+        Value used to fill in the masked values.
+        If None, use the the output of maximum_fill_value().
+    """
+    return _extrema(self, 'argmax', axis, fill_value)
+TimeSeries.max = _max
+
+def _min(self, axis=None, fill_value=None):
+    """Return the minimum of self along the given axis.
+    Masked values are filled with fill_value.
+
+    Parameters
+    ----------
+    axis : int, optional
+        Axis along which to perform the operation.
+        If None, applies to a flattened view of the array.
+    fill_value : {var}, optional
+        Value used to fill in the masked values.
+        If None, use the the output of minimum_fill_value().
+    """
+    return _extrema(self, 'argmin', axis, fill_value)
+TimeSeries.min = _min
+
+
+#.......................................
+
+
+class _tsblockedmethods(object):
+    """Defines a wrapper for array methods that should be temporarily disabled.
+    """
+    def __init__ (self, methodname):
+        """abfunc(fillx, filly) must be defined.
+           abinop(x, filly) = x for all x to enable reduce.
+        """
+        self._name = methodname
+        self.obj = None
+    #
+    def __get__(self, obj, objtype=None):
+        self.obj = obj
+        return self
+    #
+    def __call__ (self, *args, **params):
+        raise NotImplementedError
+
+TimeSeries.swapaxes = _tsarraymethod('swapaxes', ondates=True)
+
+#####---------------------------------------------------------------------------
+#---- --- Definition of functions from the corresponding methods ---
+#####---------------------------------------------------------------------------
+class _frommethod(object):
+    """Defines functions from existing MaskedArray methods.
+:ivar _methodname (String): Name of the method to transform.
+    """
+    def __init__(self, methodname):
+        self.__name__ = methodname
+        self.__doc__ = self.getdoc()
+    def getdoc(self):
+        "Returns the doc of the function (from the doc of the method)."
+        try:
+            return getattr(TimeSeries, self.__name__).__doc__
+        except:
+            return "???"
+    #
+    def __call__ (self, caller, *args, **params):
+        if hasattr(caller, self.__name__):
+            method = getattr(caller, self.__name__)
+            # If method is not callable, it's a property, and don't call it
+            if hasattr(method, '__call__'):
+                return method.__call__(*args, **params)
+            return method
+        method = getattr(np.asarray(caller), self.__name__)
+        try:
+            return method(*args, **params)
+        except SystemError:
+            return getattr(np, self.__name__).__call__(caller, *args, **params)
+#............................
+weekday = _frommethod('weekday')
+day_of_year = _frommethod('day_of_year')
+week = _frommethod('week')
+year = _frommethod('year')
+quarter = _frommethod('quarter')
+month = _frommethod('month')
+day = _frommethod('day')
+hour = _frommethod('hour')
+minute = _frommethod('minute')
+second = _frommethod('second')
+
+split = _frommethod('split')
+
+torecords = toflex = _frommethod('toflex')
+
+#
+##### ---------------------------------------------------------------------------
+#---- ... Additional methods ...
+##### ---------------------------------------------------------------------------
+def tofile(series, fileobject, format=None,
+           separator=" ", linesep='\n', precision=5,
+           suppress_small=False, keep_open=False):
+    """
+    Writes the TimeSeries to a file. The series should be 2D at most.
+
+    Parameters
+    ----------
+    series : TimeSeries
+        The array to write.
+    fileobject
+        An open file object or a string to a valid filename.
+    format : {None, string}, optional
+        Format string for the date.
+        If None, uses the default date format.
+    separator : {string}, optional
+        Separator to write between elements of the array.
+    linesep : {string}, optional
+        Separator to write between rows of array.
+    precision : {integer}, optional
+        Number of digits after the decimal place to write.
+    suppress_small : {boolean}, optional
+        Whether on-zero to round small numbers down to 0.0
+    keep_open : {boolean}, optional
+        Whether to close the file or to return the open file.
+
+    Returns
+    -------
+    file : file object
+        The open file (if ``keep_open`` is non-zero).
+    """
+
+    try:
+        import scipy.io
+    except ImportError:
+        raise ImportError("scipy is required for the tofile function/method")
+
+    (_dates, _data) = (series._dates, series._series)
+    optpars = dict(separator=separator, linesep=linesep, precision=precision,
+                   suppress_small=suppress_small, keep_open=keep_open)
+    if _dates.size == _data.size:
+        # 1D version
+        tmpfiller = ma.empty((_dates.size, 2), dtype=np.object_)
+        _data = _data.reshape(-1)
+        tmpfiller[:, 1:] = ma.atleast_2d(_data).T
+    else:
+        sshape = list(_data.shape)
+        sshape[-1] += 1
+        tmpfiller = ma.empty(sshape, dtype=np.object_)
+        tmpfiller[:, 1:] = _data
+    #
+    if format is None:
+        tmpfiller[:, 0] = _dates.ravel().tostring()
+    else:
+        tmpfiller[:, 0] = [_.strftime(format) for _ in _dates.ravel()]
+    return scipy.io.write_array(fileobject, tmpfiller, **optpars)
+
+
+TimeSeries.tofile = tofile
+
+
+def flatten(series):
+    """
+    Flattens a (multi-) time series to 1D series.
+
+    """
+    shp_ini = series.shape
+    # Already flat time series....
+    if len(shp_ini) == 1:
+        return series
+    # Folded single time series ..
+    newdates = series._dates.ravel()
+    if series._dates.size == series._series.size:
+        newshape = (series._series.size,)
+    else:
+        newshape = (np.asarray(shp_ini[:-1]).prod(), shp_ini[-1])
+    newseries = series._series.reshape(newshape)
+    return time_series(newseries, newdates)
+
+TimeSeries.flatten = flatten
+
+
+def compressed(series):
+    """
+    Suppresses missing values from a time series.
+
+    Returns a :class:`TimeSeries` object.
+
+    """
+    if series._mask is nomask:
+        return series
+    if series.ndim == 1:
+        keeper = ~(series._mask)
+    elif series.ndim == 2:
+        _dates = series._dates
+        _series = series._series
+        # Both dates and data are 2D: ravel first
+        if _dates.ndim == 2:
+            series = series.ravel()
+            keeper = ~(series._mask)
+        # 2D series w/ only one date : return a new series ....
+        elif _dates.size == 1:
+            result = _series.compressed().view(type(series))
+            result._dates = series.dates
+            return result
+        # a 2D series: suppress the rows (dates are in columns)
+        else:
+            keeper = ~(series._mask.any(-1))
+    else:
+        raise NotImplementedError
+    return series[keeper]
+TimeSeries.compressed = compressed
+
+
+##### -------------------------------------------------------------------------
+#---- --- TimeSeries constructor ---
+##### -------------------------------------------------------------------------
+
+def time_series(data, dates=None, start_date=None, length=None, freq=None,
+                mask=nomask, dtype=None, copy=False, fill_value=None,
+                keep_mask=True, hard_mask=False, autosort=True):
+    """
+    Creates a TimeSeries object.
+
+    The ``data`` parameter can be a valid :class:`TimeSeries` object.
+    In that case, the ``dates``, ``start_date`` or ``freq`` parameters are
+    optional: if none of them is given, the dates of the result are the dates of
+    ``data``.
+
+    If ``data`` is not a :class:`TimeSeries`, then ``dates`` must be either
+    ``None`` or an object recognized by the :func:`date_array` function (used
+    internally):
+
+        * an existing :class:`DateArray` object;
+        * a sequence of :class:`Date` objects with the same frequency;
+        * a sequence of :class:`datetime.datetime` objects;
+        * a sequence of dates in string format;
+        * a sequence of integers corresponding to the representation of
+          :class:`Date` objects.
+
+    In any of the last four possibilities, the ``freq`` parameter is mandatory.
+
+    If ``dates`` is ``None``, a continuous :class:`DateArray` is automatically
+    constructed as an array of size ``len(data)`` starting at ``start_date`` and
+    with a frequency ``freq``.
+
+
+    Parameters
+    ----------
+    data : array_like
+        Data portion of the array. Any data that is valid for constructing a
+        :class:`~numpy.ma.MaskedArray` can be used here.
+        :keyword:`data` can also be a :class:`TimeSeries` object.
+    dates : {None, var}, optional
+        A sequence of dates corresponding to each entry.
+    start_date : {Date}, optional
+        Date corresponding to the first entry of the data (index 0).
+        This parameter must be a valid :class:`Date` object, and is mandatory
+        if ``dates`` is None and if ``data`` has a length greater or equal to 1.
+    length : {integer}, optional
+        Length of the dates.
+    freq : {freq_spec}, optional
+        A valid frequency specification, as a string or an integer.
+        This parameter is mandatory if ``dates`` is None.
+        Otherwise, the frequency of the series is set to the frequency
+        of the ``dates`` input.
+
+    Notes
+    -----
+    * All other parameters recognized by the :func:`numpy.ma.array` constructor
+      are also recognized by the function.
+    * If ``data`` is zero-sized, only the ``freq`` parameter is mandatory.
+
+    See Also
+    --------
+    numpy.ma.masked_array
+        Constructor for the :class:`~numpy.ma.MaskedArray` class.
+    scikits.timeseries.date_array
+        Constructor for the :class:`DateArray` class.
+
+    """
+    freq = check_freq(freq)
+
+    if dates is None:
+        _dates = getattr(data, '_dates', None)
+    elif isinstance(dates, (Date, DateArray)):
+        if copy:
+            _dates = date_array(dates, autosort=False).copy()
+        else:
+            _dates = date_array(dates, autosort=False)
+    elif isinstance(dates, (tuple, list, ndarray)):
+        _dates = date_array(dlist=dates, freq=freq, autosort=False)
+    else:
+        _dates = date_array([], freq=freq)
+
+    if _dates is not None:
+        # Make sure _dates has the proper frequency
+        if (freq != _c.FR_UND) and (_dates.freq != freq):
+            _dates = _dates.asfreq(freq)
+    else:
+        dshape = np.shape(data)
+        if len(dshape) > 0:
+            length = length or dshape[0]
+            _dates = date_array(start_date=start_date, freq=freq, length=length)
+        else:
+            _dates = date_array([], freq=freq)
+
+    return TimeSeries(data=data, mask=mask, dates=_dates,
+                      copy=copy, dtype=dtype, subok=True,
+                      fill_value=fill_value, keep_mask=keep_mask,
+                      hard_mask=hard_mask, autosort=autosort)
+
+
+
+
+##### --------------------------------------------------------------------------
+#---- ... Additional functions ...
+##### --------------------------------------------------------------------------
+
+
+def sort_chronologically(series):
+    """
+    Returns a copy of series, sorted chronologically.
+    """
+    series = series.copy()
+    series.sort_chronologically()
+    return series
+
+
+def asrecords(series):
+    "Deprecated version of torecords"
+    warnings.warn("Deprecated function: use torecords instead")
+    return torecords(series)
+
+
+def adjust_endpoints(a, start_date=None, end_date=None, copy=False):
+    """
+    Returns a TimeSeries going from `start_date` to `end_date`.
+
+    Parameters
+    ----------
+    a : TimeSeries
+        TimeSeries object whose dates must be adjusted
+    start_date : Date, optional
+        New starting date. If not specified, the current starting date is used.
+    end_date : Date, optional
+        New ending date. If not specified, the current ending date is used.
+    copy : {False, True}, optional
+        Whether to return a copy of the initial array (:const:`True`)
+        or a reference to the array (:const:`False`), in the case where both
+        the `start_date` and `end_date` both fall into the initial range
+        of dates.
+
+    """
+    # Series validity tests .....................
+    if not isinstance(a, TimeSeries):
+        raise TypeError, "Argument should be a valid TimeSeries object!"
+    if a.freq == 'U':
+        errmsg = "Cannot adjust a series with 'Undefined' frequency."
+        raise TimeSeriesError(errmsg,)
+
+    if not a._dates.is_valid():
+        errmsg = "Cannot adjust a series with missing or duplicated dates."
+        raise TimeSeriesError(errmsg,)
+    # Flatten the series if needed ..............
+    a = a.flatten()
+    shp_flat = a.shape
+    # Dates validity checks .,...................
+    msg = "%s should be a valid Date object! (got %s instead)"
+    if a.dates.size >= 1:
+        (dstart, dend) = a.dates[[0, -1]]
+    else:
+        (dstart, dend) = (None, None)
+    # Skip the empty series case
+    if dstart is None and (start_date is None or end_date is None):
+        errmsg = "Both start_date and end_date must be specified"\
+                 " to adjust endpoints of a zero length series!"
+        raise TimeSeriesError(errmsg,)
+    #....
+    if start_date is None:
+        start_date = dstart
+        start_lag = 0
+    else:
+        if isinstance(start_date, basestring):
+            start_date = Date(a.freq, string=start_date)
+        elif not isinstance(start_date, Date):
+            raise TypeError, msg % ('start_date', type(start_date))
+        if dstart is not None:
+            start_lag = start_date - dstart
+        else:
+            start_lag = start_date
+    #....
+    if end_date is None:
+        end_date = dend
+        end_lag = 0
+    else:
+        if isinstance(end_date, basestring):
+            end_date = Date(a.freq, string=end_date)
+        elif not isinstance(end_date, Date):
+            raise TypeError, msg % ('end_date', type(end_date))
+        if dend is not None:
+            end_lag = end_date - dend
+        else:
+            end_lag = end_date
+    # Check if the new range is included in the old one
+    if start_lag >= 0:
+        if end_lag == 0:
+            if not copy:
+                return a[start_lag:]
+            else:
+                return a[start_lag:].copy()
+        elif end_lag < 0:
+            if not copy:
+                return a[start_lag:end_lag]
+            else:
+                return a[start_lag:end_lag].copy()
+    # Create a new series .......................
+    newdates = date_array(start_date=start_date, end_date=end_date)
+
+    newshape = list(shp_flat)
+    newshape[0] = len(newdates)
+    newshape = tuple(newshape)
+
+    newseries = np.empty(newshape, dtype=a.dtype).view(type(a))
+    #!!!: Here, we may wanna use something else than MaskType
+    newseries.__setmask__(np.ones(newseries.shape, dtype=bool_))
+    newseries._dates = newdates
+    newseries._update_from(a)
+    if dstart is not None:
+        start_date = max(start_date, dstart)
+        end_date = min(end_date, dend) + 1
+        if not copy:
+            newseries[start_date:end_date] = a[start_date:end_date]
+        else:
+            newseries[start_date:end_date] = a[start_date:end_date].copy()
+    return newseries
+TimeSeries.adjust_endpoints = adjust_endpoints
+
+
+
+def align_series(*series, **kwargs):
+    """
+    Aligns several TimeSeries, so that their starting and ending dates match.
+
+    Series are resized and filled with masked values accordingly.
+
+    The resulting series have no missing dates (ie. ``series.is_valid() == True``
+    for each of the resulting series).
+
+    The function accepts two extras parameters:
+    - `start_date` forces the series to start at that given date,
+    - `end_date` forces the series to end at that given date.
+
+    By default, `start_date` and `end_date` are set respectively to the smallest
+    and largest dates of the series.
+    """
+    if len(series) < 2:
+        return series
+    unique_freqs = np.unique([x.freqstr for x in series])
+    common_freq = _compare_frequencies(*series)
+
+    # if any of the series have missing dates, fill them in first
+    filled_series = []
+    for ser in series:
+        if ser.is_valid():
+            filled_series.append(ser)
+        else:
+            filled_series.append(ser.fill_missing_dates())
+
+    start_date = kwargs.pop('start_date',
+                            min([x.start_date for x in filled_series
+                                     if x.start_date is not None]))
+    if isinstance(start_date, str):
+        start_date = Date(common_freq, string=start_date)
+    end_date = kwargs.pop('end_date',
+                          max([x.end_date for x in filled_series
+                                   if x.end_date is not None]))
+    if isinstance(end_date, str):
+        end_date = Date(common_freq, string=end_date)
+
+    return [adjust_endpoints(x, start_date, end_date) for x in filled_series]
+aligned = align_series
+
+
+
+def align_with(*series):
+    """
+    Aligns several TimeSeries to the first of the list, so that their
+    starting and ending dates match.
+
+    The series are resized and padded with masked values accordingly.
+    """
+    if len(series) < 2:
+        return series
+    dates = series[0]._dates[[0, -1]]
+    if len(series) == 2:
+        return adjust_endpoints(series[-1], dates[0], dates[-1])
+    return [adjust_endpoints(x, dates[0], dates[-1]) for x in series[1:]]
+
+
+#....................................................................
+def _convert1d(series, freq, func, position, *args, **kwargs):
+    "helper function for `convert` function"
+    # Check the frequencies ..........................
+    to_freq = check_freq(freq)
+    from_freq = series.freq
+    # Don't do anything if not needed
+    if from_freq == to_freq:
+        return series
+    if from_freq == _c.FR_UND:
+        err_msg = "Cannot convert a series with UNDEFINED frequency."
+        raise TimeSeriesError(err_msg)
+    if to_freq == _c.FR_UND:
+        err_msg = "Cannot convert a series to UNDEFINED frequency."
+        raise TimeSeriesError(err_msg)
+    # Check the validity of the series .....
+    if not series.is_valid():
+        err_msg = "Cannot adjust a series with missing or duplicated dates."
+        raise TimeSeriesError(err_msg)
+
+    # Check the position parameter..........
+    position = position.upper()
+    if position not in ('END', 'START'):
+        err_msg = "Invalid value for position argument: (%s). "\
+                  "Should be in ['END','START']," % str(position)
+        raise ValueError(err_msg)
+
+    start_date = series._dates[0]
+
+    if series.size == 0:
+        return TimeSeries(series, freq=to_freq,
+                          start_date=start_date.asfreq(to_freq))
+
+    data_ = series._series.filled()
+    mask_ = getmaskarray(series)
+
+    if (data_.size // series._dates.size) > 1:
+        raise TimeSeriesError("convert works with 1D data only !")
+
+    cdictresult = pandas._skts.TS_convert(data_, from_freq, to_freq, position,
+                                     int(start_date), mask_)
+    start_date = Date(freq=to_freq, value=cdictresult['startindex'])
+    data_ = masked_array(cdictresult['values'], mask=cdictresult['mask'])
+
+    if data_.ndim == 2:
+        if func is None:
+            newvarshape = data_.shape[1:]
+        else:
+            # Try to use an axis argument
+            try:
+                data_ = func(data_, axis= -1, *args, **kwargs)
+            # Fall back to apply_along_axis (slower)
+            except TypeError:
+                data_ = ma.apply_along_axis(func, -1, data_, *args, **kwargs)
+            newvarshape = ()
+    elif data_.ndim == 1:
+        newvarshape = ()
+
+    newdates = DateArray(np.arange(len(data_)) + start_date, freq=to_freq)
+
+    newseries = data_.view(type(series))
+    newseries._varshape = newvarshape
+    newseries._dates = newdates
+    newseries._update_from(series)
+    return newseries
+
+
+
+def convert(series, freq, func=None, position='END', *args, **kwargs):
+    """
+    Converts a series from one frequency to another, by manipulating both the
+    `data` and `dates` attributes.
+
+    If the input series has any missing dates, it will first be filled in with
+    masked values prior to doing the conversion.
+
+    Parameters
+    ----------
+    series : TimeSeries
+        Series to convert. Skip this parameter if you are calling this as
+        a method of the TimeSeries object instead of the module function.
+    freq : freq_spec
+        Frequency to convert the TimeSeries to. Accepts any valid frequency
+        specification (string or integer)
+    func : function, optional
+        When converting a series to a lower frequency, the :keyword:`func`
+        parameter to perform a calculation on each period of values
+        to aggregate results.
+        For example, when converting a daily series to a monthly series, use
+        :func:`numpy.ma.mean` to get a series of monthly averages.
+        If the first or last value from a period, the functions
+        :func:`~scikits.timeseries.first_unmasked_val` and
+        :func:`~scikits.timeseries.last_unmasked_val` should be used instead.
+        If :keyword:`func` is not given, the output series group the points
+        of the initial series that share the same new date. Thus, if the
+        initial series has a daily frequency and is 1D, the output series is
+        2D.
+    position : {'END', 'START'}, optional
+        When converting a series to a higher frequency, use this parameter to
+        determine where the points should fall in the new period.
+        For example, when converting a monthly series to daily, using
+        position='START' will cause the values to fall on the first day of
+        each month (with all other values being masked).
+    *args : {extra arguments for func parameter}, optional
+        Mandatory parameters of the :keyword:`func` function.
+    **kwargs : {extra keyword arguments for func parameter}, optional
+        Optional keyword parameters of the :keyword:`func` function.
+
+    Returns
+    -------
+    converted_series
+        A new :class:`TimeSeries` at the given frequency, without any missing
+        nor duplicated dates
+
+    """
+    #!!!: Raise some kind of proper exception if the underlying dtype will mess things up
+    #!!!: For example, mean on string array...
+
+    if series.ndim > 2 or series.ndim == 0:
+        raise ValueError(
+            "only series with ndim == 1 or ndim == 2 may be converted")
+
+    if series.has_duplicated_dates():
+        raise TimeSeriesError("The input series must not have duplicated dates!")
+
+    if series.has_missing_dates():
+        # can only convert continuous time series, so fill in missing dates
+        series = fill_missing_dates(series)
+
+    if series.ndim == 1:
+        obj = _convert1d(series, freq, func, position, *args, **kwargs)
+    elif series.ndim == 2:
+        base = _convert1d(series[:, 0], freq, func, position, *args, **kwargs)
+        obj = ma.column_stack([_convert1d(m, freq, func, position,
+                                          *args, **kwargs)._series
+                               for m in series.split()]).view(type(series))
+        obj._dates = base._dates
+        if func is None:
+            shp = obj.shape
+            ncols = base.shape[-1]
+            obj.shape = (shp[0], shp[-1] // ncols, ncols)
+            obj = np.swapaxes(obj, 1, 2)
+
+    return obj
+TimeSeries.convert = convert
+
+
+
+def tshift(series, nper, copy=True):
+    """
+    Returns a series of the same size as `series`, with the same `start_date`
+    and `end_date`, but values shifted by `nper`.
+
+    Parameters
+    ----------
+    series : TimeSeries
+        TimeSeries object to shift. Ignore this parameter if calling this as a
+        method.
+    nper : int
+        Number of periods to shift. Negative numbers shift values to the right,
+        positive to the left.
+    copy : {True, False}, optional
+        copies the data if True, returns a view if False.
+
+    Examples
+    --------
+    >>> series = time_series([0,1,2,3], start_date=Date(freq='A', year=2005))
+    >>> series
+    timeseries(data  = [0 1 2 3],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+    >>> tshift(series, -1)
+    timeseries(data  = [-- 0 1 2],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+    >>> pct_change = 100 * (series/series.tshift(-1, copy=False) - 1)
+
+    """
+    newdata = masked_array(np.empty(series.shape, dtype=series.dtype),
+                           mask=True)
+    if copy:
+        inidata = series._series.copy()
+    else:
+        inidata = series._series
+    if nper < 0:
+        nper = max(-len(series), nper)
+        newdata[-nper:] = inidata[:nper]
+    elif nper > 0:
+        nper = min(len(series), nper)
+        newdata[:-nper] = inidata[nper:]
+    else:
+        newdata = inidata
+    newseries = newdata.view(type(series))
+    newseries._dates = series._dates
+    newseries._update_from(series)
+    return newseries
+TimeSeries.tshift = tshift
+
+#...............................................................................
+def _get_type_num_double(dtype):
+    """
+    Private used to force dtypes upcasting in certain functions
+    (eg. int -> float in pct function).
+    Adapted from function of the same name in the C source code.
+    """
+    if dtype.num < np.dtype('f').num:
+        return np.dtype('d')
+    return dtype
+
+def _pct_generic(series, nper, pct_func):
+    "helper function for the pct_* functions"
+    _dtype = _get_type_num_double(series.dtype)
+    if _dtype != series.dtype:
+        series = series.astype(_dtype)
+    newdata = masked_array(np.empty(series.shape, dtype=series.dtype),
+                           mask=True)
+    if nper < newdata.size:
+        mseries = series.view(MaskedArray)
+        newdata[nper:] = pct_func(mseries, nper)
+    newseries = newdata.view(type(series))
+    newseries._dates = series._dates
+    newseries._update_from(series)
+    return newseries
+
+def pct(series, nper=1):
+    """
+    Returns the rolling percentage change of the series.
+
+    Parameters
+    ----------
+    series : {TimeSeries}
+        TimeSeries object to to calculate percentage chage for. Ignore this
+        parameter if calling this as a method.
+    nper : {int}
+        Number of periods for percentage change.
+
+    Notes
+    -----
+    Series of integer types will be upcast
+    1.0 == 100% in result
+
+    Examples
+    --------
+    >>> series = ts.time_series(
+    ...     [2.,1.,2.,3.], start_date=ts.Date(freq='A', year=2005))
+    >>> series.pct()
+    timeseries([-- -0.5 1.0 0.5],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+    >>> series.pct(2)
+    timeseries([-- -- 0.0 2.0],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+
+    """
+    def pct_func(series, nper):
+        return series[nper:] / series[:-nper] - 1
+    return _pct_generic(series, nper, pct_func)
+TimeSeries.pct = pct
+
+def pct_log(series, nper=1):
+    """
+    Returns the rolling log percentage change of the series. This is defined as
+    the log of the ratio of series[T]/series[T-nper]
+
+    Parameters
+    ----------
+    series : {TimeSeries}
+        TimeSeries object to to calculate log percentage chage for. Ignore this
+        parameter if calling this as a method.
+    nper : {int}
+        Number of periods for percentage change.
+
+    Notes
+    -----
+    Series of integer types will be upcast
+    1.0 == 100% in result
+
+    Examples
+    --------
+    >>> series = ts.time_series(
+    ...     [2.,1.,2.,3.], start_date=ts.Date(freq='A', year=2005))
+    >>> series.pct_log()
+    timeseries([-- -0.69314718056 0.69314718056 0.405465108108],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+    >>> series.pct_log(2)
+    timeseries([-- -- 0.0 1.09861228867],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+
+    """
+    def pct_func(series, nper):
+        return ma.log(series[nper:] / series[:-nper])
+    return _pct_generic(series, nper, pct_func)
+TimeSeries.pct_log = pct_log
+
+def pct_symmetric(series, nper=1):
+    """
+    Returns the rolling symmetric percentage change of the series. This is
+    defined as 2*(series[T] - series[T-nper])/(series[T] - series[T-nper])
+
+    Parameters
+    ----------
+    series : {TimeSeries}
+        TimeSeries object to to calculate symmetric percentage chage for. Ignore
+        this parameter if calling this as a method.
+    nper : {int}
+        Number of periods for percentage change.
+
+    Notes
+    -----
+    Series of integer types will be upcast
+    1.0 == 100% in result
+
+    Examples
+    --------
+    >>> series = ts.time_series(
+    ...     [2.,1.,2.,3.], start_date=ts.Date(freq='A', year=2005))
+    >>> series.pct_symmetric()
+    timeseries([-- -0.666666666667 0.666666666667 0.4],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+    >>> series.pct_symmetric(2)
+    timeseries([-- -- 0.0 1.0],
+               dates = [2005 ... 2008],
+               freq  = A-DEC)
+
+    """
+    def pct_func(series, nper):
+        return \
+            2 * (series[nper:] - series[:-nper]) / \
+                (series[nper:] + series[:-nper])
+    return _pct_generic(series, nper, pct_func)
+TimeSeries.pct_symmetric = pct_symmetric
+
+
+
+def fill_missing_dates(data, dates=None, freq=None, fill_value=None):
+    """
+    Finds and fills the missing dates in a time series. The data
+    corresponding to the initially missing dates are masked, or filled to
+    `fill_value`.
+
+    Parameters
+    ----------
+    data : {TimeSeries, ndarray}
+        Initial array of data.
+    dates : {DateArray} (optional)
+        Initial array of dates. Specify this if you are passing a plain ndarray
+        for the data instead of a :class:`TimeSeries`.
+    freq : {freq_spec} (optional)
+        Frequency of result. If not specified, the initial frequency is used.
+    fill_value : {scalar of type data.dtype} (optional)
+        Default value for missing data. If Not specified, the data are just
+        masked.
+
+    """
+    # Check the frequency ........
+    orig_freq = freq
+    freq = check_freq(freq)
+    if orig_freq is not None and freq == _c.FR_UND:
+        freqstr = check_freq_str(freq)
+        raise ValueError, \
+              "Unable to define a proper date resolution (found %s)." % freqstr
+    # Check the dates .............
+    if dates is None:
+        if not isinstance(data, TimeSeries):
+            raise InsufficientDateError
+        dates = data._dates
+    else:
+        if not isinstance(dates, DateArray):
+            dates = DateArray(dates, freq)
+    dflat = dates.asfreq(freq).ravel()
+    if not dflat.has_missing_dates():
+        if isinstance(data, TimeSeries):
+            return data
+        data = data.view(TimeSeries)
+        data._dates = dflat
+        return data
+    # Check the data ..............
+    if isinstance(data, MaskedArray):
+        datad = data._data
+        datam = data._mask
+        if isinstance(data, TimeSeries):
+            datat = type(data)
+            datas = data._varshape
+        else:
+            datat = TimeSeries
+            datas = ()
+    else:
+        datad = np.asarray(data)
+        datam = nomask
+        datat = TimeSeries
+    # Check whether we need to flatten the data
+    if data.ndim > 1:
+        if (not datas):
+            datad.shape = -1
+        elif dflat.size != len(datad):
+            err_msg = "fill_missing_dates is not yet implemented for nD series!"
+            raise NotImplementedError(err_msg)
+    # ...and now, fill it ! ......
+    (tstart, tend) = dflat[[0, -1]]
+    newdates = date_array(start_date=tstart, end_date=tend)
+    (osize, nsize) = (dflat.size, newdates.size)
+    #.............................
+    # Get the steps between consecutive data.
+    delta = dflat.get_steps() - 1
+    gap = delta.nonzero()
+    slcid = np.concatenate(([0, ], np.arange(1, osize)[gap], [osize, ]))
+    oldslc = np.array([slice(i, e)
+                       for (i, e) in np.broadcast(slcid[:-1], slcid[1:])])
+    addidx = delta[gap].astype(int).cumsum()
+    newslc = np.concatenate(([oldslc[0]],
+                             [slice(i + d, e + d) for (i, e, d) in \
+                              np.broadcast(slcid[1:-1], slcid[2:], addidx)]
+                             ))
+    #.............................
+    # Just a quick check
+    vdflat = np.asarray(dflat)
+    vnewdates = np.asarray(newdates)
+    for (osl, nsl) in zip(oldslc, newslc):
+        assert np.equal(vdflat[osl], vnewdates[nsl]).all(), \
+            "Slicing mishap ! Please check %s (old) and %s (new)" % (osl, nsl)
+    #.............................
+    newshape = list(datad.shape)
+    newshape[0] = nsize
+    newdatad = np.empty(newshape, dtype=data.dtype)
+    newdatam = np.ones(newshape, dtype=ma.make_mask_descr(datad.dtype))
+    #....
+    if datam is nomask:
+        for (new, old) in zip(newslc, oldslc):
+            newdatad[new] = datad[old]
+            newdatam[new] = False
+    else:
+        for (new, old) in zip(newslc, oldslc):
+            newdatad[new] = datad[old]
+            newdatam[new] = datam[old]
+    if fill_value is None:
+        fill_value = getattr(data, '_fill_value', None)
+    newdata = ma.masked_array(newdatad, mask=newdatam, fill_value=fill_value)
+    _data = newdata.view(datat)
+    _data._dates = newdates
+    return _data
+TimeSeries.fill_missing_dates = fill_missing_dates
+
+
+
+def find_duplicated_dates(series):
+    """
+    Return a dictionary (duplicated dates <> indices) for the input series.
+
+    The indices are given as a tuple of ndarrays, a la :meth:`nonzero`.
+
+    Parameters
+    ----------
+    series : TimeSeries, DateArray
+        A valid :class:`TimeSeries` or :class:`DateArray` object.
+
+    Examples
+    --------
+    >>> series = time_series(np.arange(10),
+                            dates=[2000, 2001, 2002, 2003, 2003,
+                                   2003, 2004, 2005, 2005, 2006], freq='A')
+    >>> test = find_duplicated_dates(series)
+     {<A-DEC : 2003>: (array([3, 4, 5]),), <A-DEC : 2005>: (array([7, 8]),)}
+    """
+    dates = getattr(series, '_dates', series)
+    steps = dates.get_steps()
+    duplicated_dates = tuple(set(dates[steps == 0]))
+    indices = {}
+    for d in duplicated_dates:
+        indices[d] = (dates == d).nonzero()
+    return indices
+
+
+
+def remove_duplicated_dates(series):
+    """
+    Remove the entries of `series` corresponding to duplicated dates.
+
+    The series is first sorted in chronological order.
+    Only the first occurence of a date is then kept, the others are discarded.
+
+    Parameters
+    ----------
+    series : TimeSeries
+        Time series to process
+    """
+    dates = getattr(series, '_dates', series)
+    steps = np.concatenate(([1, ], dates.get_steps()))
+    if not dates.is_chronological():
+        series = series.copy()
+        series.sort_chronologically()
+        dates = series._dates
+    return series[steps.nonzero()]
+
+
+
+
+def stack(*series):
+    """
+    Performs a column_stack on the data from each series, and the
+    resulting series has the same dates as each individual series. The series
+    must have compatible dates (same starting and ending dates, same frequency).
+
+    Parameters
+    ----------
+    series : the series to be stacked
+    """
+    _timeseriescompat_multiple(*series)
+    return time_series(ma.column_stack(series), series[0]._dates,
+                       **_attrib_dict(series[0]))
+
+
+
+def concatenate(series, axis=0, remove_duplicates=True, fill_missing=False):
+    """
+    Joins series together.
+
+    The series are joined in chronological order.
+    Duplicated dates are handled with the `remove_duplicates` parameter.
+    If `remove_duplicate` is False, duplicated dates are saved.
+    Otherwise, only the first occurence of the date is conserved.
+
+
+    Parameters
+    ----------
+    series : {sequence}
+        Sequence of time series to join
+    axis : {0, None, int}, optional
+        Axis along which to join
+    remove_duplicates : {False, True}, optional
+        Whether to remove duplicated dates.
+    fill_missing : {False, True}, optional
+        Whether to fill the missing dates with missing values.
+
+    Examples
+    --------
+    >>> a = time_series([1,2,3], start_date=now('D'))
+    >>> b = time_series([10,20,30], start_date=now('D')+1)
+    >>> c = concatenate((a,b))
+    >>> c._series
+    masked_array(data = [ 1  2  3 30],
+          mask = False,
+          fill_value=999999)
+
+    """
+    # Get the common frequency, raise an error if incompatibility
+    common_f = _compare_frequencies(*series)
+    # Concatenate the order of series
+    sidx = np.concatenate([np.repeat(i, len(s))
+                           for (i, s) in enumerate(series)], axis=axis)
+    # Concatenate the dates and data
+    ndates = np.concatenate([s._dates for s in series], axis=axis)
+    ndata = ma.concatenate([s._series for s in series], axis=axis)
+    # Resort the data chronologically
+    norder = ndates.argsort(kind='mergesort')
+    ndates = ndates[norder]
+    ndata = ndata[norder]
+    sidx = sidx[norder]
+    #
+    if not remove_duplicates:
+        ndates = date_array(ndates, freq=common_f)
+        result = time_series(ndata, dates=ndates)
+    else:
+        # Find the original dates
+        orig = np.concatenate([[True], (np.diff(ndates) != 0)])
+        result = time_series(ndata.compress(orig, axis=axis),
+                             dates=ndates.compress(orig, axis=axis),
+                             freq=common_f)
+    if fill_missing:
+        result = fill_missing_dates(result)
+    return result
+
+
+
+def empty_like(series):
+    """
+    Returns an empty series with the same dtype, mask and dates as series.
+    """
+    result = np.empty_like(series).view(type(series))
+    result._dates = series._dates
+    result._mask = series._mask.copy()
+    return result
+
+################################################################################
diff --git a/pandas/timeseries/version.py b/pandas/timeseries/version.py
new file mode 100644
index 000000000..bcabcdd1d
--- /dev/null
+++ b/pandas/timeseries/version.py
@@ -0,0 +1,23 @@
+from pkg_resources import require, DistributionNotFound
+
+try:
+    __version__ = require('scikits.timeseries')[0].version
+except DistributionNotFound:
+    # package hasn't actually been installed. Importing directly from source
+    # folder. Explicitly import setup.py to extract version number
+    # This should only happen for developers of the package
+    import imp
+    import os
+    _parent_dir = os.path.split(os.path.dirname(__file__))[0]
+    _setup_dir = os.path.split(_parent_dir)[0]
+    _setup_py = os.path.join(_setup_dir, "setup.py")
+
+    if os.path.exists(_setup_py):
+        _setup = imp.load_source("setup", os.path.join(_setup_dir, "setup.py"))
+        __version__ = _setup.version
+    else:
+        # package not installed through setup tools, just leave version undefined
+        __version__ = None
+
+# Make an alias (sometimes it's easier to drop the _).
+version = __version__
\ No newline at end of file
