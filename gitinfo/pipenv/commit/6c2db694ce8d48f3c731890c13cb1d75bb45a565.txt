commit 6c2db694ce8d48f3c731890c13cb1d75bb45a565
Author: Dan Ryan <dan@danryan.co>
Date:   Fri Sep 7 02:30:51 2018 -0400

    Update to pip 18
    
    Signed-off-by: Dan Ryan <dan@danryan.co>

diff --git a/pipenv/patched/notpip/LICENSE.txt b/pipenv/patched/notpip/LICENSE.txt
index f63eac3d..d3379fac 100644
--- a/pipenv/patched/notpip/LICENSE.txt
+++ b/pipenv/patched/notpip/LICENSE.txt
@@ -1,4 +1,4 @@
-Copyright (c) 2008-2016 The pip developers (see AUTHORS.txt file)
+Copyright (c) 2008-2018 The pip developers (see AUTHORS.txt file)
 
 Permission is hereby granted, free of charge, to any person obtaining
 a copy of this software and associated documentation files (the
diff --git a/pipenv/patched/notpip/__init__.py b/pipenv/patched/notpip/__init__.py
index ab649641..9227d0ea 100644
--- a/pipenv/patched/notpip/__init__.py
+++ b/pipenv/patched/notpip/__init__.py
@@ -1 +1 @@
-__version__ = "10.0.1"
+__version__ = "18.0"
diff --git a/pipenv/patched/notpip/__main__.py b/pipenv/patched/notpip/__main__.py
index e63d7ae9..a4879980 100644
--- a/pipenv/patched/notpip/__main__.py
+++ b/pipenv/patched/notpip/__main__.py
@@ -13,7 +13,7 @@ if __package__ == '':
     path = os.path.dirname(os.path.dirname(__file__))
     sys.path.insert(0, path)
 
-from pipenv.patched.notpip._internal import main as _main  # noqa
+from pipenv.patched.notpip._internal import main as _main  # isort:skip # noqa
 
 if __name__ == '__main__':
     sys.exit(_main())
diff --git a/pipenv/patched/notpip/_internal/__init__.py b/pipenv/patched/notpip/_internal/__init__.py
index dadf6e11..dcd0937e 100644
--- a/pipenv/patched/notpip/_internal/__init__.py
+++ b/pipenv/patched/notpip/_internal/__init__.py
@@ -116,6 +116,15 @@ def autocomplete():
         options = [(x, v) for (x, v) in options if x not in prev_opts]
         # filter options by current input
         options = [(k, v) for k, v in options if k.startswith(current)]
+        # get completion type given cwords and available subcommand options
+        completion_type = get_path_completion_type(
+            cwords, cword, subcommand.parser.option_list_all,
+        )
+        # get completion files and directories if ``completion_type`` is
+        # ``<file>``, ``<dir>`` or ``<path>``
+        if completion_type:
+            options = auto_complete_paths(current, completion_type)
+            options = ((opt, 0) for opt in options)
         for option in options:
             opt_label = option[0]
             # append '=' to options which require args
@@ -124,19 +133,74 @@ def autocomplete():
             print(opt_label)
     else:
         # show main parser options only when necessary
-        if current.startswith('-') or current.startswith('--'):
-            opts = [i.option_list for i in parser.option_groups]
-            opts.append(parser.option_list)
-            opts = (o for it in opts for o in it)
 
+        opts = [i.option_list for i in parser.option_groups]
+        opts.append(parser.option_list)
+        opts = (o for it in opts for o in it)
+        if current.startswith('-'):
             for opt in opts:
                 if opt.help != optparse.SUPPRESS_HELP:
                     subcommands += opt._long_opts + opt._short_opts
+        else:
+            # get completion type given cwords and all available options
+            completion_type = get_path_completion_type(cwords, cword, opts)
+            if completion_type:
+                subcommands = auto_complete_paths(current, completion_type)
 
         print(' '.join([x for x in subcommands if x.startswith(current)]))
     sys.exit(1)
 
 
+def get_path_completion_type(cwords, cword, opts):
+    """Get the type of path completion (``file``, ``dir``, ``path`` or None)
+
+    :param cwords: same as the environmental variable ``COMP_WORDS``
+    :param cword: same as the environmental variable ``COMP_CWORD``
+    :param opts: The available options to check
+    :return: path completion type (``file``, ``dir``, ``path`` or None)
+    """
+    if cword < 2 or not cwords[cword - 2].startswith('-'):
+        return
+    for opt in opts:
+        if opt.help == optparse.SUPPRESS_HELP:
+            continue
+        for o in str(opt).split('/'):
+            if cwords[cword - 2].split('=')[0] == o:
+                if any(x in ('path', 'file', 'dir')
+                        for x in opt.metavar.split('/')):
+                    return opt.metavar
+
+
+def auto_complete_paths(current, completion_type):
+    """If ``completion_type`` is ``file`` or ``path``, list all regular files
+    and directories starting with ``current``; otherwise only list directories
+    starting with ``current``.
+
+    :param current: The word to be completed
+    :param completion_type: path completion type(`file`, `path` or `dir`)i
+    :return: A generator of regular files and/or directories
+    """
+    directory, filename = os.path.split(current)
+    current_path = os.path.abspath(directory)
+    # Don't complete paths if they can't be accessed
+    if not os.access(current_path, os.R_OK):
+        return
+    filename = os.path.normcase(filename)
+    # list all files that start with ``filename``
+    file_list = (x for x in os.listdir(current_path)
+                 if os.path.normcase(x).startswith(filename))
+    for f in file_list:
+        opt = os.path.join(current_path, f)
+        comp_file = os.path.normcase(os.path.join(directory, f))
+        # complete regular files when there is not ``<dir>`` after option
+        # complete directories when there is ``<file>``, ``<path>`` or
+        # ``<dir>``after option
+        if completion_type != 'dir' and os.path.isfile(opt):
+            yield comp_file
+        elif os.path.isdir(opt):
+            yield os.path.join(comp_file, '')
+
+
 def create_main_parser():
     parser_kw = {
         'usage': '\n%prog <command> [options]',
diff --git a/pipenv/patched/notpip/_internal/basecommand.py b/pipenv/patched/notpip/_internal/basecommand.py
index e35bf3d1..60199d55 100644
--- a/pipenv/patched/notpip/_internal/basecommand.py
+++ b/pipenv/patched/notpip/_internal/basecommand.py
@@ -6,13 +6,11 @@ import logging.config
 import optparse
 import os
 import sys
-import warnings
 
 from pipenv.patched.notpip._internal import cmdoptions
 from pipenv.patched.notpip._internal.baseparser import (
     ConfigOptionParser, UpdatingDefaultsHelpFormatter,
 )
-from pipenv.patched.notpip._internal.compat import WINDOWS
 from pipenv.patched.notpip._internal.download import PipSession
 from pipenv.patched.notpip._internal.exceptions import (
     BadCommand, CommandError, InstallationError, PreviousBuildDirError,
@@ -26,14 +24,13 @@ from pipenv.patched.notpip._internal.status_codes import (
     ERROR, PREVIOUS_BUILD_DIR_ERROR, SUCCESS, UNKNOWN_ERROR,
     VIRTUALENV_NOT_FOUND,
 )
-from pipenv.patched.notpip._internal.utils import deprecation
-from pipenv.patched.notpip._internal.utils.logging import IndentingFormatter
+from pipenv.patched.notpip._internal.utils.logging import setup_logging
 from pipenv.patched.notpip._internal.utils.misc import get_prog, normalize_path
 from pipenv.patched.notpip._internal.utils.outdated import pip_version_check
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
 if MYPY_CHECK_RUNNING:
-    from typing import Optional
+    from typing import Optional  # noqa: F401
 
 __all__ = ['Command']
 
@@ -45,7 +42,6 @@ class Command(object):
     usage = None  # type: Optional[str]
     hidden = False  # type: bool
     ignore_require_venv = False  # type: bool
-    log_streams = ("ext://sys.stdout", "ext://sys.stderr")
 
     def __init__(self, isolated=False):
         parser_kw = {
@@ -117,96 +113,15 @@ class Command(object):
         # Set verbosity so that it can be used elsewhere.
         self.verbosity = options.verbose - options.quiet
 
-        if self.verbosity >= 1:
-            level = "DEBUG"
-        elif self.verbosity == -1:
-            level = "WARNING"
-        elif self.verbosity == -2:
-            level = "ERROR"
-        elif self.verbosity <= -3:
-            level = "CRITICAL"
-        else:
-            level = "INFO"
-
-        # The root logger should match the "console" level *unless* we
-        # specified "--log" to send debug logs to a file.
-        root_level = level
-        if options.log:
-            root_level = "DEBUG"
-
-        logger_class = "pip._internal.utils.logging.ColorizedStreamHandler"
-        handler_class = "pip._internal.utils.logging.BetterRotatingFileHandler"
-
-        logging.config.dictConfig({
-            "version": 1,
-            "disable_existing_loggers": False,
-            "filters": {
-                "exclude_warnings": {
-                    "()": "pip._internal.utils.logging.MaxLevelFilter",
-                    "level": logging.WARNING,
-                },
-            },
-            "formatters": {
-                "indent": {
-                    "()": IndentingFormatter,
-                    "format": "%(message)s",
-                },
-            },
-            "handlers": {
-                "console": {
-                    "level": level,
-                    "class": logger_class,
-                    "no_color": options.no_color,
-                    "stream": self.log_streams[0],
-                    "filters": ["exclude_warnings"],
-                    "formatter": "indent",
-                },
-                "console_errors": {
-                    "level": "WARNING",
-                    "class": logger_class,
-                    "no_color": options.no_color,
-                    "stream": self.log_streams[1],
-                    "formatter": "indent",
-                },
-                "user_log": {
-                    "level": "DEBUG",
-                    "class": handler_class,
-                    "filename": options.log or "/dev/null",
-                    "delay": True,
-                    "formatter": "indent",
-                },
-            },
-            "root": {
-                "level": root_level,
-                "handlers": list(filter(None, [
-                    "console",
-                    "console_errors",
-                    "user_log" if options.log else None,
-                ])),
-            },
-            # Disable any logging besides WARNING unless we have DEBUG level
-            # logging enabled. These use both pip._vendor and the bare names
-            # for the case where someone unbundles our libraries.
-            "loggers": {
-                name: {
-                    "level": (
-                        "WARNING" if level in ["INFO", "ERROR"] else "DEBUG"
-                    )
-                } for name in [
-                    "pip._vendor", "distlib", "requests", "urllib3"
-                ]
-            },
-        })
-
-        if sys.version_info[:2] == (3, 3):
-            warnings.warn(
-                "Python 3.3 supported has been deprecated and support for it "
-                "will be dropped in the future. Please upgrade your Python.",
-                deprecation.RemovedInPip11Warning,
-            )
+        setup_logging(
+            verbosity=self.verbosity,
+            no_color=options.no_color,
+            user_log_file=options.log,
+        )
 
-        # TODO: try to get these passing down from the command?
-        #      without resorting to os.environ to hold these.
+        # TODO: Try to get these passing down from the command?
+        #       without resorting to os.environ to hold these.
+        #       This also affects isolated builds and it should.
 
         if options.no_input:
             os.environ['PIP_NO_INPUT'] = '1'
@@ -222,8 +137,6 @@ class Command(object):
                 )
                 sys.exit(VIRTUALENV_NOT_FOUND)
 
-        original_root_handlers = set(logging.root.handlers)
-
         try:
             status = self.run(options, args)
             # FIXME: all commands should return an exit status
@@ -250,23 +163,27 @@ class Command(object):
             logger.debug('Exception information:', exc_info=True)
 
             return ERROR
-        except:
+        except BaseException:
             logger.critical('Exception:', exc_info=True)
 
             return UNKNOWN_ERROR
         finally:
             # Check if we're using the latest version of pip available
-            if (not options.disable_pip_version_check and not
-                    getattr(options, "no_index", False)):
-                with self._build_session(
-                        options,
-                        retries=0,
-                        timeout=min(5, options.timeout)) as session:
+            skip_version_check = (
+                options.disable_pip_version_check or
+                getattr(options, "no_index", False)
+            )
+            if not skip_version_check:
+                session = self._build_session(
+                    options,
+                    retries=0,
+                    timeout=min(5, options.timeout)
+                )
+                with session:
                     pip_version_check(session, options)
-            # Avoid leaking loggers
-            for handler in set(logging.root.handlers) - original_root_handlers:
-                # this method benefit from the Logger class internal lock
-                logging.root.removeHandler(handler)
+
+            # Shutdown the logging module
+            logging.shutdown()
 
         return SUCCESS
 
@@ -330,23 +247,6 @@ class RequirementCommand(Command):
                     'You must give at least one requirement to %(name)s '
                     '(see "pip help %(name)s")' % opts)
 
-        # On Windows, any operation modifying pip should be run as:
-        #     python -m pip ...
-        # See https://github.com/pypa/pip/issues/1299 for more discussion
-        should_show_use_python_msg = (
-            WINDOWS and
-            requirement_set.has_requirement("pip") and
-            os.path.basename(sys.argv[0]).startswith("pip")
-        )
-        if should_show_use_python_msg:
-            new_command = [
-                sys.executable, "-m", "pip"
-            ] + sys.argv[1:]
-            raise CommandError(
-                'To modify pip, please run the following command:\n{}'
-                .format(" ".join(new_command))
-            )
-
     def _build_package_finder(self, options, session,
                               platform=None, python_versions=None,
                               abi=None, implementation=None):
@@ -370,4 +270,5 @@ class RequirementCommand(Command):
             versions=python_versions,
             abi=abi,
             implementation=implementation,
+            prefer_binary=options.prefer_binary,
         )
diff --git a/pipenv/patched/notpip/_internal/build_env.py b/pipenv/patched/notpip/_internal/build_env.py
index c41696ef..1d351b5c 100644
--- a/pipenv/patched/notpip/_internal/build_env.py
+++ b/pipenv/patched/notpip/_internal/build_env.py
@@ -1,28 +1,32 @@
 """Build Environment used for isolation during sdist building
 """
 
+import logging
 import os
+import sys
 from distutils.sysconfig import get_python_lib
 from sysconfig import get_paths
 
+from pipenv.patched.notpip._internal.utils.misc import call_subprocess
 from pipenv.patched.notpip._internal.utils.temp_dir import TempDirectory
+from pipenv.patched.notpip._internal.utils.ui import open_spinner
+
+logger = logging.getLogger(__name__)
 
 
 class BuildEnvironment(object):
     """Creates and manages an isolated environment to install build deps
     """
 
-    def __init__(self, no_clean):
+    def __init__(self):
         self._temp_dir = TempDirectory(kind="build-env")
-        self._no_clean = no_clean
+        self._temp_dir.create()
 
     @property
     def path(self):
         return self._temp_dir.path
 
     def __enter__(self):
-        self._temp_dir.create()
-
         self.save_path = os.environ.get('PATH', None)
         self.save_pythonpath = os.environ.get('PYTHONPATH', None)
         self.save_nousersite = os.environ.get('PYTHONNOUSERSITE', None)
@@ -58,9 +62,6 @@ class BuildEnvironment(object):
         return self.path
 
     def __exit__(self, exc_type, exc_val, exc_tb):
-        if not self._no_clean:
-            self._temp_dir.cleanup()
-
         def restore_var(varname, old_value):
             if old_value is None:
                 os.environ.pop(varname, None)
@@ -74,12 +75,42 @@ class BuildEnvironment(object):
     def cleanup(self):
         self._temp_dir.cleanup()
 
+    def install_requirements(self, finder, requirements, message):
+        args = [
+            sys.executable, '-m', 'pip', 'install', '--ignore-installed',
+            '--no-user', '--prefix', self.path, '--no-warn-script-location',
+        ]
+        if logger.getEffectiveLevel() <= logging.DEBUG:
+            args.append('-v')
+        for format_control in ('no_binary', 'only_binary'):
+            formats = getattr(finder.format_control, format_control)
+            args.extend(('--' + format_control.replace('_', '-'),
+                         ','.join(sorted(formats or {':none:'}))))
+        if finder.index_urls:
+            args.extend(['-i', finder.index_urls[0]])
+            for extra_index in finder.index_urls[1:]:
+                args.extend(['--extra-index-url', extra_index])
+        else:
+            args.append('--no-index')
+        for link in finder.find_links:
+            args.extend(['--find-links', link])
+        for _, host, _ in finder.secure_origins:
+            args.extend(['--trusted-host', host])
+        if finder.allow_all_prereleases:
+            args.append('--pre')
+        if finder.process_dependency_links:
+            args.append('--process-dependency-links')
+        args.append('--')
+        args.extend(requirements)
+        with open_spinner(message) as spinner:
+            call_subprocess(args, show_stdout=False, spinner=spinner)
+
 
 class NoOpBuildEnvironment(BuildEnvironment):
     """A no-op drop-in replacement for BuildEnvironment
     """
 
-    def __init__(self, no_clean):
+    def __init__(self):
         pass
 
     def __enter__(self):
@@ -90,3 +121,6 @@ class NoOpBuildEnvironment(BuildEnvironment):
 
     def cleanup(self):
         pass
+
+    def install_requirements(self, finder, requirements, message):
+        raise NotImplementedError()
diff --git a/pipenv/patched/notpip/_internal/cmdoptions.py b/pipenv/patched/notpip/_internal/cmdoptions.py
index 0b1c031a..c25e769f 100644
--- a/pipenv/patched/notpip/_internal/cmdoptions.py
+++ b/pipenv/patched/notpip/_internal/cmdoptions.py
@@ -17,13 +17,13 @@ from pipenv.patched.notpip._internal.index import (
     FormatControl, fmt_ctl_handle_mutual_exclude, fmt_ctl_no_binary,
 )
 from pipenv.patched.notpip._internal.locations import USER_CACHE_DIR, src_prefix
-from pipenv.patched.notpip._internal.models import PyPI
+from pipenv.patched.notpip._internal.models.index import PyPI
 from pipenv.patched.notpip._internal.utils.hashes import STRONG_HASHES
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 from pipenv.patched.notpip._internal.utils.ui import BAR_TYPES
 
 if MYPY_CHECK_RUNNING:
-    from typing import Any
+    from typing import Any  # noqa: F401
 
 
 def make_option_group(group, parser):
@@ -406,6 +406,16 @@ def only_binary():
     )
 
 
+def prefer_binary():
+    return Option(
+        "--prefer-binary",
+        dest="prefer_binary",
+        action="store_true",
+        default=False,
+        help="Prefer older binary packages over newer source packages."
+    )
+
+
 cache_dir = partial(
     Option,
     "--cache-dir",
diff --git a/pipenv/patched/notpip/_internal/commands/__init__.py b/pipenv/patched/notpip/_internal/commands/__init__.py
index 101fa373..140c4609 100644
--- a/pipenv/patched/notpip/_internal/commands/__init__.py
+++ b/pipenv/patched/notpip/_internal/commands/__init__.py
@@ -20,8 +20,8 @@ from pipenv.patched.notpip._internal.commands.wheel import WheelCommand
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
 if MYPY_CHECK_RUNNING:
-    from typing import List, Type
-    from pipenv.patched.notpip._internal.basecommand import Command
+    from typing import List, Type  # noqa: F401
+    from pipenv.patched.notpip._internal.basecommand import Command  # noqa: F401
 
 commands_order = [
     InstallCommand,
diff --git a/pipenv/patched/notpip/_internal/commands/check.py b/pipenv/patched/notpip/_internal/commands/check.py
index c9acaff5..cd5ffb5f 100644
--- a/pipenv/patched/notpip/_internal/commands/check.py
+++ b/pipenv/patched/notpip/_internal/commands/check.py
@@ -4,7 +4,6 @@ from pipenv.patched.notpip._internal.basecommand import Command
 from pipenv.patched.notpip._internal.operations.check import (
     check_package_set, create_package_set_from_installed,
 )
-from pipenv.patched.notpip._internal.utils.misc import get_installed_distributions
 
 logger = logging.getLogger(__name__)
 
diff --git a/pipenv/patched/notpip/_internal/commands/download.py b/pipenv/patched/notpip/_internal/commands/download.py
index e2cdae5e..63d91b04 100644
--- a/pipenv/patched/notpip/_internal/commands/download.py
+++ b/pipenv/patched/notpip/_internal/commands/download.py
@@ -9,6 +9,7 @@ from pipenv.patched.notpip._internal.exceptions import CommandError
 from pipenv.patched.notpip._internal.index import FormatControl
 from pipenv.patched.notpip._internal.operations.prepare import RequirementPreparer
 from pipenv.patched.notpip._internal.req import RequirementSet
+from pipenv.patched.notpip._internal.req.req_tracker import RequirementTracker
 from pipenv.patched.notpip._internal.resolve import Resolver
 from pipenv.patched.notpip._internal.utils.filesystem import check_path_owner
 from pipenv.patched.notpip._internal.utils.misc import ensure_dir, normalize_path
@@ -52,6 +53,7 @@ class DownloadCommand(RequirementCommand):
         cmd_opts.add_option(cmdoptions.global_options())
         cmd_opts.add_option(cmdoptions.no_binary())
         cmd_opts.add_option(cmdoptions.only_binary())
+        cmd_opts.add_option(cmdoptions.prefer_binary())
         cmd_opts.add_option(cmdoptions.src())
         cmd_opts.add_option(cmdoptions.pre())
         cmd_opts.add_option(cmdoptions.no_clean())
@@ -179,7 +181,7 @@ class DownloadCommand(RequirementCommand):
                 )
                 options.cache_dir = None
 
-            with TempDirectory(
+            with RequirementTracker() as req_tracker, TempDirectory(
                 options.build_dir, delete=build_delete, kind="download"
             ) as directory:
 
@@ -203,6 +205,7 @@ class DownloadCommand(RequirementCommand):
                     wheel_download_dir=None,
                     progress_bar=options.progress_bar,
                     build_isolation=options.build_isolation,
+                    req_tracker=req_tracker,
                 )
 
                 resolver = Resolver(
diff --git a/pipenv/patched/notpip/_internal/commands/install.py b/pipenv/patched/notpip/_internal/commands/install.py
index 136310de..ebdf07d7 100644
--- a/pipenv/patched/notpip/_internal/commands/install.py
+++ b/pipenv/patched/notpip/_internal/commands/install.py
@@ -7,6 +7,8 @@ import os
 import shutil
 from optparse import SUPPRESS_HELP
 
+from pipenv.patched.notpip._vendor import pkg_resources
+
 from pipenv.patched.notpip._internal import cmdoptions
 from pipenv.patched.notpip._internal.basecommand import RequirementCommand
 from pipenv.patched.notpip._internal.cache import WheelCache
@@ -17,10 +19,14 @@ from pipenv.patched.notpip._internal.locations import distutils_scheme, virtuale
 from pipenv.patched.notpip._internal.operations.check import check_install_conflicts
 from pipenv.patched.notpip._internal.operations.prepare import RequirementPreparer
 from pipenv.patched.notpip._internal.req import RequirementSet, install_given_reqs
+from pipenv.patched.notpip._internal.req.req_tracker import RequirementTracker
 from pipenv.patched.notpip._internal.resolve import Resolver
 from pipenv.patched.notpip._internal.status_codes import ERROR
 from pipenv.patched.notpip._internal.utils.filesystem import check_path_owner
-from pipenv.patched.notpip._internal.utils.misc import ensure_dir, get_installed_version
+from pipenv.patched.notpip._internal.utils.misc import (
+    ensure_dir, get_installed_version,
+    protect_pip_from_modification_on_windows,
+)
 from pipenv.patched.notpip._internal.utils.temp_dir import TempDirectory
 from pipenv.patched.notpip._internal.wheel import WheelBuilder
 
@@ -183,6 +189,7 @@ class InstallCommand(RequirementCommand):
 
         cmd_opts.add_option(cmdoptions.no_binary())
         cmd_opts.add_option(cmdoptions.only_binary())
+        cmd_opts.add_option(cmdoptions.prefer_binary())
         cmd_opts.add_option(cmdoptions.no_clean())
         cmd_opts.add_option(cmdoptions.require_hashes())
         cmd_opts.add_option(cmdoptions.progress_bar())
@@ -254,7 +261,7 @@ class InstallCommand(RequirementCommand):
                 )
                 options.cache_dir = None
 
-            with TempDirectory(
+            with RequirementTracker() as req_tracker, TempDirectory(
                 options.build_dir, delete=build_delete, kind="install"
             ) as directory:
                 requirement_set = RequirementSet(
@@ -273,6 +280,7 @@ class InstallCommand(RequirementCommand):
                         wheel_download_dir=None,
                         progress_bar=options.progress_bar,
                         build_isolation=options.build_isolation,
+                        req_tracker=req_tracker,
                     )
 
                     resolver = Resolver(
@@ -290,6 +298,10 @@ class InstallCommand(RequirementCommand):
                     )
                     resolver.resolve(requirement_set)
 
+                    protect_pip_from_modification_on_windows(
+                        modifying_pip=requirement_set.has_requirement("pip")
+                    )
+
                     # If caching is disabled or wheel is not installed don't
                     # try to build wheels.
                     if wheel and options.cache_dir:
@@ -335,20 +347,22 @@ class InstallCommand(RequirementCommand):
                         use_user_site=options.use_user_site,
                     )
 
-                    possible_lib_locations = get_lib_location_guesses(
+                    lib_locations = get_lib_location_guesses(
                         user=options.use_user_site,
                         home=target_temp_dir.path,
                         root=options.root_path,
                         prefix=options.prefix_path,
                         isolated=options.isolated_mode,
                     )
+                    working_set = pkg_resources.WorkingSet(lib_locations)
+
                     reqs = sorted(installed, key=operator.attrgetter('name'))
                     items = []
                     for req in reqs:
                         item = req.name
                         try:
                             installed_version = get_installed_version(
-                                req.name, possible_lib_locations
+                                req.name, working_set=working_set
                             )
                             if installed_version:
                                 item += '-' + installed_version
diff --git a/pipenv/patched/notpip/_internal/commands/list.py b/pipenv/patched/notpip/_internal/commands/list.py
index dc3c0f2c..99aee99f 100644
--- a/pipenv/patched/notpip/_internal/commands/list.py
+++ b/pipenv/patched/notpip/_internal/commands/list.py
@@ -2,7 +2,6 @@ from __future__ import absolute_import
 
 import json
 import logging
-import warnings
 
 from pipenv.patched.notpip._vendor import six
 from pipenv.patched.notpip._vendor.six.moves import zip_longest
@@ -11,7 +10,6 @@ from pipenv.patched.notpip._internal.basecommand import Command
 from pipenv.patched.notpip._internal.cmdoptions import index_group, make_option_group
 from pipenv.patched.notpip._internal.exceptions import CommandError
 from pipenv.patched.notpip._internal.index import PackageFinder
-from pipenv.patched.notpip._internal.utils.deprecation import RemovedInPip11Warning
 from pipenv.patched.notpip._internal.utils.misc import (
     dist_is_editable, get_installed_distributions,
 )
@@ -78,9 +76,9 @@ class ListCommand(Command):
             action='store',
             dest='list_format',
             default="columns",
-            choices=('legacy', 'columns', 'freeze', 'json'),
+            choices=('columns', 'freeze', 'json'),
             help="Select the output format among: columns (default), freeze, "
-                 "json, or legacy.",
+                 "or json",
         )
 
         cmd_opts.add_option(
@@ -123,13 +121,6 @@ class ListCommand(Command):
         )
 
     def run(self, options, args):
-        if options.list_format == "legacy":
-            warnings.warn(
-                "The legacy format has been deprecated and will be removed "
-                "in the future.",
-                RemovedInPip11Warning,
-            )
-
         if options.outdated and options.uptodate:
             raise CommandError(
                 "Options --outdated and --uptodate cannot be combined.")
@@ -208,30 +199,6 @@ class ListCommand(Command):
                 dist.latest_filetype = typ
                 yield dist
 
-    def output_legacy(self, dist, options):
-        if options.verbose >= 1:
-            return '%s (%s, %s, %s)' % (
-                dist.project_name,
-                dist.version,
-                dist.location,
-                get_installer(dist),
-            )
-        elif dist_is_editable(dist):
-            return '%s (%s, %s)' % (
-                dist.project_name,
-                dist.version,
-                dist.location,
-            )
-        else:
-            return '%s (%s)' % (dist.project_name, dist.version)
-
-    def output_legacy_latest(self, dist, options):
-        return '%s - Latest: %s [%s]' % (
-            self.output_legacy(dist, options),
-            dist.latest_version,
-            dist.latest_filetype,
-        )
-
     def output_package_listing(self, packages, options):
         packages = sorted(
             packages,
@@ -249,12 +216,6 @@ class ListCommand(Command):
                     logger.info("%s==%s", dist.project_name, dist.version)
         elif options.list_format == 'json':
             logger.info(format_for_json(packages, options))
-        elif options.list_format == "legacy":
-            for dist in packages:
-                if options.outdated:
-                    logger.info(self.output_legacy_latest(dist, options))
-                else:
-                    logger.info(self.output_legacy(dist, options))
 
     def output_package_listing_columns(self, data, header):
         # insert the header first: we need to know the size of column names
diff --git a/pipenv/patched/notpip/_internal/commands/search.py b/pipenv/patched/notpip/_internal/commands/search.py
index 32a111ae..ac111c14 100644
--- a/pipenv/patched/notpip/_internal/commands/search.py
+++ b/pipenv/patched/notpip/_internal/commands/search.py
@@ -15,7 +15,7 @@ from pipenv.patched.notpip._internal.basecommand import SUCCESS, Command
 from pipenv.patched.notpip._internal.compat import get_terminal_size
 from pipenv.patched.notpip._internal.download import PipXmlrpcTransport
 from pipenv.patched.notpip._internal.exceptions import CommandError
-from pipenv.patched.notpip._internal.models import PyPI
+from pipenv.patched.notpip._internal.models.index import PyPI
 from pipenv.patched.notpip._internal.status_codes import NO_MATCHES_FOUND
 from pipenv.patched.notpip._internal.utils.logging import indent_log
 
diff --git a/pipenv/patched/notpip/_internal/commands/show.py b/pipenv/patched/notpip/_internal/commands/show.py
index 3bac8b4f..8de6b6b8 100644
--- a/pipenv/patched/notpip/_internal/commands/show.py
+++ b/pipenv/patched/notpip/_internal/commands/show.py
@@ -14,7 +14,11 @@ logger = logging.getLogger(__name__)
 
 
 class ShowCommand(Command):
-    """Show information about one or more installed packages."""
+    """
+    Show information about one or more installed packages.
+
+    The output is in RFC-compliant mail header format.
+    """
     name = 'show'
     usage = """
       %prog [options] <package> ..."""
diff --git a/pipenv/patched/notpip/_internal/commands/uninstall.py b/pipenv/patched/notpip/_internal/commands/uninstall.py
index 262c2c45..45a0eba5 100644
--- a/pipenv/patched/notpip/_internal/commands/uninstall.py
+++ b/pipenv/patched/notpip/_internal/commands/uninstall.py
@@ -5,6 +5,7 @@ from pipenv.patched.notpip._vendor.packaging.utils import canonicalize_name
 from pipenv.patched.notpip._internal.basecommand import Command
 from pipenv.patched.notpip._internal.exceptions import InstallationError
 from pipenv.patched.notpip._internal.req import InstallRequirement, parse_requirements
+from pipenv.patched.notpip._internal.utils.misc import protect_pip_from_modification_on_windows
 
 
 class UninstallCommand(Command):
@@ -63,6 +64,11 @@ class UninstallCommand(Command):
                     'You must give at least one requirement to %(name)s (see '
                     '"pip help %(name)s")' % dict(name=self.name)
                 )
+
+            protect_pip_from_modification_on_windows(
+                modifying_pip="pip" in reqs_to_uninstall
+            )
+
             for req in reqs_to_uninstall.values():
                 uninstall_pathset = req.uninstall(
                     auto_confirm=options.yes, verbose=self.verbosity > 0,
diff --git a/pipenv/patched/notpip/_internal/commands/wheel.py b/pipenv/patched/notpip/_internal/commands/wheel.py
index e97d5988..c04d58ed 100644
--- a/pipenv/patched/notpip/_internal/commands/wheel.py
+++ b/pipenv/patched/notpip/_internal/commands/wheel.py
@@ -10,6 +10,7 @@ from pipenv.patched.notpip._internal.cache import WheelCache
 from pipenv.patched.notpip._internal.exceptions import CommandError, PreviousBuildDirError
 from pipenv.patched.notpip._internal.operations.prepare import RequirementPreparer
 from pipenv.patched.notpip._internal.req import RequirementSet
+from pipenv.patched.notpip._internal.req.req_tracker import RequirementTracker
 from pipenv.patched.notpip._internal.resolve import Resolver
 from pipenv.patched.notpip._internal.utils.temp_dir import TempDirectory
 from pipenv.patched.notpip._internal.wheel import WheelBuilder
@@ -57,6 +58,7 @@ class WheelCommand(RequirementCommand):
         )
         cmd_opts.add_option(cmdoptions.no_binary())
         cmd_opts.add_option(cmdoptions.only_binary())
+        cmd_opts.add_option(cmdoptions.prefer_binary())
         cmd_opts.add_option(
             '--build-option',
             dest='build_options',
@@ -119,9 +121,10 @@ class WheelCommand(RequirementCommand):
             build_delete = (not (options.no_clean or options.build_dir))
             wheel_cache = WheelCache(options.cache_dir, options.format_control)
 
-            with TempDirectory(
+            with RequirementTracker() as req_tracker, TempDirectory(
                 options.build_dir, delete=build_delete, kind="wheel"
             ) as directory:
+
                 requirement_set = RequirementSet(
                     require_hashes=options.require_hashes,
                 )
@@ -139,6 +142,7 @@ class WheelCommand(RequirementCommand):
                         wheel_download_dir=options.wheel_dir,
                         progress_bar=options.progress_bar,
                         build_isolation=options.build_isolation,
+                        req_tracker=req_tracker,
                     )
 
                     resolver = Resolver(
diff --git a/pipenv/patched/notpip/_internal/compat.py b/pipenv/patched/notpip/_internal/compat.py
index 3407880c..6e51e32a 100644
--- a/pipenv/patched/notpip/_internal/compat.py
+++ b/pipenv/patched/notpip/_internal/compat.py
@@ -164,7 +164,7 @@ def expanduser(path):
     """
     Expand ~ and ~user constructions.
 
-    Includes a workaround for http://bugs.python.org/issue14768
+    Includes a workaround for https://bugs.python.org/issue14768
     """
     expanded = os.path.expanduser(path)
     if path.startswith('~/') and expanded.startswith('//'):
@@ -217,7 +217,7 @@ else:
                     'hh',
                     fcntl.ioctl(fd, termios.TIOCGWINSZ, '12345678')
                 )
-            except:
+            except Exception:
                 return None
             if cr == (0, 0):
                 return None
@@ -228,7 +228,7 @@ else:
                 fd = os.open(os.ctermid(), os.O_RDONLY)
                 cr = ioctl_GWINSZ(fd)
                 os.close(fd)
-            except:
+            except Exception:
                 pass
         if not cr:
             cr = (os.environ.get('LINES', 25), os.environ.get('COLUMNS', 80))
diff --git a/pipenv/patched/notpip/_internal/configuration.py b/pipenv/patched/notpip/_internal/configuration.py
index 7b9b5a45..3df185f7 100644
--- a/pipenv/patched/notpip/_internal/configuration.py
+++ b/pipenv/patched/notpip/_internal/configuration.py
@@ -27,7 +27,9 @@ from pipenv.patched.notpip._internal.utils.misc import ensure_dir, enum
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
 if MYPY_CHECK_RUNNING:
-    from typing import Any, Dict, Iterable, List, NewType, Optional, Tuple
+    from typing import (  # noqa: F401
+        Any, Dict, Iterable, List, NewType, Optional, Tuple
+    )
 
     RawConfigParser = configparser.RawConfigParser  # Shorthand
     Kind = NewType("Kind", str)
diff --git a/pipenv/patched/notpip/_internal/download.py b/pipenv/patched/notpip/_internal/download.py
index f00e3984..06a45644 100644
--- a/pipenv/patched/notpip/_internal/download.py
+++ b/pipenv/patched/notpip/_internal/download.py
@@ -31,10 +31,9 @@ from pipenv.patched.notpip._vendor.six.moves.urllib.parse import unquote as urll
 from pipenv.patched.notpip._vendor.urllib3.util import IS_PYOPENSSL
 
 import pipenv.patched.notpip
-from pipenv.patched.notpip._internal.compat import WINDOWS
 from pipenv.patched.notpip._internal.exceptions import HashMismatch, InstallationError
 from pipenv.patched.notpip._internal.locations import write_delete_marker_file
-from pipenv.patched.notpip._internal.models import PyPI
+from pipenv.patched.notpip._internal.models.index import PyPI
 from pipenv.patched.notpip._internal.utils.encoding import auto_decode
 from pipenv.patched.notpip._internal.utils.filesystem import check_path_owner
 from pipenv.patched.notpip._internal.utils.glibc import libc_ver
diff --git a/pipenv/patched/notpip/_internal/index.py b/pipenv/patched/notpip/_internal/index.py
index 8fa39c4f..98102f3f 100644
--- a/pipenv/patched/notpip/_internal/index.py
+++ b/pipenv/patched/notpip/_internal/index.py
@@ -9,7 +9,6 @@ import os
 import posixpath
 import re
 import sys
-import warnings
 from collections import namedtuple
 
 from pipenv.patched.notpip._vendor import html5lib, requests, six
@@ -27,13 +26,13 @@ from pipenv.patched.notpip._internal.exceptions import (
     BestVersionAlreadyInstalled, DistributionNotFound, InvalidWheelFilename,
     UnsupportedWheel,
 )
-from pipenv.patched.notpip._internal.models import PyPI
+from pipenv.patched.notpip._internal.models.index import PyPI
 from pipenv.patched.notpip._internal.pep425tags import get_supported
-from pipenv.patched.notpip._internal.utils.deprecation import RemovedInPip11Warning
+from pipenv.patched.notpip._internal.utils.deprecation import deprecated
 from pipenv.patched.notpip._internal.utils.logging import indent_log
 from pipenv.patched.notpip._internal.utils.misc import (
     ARCHIVE_EXTENSIONS, SUPPORTED_EXTENSIONS, cached_property, normalize_path,
-    splitext,
+    remove_auth_from_url, splitext,
 )
 from pipenv.patched.notpip._internal.utils.packaging import check_requires_python
 from pipenv.patched.notpip._internal.wheel import Wheel, wheel_ext
@@ -59,7 +58,7 @@ logger = logging.getLogger(__name__)
 
 class InstallationCandidate(object):
 
-    def __init__(self, project, version, location, requires_python=''):
+    def __init__(self, project, version, location, requires_python=None):
         self.project = project
         self.version = parse_version(version)
         self.location = location
@@ -109,7 +108,8 @@ class PackageFinder(object):
     def __init__(self, find_links, index_urls, allow_all_prereleases=False,
                  trusted_hosts=None, process_dependency_links=False,
                  session=None, format_control=None, platform=None,
-                 versions=None, abi=None, implementation=None):
+                 versions=None, abi=None, implementation=None,
+                 prefer_binary=False):
         """Create a PackageFinder.
 
         :param format_control: A FormatControl object or None. Used to control
@@ -169,7 +169,7 @@ class PackageFinder(object):
         # The Session we'll use to make requests
         self.session = session
 
-        # Kenneth's Hack.
+        # Kenneth's Hack
         self.extra = None
 
         # The valid tags to check potential found wheel candidates against
@@ -180,6 +180,9 @@ class PackageFinder(object):
             impl=implementation,
         )
 
+        # Do we prefer old, but valid, binary dist over new source dist
+        self.prefer_binary = prefer_binary
+
         # If we don't have TLS enabled, then WARN if anyplace we're looking
         # relies on TLS.
         if not HAS_TLS:
@@ -197,7 +200,8 @@ class PackageFinder(object):
         lines = []
         if self.index_urls and self.index_urls != [PyPI.simple_url]:
             lines.append(
-                "Looking in indexes: {}".format(", ".join(self.index_urls))
+                "Looking in indexes: {}".format(", ".join(
+                    remove_auth_from_url(url) for url in self.index_urls))
             )
         if self.find_links:
             lines.append(
@@ -211,10 +215,12 @@ class PackageFinder(object):
         # # dependency_links value
         # # FIXME: also, we should track comes_from (i.e., use Link)
         if self.process_dependency_links:
-            warnings.warn(
+            deprecated(
                 "Dependency Links processing has been deprecated and will be "
                 "removed in a future release.",
-                RemovedInPip11Warning,
+                replacement=None,
+                gone_in="18.2",
+                issue=4187,
             )
             self.dependency_links.extend(links)
 
@@ -288,7 +294,7 @@ class PackageFinder(object):
 
         return files, urls
 
-    def _candidate_sort_key(self, candidate, ignore_compatibility=True):
+    def _candidate_sort_key(self, candidate, ignore_compatibility=False):
         """
         Function used to generate link sort key for link tuples.
         The greater the return value, the more preferred it is.
@@ -297,12 +303,14 @@ class PackageFinder(object):
           1. existing installs
           2. wheels ordered via Wheel.support_index_min(self.valid_tags)
           3. source archives
+        If prefer_binary was set, then all wheels are sorted above sources.
         Note: it was considered to embed this logic into the Link
               comparison operators, but then different sdist links
               with the same version, would have to be considered equal
         """
         support_num = len(self.valid_tags)
         build_tag = tuple()
+        binary_preference = 0
         if candidate.location.is_wheel:
             # can raise InvalidWheelFilename
             wheel = Wheel(candidate.location.filename)
@@ -311,7 +319,8 @@ class PackageFinder(object):
                     "%s is not a supported wheel for this platform. It "
                     "can't be sorted." % wheel.filename
                 )
-
+            if self.prefer_binary:
+                binary_preference = 1
             tags = self.valid_tags if not ignore_compatibility else None
             try:
                 pri = -(wheel.support_index_min(tags=tags))
@@ -324,7 +333,7 @@ class PackageFinder(object):
                 build_tag = (int(build_tag_groups[0]), build_tag_groups[1])
         else:  # sdist
             pri = -(support_num)
-        return (candidate.version, build_tag, pri)
+        return (binary_preference, candidate.version, build_tag, pri)
 
     def _validate_secure_origin(self, logger, location):
         # Determine if this url used a secure transport mechanism
@@ -512,25 +521,22 @@ class PackageFinder(object):
         all_candidates = self.find_all_candidates(req.name)
 
         # Filter out anything which doesn't match our specifier
-        if not ignore_compatibility:
-            compatible_versions = set(
-                req.specifier.filter(
-                    # We turn the version object into a str here because otherwise
-                    # when we're debundled but setuptools isn't, Python will see
-                    # packaging.version.Version and
-                    # pkg_resources._vendor.packaging.version.Version as different
-                    # types. This way we'll use a str as a common data interchange
-                    # format. If we stop using the pkg_resources provided specifier
-                    # and start using our own, we can drop the cast to str().
-                    [str(c.version) for c in all_candidates],
-                    prereleases=(
-                        self.allow_all_prereleases
-                        if self.allow_all_prereleases else None
-                    ),
-                )
+        compatible_versions = set(
+            req.specifier.filter(
+                # We turn the version object into a str here because otherwise
+                # when we're debundled but setuptools isn't, Python will see
+                # packaging.version.Version and
+                # pkg_resources._vendor.packaging.version.Version as different
+                # types. This way we'll use a str as a common data interchange
+                # format. If we stop using the pkg_resources provided specifier
+                # and start using our own, we can drop the cast to str().
+                [str(c.version) for c in all_candidates],
+                prereleases=(
+                    self.allow_all_prereleases
+                    if self.allow_all_prereleases else None
+                ),
             )
-        else:
-            compatible_versions = [str(c.version) for c in all_candidates]
+        )
         applicable_candidates = [
             # Again, converting to str to deal with debundling.
             c for c in all_candidates if str(c.version) in compatible_versions
@@ -618,8 +624,6 @@ class PackageFinder(object):
             try:
                 page = self._get_page(location)
             except requests.HTTPError as e:
-                page = None
-            if page is None:
                 continue
 
             yield page
@@ -666,7 +670,6 @@ class PackageFinder(object):
             if not ext:
                 self._log_skipped_link(link, 'not a file')
                 return
-            # Always ignore unsupported extensions even when we ignore compatibility
             if ext not in SUPPORTED_EXTENSIONS:
                 self._log_skipped_link(
                     link, 'unsupported archive format: %s' % ext,
@@ -709,7 +712,7 @@ class PackageFinder(object):
             version = egg_info_matches(egg_info, search.supplied, link)
         if version is None:
             self._log_skipped_link(
-                link, 'wrong project name (not %s)' % search.supplied)
+                link, 'Missing project version for %s' % search.supplied)
             return
 
         match = self._py_version_re.search(version)
diff --git a/pipenv/patched/notpip/_internal/locations.py b/pipenv/patched/notpip/_internal/locations.py
index 43586f4b..29c6db79 100644
--- a/pipenv/patched/notpip/_internal/locations.py
+++ b/pipenv/patched/notpip/_internal/locations.py
@@ -8,7 +8,7 @@ import site
 import sys
 import sysconfig
 from distutils import sysconfig as distutils_sysconfig
-from distutils.command.install import SCHEME_KEYS, install  # type: ignore
+from distutils.command.install import SCHEME_KEYS  # type: ignore
 
 from pipenv.patched.notpip._internal.compat import WINDOWS, expanduser
 from pipenv.patched.notpip._internal.utils import appdirs
diff --git a/pipenv/patched/notpip/_internal/models/__init__.py b/pipenv/patched/notpip/_internal/models/__init__.py
index e4de2e9d..7855226e 100644
--- a/pipenv/patched/notpip/_internal/models/__init__.py
+++ b/pipenv/patched/notpip/_internal/models/__init__.py
@@ -1,4 +1,2 @@
-from pipenv.patched.notpip._internal.models.index import Index, PyPI
-
-
-__all__ = ["Index", "PyPI"]
+"""A package that contains models that represent entities.
+"""
diff --git a/pipenv/patched/notpip/_internal/operations/check.py b/pipenv/patched/notpip/_internal/operations/check.py
index bc2c2b61..9c8ea08e 100644
--- a/pipenv/patched/notpip/_internal/operations/check.py
+++ b/pipenv/patched/notpip/_internal/operations/check.py
@@ -6,13 +6,14 @@ from collections import namedtuple
 from pipenv.patched.notpip._vendor.packaging.utils import canonicalize_name
 
 from pipenv.patched.notpip._internal.operations.prepare import make_abstract_dist
-
 from pipenv.patched.notpip._internal.utils.misc import get_installed_distributions
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
 if MYPY_CHECK_RUNNING:
-    from pipenv.patched.notpip._internal.req.req_install import InstallRequirement
-    from typing import Any, Dict, Iterator, Set, Tuple, List
+    from pipenv.patched.notpip._internal.req.req_install import InstallRequirement  # noqa: F401
+    from typing import (  # noqa: F401
+        Any, Callable, Dict, Iterator, Optional, Set, Tuple, List
+    )
 
     # Shorthands
     PackageSet = Dict[str, 'PackageDetails']
@@ -33,17 +34,25 @@ def create_package_set_from_installed(**kwargs):
     # Default to using all packages installed on the system
     if kwargs == {}:
         kwargs = {"local_only": False, "skip": ()}
-    retval = {}
+
+    package_set = {}
     for dist in get_installed_distributions(**kwargs):
         name = canonicalize_name(dist.project_name)
-        retval[name] = PackageDetails(dist.version, dist.requires())
-    return retval
+        package_set[name] = PackageDetails(dist.version, dist.requires())
+    return package_set
 
 
-def check_package_set(package_set):
-    # type: (PackageSet) -> CheckResult
+def check_package_set(package_set, should_ignore=None):
+    # type: (PackageSet, Optional[Callable[[str], bool]]) -> CheckResult
     """Check if a package set is consistent
+
+    If should_ignore is passed, it should be a callable that takes a
+    package name and returns a boolean.
     """
+    if should_ignore is None:
+        def should_ignore(name):
+            return False
+
     missing = dict()
     conflicting = dict()
 
@@ -52,6 +61,9 @@ def check_package_set(package_set):
         missing_deps = set()  # type: Set[Missing]
         conflicting_deps = set()  # type: Set[Conflicting]
 
+        if should_ignore(package_name):
+            continue
+
         for req in package_set[package_name].requires:
             name = canonicalize_name(req.project_name)  # type: str
 
@@ -69,13 +81,10 @@ def check_package_set(package_set):
             if not req.specifier.contains(version, prereleases=True):
                 conflicting_deps.add((name, version, req))
 
-        def str_key(x):
-            return str(x)
-
         if missing_deps:
-            missing[package_name] = sorted(missing_deps, key=str_key)
+            missing[package_name] = sorted(missing_deps, key=str)
         if conflicting_deps:
-            conflicting[package_name] = sorted(conflicting_deps, key=str_key)
+            conflicting[package_name] = sorted(conflicting_deps, key=str)
 
     return missing, conflicting
 
@@ -86,21 +95,54 @@ def check_install_conflicts(to_install):
     installing given requirements
     """
     # Start from the current state
-    state = create_package_set_from_installed()
-    _simulate_installation_of(to_install, state)
-    return state, check_package_set(state)
+    package_set = create_package_set_from_installed()
+    # Install packages
+    would_be_installed = _simulate_installation_of(to_install, package_set)
+
+    # Only warn about directly-dependent packages; create a whitelist of them
+    whitelist = _create_whitelist(would_be_installed, package_set)
+
+    return (
+        package_set,
+        check_package_set(
+            package_set, should_ignore=lambda name: name not in whitelist
+        )
+    )
 
 
 # NOTE from @pradyunsg
 # This required a minor update in dependency link handling logic over at
 # operations.prepare.IsSDist.dist() to get it working
-def _simulate_installation_of(to_install, state):
-    # type: (List[InstallRequirement], PackageSet) -> None
+def _simulate_installation_of(to_install, package_set):
+    # type: (List[InstallRequirement], PackageSet) -> Set[str]
     """Computes the version of packages after installing to_install.
     """
 
+    # Keep track of packages that were installed
+    installed = set()
+
     # Modify it as installing requirement_set would (assuming no errors)
     for inst_req in to_install:
         dist = make_abstract_dist(inst_req).dist(finder=None)
         name = canonicalize_name(dist.key)
-        state[name] = PackageDetails(dist.version, dist.requires())
+        package_set[name] = PackageDetails(dist.version, dist.requires())
+
+        installed.add(name)
+
+    return installed
+
+
+def _create_whitelist(would_be_installed, package_set):
+    # type: (Set[str], PackageSet) -> Set[str]
+    packages_affected = set(would_be_installed)
+
+    for package_name in package_set:
+        if package_name in packages_affected:
+            continue
+
+        for req in package_set[package_name].requires:
+            if canonicalize_name(req.name) in packages_affected:
+                packages_affected.add(package_name)
+                break
+
+    return packages_affected
diff --git a/pipenv/patched/notpip/_internal/operations/freeze.py b/pipenv/patched/notpip/_internal/operations/freeze.py
index e81cc1cb..532989bc 100644
--- a/pipenv/patched/notpip/_internal/operations/freeze.py
+++ b/pipenv/patched/notpip/_internal/operations/freeze.py
@@ -4,7 +4,6 @@ import collections
 import logging
 import os
 import re
-import warnings
 
 from pipenv.patched.notpip._vendor import pkg_resources, six
 from pipenv.patched.notpip._vendor.packaging.utils import canonicalize_name
@@ -13,7 +12,7 @@ from pipenv.patched.notpip._vendor.pkg_resources import RequirementParseError
 from pipenv.patched.notpip._internal.exceptions import InstallationError
 from pipenv.patched.notpip._internal.req import InstallRequirement
 from pipenv.patched.notpip._internal.req.req_file import COMMENT_RE
-from pipenv.patched.notpip._internal.utils.deprecation import RemovedInPip11Warning
+from pipenv.patched.notpip._internal.utils.deprecation import deprecated
 from pipenv.patched.notpip._internal.utils.misc import (
     dist_is_editable, get_installed_distributions,
 )
@@ -216,10 +215,12 @@ class FrozenRequirement(object):
                         'for this package:'
                     )
                 else:
-                    warnings.warn(
+                    deprecated(
                         "SVN editable detection based on dependency links "
                         "will be dropped in the future.",
-                        RemovedInPip11Warning,
+                        replacement=None,
+                        gone_in="18.2",
+                        issue=4187,
                     )
                     comments.append(
                         '# Installing as editable to satisfy requirement %s:' %
diff --git a/pipenv/patched/notpip/_internal/operations/prepare.py b/pipenv/patched/notpip/_internal/operations/prepare.py
index 46538373..9ebc3ebd 100644
--- a/pipenv/patched/notpip/_internal/operations/prepare.py
+++ b/pipenv/patched/notpip/_internal/operations/prepare.py
@@ -1,15 +1,12 @@
 """Prepares a distribution for installation
 """
 
-import itertools
 import logging
 import os
-import sys
-from copy import copy
 
 from pipenv.patched.notpip._vendor import pkg_resources, requests
 
-from pipenv.patched.notpip._internal.build_env import NoOpBuildEnvironment
+from pipenv.patched.notpip._internal.build_env import BuildEnvironment
 from pipenv.patched.notpip._internal.compat import expanduser
 from pipenv.patched.notpip._internal.download import (
     is_dir_url, is_file_url, is_vcs_url, unpack_url, url_to_path,
@@ -18,14 +15,9 @@ from pipenv.patched.notpip._internal.exceptions import (
     DirectoryUrlHashUnsupported, HashUnpinned, InstallationError,
     PreviousBuildDirError, VcsHashUnsupported,
 )
-from pipenv.patched.notpip._internal.index import FormatControl
-from pipenv.patched.notpip._internal.req.req_install import InstallRequirement
 from pipenv.patched.notpip._internal.utils.hashes import MissingHashes
 from pipenv.patched.notpip._internal.utils.logging import indent_log
-from pipenv.patched.notpip._internal.utils.misc import (
-    call_subprocess, display_path, normalize_path,
-)
-from pipenv.patched.notpip._internal.utils.ui import open_spinner
+from pipenv.patched.notpip._internal.utils.misc import display_path, normalize_path, rmtree
 from pipenv.patched.notpip._internal.vcs import vcs
 
 logger = logging.getLogger(__name__)
@@ -47,26 +39,6 @@ def make_abstract_dist(req):
         return IsSDist(req)
 
 
-def _install_build_reqs(finder, prefix, build_requirements):
-    # NOTE: What follows is not a very good thing.
-    #       Eventually, this should move into the BuildEnvironment class and
-    #       that should handle all the isolation and sub-process invocation.
-    finder = copy(finder)
-    finder.format_control = FormatControl(set(), set([":all:"]))
-    urls = [
-        finder.find_requirement(
-            InstallRequirement.from_line(r), upgrade=False).url
-        for r in build_requirements
-    ]
-    args = [
-        sys.executable, '-m', 'pip', 'install', '--ignore-installed',
-        '--no-user', '--prefix', prefix,
-    ] + list(urls)
-
-    with open_spinner("Installing build dependencies") as spinner:
-        call_subprocess(args, show_stdout=False, spinner=spinner)
-
-
 class DistAbstraction(object):
     """Abstracts out the wheel vs non-wheel Resolver.resolve() logic.
 
@@ -123,33 +95,33 @@ class IsSDist(DistAbstraction):
     def prep_for_dist(self, finder, build_isolation):
         # Before calling "setup.py egg_info", we need to set-up the build
         # environment.
-        build_requirements, isolate = self.req.get_pep_518_info()
-        should_isolate = build_isolation and isolate
-
-        minimum_requirements = ('setuptools', 'wheel')
-        missing_requirements = set(minimum_requirements) - set(
-            pkg_resources.Requirement(r).key
-            for r in build_requirements
-        )
-        if missing_requirements:
-            def format_reqs(rs):
-                return ' and '.join(map(repr, sorted(rs)))
-            logger.warning(
-                "Missing build time requirements in pyproject.toml for %s: "
-                "%s.", self.req, format_reqs(missing_requirements)
-            )
-            logger.warning(
-                "This version of pip does not implement PEP 517 so it cannot "
-                "build a wheel without %s.", format_reqs(minimum_requirements)
-            )
+        build_requirements = self.req.get_pep_518_info()
+        should_isolate = build_isolation and build_requirements is not None
 
         if should_isolate:
-            with self.req.build_env:
-                pass
-            _install_build_reqs(finder, self.req.build_env.path,
-                                build_requirements)
-        else:
-            self.req.build_env = NoOpBuildEnvironment(no_clean=False)
+            # Haven't implemented PEP 517 yet, so spew a warning about it if
+            # build-requirements don't include setuptools and wheel.
+            missing_requirements = {'setuptools', 'wheel'} - {
+                pkg_resources.Requirement(r).key for r in build_requirements
+            }
+            if missing_requirements:
+                logger.warning(
+                    "Missing build requirements in pyproject.toml for %s.",
+                    self.req,
+                )
+                logger.warning(
+                    "This version of pip does not implement PEP 517 so it "
+                    "cannot build a wheel without %s.",
+                    " and ".join(map(repr, sorted(missing_requirements)))
+                )
+
+            # Isolate in a BuildEnvironment and install the build-time
+            # requirements.
+            self.req.build_env = BuildEnvironment()
+            self.req.build_env.install_requirements(
+                finder, build_requirements,
+                "Installing build dependencies"
+            )
 
         try:
             self.req.run_egg_info()
@@ -173,11 +145,12 @@ class RequirementPreparer(object):
     """
 
     def __init__(self, build_dir, download_dir, src_dir, wheel_download_dir,
-                 progress_bar, build_isolation):
+                 progress_bar, build_isolation, req_tracker):
         super(RequirementPreparer, self).__init__()
 
         self.src_dir = src_dir
         self.build_dir = build_dir
+        self.req_tracker = req_tracker
 
         # Where still packed archives should be written to. If None, they are
         # not saved, and are deleted immediately after unpacking.
@@ -236,16 +209,8 @@ class RequirementPreparer(object):
             # installation.
             # FIXME: this won't upgrade when there's an existing
             # package unpacked in `req.source_dir`
-            # package unpacked in `req.source_dir`
-            # if os.path.exists(os.path.join(req.source_dir, 'setup.py')):
-            #     raise PreviousBuildDirError(
-            #         "pip can't proceed with requirements '%s' due to a"
-            #         " pre-existing build directory (%s). This is "
-            #         "likely due to a previous installation that failed"
-            #         ". pip is being responsible and not assuming it "
-            #         "can delete this. Please delete it and try again."
-            #         % (req, req.source_dir)
-            #     )
+            if os.path.exists(os.path.join(req.source_dir, 'setup.py')):
+                rmtree(req.source_dir)
             req.populate_link(finder, upgrade_allowed, require_hashes)
 
             # We can't hit this spot and have populate_link return None.
@@ -325,7 +290,8 @@ class RequirementPreparer(object):
                     (req, exc, req.link)
                 )
             abstract_dist = make_abstract_dist(req)
-            abstract_dist.prep_for_dist(finder, self.build_isolation)
+            with self.req_tracker.track(req):
+                abstract_dist.prep_for_dist(finder, self.build_isolation)
             if self._download_should_save:
                 # Make a .zip of the source_dir we already created.
                 if req.link.scheme in vcs.all_schemes:
@@ -351,7 +317,8 @@ class RequirementPreparer(object):
             req.update_editable(not self._download_should_save)
 
             abstract_dist = make_abstract_dist(req)
-            abstract_dist.prep_for_dist(finder, self.build_isolation)
+            with self.req_tracker.track(req):
+                abstract_dist.prep_for_dist(finder, self.build_isolation)
 
             if self._download_should_save:
                 req.archive(self.download_dir)
diff --git a/pipenv/patched/notpip/_internal/req/__init__.py b/pipenv/patched/notpip/_internal/req/__init__.py
index 9eece6ec..72aecb7c 100644
--- a/pipenv/patched/notpip/_internal/req/__init__.py
+++ b/pipenv/patched/notpip/_internal/req/__init__.py
@@ -48,7 +48,7 @@ def install_given_reqs(to_install, install_options, global_options=(),
                     *args,
                     **kwargs
                 )
-            except:
+            except Exception:
                 should_rollback = (
                     requirement.conflicts_with and
                     not requirement.install_succeeded
diff --git a/pipenv/patched/notpip/_internal/req/req_install.py b/pipenv/patched/notpip/_internal/req/req_install.py
index 6347200c..a9c642c0 100644
--- a/pipenv/patched/notpip/_internal/req/req_install.py
+++ b/pipenv/patched/notpip/_internal/req/req_install.py
@@ -1,5 +1,6 @@
 from __future__ import absolute_import
 
+import io
 import logging
 import os
 import re
@@ -7,7 +8,6 @@ import shutil
 import sys
 import sysconfig
 import traceback
-import warnings
 import zipfile
 from distutils.util import change_root
 from email.parser import FeedParser  # type: ignore
@@ -17,22 +17,21 @@ from pipenv.patched.notpip._vendor.packaging import specifiers
 from pipenv.patched.notpip._vendor.packaging.markers import Marker
 from pipenv.patched.notpip._vendor.packaging.requirements import InvalidRequirement, Requirement
 from pipenv.patched.notpip._vendor.packaging.utils import canonicalize_name
-from pipenv.patched.notpip._vendor.packaging.version import parse as parse_version
 from pipenv.patched.notpip._vendor.packaging.version import Version
+from pipenv.patched.notpip._vendor.packaging.version import parse as parse_version
 from pipenv.patched.notpip._vendor.pkg_resources import RequirementParseError, parse_requirements
 
 from pipenv.patched.notpip._internal import wheel
-from pipenv.patched.notpip._internal.build_env import BuildEnvironment
+from pipenv.patched.notpip._internal.build_env import NoOpBuildEnvironment
 from pipenv.patched.notpip._internal.compat import native_str
 from pipenv.patched.notpip._internal.download import (
     is_archive_file, is_url, path_to_url, url_to_path,
 )
-from pipenv.patched.notpip._internal.exceptions import InstallationError, UninstallationError
+from pipenv.patched.notpip._internal.exceptions import InstallationError
 from pipenv.patched.notpip._internal.locations import (
     PIP_DELETE_MARKER_FILENAME, running_under_virtualenv,
 )
 from pipenv.patched.notpip._internal.req.req_uninstall import UninstallPathSet
-from pipenv.patched.notpip._internal.utils.deprecation import RemovedInPip11Warning
 from pipenv.patched.notpip._internal.utils.hashes import Hashes
 from pipenv.patched.notpip._internal.utils.logging import indent_log
 from pipenv.patched.notpip._internal.utils.misc import (
@@ -127,8 +126,10 @@ class InstallRequirement(object):
         self.is_direct = False
 
         self.isolated = isolated
-        self.build_env = BuildEnvironment(no_clean=True)
+        self.build_env = NoOpBuildEnvironment()
 
+    # Constructors
+    #   TODO: Move these out of this class into custom methods.
     @classmethod
     def from_editable(cls, editable_req, comes_from=None, isolated=False,
                       options=None, wheel_cache=None, constraint=False):
@@ -311,6 +312,13 @@ class InstallRequirement(object):
             if old_link != self.link:
                 logger.debug('Using cached wheel link: %s', self.link)
 
+    # Things that are valid for all kinds of requirements?
+    @property
+    def name(self):
+        if self.req is None:
+            return None
+        return native_str(pkg_resources.safe_name(self.req.name))
+
     @property
     def specifier(self):
         return self.req.specifier
@@ -325,7 +333,56 @@ class InstallRequirement(object):
         return (len(specifiers) == 1 and
                 next(iter(specifiers)).operator in {'==', '==='})
 
+    @property
+    def installed_version(self):
+        return get_installed_version(self.name)
+
+    def match_markers(self, extras_requested=None):
+        if not extras_requested:
+            # Provide an extra to safely evaluate the markers
+            # without matching any extra
+            extras_requested = ('',)
+        if self.markers is not None:
+            return any(
+                self.markers.evaluate({'extra': extra})
+                for extra in extras_requested)
+        else:
+            return True
+
+    @property
+    def has_hash_options(self):
+        """Return whether any known-good hashes are specified as options.
+
+        These activate --require-hashes mode; hashes specified as part of a
+        URL do not.
+
+        """
+        return bool(self.options.get('hashes', {}))
+
+    def hashes(self, trust_internet=True):
+        """Return a hash-comparer that considers my option- and URL-based
+        hashes to be known-good.
+
+        Hashes in URLs--ones embedded in the requirements file, not ones
+        downloaded from an index server--are almost peers with ones from
+        flags. They satisfy --require-hashes (whether it was implicitly or
+        explicitly activated) but do not activate it. md5 and sha224 are not
+        allowed in flags, which should nudge people toward good algos. We
+        always OR all hashes together, even ones from URLs.
+
+        :param trust_internet: Whether to trust URL-based (#md5=...) hashes
+            downloaded from the internet, as by populate_link()
+
+        """
+        good_hashes = self.options.get('hashes', {}).copy()
+        link = self.link if trust_internet else self.original_link
+        if link and link.hash:
+            good_hashes.setdefault(link.hash_name, []).append(link.hash)
+        return Hashes(good_hashes)
+
     def from_path(self):
+        """Format a nice indicator to show where this "comes from"
+        """
         if self.req is None:
             return None
         s = str(self.req)
@@ -398,12 +455,78 @@ class InstallRequirement(object):
         self.source_dir = os.path.normpath(os.path.abspath(new_location))
         self._egg_info_path = None
 
-    @property
-    def name(self):
+    def remove_temporary_source(self):
+        """Remove the source files from this requirement, if they are marked
+        for deletion"""
+        if self.source_dir and os.path.exists(
+                os.path.join(self.source_dir, PIP_DELETE_MARKER_FILENAME)):
+            logger.debug('Removing source in %s', self.source_dir)
+            rmtree(self.source_dir)
+        self.source_dir = None
+        self._temp_build_dir.cleanup()
+        self.build_env.cleanup()
+
+    def check_if_exists(self, use_user_site):
+        """Find an installed distribution that satisfies or conflicts
+        with this requirement, and set self.satisfied_by or
+        self.conflicts_with appropriately.
+        """
         if self.req is None:
-            return None
-        return native_str(pkg_resources.safe_name(self.req.name))
+            return False
+        try:
+            # get_distribution() will resolve the entire list of requirements
+            # anyway, and we've already determined that we need the requirement
+            # in question, so strip the marker so that we don't try to
+            # evaluate it.
+            no_marker = Requirement(str(self.req))
+            no_marker.marker = None
+            self.satisfied_by = pkg_resources.get_distribution(str(no_marker))
+            if self.editable and self.satisfied_by:
+                self.conflicts_with = self.satisfied_by
+                # when installing editables, nothing pre-existing should ever
+                # satisfy
+                self.satisfied_by = None
+                return True
+        except pkg_resources.DistributionNotFound:
+            return False
+        except pkg_resources.VersionConflict:
+            existing_dist = pkg_resources.get_distribution(
+                self.req.name
+            )
+            if use_user_site:
+                if dist_in_usersite(existing_dist):
+                    self.conflicts_with = existing_dist
+                elif (running_under_virtualenv() and
+                        dist_in_site_packages(existing_dist)):
+                    raise InstallationError(
+                        "Will not install to the user site because it will "
+                        "lack sys.path precedence to %s in %s" %
+                        (existing_dist.project_name, existing_dist.location)
+                    )
+            else:
+                self.conflicts_with = existing_dist
+        return True
 
+    # Things valid for wheels
+    @property
+    def is_wheel(self):
+        return self.link and self.link.is_wheel
+
+    def move_wheel_files(self, wheeldir, root=None, home=None, prefix=None,
+                         warn_script_location=True, use_user_site=False,
+                         pycompile=True):
+        move_wheel_files(
+            self.name, self.req, wheeldir,
+            user=use_user_site,
+            home=home,
+            root=root,
+            prefix=prefix,
+            pycompile=pycompile,
+            isolated=self.isolated,
+            warn_script_location=warn_script_location,
+        )
+
+    # Things valid for sdists
     @property
     def setup_py_dir(self):
         return os.path.join(
@@ -435,20 +558,47 @@ class InstallRequirement(object):
         return pp_toml
 
     def get_pep_518_info(self):
-        """Get a list of the packages required to build the project, if any,
-        and a flag indicating whether pyproject.toml is present, indicating
-        that the build should be isolated.
+        """Get PEP 518 build-time requirements.
 
-        Build requirements can be specified in a pyproject.toml, as described
-        in PEP 518. If this file exists but doesn't specify build
-        requirements, pip will default to installing setuptools and wheel.
+        Returns the list of the packages required to build the project,
+        specified as per PEP 518 within the package. If `pyproject.toml` is not
+        present, returns None to signify not using the same.
         """
-        if os.path.isfile(self.pyproject_toml):
-            with open(self.pyproject_toml) as f:
-                pp_toml = pytoml.load(f)
-            build_sys = pp_toml.get('build-system', {})
-            return (build_sys.get('requires', ['setuptools', 'wheel']), True)
-        return (['setuptools', 'wheel'], False)
+        # If pyproject.toml does not exist, don't do anything.
+        if not os.path.isfile(self.pyproject_toml):
+            return None
+
+        error_template = (
+            "{package} has a pyproject.toml file that does not comply "
+            "with PEP 518: {reason}"
+        )
+
+        with io.open(self.pyproject_toml, encoding="utf-8") as f:
+            pp_toml = pytoml.load(f)
+
+        # If there is no build-system table, just use setuptools and wheel.
+        if "build-system" not in pp_toml:
+            return ["setuptools", "wheel"]
+
+        # Specifying the build-system table but not the requires key is invalid
+        build_system = pp_toml["build-system"]
+        if "requires" not in build_system:
+            raise InstallationError(
+                error_template.format(package=self, reason=(
+                    "it has a 'build-system' table but not "
+                    "'build-system.requires' which is mandatory in the table"
+                ))
+            )
+
+        # Error out if it's not a list of strings
+        requires = build_system["requires"]
+        if not _is_list_of_str(requires):
+            raise InstallationError(error_template.format(
+                package=self,
+                reason="'build-system.requires' is not a list of strings.",
+            ))
+
+        return requires
 
     def run_egg_info(self):
         assert self.source_dir
@@ -559,11 +709,9 @@ class InstallRequirement(object):
 
             if not filenames:
                 raise InstallationError(
-                    'No files/directories in %s (from %s)' % (base, filename)
+                    "Files/directories (from %s) not found in %s"
+                    % (filename, base)
                 )
-            assert filenames, \
-                "No files/directories in %s (from %s)" % (base, filename)
-
             # if we have more than one match, we pick the toplevel one.  This
             # can easily be the case if there is a dist folder which contains
             # an extracted tarball for testing purposes.
@@ -588,9 +736,17 @@ class InstallRequirement(object):
 
     _requirements_section_re = re.compile(r'\[(.*?)\]')
 
-    @property
-    def installed_version(self):
-        return get_installed_version(self.name)
+    def get_dist(self):
+        """Return a pkg_resources.Distribution built from self.egg_info_path"""
+        egg_info = self.egg_info_path('').rstrip(os.path.sep)
+        base_dir = os.path.dirname(egg_info)
+        metadata = pkg_resources.PathMetadata(base_dir, egg_info)
+        dist_name = os.path.splitext(os.path.basename(egg_info))[0]
+        return pkg_resources.Distribution(
+            os.path.dirname(egg_info),
+            project_name=dist_name,
+            metadata=metadata,
+        )
 
     def assert_source_matches_version(self):
         assert self.source_dir
@@ -609,6 +765,52 @@ class InstallRequirement(object):
                 self,
             )
 
+    # For both source distributions and editables
+    def ensure_has_source_dir(self, parent_dir):
+        """Ensure that a source_dir is set.
+
+        This will create a temporary build dir if the name of the requirement
+        isn't known yet.
+
+        :param parent_dir: The ideal pip parent_dir for the source_dir.
+            Generally src_dir for editables and build_dir for sdists.
+        :return: self.source_dir
+        """
+        if self.source_dir is None:
+            self.source_dir = self.build_location(parent_dir)
+        return self.source_dir
+
+    # For editable installations
+    def install_editable(self, install_options,
+                         global_options=(), prefix=None):
+        logger.info('Running setup.py develop for %s', self.name)
+
+        if self.isolated:
+            global_options = list(global_options) + ["--no-user-cfg"]
+
+        if prefix:
+            prefix_param = ['--prefix={}'.format(prefix)]
+            install_options = list(install_options) + prefix_param
+
+        with indent_log():
+            # FIXME: should we do --install-headers here too?
+            with self.build_env:
+                call_subprocess(
+                    [
+                        os.environ.get('PIP_PYTHON_PATH', sys.executable),
+                        '-c',
+                        SETUPTOOLS_SHIM % self.setup_py
+                    ] +
+                    list(global_options) +
+                    ['develop', '--no-deps'] +
+                    list(install_options),
+
+                    cwd=self.setup_py_dir,
+                    show_stdout=False,
+                )
+
+        self.install_succeeded = True
+
     def update_editable(self, obtain=True):
         if not self.link:
             logger.debug(
@@ -638,6 +840,7 @@ class InstallRequirement(object):
                 'Unexpected version control type (in %s): %s'
                 % (self.link, vc_type))
 
+    # Top-level Actions
     def uninstall(self, auto_confirm=False, verbose=False,
                   use_user_site=False):
         """
@@ -661,6 +864,16 @@ class InstallRequirement(object):
         uninstalled_pathset.remove(auto_confirm, verbose)
         return uninstalled_pathset
 
+    def _clean_zip_name(self, name, prefix):  # only used by archive.
+        assert name.startswith(prefix + os.path.sep), (
+            "name %r doesn't start with prefix %r" % (name, prefix)
+        )
+        name = name[len(prefix) + 1:]
+        name = name.replace(os.path.sep, '/')
+        return name
+
+    # TODO: Investigate if this should be kept in InstallRequirement
+    #       Seems to be used only when VCS + downloads
     def archive(self, build_dir):
         assert self.source_dir
         create_archive = True
@@ -709,26 +922,6 @@ class InstallRequirement(object):
             zip.close()
             logger.info('Saved %s', display_path(archive_path))
 
-    def _clean_zip_name(self, name, prefix):
-        assert name.startswith(prefix + os.path.sep), (
-            "name %r doesn't start with prefix %r" % (name, prefix)
-        )
-        name = name[len(prefix) + 1:]
-        name = name.replace(os.path.sep, '/')
-        return name
-
-    def match_markers(self, extras_requested=None):
-        if not extras_requested:
-            # Provide an extra to safely evaluate the markers
-            # without matching any extra
-            extras_requested = ('',)
-        if self.markers is not None:
-            return any(
-                self.markers.evaluate({'extra': extra})
-                for extra in extras_requested)
-        else:
-            return True
-
     def install(self, install_options, global_options=None, root=None,
                 home=None, prefix=None, warn_script_location=True,
                 use_user_site=False, pycompile=True):
@@ -820,20 +1013,6 @@ class InstallRequirement(object):
             with open(inst_files_path, 'w') as f:
                 f.write('\n'.join(new_lines) + '\n')
 
-    def ensure_has_source_dir(self, parent_dir):
-        """Ensure that a source_dir is set.
-
-        This will create a temporary build dir if the name of the requirement
-        isn't known yet.
-
-        :param parent_dir: The ideal pip parent_dir for the source_dir.
-            Generally src_dir for editables and build_dir for sdists.
-        :return: self.source_dir
-        """
-        if self.source_dir is None:
-            self.source_dir = self.build_location(parent_dir)
-        return self.source_dir
-
     def get_install_args(self, global_options, record_filename, root, prefix,
                          pycompile):
         install_args = [os.environ.get('PIP_PYTHON_PATH', sys.executable), "-u"]
@@ -861,165 +1040,6 @@ class InstallRequirement(object):
 
         return install_args
 
-    def remove_temporary_source(self):
-        """Remove the source files from this requirement, if they are marked
-        for deletion"""
-        if self.source_dir and os.path.exists(
-                os.path.join(self.source_dir, PIP_DELETE_MARKER_FILENAME)):
-            logger.debug('Removing source in %s', self.source_dir)
-            rmtree(self.source_dir)
-        self.source_dir = None
-        self._temp_build_dir.cleanup()
-        self.build_env.cleanup()
-
-    def install_editable(self, install_options,
-                         global_options=(), prefix=None):
-        logger.info('Running setup.py develop for %s', self.name)
-
-        if self.isolated:
-            global_options = list(global_options) + ["--no-user-cfg"]
-
-        if prefix:
-            prefix_param = ['--prefix={}'.format(prefix)]
-            install_options = list(install_options) + prefix_param
-
-        with indent_log():
-            # FIXME: should we do --install-headers here too?
-            with self.build_env:
-                call_subprocess(
-                    [
-                        os.environ.get('PIP_PYTHON_PATH', sys.executable),
-                        '-c',
-                        SETUPTOOLS_SHIM % self.setup_py
-                    ] +
-                    list(global_options) +
-                    ['develop', '--no-deps'] +
-                    list(install_options),
-
-                    cwd=self.setup_py_dir,
-                    show_stdout=False,
-                )
-
-        self.install_succeeded = True
-
-    def check_if_exists(self, use_user_site):
-        """Find an installed distribution that satisfies or conflicts
-        with this requirement, and set self.satisfied_by or
-        self.conflicts_with appropriately.
-        """
-        if self.req is None:
-            return False
-        try:
-            # get_distribution() will resolve the entire list of requirements
-            # anyway, and we've already determined that we need the requirement
-            # in question, so strip the marker so that we don't try to
-            # evaluate it.
-            no_marker = Requirement(str(self.req))
-            no_marker.marker = None
-            self.satisfied_by = pkg_resources.get_distribution(str(no_marker))
-            if self.editable and self.satisfied_by:
-                self.conflicts_with = self.satisfied_by
-                # when installing editables, nothing pre-existing should ever
-                # satisfy
-                self.satisfied_by = None
-                return True
-        except pkg_resources.DistributionNotFound:
-            return False
-        except pkg_resources.VersionConflict:
-            existing_dist = pkg_resources.get_distribution(
-                self.req.name
-            )
-            if use_user_site:
-                if dist_in_usersite(existing_dist):
-                    self.conflicts_with = existing_dist
-                elif (running_under_virtualenv() and
-                        dist_in_site_packages(existing_dist)):
-                    raise InstallationError(
-                        "Will not install to the user site because it will "
-                        "lack sys.path precedence to %s in %s" %
-                        (existing_dist.project_name, existing_dist.location)
-                    )
-            else:
-                self.conflicts_with = existing_dist
-        return True
-
-    @property
-    def is_wheel(self):
-        return self.link and self.link.is_wheel
-
-    def move_wheel_files(self, wheeldir, root=None, home=None, prefix=None,
-                         warn_script_location=True, use_user_site=False,
-                         pycompile=True):
-        move_wheel_files(
-            self.name, self.req, wheeldir,
-            user=use_user_site,
-            home=home,
-            root=root,
-            prefix=prefix,
-            pycompile=pycompile,
-            isolated=self.isolated,
-            warn_script_location=warn_script_location,
-        )
-
-    def get_dist(self):
-        """Return a pkg_resources.Distribution built from self.egg_info_path"""
-        egg_info = self.egg_info_path('').rstrip(os.path.sep)
-        base_dir = os.path.dirname(egg_info)
-        metadata = pkg_resources.PathMetadata(base_dir, egg_info)
-        dist_name = os.path.splitext(os.path.basename(egg_info))[0]
-        return pkg_resources.Distribution(
-            os.path.dirname(egg_info),
-            project_name=dist_name,
-            metadata=metadata,
-        )
-
-    @property
-    def has_hash_options(self):
-        """Return whether any known-good hashes are specified as options.
-
-        These activate --require-hashes mode; hashes specified as part of a
-        URL do not.
-
-        """
-        return bool(self.options.get('hashes', {}))
-
-    def hashes(self, trust_internet=True):
-        """Return a hash-comparer that considers my option- and URL-based
-        hashes to be known-good.
-
-        Hashes in URLs--ones embedded in the requirements file, not ones
-        downloaded from an index server--are almost peers with ones from
-        flags. They satisfy --require-hashes (whether it was implicitly or
-        explicitly activated) but do not activate it. md5 and sha224 are not
-        allowed in flags, which should nudge people toward good algos. We
-        always OR all hashes together, even ones from URLs.
-
-        :param trust_internet: Whether to trust URL-based (#md5=...) hashes
-            downloaded from the internet, as by populate_link()
-
-        """
-        good_hashes = self.options.get('hashes', {}).copy()
-        link = self.link if trust_internet else self.original_link
-        if link and link.hash:
-            good_hashes.setdefault(link.hash_name, []).append(link.hash)
-        return Hashes(good_hashes)
-
-
-def _strip_postfix(req):
-    """
-        Strip req postfix ( -dev, 0.2, etc )
-    """
-    # FIXME: use package_to_requirement?
-    match = re.search(r'^(.*?)(?:-dev|-\d.*)$', req)
-    if match:
-        # Strip off -dev, -0.2, etc.
-        warnings.warn(
-            "#egg cleanup for editable urls will be dropped in the future",
-            RemovedInPip11Warning,
-        )
-        req = match.group(1)
-    return req
-
 
 def parse_editable(editable_req):
     """Parses an editable requirement into:
@@ -1085,7 +1105,7 @@ def parse_editable(editable_req):
             "Could not detect requirement name for '%s', please specify one "
             "with #egg=your_package_name" % editable_req
         )
-    return _strip_postfix(package_name), url, None
+    return package_name, url, None
 
 
 def deduce_helpful_msg(req):
@@ -1113,3 +1133,10 @@ def deduce_helpful_msg(req):
     else:
         msg += " File '%s' does not exist." % (req)
     return msg
+
+
+def _is_list_of_str(obj):
+    return (
+        isinstance(obj, list) and
+        all(isinstance(item, six.string_types) for item in obj)
+    )
diff --git a/pipenv/patched/notpip/_internal/req/req_set.py b/pipenv/patched/notpip/_internal/req/req_set.py
index 2fd09f5e..2c54c85a 100644
--- a/pipenv/patched/notpip/_internal/req/req_set.py
+++ b/pipenv/patched/notpip/_internal/req/req_set.py
@@ -14,9 +14,6 @@ class RequirementSet(object):
 
     def __init__(self, require_hashes=False, ignore_compatibility=True):
         """Create a RequirementSet.
-
-        :param wheel_cache: The pip wheel cache, for passing to
-            InstallRequirement.
         """
 
         self.requirements = OrderedDict()
diff --git a/pipenv/patched/notpip/_internal/req/req_tracker.py b/pipenv/patched/notpip/_internal/req/req_tracker.py
new file mode 100644
index 00000000..6e0201fe
--- /dev/null
+++ b/pipenv/patched/notpip/_internal/req/req_tracker.py
@@ -0,0 +1,76 @@
+from __future__ import absolute_import
+
+import contextlib
+import errno
+import hashlib
+import logging
+import os
+
+from pipenv.patched.notpip._internal.utils.temp_dir import TempDirectory
+
+logger = logging.getLogger(__name__)
+
+
+class RequirementTracker(object):
+
+    def __init__(self):
+        self._root = os.environ.get('PIP_REQ_TRACKER')
+        if self._root is None:
+            self._temp_dir = TempDirectory(delete=False, kind='req-tracker')
+            self._temp_dir.create()
+            self._root = os.environ['PIP_REQ_TRACKER'] = self._temp_dir.path
+            logger.debug('Created requirements tracker %r', self._root)
+        else:
+            self._temp_dir = None
+            logger.debug('Re-using requirements tracker %r', self._root)
+        self._entries = set()
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self.cleanup()
+
+    def _entry_path(self, link):
+        hashed = hashlib.sha224(link.url_without_fragment.encode()).hexdigest()
+        return os.path.join(self._root, hashed)
+
+    def add(self, req):
+        link = req.link
+        info = str(req)
+        entry_path = self._entry_path(link)
+        try:
+            with open(entry_path) as fp:
+                # Error, these's already a build in progress.
+                raise LookupError('%s is already being built: %s'
+                                  % (link, fp.read()))
+        except IOError as e:
+            if e.errno != errno.ENOENT:
+                raise
+            assert req not in self._entries
+            with open(entry_path, 'w') as fp:
+                fp.write(info)
+            self._entries.add(req)
+            logger.debug('Added %s to build tracker %r', req, self._root)
+
+    def remove(self, req):
+        link = req.link
+        self._entries.remove(req)
+        os.unlink(self._entry_path(link))
+        logger.debug('Removed %s from build tracker %r', req, self._root)
+
+    def cleanup(self):
+        for req in set(self._entries):
+            self.remove(req)
+        remove = self._temp_dir is not None
+        if remove:
+            self._temp_dir.cleanup()
+        logger.debug('%s build tracker %r',
+                     'Removed' if remove else 'Cleaned',
+                     self._root)
+
+    @contextlib.contextmanager
+    def track(self, req):
+        self.add(req)
+        yield
+        self.remove(req)
diff --git a/pipenv/patched/notpip/_internal/req/req_uninstall.py b/pipenv/patched/notpip/_internal/req/req_uninstall.py
index 0104a09e..3ccd3265 100644
--- a/pipenv/patched/notpip/_internal/req/req_uninstall.py
+++ b/pipenv/patched/notpip/_internal/req/req_uninstall.py
@@ -57,12 +57,12 @@ def _unique(fn):
 @_unique
 def uninstallation_paths(dist):
     """
-    Yield all the uninstallation paths for dist based on RECORD-without-.pyc
+    Yield all the uninstallation paths for dist based on RECORD-without-.py[co]
 
     Yield paths to all the files in RECORD. For each .py file in RECORD, add
-    the .pyc in the same directory.
+    the .pyc and .pyo in the same directory.
 
-    UninstallPathSet.add() takes care of the __pycache__ .pyc.
+    UninstallPathSet.add() takes care of the __pycache__ .py[co].
     """
     r = csv.reader(FakeFile(dist.get_metadata_lines('RECORD')))
     for row in r:
@@ -73,6 +73,8 @@ def uninstallation_paths(dist):
             base = fn[:-3]
             path = os.path.join(dn, base + '.pyc')
             yield path
+            path = os.path.join(dn, base + '.pyo')
+            yield path
 
 
 def compact(paths):
diff --git a/pipenv/patched/notpip/_internal/resolve.py b/pipenv/patched/notpip/_internal/resolve.py
index 5b3d0c00..461a2bb8 100644
--- a/pipenv/patched/notpip/_internal/resolve.py
+++ b/pipenv/patched/notpip/_internal/resolve.py
@@ -18,7 +18,6 @@ from pipenv.patched.notpip._internal.exceptions import (
     BestVersionAlreadyInstalled, DistributionNotFound, HashError, HashErrors,
     UnsupportedPythonVersion,
 )
-
 from pipenv.patched.notpip._internal.req.req_install import InstallRequirement
 from pipenv.patched.notpip._internal.utils.logging import indent_log
 from pipenv.patched.notpip._internal.utils.misc import dist_in_usersite, ensure_dir
@@ -164,7 +163,7 @@ class Resolver(object):
 
         if not self._is_upgrade_allowed(req_to_install):
             if self.upgrade_strategy == "only-if-needed":
-                return 'not upgraded as not directly required'
+                return 'already satisfied, skipping upgrade'
             return 'already satisfied'
 
         # Check for the possibility of an upgrade.  For link-based
diff --git a/pipenv/patched/notpip/_internal/utils/deprecation.py b/pipenv/patched/notpip/_internal/utils/deprecation.py
index e5ab59ab..b140ac71 100644
--- a/pipenv/patched/notpip/_internal/utils/deprecation.py
+++ b/pipenv/patched/notpip/_internal/utils/deprecation.py
@@ -6,72 +6,84 @@ from __future__ import absolute_import
 import logging
 import warnings
 
+from pipenv.patched.notpip._vendor.packaging.version import parse
+
+from pipenv.patched.notpip import __version__ as current_version
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
 if MYPY_CHECK_RUNNING:
-    from typing import Any
+    from typing import Any, Optional  # noqa: F401
 
 
 class PipDeprecationWarning(Warning):
     pass
 
 
-class Pending(object):
-    pass
-
-
-class RemovedInPip11Warning(PipDeprecationWarning):
-    pass
-
-
-class RemovedInPip12Warning(PipDeprecationWarning, Pending):
-    pass
+_original_showwarning = None  # type: Any
 
 
 # Warnings <-> Logging Integration
-
-
-_warnings_showwarning = None  # type: Any
-
-
 def _showwarning(message, category, filename, lineno, file=None, line=None):
     if file is not None:
-        if _warnings_showwarning is not None:
-            _warnings_showwarning(
+        if _original_showwarning is not None:
+            _original_showwarning(
                 message, category, filename, lineno, file, line,
             )
+    elif issubclass(category, PipDeprecationWarning):
+        # We use a specially named logger which will handle all of the
+        # deprecation messages for pip.
+        logger = logging.getLogger("pip._internal.deprecations")
+        logger.warning(message)
     else:
-        if issubclass(category, PipDeprecationWarning):
-            # We use a specially named logger which will handle all of the
-            # deprecation messages for pip.
-            logger = logging.getLogger("pip._internal.deprecations")
-
-            # This is purposely using the % formatter here instead of letting
-            # the logging module handle the interpolation. This is because we
-            # want it to appear as if someone typed this entire message out.
-            log_message = "DEPRECATION: %s" % message
-
-            # PipDeprecationWarnings that are Pending still have at least 2
-            # versions to go until they are removed so they can just be
-            # warnings.  Otherwise, they will be removed in the very next
-            # version of pip. We want these to be more obvious so we use the
-            # ERROR logging level.
-            if issubclass(category, Pending):
-                logger.warning(log_message)
-            else:
-                logger.error(log_message)
-        else:
-            _warnings_showwarning(
-                message, category, filename, lineno, file, line,
-            )
+        _original_showwarning(
+            message, category, filename, lineno, file, line,
+        )
 
 
 def install_warning_logger():
     # Enable our Deprecation Warnings
     warnings.simplefilter("default", PipDeprecationWarning, append=True)
 
-    global _warnings_showwarning
+    global _original_showwarning
 
-    if _warnings_showwarning is None:
-        _warnings_showwarning = warnings.showwarning
+    if _original_showwarning is None:
+        _original_showwarning = warnings.showwarning
         warnings.showwarning = _showwarning
+
+
+def deprecated(reason, replacement, gone_in, issue=None):
+    # type: (str, Optional[str], Optional[str], Optional[int]) -> None
+    """Helper to deprecate existing functionality.
+
+    reason:
+        Textual reason shown to the user about why this functionality has
+        been deprecated.
+    replacement:
+        Textual suggestion shown to the user about what alternative
+        functionality they can use.
+    gone_in:
+        The version of pip does this functionality should get removed in.
+        Raises errors if pip's current version is greater than or equal to
+        this.
+    issue:
+        Issue number on the tracker that would serve as a useful place for
+        users to find related discussion and provide feedback.
+
+    Always pass replacement, gone_in and issue as keyword arguments for clarity
+    at the call site.
+    """
+
+    # Construct a nice message.
+    # This is purposely eagerly formatted as we want it to appear as if someone
+    # typed this entire message out.
+    message = "DEPRECATION: " + reason
+    if replacement is not None:
+        message += " A possible replacement is {}.".format(replacement)
+    if issue is not None:
+        url = "https://github.com/pypa/pip/issues/" + str(issue)
+        message += " You can find discussion regarding this at {}.".format(url)
+
+    # Raise as an error if it has to be removed.
+    if gone_in is not None and parse(current_version) >= parse(gone_in):
+        raise PipDeprecationWarning(message)
+    warnings.warn(message, category=PipDeprecationWarning, stacklevel=2)
diff --git a/pipenv/patched/notpip/_internal/utils/logging.py b/pipenv/patched/notpip/_internal/utils/logging.py
index 5864fe14..257a6234 100644
--- a/pipenv/patched/notpip/_internal/utils/logging.py
+++ b/pipenv/patched/notpip/_internal/utils/logging.py
@@ -97,7 +97,7 @@ class ColorizedStreamHandler(logging.StreamHandler):
         if hasattr(real_stream, "isatty") and real_stream.isatty():
             return True
 
-        # If we have an ASNI term we should color it
+        # If we have an ANSI term we should color it
         if os.environ.get("TERM") == "ANSI":
             return True
 
@@ -130,3 +130,96 @@ class MaxLevelFilter(logging.Filter):
 
     def filter(self, record):
         return record.levelno < self.level
+
+
+def setup_logging(verbosity, no_color, user_log_file):
+    """Configures and sets up all of the logging
+    """
+
+    # Determine the level to be logging at.
+    if verbosity >= 1:
+        level = "DEBUG"
+    elif verbosity == -1:
+        level = "WARNING"
+    elif verbosity == -2:
+        level = "ERROR"
+    elif verbosity <= -3:
+        level = "CRITICAL"
+    else:
+        level = "INFO"
+
+    # The "root" logger should match the "console" level *unless* we also need
+    # to log to a user log file.
+    include_user_log = user_log_file is not None
+    if include_user_log:
+        additional_log_file = user_log_file
+        root_level = "DEBUG"
+    else:
+        additional_log_file = "/dev/null"
+        root_level = level
+
+    # Disable any logging besides WARNING unless we have DEBUG level logging
+    # enabled for vendored libraries.
+    vendored_log_level = "WARNING" if level in ["INFO", "ERROR"] else "DEBUG"
+
+    # Shorthands for clarity
+    log_streams = {
+        "stdout": "ext://sys.stdout",
+        "stderr": "ext://sys.stderr",
+    }
+    handler_classes = {
+        "stream": "pip._internal.utils.logging.ColorizedStreamHandler",
+        "file": "pip._internal.utils.logging.BetterRotatingFileHandler",
+    }
+
+    logging.config.dictConfig({
+        "version": 1,
+        "disable_existing_loggers": False,
+        "filters": {
+            "exclude_warnings": {
+                "()": "pip._internal.utils.logging.MaxLevelFilter",
+                "level": logging.WARNING,
+            },
+        },
+        "formatters": {
+            "indent": {
+                "()": IndentingFormatter,
+                "format": "%(message)s",
+            },
+        },
+        "handlers": {
+            "console": {
+                "level": level,
+                "class": handler_classes["stream"],
+                "no_color": no_color,
+                "stream": log_streams["stdout"],
+                "filters": ["exclude_warnings"],
+                "formatter": "indent",
+            },
+            "console_errors": {
+                "level": "WARNING",
+                "class": handler_classes["stream"],
+                "no_color": no_color,
+                "stream": log_streams["stderr"],
+                "formatter": "indent",
+            },
+            "user_log": {
+                "level": "DEBUG",
+                "class": handler_classes["file"],
+                "filename": additional_log_file,
+                "delay": True,
+                "formatter": "indent",
+            },
+        },
+        "root": {
+            "level": root_level,
+            "handlers": ["console", "console_errors"] + (
+                ["user_log"] if include_user_log else []
+            ),
+        },
+        "loggers": {
+            "pip._vendor": {
+                "level": vendored_log_level
+            }
+        },
+    })
diff --git a/pipenv/patched/notpip/_internal/utils/misc.py b/pipenv/patched/notpip/_internal/utils/misc.py
index 459b309a..e254f3d4 100644
--- a/pipenv/patched/notpip/_internal/utils/misc.py
+++ b/pipenv/patched/notpip/_internal/utils/misc.py
@@ -24,9 +24,12 @@ from pipenv.patched.notpip._vendor import pkg_resources
 from pipenv.patched.notpip._vendor.retrying import retry  # type: ignore
 from pipenv.patched.notpip._vendor.six import PY2
 from pipenv.patched.notpip._vendor.six.moves import input
+from pipenv.patched.notpip._vendor.six.moves.urllib import parse as urllib_parse
 
-from pipenv.patched.notpip._internal.compat import console_to_str, expanduser, stdlib_pkgs
-from pipenv.patched.notpip._internal.exceptions import InstallationError
+from pipenv.patched.notpip._internal.compat import (
+    WINDOWS, console_to_str, expanduser, stdlib_pkgs,
+)
+from pipenv.patched.notpip._internal.exceptions import CommandError, InstallationError
 from pipenv.patched.notpip._internal.locations import (
     running_under_virtualenv, site_packages, user_site, virtualenv_no_global,
     write_delete_marker_file,
@@ -47,7 +50,7 @@ __all__ = ['rmtree', 'display_path', 'backup_dir',
            'unzip_file', 'untar_file', 'unpack_file', 'call_subprocess',
            'captured_stdout', 'ensure_dir',
            'ARCHIVE_EXTENSIONS', 'SUPPORTED_EXTENSIONS',
-           'get_installed_version']
+           'get_installed_version', 'remove_auth_from_url']
 
 
 logger = std_logging.getLogger(__name__)
@@ -818,17 +821,15 @@ class cached_property(object):
         return value
 
 
-def get_installed_version(dist_name, lookup_dirs=None):
+def get_installed_version(dist_name, working_set=None):
     """Get the installed version of dist_name avoiding pkg_resources cache"""
     # Create a requirement that we'll look for inside of setuptools.
     req = pkg_resources.Requirement.parse(dist_name)
 
-    # We want to avoid having this cached, so we need to construct a new
-    # working set each time.
-    if lookup_dirs is None:
+    if working_set is None:
+        # We want to avoid having this cached, so we need to construct a new
+        # working set each time.
         working_set = pkg_resources.WorkingSet()
-    else:
-        working_set = pkg_resources.WorkingSet(lookup_dirs)
 
     # Get the installed distribution from our working set
     dist = working_set.find(req)
@@ -849,3 +850,50 @@ def enum(*sequential, **named):
     reverse = {value: key for key, value in enums.items()}
     enums['reverse_mapping'] = reverse
     return type('Enum', (), enums)
+
+
+def remove_auth_from_url(url):
+    # Return a copy of url with 'username:password@' removed.
+    # username/pass params are passed to subversion through flags
+    # and are not recognized in the url.
+
+    # parsed url
+    purl = urllib_parse.urlsplit(url)
+    stripped_netloc = \
+        purl.netloc.split('@')[-1]
+
+    # stripped url
+    url_pieces = (
+        purl.scheme, stripped_netloc, purl.path, purl.query, purl.fragment
+    )
+    surl = urllib_parse.urlunsplit(url_pieces)
+    return surl
+
+
+def protect_pip_from_modification_on_windows(modifying_pip):
+    """Protection of pip.exe from modification on Windows
+
+    On Windows, any operation modifying pip should be run as:
+        python -m pip ...
+    """
+    pip_names = [
+        "pip.exe",
+        "pip{}.exe".format(sys.version_info[0]),
+        "pip{}.{}.exe".format(*sys.version_info[:2])
+    ]
+
+    # See https://github.com/pypa/pip/issues/1299 for more discussion
+    should_show_use_python_msg = (
+        modifying_pip and
+        WINDOWS and
+        os.path.basename(sys.argv[0]) in pip_names
+    )
+
+    if should_show_use_python_msg:
+        new_command = [
+            sys.executable, "-m", "pip"
+        ] + sys.argv[1:]
+        raise CommandError(
+            'To modify pip, please run the following command:\n{}'
+            .format(" ".join(new_command))
+        )
diff --git a/pipenv/patched/notpip/_internal/utils/outdated.py b/pipenv/patched/notpip/_internal/utils/outdated.py
index 87911380..6133e6fd 100644
--- a/pipenv/patched/notpip/_internal/utils/outdated.py
+++ b/pipenv/patched/notpip/_internal/utils/outdated.py
@@ -6,12 +6,11 @@ import logging
 import os.path
 import sys
 
-from pipenv.patched.notpip._vendor import lockfile
+from pipenv.patched.notpip._vendor import lockfile, pkg_resources
 from pipenv.patched.notpip._vendor.packaging import version as packaging_version
 
 from pipenv.patched.notpip._internal.compat import WINDOWS
 from pipenv.patched.notpip._internal.index import PackageFinder
-from pipenv.patched.notpip._internal.locations import USER_CACHE_DIR, running_under_virtualenv
 from pipenv.patched.notpip._internal.utils.filesystem import check_path_owner
 from pipenv.patched.notpip._internal.utils.misc import ensure_dir, get_installed_version
 
@@ -21,34 +20,9 @@ SELFCHECK_DATE_FMT = "%Y-%m-%dT%H:%M:%SZ"
 logger = logging.getLogger(__name__)
 
 
-class VirtualenvSelfCheckState(object):
-    def __init__(self):
-        self.statefile_path = os.path.join(sys.prefix, "pip-selfcheck.json")
-
-        # Load the existing state
-        try:
-            with open(self.statefile_path) as statefile:
-                self.state = json.load(statefile)
-        except (IOError, ValueError):
-            self.state = {}
-
-    def save(self, pypi_version, current_time):
-        # Attempt to write out our version check file
-        with open(self.statefile_path, "w") as statefile:
-            json.dump(
-                {
-                    "last_check": current_time.strftime(SELFCHECK_DATE_FMT),
-                    "pypi_version": pypi_version,
-                },
-                statefile,
-                sort_keys=True,
-                separators=(",", ":")
-            )
-
-
-class GlobalSelfCheckState(object):
-    def __init__(self):
-        self.statefile_path = os.path.join(USER_CACHE_DIR, "selfcheck.json")
+class SelfCheckState(object):
+    def __init__(self, cache_dir):
+        self.statefile_path = os.path.join(cache_dir, "selfcheck.json")
 
         # Load the existing state
         try:
@@ -84,11 +58,18 @@ class GlobalSelfCheckState(object):
                           separators=(",", ":"))
 
 
-def load_selfcheck_statefile():
-    if running_under_virtualenv():
-        return VirtualenvSelfCheckState()
-    else:
-        return GlobalSelfCheckState()
+def was_installed_by_pip(pkg):
+    """Checks whether pkg was installed by pip
+
+    This is used not to display the upgrade message when pip is in fact
+    installed by system package manager, such as dnf on Fedora.
+    """
+    try:
+        dist = pkg_resources.get_distribution(pkg)
+        return (dist.has_metadata('INSTALLER') and
+                'pip' in dist.get_metadata_lines('INSTALLER'))
+    except pkg_resources.DistributionNotFound:
+        return False
 
 
 def pip_version_check(session, options):
@@ -106,7 +87,7 @@ def pip_version_check(session, options):
     pypi_version = None
 
     try:
-        state = load_selfcheck_statefile()
+        state = SelfCheckState(cache_dir=options.cache_dir)
 
         current_time = datetime.datetime.utcnow()
         # Determine if we need to refresh the state
@@ -143,7 +124,8 @@ def pip_version_check(session, options):
 
         # Determine if our pypi_version is older
         if (pip_version < remote_version and
-                pip_version.base_version != remote_version.base_version):
+                pip_version.base_version != remote_version.base_version and
+                was_installed_by_pip('pip')):
             # Advise "python -m pip" on Windows to avoid issues
             # with overwriting pip.exe.
             if WINDOWS:
diff --git a/pipenv/patched/notpip/_internal/utils/packaging.py b/pipenv/patched/notpip/_internal/utils/packaging.py
index eee21469..13547743 100644
--- a/pipenv/patched/notpip/_internal/utils/packaging.py
+++ b/pipenv/patched/notpip/_internal/utils/packaging.py
@@ -1,7 +1,6 @@
 from __future__ import absolute_import
 
 import logging
-import os
 import sys
 from email.parser import FeedParser  # type: ignore
 
@@ -51,13 +50,7 @@ def check_dist_requires_python(dist, absorb=True):
         return requires_python
     try:
         if not check_requires_python(requires_python):
-            # raise exceptions.UnsupportedPythonVersion(
-            #     "%s requires Python '%s' but the running Python is %s" % (
-            #         dist.project_name,
-            #         requires_python,
-            #         '.'.join(map(str, sys.version_info[:3])),)
-            # )
-            return
+            return requires_python
     except specifiers.InvalidSpecifier as e:
         logger.warning(
             "Package %s has an invalid Requires-Python entry %s - %s",
diff --git a/pipenv/patched/notpip/_internal/utils/typing.py b/pipenv/patched/notpip/_internal/utils/typing.py
index 170cc6c0..56f2fa87 100644
--- a/pipenv/patched/notpip/_internal/utils/typing.py
+++ b/pipenv/patched/notpip/_internal/utils/typing.py
@@ -18,10 +18,10 @@ curious maintainer can reach here to read this.
 
 In pip, all static-typing related imports should be guarded as follows:
 
-    from pipenv.patched.notpip.utils.typing import MYPY_CHECK_RUNNING
+    from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
     if MYPY_CHECK_RUNNING:
-        from typing import ...
+        from typing import ...  # noqa: F401
 
 Ref: https://github.com/python/mypy/issues/3216
 """
diff --git a/pipenv/patched/notpip/_internal/utils/ui.py b/pipenv/patched/notpip/_internal/utils/ui.py
index 2b692a81..b96863cc 100644
--- a/pipenv/patched/notpip/_internal/utils/ui.py
+++ b/pipenv/patched/notpip/_internal/utils/ui.py
@@ -21,7 +21,7 @@ from pipenv.patched.notpip._internal.utils.misc import format_size
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
 if MYPY_CHECK_RUNNING:
-    from typing import Any
+    from typing import Any  # noqa: F401
 
 try:
     from pipenv.patched.notpip._vendor import colorama
diff --git a/pipenv/patched/notpip/_internal/vcs/__init__.py b/pipenv/patched/notpip/_internal/vcs/__init__.py
index f115de4c..146f2829 100644
--- a/pipenv/patched/notpip/_internal/vcs/__init__.py
+++ b/pipenv/patched/notpip/_internal/vcs/__init__.py
@@ -1,7 +1,6 @@
 """Handles all VCS (version control) support"""
 from __future__ import absolute_import
 
-import copy
 import errno
 import logging
 import os
@@ -17,8 +16,8 @@ from pipenv.patched.notpip._internal.utils.misc import (
 from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 
 if MYPY_CHECK_RUNNING:
-    from typing import Dict, Optional, Tuple
-    from pipenv.patched.notpip._internal.basecommand import Command
+    from typing import Dict, Optional, Tuple  # noqa: F401
+    from pipenv.patched.notpip._internal.basecommand import Command  # noqa: F401
 
 __all__ = ['vcs', 'get_src_requirement']
 
@@ -214,7 +213,7 @@ class VersionControl(object):
         """
         raise NotImplementedError
 
-    def get_url_rev(self):
+    def get_url_rev(self, url):
         """
         Returns the correct repository URL and revision by parsing the given
         repository URL
@@ -224,8 +223,8 @@ class VersionControl(object):
             "The format is <vcs>+<protocol>://<url>, "
             "e.g. svn+http://myrepo/svn/MyApp#egg=MyApp"
         )
-        assert '+' in self.url, error_message % self.url
-        url = self.url.split('+', 1)[1]
+        assert '+' in url, error_message % url
+        url = url.split('+', 1)[1]
         scheme, netloc, path, query, frag = urllib_parse.urlsplit(url)
         rev = None
         if '@' in path:
@@ -233,6 +232,24 @@ class VersionControl(object):
         url = urllib_parse.urlunsplit((scheme, netloc, path, query, ''))
         return url, rev
 
+    def get_url_rev_args(self, url):
+        """
+        Return the URL and RevOptions "extra arguments" to use in obtain(),
+        as a tuple (url, extra_args).
+        """
+        return url, []
+
+    def get_url_rev_options(self, url):
+        """
+        Return the URL and RevOptions object to use in obtain() and in
+        some cases export(), as a tuple (url, rev_options).
+        """
+        url, rev = self.get_url_rev(url)
+        url, extra_args = self.get_url_rev_args(url)
+        rev_options = self.make_rev_options(rev, extra_args=extra_args)
+
+        return url, rev_options
+
     def get_info(self, location):
         """
         Returns (url, revision), where both are strings
@@ -254,10 +271,14 @@ class VersionControl(object):
         """
         return (self.normalize_url(url1) == self.normalize_url(url2))
 
-    def obtain(self, dest):
+    def fetch_new(self, dest, url, rev_options):
         """
-        Called when installing or updating an editable package, takes the
-        source path of the checkout.
+        Fetch a revision from a repository, in the case that this is the
+        first fetch from the repository.
+
+        Args:
+          dest: the directory to fetch the repository to.
+          rev_options: a RevOptions object.
         """
         raise NotImplementedError
 
@@ -289,94 +310,95 @@ class VersionControl(object):
         """
         raise NotImplementedError
 
-    def check_destination(self, dest, url, rev_options):
+    def obtain(self, dest):
         """
-        Prepare a location to receive a checkout/clone.
-
-        Return True if the location is ready for (and requires) a
-        checkout/clone, False otherwise.
+        Install or update in editable mode the package represented by this
+        VersionControl object.
 
         Args:
-          rev_options: a RevOptions object.
+          dest: the repository directory in which to install or update.
         """
-        checkout = True
-        prompt = False
+        url, rev_options = self.get_url_rev_options(self.url)
+
+        if not os.path.exists(dest):
+            self.fetch_new(dest, url, rev_options)
+            return
+
         rev_display = rev_options.to_display()
-        if os.path.exists(dest):
-            checkout = False
-            if os.path.exists(os.path.join(dest, self.dirname)):
-                existing_url = self.get_url(dest)
-                if self.compare_urls(existing_url, url):
-                    logger.debug(
-                        '%s in %s exists, and has correct URL (%s)',
-                        self.repo_name.title(),
+        if self.is_repository_directory(dest):
+            existing_url = self.get_url(dest)
+            if self.compare_urls(existing_url, url):
+                logger.debug(
+                    '%s in %s exists, and has correct URL (%s)',
+                    self.repo_name.title(),
+                    display_path(dest),
+                    url,
+                )
+                if not self.is_commit_id_equal(dest, rev_options.rev):
+                    logger.info(
+                        'Updating %s %s%s',
                         display_path(dest),
-                        url,
-                    )
-                    if not self.is_commit_id_equal(dest, rev_options.rev):
-                        logger.info(
-                            'Updating %s %s%s',
-                            display_path(dest),
-                            self.repo_name,
-                            rev_display,
-                        )
-                        self.update(dest, rev_options)
-                    else:
-                        logger.info(
-                            'Skipping because already up-to-date.')
-                else:
-                    logger.warning(
-                        '%s %s in %s exists with URL %s',
-                        self.name,
                         self.repo_name,
-                        display_path(dest),
-                        existing_url,
+                        rev_display,
                     )
-                    prompt = ('(s)witch, (i)gnore, (w)ipe, (b)ackup ',
-                              ('s', 'i', 'w', 'b'))
-            else:
-                logger.warning(
-                    'Directory %s already exists, and is not a %s %s.',
-                    dest,
-                    self.name,
-                    self.repo_name,
-                )
-                prompt = ('(i)gnore, (w)ipe, (b)ackup ', ('i', 'w', 'b'))
-        if prompt:
+                    self.update(dest, rev_options)
+                else:
+                    logger.info('Skipping because already up-to-date.')
+                return
+
             logger.warning(
-                'The plan is to install the %s repository %s',
+                '%s %s in %s exists with URL %s',
                 self.name,
-                url,
+                self.repo_name,
+                display_path(dest),
+                existing_url,
             )
-            response = ask_path_exists('What to do?  %s' % prompt[0],
-                                       prompt[1])
+            prompt = ('(s)witch, (i)gnore, (w)ipe, (b)ackup ',
+                      ('s', 'i', 'w', 'b'))
+        else:
+            logger.warning(
+                'Directory %s already exists, and is not a %s %s.',
+                dest,
+                self.name,
+                self.repo_name,
+            )
+            prompt = ('(i)gnore, (w)ipe, (b)ackup ', ('i', 'w', 'b'))
 
-            if response == 's':
-                logger.info(
-                    'Switching %s %s to %s%s',
-                    self.repo_name,
-                    display_path(dest),
-                    url,
-                    rev_display,
-                )
-                self.switch(dest, url, rev_options)
-            elif response == 'i':
-                # do nothing
-                pass
-            elif response == 'w':
-                logger.warning('Deleting %s', display_path(dest))
-                rmtree(dest)
-                checkout = True
-            elif response == 'b':
-                dest_dir = backup_dir(dest)
-                logger.warning(
-                    'Backing up %s to %s', display_path(dest), dest_dir,
-                )
-                shutil.move(dest, dest_dir)
-                checkout = True
-            elif response == 'a':
-                sys.exit(-1)
-        return checkout
+        logger.warning(
+            'The plan is to install the %s repository %s',
+            self.name,
+            url,
+        )
+        response = ask_path_exists('What to do?  %s' % prompt[0], prompt[1])
+
+        if response == 'a':
+            sys.exit(-1)
+
+        if response == 'w':
+            logger.warning('Deleting %s', display_path(dest))
+            rmtree(dest)
+            self.fetch_new(dest, url, rev_options)
+            return
+
+        if response == 'b':
+            dest_dir = backup_dir(dest)
+            logger.warning(
+                'Backing up %s to %s', display_path(dest), dest_dir,
+            )
+            shutil.move(dest, dest_dir)
+            self.fetch_new(dest, url, rev_options)
+            return
+
+        # Do nothing if the response is "i".
+        if response == 's':
+            logger.info(
+                'Switching %s %s to %s%s',
+                self.repo_name,
+                display_path(dest),
+                url,
+                rev_display,
+            )
+            self.switch(dest, url, rev_options)
 
     def unpack(self, location):
         """
@@ -399,7 +421,8 @@ class VersionControl(object):
     def get_url(self, location):
         """
         Return the url used at location
-        Used in get_info or check_destination
+
+        This is used in get_info() and obtain().
         """
         raise NotImplementedError
 
@@ -436,17 +459,26 @@ class VersionControl(object):
             else:
                 raise  # re-raise exception if a different error occurred
 
+    @classmethod
+    def is_repository_directory(cls, path):
+        """
+        Return whether a directory path is a repository directory.
+        """
+        logger.debug('Checking in %s for %s (%s)...',
+                     path, cls.dirname, cls.name)
+        return os.path.exists(os.path.join(path, cls.dirname))
+
     @classmethod
     def controls_location(cls, location):
         """
         Check if a location is controlled by the vcs.
         It is meant to be overridden to implement smarter detection
         mechanisms for specific vcs.
+
+        This can do more than is_repository_directory() alone.  For example,
+        the Git override checks that Git is actually available.
         """
-        logger.debug('Checking in %s for %s (%s)...',
-                     location, cls.dirname, cls.name)
-        path = os.path.join(location, cls.dirname)
-        return os.path.exists(path)
+        return cls.is_repository_directory(location)
 
 
 def get_src_requirement(dist, location):
diff --git a/pipenv/patched/notpip/_internal/vcs/bazaar.py b/pipenv/patched/notpip/_internal/vcs/bazaar.py
index f9a36a93..b2664cd8 100644
--- a/pipenv/patched/notpip/_internal/vcs/bazaar.py
+++ b/pipenv/patched/notpip/_internal/vcs/bazaar.py
@@ -48,6 +48,17 @@ class Bazaar(VersionControl):
                 cwd=temp_dir.path, show_stdout=False,
             )
 
+    def fetch_new(self, dest, url, rev_options):
+        rev_display = rev_options.to_display()
+        logger.info(
+            'Checking out %s%s to %s',
+            url,
+            rev_display,
+            display_path(dest),
+        )
+        cmd_args = ['branch', '-q'] + rev_options.to_args() + [url, dest]
+        self.run_command(cmd_args)
+
     def switch(self, dest, url, rev_options):
         self.run_command(['switch', url], cwd=dest)
 
@@ -55,23 +66,9 @@ class Bazaar(VersionControl):
         cmd_args = ['pull', '-q'] + rev_options.to_args()
         self.run_command(cmd_args, cwd=dest)
 
-    def obtain(self, dest):
-        url, rev = self.get_url_rev()
-        rev_options = self.make_rev_options(rev)
-        if self.check_destination(dest, url, rev_options):
-            rev_display = rev_options.to_display()
-            logger.info(
-                'Checking out %s%s to %s',
-                url,
-                rev_display,
-                display_path(dest),
-            )
-            cmd_args = ['branch', '-q'] + rev_options.to_args() + [url, dest]
-            self.run_command(cmd_args)
-
-    def get_url_rev(self):
+    def get_url_rev(self, url):
         # hotfix the URL scheme after removing bzr+ from bzr+ssh:// readd it
-        url, rev = super(Bazaar, self).get_url_rev()
+        url, rev = super(Bazaar, self).get_url_rev(url)
         if url.startswith('ssh://'):
             url = 'bzr+' + url
         return url, rev
diff --git a/pipenv/patched/notpip/_internal/vcs/git.py b/pipenv/patched/notpip/_internal/vcs/git.py
index dd862bfe..ef2dd908 100644
--- a/pipenv/patched/notpip/_internal/vcs/git.py
+++ b/pipenv/patched/notpip/_internal/vcs/git.py
@@ -43,7 +43,7 @@ class Git(VersionControl):
     def __init__(self, url=None, *args, **kwargs):
 
         # Works around an apparent Git bug
-        # (see http://article.gmane.org/gmane.comp.version-control.git/146500)
+        # (see https://article.gmane.org/gmane.comp.version-control.git/146500)
         if url:
             scheme, netloc, path, query, fragment = urlsplit(url)
             if scheme.endswith('file'):
@@ -155,6 +155,33 @@ class Git(VersionControl):
 
         return self.get_revision(dest) == name
 
+    def fetch_new(self, dest, url, rev_options):
+        rev_display = rev_options.to_display()
+        logger.info(
+            'Cloning %s%s to %s', url, rev_display, display_path(dest),
+        )
+        self.run_command(['clone', '-q', url, dest])
+
+        if rev_options.rev:
+            # Then a specific revision was requested.
+            rev_options = self.check_rev_options(dest, rev_options)
+            # Only do a checkout if the current commit id doesn't match
+            # the requested revision.
+            if not self.is_commit_id_equal(dest, rev_options.rev):
+                rev = rev_options.rev
+                # Only fetch the revision if it's a ref
+                if rev.startswith('refs/'):
+                    self.run_command(
+                        ['fetch', '-q', url] + rev_options.to_args(),
+                        cwd=dest,
+                    )
+                    # Change the revision to the SHA of the ref we fetched
+                    rev = 'FETCH_HEAD'
+                self.run_command(['checkout', '-q', rev], cwd=dest)
+
+        #: repo may contain submodules
+        self.update_submodules(dest)
+
     def switch(self, dest, url, rev_options):
         self.run_command(['config', 'remote.origin.url', url], cwd=dest)
         cmd_args = ['checkout', '-q'] + rev_options.to_args()
@@ -176,35 +203,6 @@ class Git(VersionControl):
         #: update submodules
         self.update_submodules(dest)
 
-    def obtain(self, dest):
-        url, rev = self.get_url_rev()
-        rev_options = self.make_rev_options(rev)
-        if self.check_destination(dest, url, rev_options):
-            rev_display = rev_options.to_display()
-            logger.info(
-                'Cloning %s%s to %s', url, rev_display, display_path(dest),
-            )
-            self.run_command(['clone', '-q', url, dest])
-
-            if rev:
-                rev_options = self.check_rev_options(dest, rev_options)
-                # Only do a checkout if the current commit id doesn't match
-                # the requested revision.
-                if not self.is_commit_id_equal(dest, rev_options.rev):
-                    rev = rev_options.rev
-                    # Only fetch the revision if it's a ref
-                    if rev.startswith('refs/'):
-                        self.run_command(
-                            ['fetch', '-q', url] + rev_options.to_args(),
-                            cwd=dest,
-                        )
-                        # Change the revision to the SHA of the ref we fetched
-                        rev = 'FETCH_HEAD'
-                    self.run_command(['checkout', '-q', rev], cwd=dest)
-
-            #: repo may contain submodules
-            self.update_submodules(dest)
-
     def get_url(self, location):
         """Return URL of the first remote encountered."""
         remotes = self.run_command(
@@ -267,20 +265,20 @@ class Git(VersionControl):
             req += '&subdirectory=' + subdirectory
         return req
 
-    def get_url_rev(self):
+    def get_url_rev(self, url):
         """
         Prefixes stub URLs like 'user@hostname:user/repo.git' with 'ssh://'.
-        That's required because although they use SSH they sometimes doesn't
-        work with a ssh:// scheme (e.g. Github). But we need a scheme for
+        That's required because although they use SSH they sometimes don't
+        work with a ssh:// scheme (e.g. GitHub). But we need a scheme for
         parsing. Hence we remove it again afterwards and return it as a stub.
         """
-        if '://' not in self.url:
-            assert 'file:' not in self.url
-            self.url = self.url.replace('git+', 'git+ssh://')
-            url, rev = super(Git, self).get_url_rev()
+        if '://' not in url:
+            assert 'file:' not in url
+            url = url.replace('git+', 'git+ssh://')
+            url, rev = super(Git, self).get_url_rev(url)
             url = url.replace('ssh://', '')
         else:
-            url, rev = super(Git, self).get_url_rev()
+            url, rev = super(Git, self).get_url_rev(url)
 
         return url, rev
 
diff --git a/pipenv/patched/notpip/_internal/vcs/mercurial.py b/pipenv/patched/notpip/_internal/vcs/mercurial.py
index e28228fc..a143e765 100644
--- a/pipenv/patched/notpip/_internal/vcs/mercurial.py
+++ b/pipenv/patched/notpip/_internal/vcs/mercurial.py
@@ -31,6 +31,18 @@ class Mercurial(VersionControl):
                 ['archive', location], show_stdout=False, cwd=temp_dir.path
             )
 
+    def fetch_new(self, dest, url, rev_options):
+        rev_display = rev_options.to_display()
+        logger.info(
+            'Cloning hg %s%s to %s',
+            url,
+            rev_display,
+            display_path(dest),
+        )
+        self.run_command(['clone', '--noupdate', '-q', url, dest])
+        cmd_args = ['update', '-q'] + rev_options.to_args()
+        self.run_command(cmd_args, cwd=dest)
+
     def switch(self, dest, url, rev_options):
         repo_config = os.path.join(dest, self.dirname, 'hgrc')
         config = configparser.SafeConfigParser()
@@ -52,21 +64,6 @@ class Mercurial(VersionControl):
         cmd_args = ['update', '-q'] + rev_options.to_args()
         self.run_command(cmd_args, cwd=dest)
 
-    def obtain(self, dest):
-        url, rev = self.get_url_rev()
-        rev_options = self.make_rev_options(rev)
-        if self.check_destination(dest, url, rev_options):
-            rev_display = rev_options.to_display()
-            logger.info(
-                'Cloning hg %s%s to %s',
-                url,
-                rev_display,
-                display_path(dest),
-            )
-            self.run_command(['clone', '--noupdate', '-q', url, dest])
-            cmd_args = ['update', '-q'] + rev_options.to_args()
-            self.run_command(cmd_args, cwd=dest)
-
     def get_url(self, location):
         url = self.run_command(
             ['showconfig', 'paths.default'],
diff --git a/pipenv/patched/notpip/_internal/vcs/subversion.py b/pipenv/patched/notpip/_internal/vcs/subversion.py
index 6f480ae3..5adbdaa3 100644
--- a/pipenv/patched/notpip/_internal/vcs/subversion.py
+++ b/pipenv/patched/notpip/_internal/vcs/subversion.py
@@ -8,7 +8,7 @@ from pipenv.patched.notpip._vendor.six.moves.urllib import parse as urllib_parse
 
 from pipenv.patched.notpip._internal.index import Link
 from pipenv.patched.notpip._internal.utils.logging import indent_log
-from pipenv.patched.notpip._internal.utils.misc import display_path, rmtree
+from pipenv.patched.notpip._internal.utils.misc import display_path, remove_auth_from_url, rmtree
 from pipenv.patched.notpip._internal.vcs import VersionControl, vcs
 
 _svn_xml_url_re = re.compile('url="([^"]+)"')
@@ -61,9 +61,8 @@ class Subversion(VersionControl):
 
     def export(self, location):
         """Export the svn repository at the url to the destination location"""
-        url, rev = self.get_url_rev()
-        rev_options = get_rev_options(self, url, rev)
-        url = self.remove_auth_from_url(url)
+        url, rev_options = self.get_url_rev_options(self.url)
+
         logger.info('Exporting svn repository %s to %s', url, location)
         with indent_log():
             if os.path.exists(location):
@@ -73,6 +72,17 @@ class Subversion(VersionControl):
             cmd_args = ['export'] + rev_options.to_args() + [url, location]
             self.run_command(cmd_args, show_stdout=False)
 
+    def fetch_new(self, dest, url, rev_options):
+        rev_display = rev_options.to_display()
+        logger.info(
+            'Checking out %s%s to %s',
+            url,
+            rev_display,
+            display_path(dest),
+        )
+        cmd_args = ['checkout', '-q'] + rev_options.to_args() + [url, dest]
+        self.run_command(cmd_args)
+
     def switch(self, dest, url, rev_options):
         cmd_args = ['switch'] + rev_options.to_args() + [url, dest]
         self.run_command(cmd_args)
@@ -81,21 +91,6 @@ class Subversion(VersionControl):
         cmd_args = ['update'] + rev_options.to_args() + [dest]
         self.run_command(cmd_args)
 
-    def obtain(self, dest):
-        url, rev = self.get_url_rev()
-        rev_options = get_rev_options(self, url, rev)
-        url = self.remove_auth_from_url(url)
-        if self.check_destination(dest, url, rev_options):
-            rev_display = rev_options.to_display()
-            logger.info(
-                'Checking out %s%s to %s',
-                url,
-                rev_display,
-                display_path(dest),
-            )
-            cmd_args = ['checkout', '-q'] + rev_options.to_args() + [url, dest]
-            self.run_command(cmd_args)
-
     def get_location(self, dist, dependency_links):
         for url in dependency_links:
             egg_fragment = Link(url).egg_fragment
@@ -137,13 +132,19 @@ class Subversion(VersionControl):
             revision = max(revision, localrev)
         return revision
 
-    def get_url_rev(self):
+    def get_url_rev(self, url):
         # hotfix the URL scheme after removing svn+ from svn+ssh:// readd it
-        url, rev = super(Subversion, self).get_url_rev()
+        url, rev = super(Subversion, self).get_url_rev(url)
         if url.startswith('ssh://'):
             url = 'svn+' + url
         return url, rev
 
+    def get_url_rev_args(self, url):
+        extra_args = get_rev_options_args(url)
+        url = remove_auth_from_url(url)
+
+        return url, extra_args
+
     def get_url(self, location):
         # In cases where the source is in a subdirectory, not alongside
         # setup.py we have to look up in the location until we find a real
@@ -221,28 +222,10 @@ class Subversion(VersionControl):
         """Always assume the versions don't match"""
         return False
 
-    @staticmethod
-    def remove_auth_from_url(url):
-        # Return a copy of url with 'username:password@' removed.
-        # username/pass params are passed to subversion through flags
-        # and are not recognized in the url.
-
-        # parsed url
-        purl = urllib_parse.urlsplit(url)
-        stripped_netloc = \
-            purl.netloc.split('@')[-1]
-
-        # stripped url
-        url_pieces = (
-            purl.scheme, stripped_netloc, purl.path, purl.query, purl.fragment
-        )
-        surl = urllib_parse.urlunsplit(url_pieces)
-        return surl
-
 
-def get_rev_options(vcs, url, rev):
+def get_rev_options_args(url):
     """
-    Return a RevOptions object.
+    Return the extra arguments to pass to RevOptions.
     """
     r = urllib_parse.urlsplit(url)
     if hasattr(r, 'username'):
@@ -265,7 +248,7 @@ def get_rev_options(vcs, url, rev):
     if password:
         extra_args += ['--password', password]
 
-    return vcs.make_rev_options(rev, extra_args=extra_args)
+    return extra_args
 
 
 vcs.register(Subversion)
diff --git a/pipenv/patched/notpip/_internal/wheel.py b/pipenv/patched/notpip/_internal/wheel.py
index fe3bf7d2..14ec0014 100644
--- a/pipenv/patched/notpip/_internal/wheel.py
+++ b/pipenv/patched/notpip/_internal/wheel.py
@@ -5,7 +5,6 @@ from __future__ import absolute_import
 
 import collections
 import compileall
-import copy
 import csv
 import hashlib
 import logging
@@ -24,7 +23,6 @@ from pipenv.patched.notpip._vendor.packaging.utils import canonicalize_name
 from pipenv.patched.notpip._vendor.six import StringIO
 
 from pipenv.patched.notpip._internal import pep425tags
-from pipenv.patched.notpip._internal.build_env import BuildEnvironment
 from pipenv.patched.notpip._internal.download import path_to_url, unpack_url
 from pipenv.patched.notpip._internal.exceptions import (
     InstallationError, InvalidWheelFilename, UnsupportedWheel,
@@ -42,7 +40,7 @@ from pipenv.patched.notpip._internal.utils.typing import MYPY_CHECK_RUNNING
 from pipenv.patched.notpip._internal.utils.ui import open_spinner
 
 if MYPY_CHECK_RUNNING:
-    from typing import Dict, List, Optional
+    from typing import Dict, List, Optional  # noqa: F401
 
 wheel_ext = '.whl'
 
@@ -52,9 +50,9 @@ VERSION_COMPATIBLE = (1, 0)
 logger = logging.getLogger(__name__)
 
 
-def rehash(path, algo='sha256', blocksize=1 << 20):
-    """Return (hash, length) for path using hashlib.new(algo)"""
-    h = hashlib.new(algo)
+def rehash(path, blocksize=1 << 20):
+    """Return (hash, length) for path using hashlib.sha256()"""
+    h = hashlib.sha256()
     length = 0
     with open(path, 'rb') as f:
         for block in read_chunks(f, size=blocksize):
@@ -164,7 +162,8 @@ def message_about_scripts_not_on_PATH(scripts):
 
     # We don't want to warn for directories that are on PATH.
     not_warn_dirs = [
-        os.path.normcase(i) for i in os.environ["PATH"].split(os.pathsep)
+        os.path.normcase(i).rstrip(os.sep) for i in
+        os.environ.get("PATH", "").split(os.pathsep)
     ]
     # If an executable sits with sys.executable, we don't warn for it.
     #     This covers the case of venv invocations without activating the venv.
@@ -284,6 +283,17 @@ def move_wheel_files(name, req, wheeldir, user=False, home=None, root=None,
                 # uninstalled.
                 ensure_dir(destdir)
 
+                # copyfile (called below) truncates the destination if it
+                # exists and then writes the new contents. This is fine in most
+                # cases, but can cause a segfault if pip has loaded a shared
+                # object (e.g. from pyopenssl through its vendored urllib3)
+                # Since the shared object is mmap'd an attempt to call a
+                # symbol in it will then cause a segfault. Unlinking the file
+                # allows writing of new contents while allowing the process to
+                # continue to use the old copy.
+                if os.path.exists(destfile):
+                    os.unlink(destfile)
+
                 # We use copyfile (not move, copy, or copy2) to be extra sure
                 # that we are not moving directories over (copyfile fails for
                 # directories) as well as to ensure that we are not copying
@@ -496,8 +506,8 @@ if __name__ == '__main__':
                     row[1], row[2] = rehash(row[0])
                 writer.writerow(row)
             for f in generated:
-                h, l = rehash(f)
-                writer.writerow((normpath(f, lib_dir), h, l))
+                digest, length = rehash(f)
+                writer.writerow((normpath(f, lib_dir), digest, length))
             for f in installed:
                 writer.writerow((installed[f], '', ''))
     shutil.move(temp_record, record)
@@ -518,7 +528,7 @@ def wheel_version(source_dir):
         version = wheel_data['Wheel-Version'].strip()
         version = tuple(map(int, version.split('.')))
         return version
-    except:
+    except Exception:
         return False
 
 
@@ -643,7 +653,7 @@ class WheelBuilder(object):
                     )
                     logger.info('Stored in directory: %s', output_dir)
                     return wheel_path
-                except:
+                except Exception:
                     pass
             # Ignore return, we can't do anything else useful.
             self._clean_one(req)
@@ -675,7 +685,7 @@ class WheelBuilder(object):
                 call_subprocess(wheel_args, cwd=req.setup_py_dir,
                                 show_stdout=False, spinner=spinner)
                 return True
-            except:
+            except Exception:
                 spinner.finish("error")
                 logger.error('Failed building wheel for %s', req.name)
                 return False
@@ -688,7 +698,7 @@ class WheelBuilder(object):
         try:
             call_subprocess(clean_args, cwd=req.source_dir, show_stdout=False)
             return True
-        except:
+        except Exception:
             logger.error('Failed cleaning build dir for %s', req.name)
             return False
 
diff --git a/pipenv/patched/notpip/_vendor/__init__.py b/pipenv/patched/notpip/_vendor/__init__.py
index 40ce7a01..b6294b21 100644
--- a/pipenv/patched/notpip/_vendor/__init__.py
+++ b/pipenv/patched/notpip/_vendor/__init__.py
@@ -107,5 +107,4 @@ if DEBUNDLED:
     vendored("requests.packages.urllib3.util.ssl_")
     vendored("requests.packages.urllib3.util.timeout")
     vendored("requests.packages.urllib3.util.url")
-
-import requests
+    vendored("urllib3")
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/__init__.py b/pipenv/patched/notpip/_vendor/cachecontrol/__init__.py
index f386d492..8fdee66f 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/__init__.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/__init__.py
@@ -2,9 +2,9 @@
 
 Make it easy to import from cachecontrol without long namespaces.
 """
-__author__ = 'Eric Larson'
-__email__ = 'eric@ionrock.org'
-__version__ = '0.12.4'
+__author__ = "Eric Larson"
+__email__ = "eric@ionrock.org"
+__version__ = "0.12.5"
 
 from .wrapper import CacheControl
 from .adapter import CacheControlAdapter
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/_cmd.py b/pipenv/patched/notpip/_vendor/cachecontrol/_cmd.py
index b5e44db8..fb90adf4 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/_cmd.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/_cmd.py
@@ -17,14 +17,11 @@ def setup_logging():
 
 def get_session():
     adapter = CacheControlAdapter(
-        DictCache(),
-        cache_etags=True,
-        serializer=None,
-        heuristic=None,
+        DictCache(), cache_etags=True, serializer=None, heuristic=None
     )
     sess = requests.Session()
-    sess.mount('http://', adapter)
-    sess.mount('https://', adapter)
+    sess.mount("http://", adapter)
+    sess.mount("https://", adapter)
 
     sess.cache_controller = adapter.controller
     return sess
@@ -32,7 +29,7 @@ def get_session():
 
 def get_args():
     parser = ArgumentParser()
-    parser.add_argument('url', help='The URL to try and cache')
+    parser.add_argument("url", help="The URL to try and cache")
     return parser.parse_args()
 
 
@@ -51,10 +48,10 @@ def main(args=None):
 
     # Now try to get it
     if sess.cache_controller.cached_request(resp.request):
-        print('Cached!')
+        print("Cached!")
     else:
-        print('Not cached :(')
+        print("Not cached :(")
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/adapter.py b/pipenv/patched/notpip/_vendor/cachecontrol/adapter.py
index bcf107ee..2f290998 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/adapter.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/adapter.py
@@ -10,25 +10,27 @@ from .filewrapper import CallbackFileWrapper
 
 
 class CacheControlAdapter(HTTPAdapter):
-    invalidating_methods = set(['PUT', 'DELETE'])
-
-    def __init__(self, cache=None,
-                 cache_etags=True,
-                 controller_class=None,
-                 serializer=None,
-                 heuristic=None,
-                 cacheable_methods=None,
-                 *args, **kw):
+    invalidating_methods = {"PUT", "DELETE"}
+
+    def __init__(
+        self,
+        cache=None,
+        cache_etags=True,
+        controller_class=None,
+        serializer=None,
+        heuristic=None,
+        cacheable_methods=None,
+        *args,
+        **kw
+    ):
         super(CacheControlAdapter, self).__init__(*args, **kw)
         self.cache = cache or DictCache()
         self.heuristic = heuristic
-        self.cacheable_methods = cacheable_methods or ('GET',)
+        self.cacheable_methods = cacheable_methods or ("GET",)
 
         controller_factory = controller_class or CacheController
         self.controller = controller_factory(
-            self.cache,
-            cache_etags=cache_etags,
-            serializer=serializer,
+            self.cache, cache_etags=cache_etags, serializer=serializer
         )
 
     def send(self, request, cacheable_methods=None, **kw):
@@ -43,20 +45,18 @@ class CacheControlAdapter(HTTPAdapter):
             except zlib.error:
                 cached_response = None
             if cached_response:
-                return self.build_response(request, cached_response,
-                                           from_cache=True)
+                return self.build_response(request, cached_response, from_cache=True)
 
             # check for etags and add headers if appropriate
-            request.headers.update(
-                self.controller.conditional_headers(request)
-            )
+            request.headers.update(self.controller.conditional_headers(request))
 
         resp = super(CacheControlAdapter, self).send(request, **kw)
 
         return resp
 
-    def build_response(self, request, response, from_cache=False,
-                       cacheable_methods=None):
+    def build_response(
+        self, request, response, from_cache=False, cacheable_methods=None
+    ):
         """
         Build a response by making a request or using the cache.
 
@@ -101,10 +101,8 @@ class CacheControlAdapter(HTTPAdapter):
                 response._fp = CallbackFileWrapper(
                     response._fp,
                     functools.partial(
-                        self.controller.cache_response,
-                        request,
-                        response,
-                    )
+                        self.controller.cache_response, request, response
+                    ),
                 )
                 if response.chunked:
                     super_update_chunk_length = response._update_chunk_length
@@ -113,11 +111,12 @@ class CacheControlAdapter(HTTPAdapter):
                         super_update_chunk_length()
                         if self.chunk_left == 0:
                             self._fp._close()
-                    response._update_chunk_length = types.MethodType(_update_chunk_length, response)
 
-        resp = super(CacheControlAdapter, self).build_response(
-            request, response
-        )
+                    response._update_chunk_length = types.MethodType(
+                        _update_chunk_length, response
+                    )
+
+        resp = super(CacheControlAdapter, self).build_response(request, response)
 
         # See if we should invalidate the cache.
         if request.method in self.invalidating_methods and resp.ok:
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/cache.py b/pipenv/patched/notpip/_vendor/cachecontrol/cache.py
index 7389a73f..94e07732 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/cache.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/cache.py
@@ -8,13 +8,13 @@ from threading import Lock
 class BaseCache(object):
 
     def get(self, key):
-        raise NotImplemented()
+        raise NotImplementedError()
 
     def set(self, key, value):
-        raise NotImplemented()
+        raise NotImplementedError()
 
     def delete(self, key):
-        raise NotImplemented()
+        raise NotImplementedError()
 
     def close(self):
         pass
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/caches/file_cache.py b/pipenv/patched/notpip/_vendor/cachecontrol/caches/file_cache.py
index cbe75851..06f7d09e 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/caches/file_cache.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/caches/file_cache.py
@@ -9,7 +9,7 @@ try:
     FileNotFoundError
 except NameError:
     # py2.X
-    FileNotFoundError = OSError
+    FileNotFoundError = (IOError, OSError)
 
 
 def _secure_open_write(filename, fmode):
@@ -46,6 +46,7 @@ def _secure_open_write(filename, fmode):
     fd = os.open(filename, flags, fmode)
     try:
         return os.fdopen(fd, "wb")
+
     except:
         # An error occurred wrapping our FD in a file object
         os.close(fd)
@@ -53,8 +54,16 @@ def _secure_open_write(filename, fmode):
 
 
 class FileCache(BaseCache):
-    def __init__(self, directory, forever=False, filemode=0o0600,
-                 dirmode=0o0700, use_dir_lock=None, lock_class=None):
+
+    def __init__(
+        self,
+        directory,
+        forever=False,
+        filemode=0o0600,
+        dirmode=0o0700,
+        use_dir_lock=None,
+        lock_class=None,
+    ):
 
         if use_dir_lock is not None and lock_class is not None:
             raise ValueError("Cannot use use_dir_lock and lock_class together")
@@ -63,12 +72,15 @@ class FileCache(BaseCache):
             from pipenv.patched.notpip._vendor.lockfile import LockFile
             from pipenv.patched.notpip._vendor.lockfile.mkdirlockfile import MkdirLockFile
         except ImportError:
-            notice = dedent("""
+            notice = dedent(
+                """
             NOTE: In order to use the FileCache you must have
             lockfile installed. You can install it via pip:
               pip install lockfile
-            """)
+            """
+            )
             raise ImportError(notice)
+
         else:
             if use_dir_lock:
                 lock_class = MkdirLockFile
@@ -95,11 +107,12 @@ class FileCache(BaseCache):
 
     def get(self, key):
         name = self._fn(key)
-        if not os.path.exists(name):
-            return None
+        try:
+            with open(name, "rb") as fh:
+                return fh.read()
 
-        with open(name, 'rb') as fh:
-            return fh.read()
+        except FileNotFoundError:
+            return None
 
     def set(self, key, value):
         name = self._fn(key)
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/caches/redis_cache.py b/pipenv/patched/notpip/_vendor/cachecontrol/caches/redis_cache.py
index c0458856..0cc5e979 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/caches/redis_cache.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/caches/redis_cache.py
@@ -4,16 +4,6 @@ from datetime import datetime
 from pipenv.patched.notpip._vendor.cachecontrol.cache import BaseCache
 
 
-def total_seconds(td):
-    """Python 2.6 compatability"""
-    if hasattr(td, 'total_seconds'):
-        return int(td.total_seconds())
-
-    ms = td.microseconds
-    secs = (td.seconds + td.days * 24 * 3600)
-    return int((ms + secs * 10**6) / 10**6)
-
-
 class RedisCache(BaseCache):
 
     def __init__(self, conn):
@@ -27,7 +17,7 @@ class RedisCache(BaseCache):
             self.conn.set(key, value)
         else:
             expires = expires - datetime.utcnow()
-            self.conn.setex(key, total_seconds(expires), value)
+            self.conn.setex(key, int(expires.total_seconds()), value)
 
     def delete(self, key):
         self.conn.delete(key)
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/controller.py b/pipenv/patched/notpip/_vendor/cachecontrol/controller.py
index 1621813c..0448910f 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/controller.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/controller.py
@@ -30,8 +30,10 @@ def parse_uri(uri):
 class CacheController(object):
     """An interface to see if request should cached or not.
     """
-    def __init__(self, cache=None, cache_etags=True, serializer=None,
-                 status_codes=None):
+
+    def __init__(
+        self, cache=None, cache_etags=True, serializer=None, status_codes=None
+    ):
         self.cache = cache or DictCache()
         self.cache_etags = cache_etags
         self.serializer = serializer or Serializer()
@@ -64,34 +66,35 @@ class CacheController(object):
     def parse_cache_control(self, headers):
         known_directives = {
             # https://tools.ietf.org/html/rfc7234#section-5.2
-            'max-age': (int, True,),
-            'max-stale': (int, False,),
-            'min-fresh': (int, True,),
-            'no-cache': (None, False,),
-            'no-store': (None, False,),
-            'no-transform': (None, False,),
-            'only-if-cached' : (None, False,),
-            'must-revalidate': (None, False,),
-            'public': (None, False,),
-            'private': (None, False,),
-            'proxy-revalidate': (None, False,),
-            's-maxage': (int, True,)
+            "max-age": (int, True),
+            "max-stale": (int, False),
+            "min-fresh": (int, True),
+            "no-cache": (None, False),
+            "no-store": (None, False),
+            "no-transform": (None, False),
+            "only-if-cached": (None, False),
+            "must-revalidate": (None, False),
+            "public": (None, False),
+            "private": (None, False),
+            "proxy-revalidate": (None, False),
+            "s-maxage": (int, True),
         }
 
-        cc_headers = headers.get('cache-control',
-                                 headers.get('Cache-Control', ''))
+        cc_headers = headers.get("cache-control", headers.get("Cache-Control", ""))
 
         retval = {}
 
-        for cc_directive in cc_headers.split(','):
-            parts = cc_directive.split('=', 1)
+        for cc_directive in cc_headers.split(","):
+            if not cc_directive.strip():
+                continue
+
+            parts = cc_directive.split("=", 1)
             directive = parts[0].strip()
 
             try:
                 typ, required = known_directives[directive]
             except KeyError:
-                logger.debug('Ignoring unknown cache-control directive: %s',
-                             directive)
+                logger.debug("Ignoring unknown cache-control directive: %s", directive)
                 continue
 
             if not typ or not required:
@@ -101,11 +104,16 @@ class CacheController(object):
                     retval[directive] = typ(parts[1].strip())
                 except IndexError:
                     if required:
-                        logger.debug('Missing value for cache-control '
-                                     'directive: %s', directive)
+                        logger.debug(
+                            "Missing value for cache-control " "directive: %s",
+                            directive,
+                        )
                 except ValueError:
-                    logger.debug('Invalid value for cache-control directive '
-                                 '%s, must be %s', directive, typ.__name__)
+                    logger.debug(
+                        "Invalid value for cache-control directive " "%s, must be %s",
+                        directive,
+                        typ.__name__,
+                    )
 
         return retval
 
@@ -119,24 +127,24 @@ class CacheController(object):
         cc = self.parse_cache_control(request.headers)
 
         # Bail out if the request insists on fresh data
-        if 'no-cache' in cc:
+        if "no-cache" in cc:
             logger.debug('Request header has "no-cache", cache bypassed')
             return False
 
-        if 'max-age' in cc and cc['max-age'] == 0:
+        if "max-age" in cc and cc["max-age"] == 0:
             logger.debug('Request header has "max_age" as 0, cache bypassed')
             return False
 
         # Request allows serving from the cache, let's see if we find something
         cache_data = self.cache.get(cache_url)
         if cache_data is None:
-            logger.debug('No cache entry available')
+            logger.debug("No cache entry available")
             return False
 
         # Check whether it can be deserialized
         resp = self.serializer.loads(request, cache_data)
         if not resp:
-            logger.warning('Cache entry deserialization failed, entry ignored')
+            logger.warning("Cache entry deserialization failed, entry ignored")
             return False
 
         # If we have a cached 301, return it immediately. We don't
@@ -148,27 +156,27 @@ class CacheController(object):
         # Client can try to refresh the value by repeating the request
         # with cache busting headers as usual (ie no-cache).
         if resp.status == 301:
-            msg = ('Returning cached "301 Moved Permanently" response '
-                   '(ignoring date and etag information)')
+            msg = (
+                'Returning cached "301 Moved Permanently" response '
+                "(ignoring date and etag information)"
+            )
             logger.debug(msg)
             return resp
 
         headers = CaseInsensitiveDict(resp.headers)
-        if not headers or 'date' not in headers:
-            if 'etag' not in headers:
+        if not headers or "date" not in headers:
+            if "etag" not in headers:
                 # Without date or etag, the cached response can never be used
                 # and should be deleted.
-                logger.debug('Purging cached response: no date or etag')
+                logger.debug("Purging cached response: no date or etag")
                 self.cache.delete(cache_url)
-            logger.debug('Ignoring cached response: no date')
+            logger.debug("Ignoring cached response: no date")
             return False
 
         now = time.time()
-        date = calendar.timegm(
-            parsedate_tz(headers['date'])
-        )
+        date = calendar.timegm(parsedate_tz(headers["date"]))
         current_age = max(0, now - date)
-        logger.debug('Current age based on date: %i', current_age)
+        logger.debug("Current age based on date: %i", current_age)
 
         # TODO: There is an assumption that the result will be a
         #       urllib3 response object. This may not be best since we
@@ -180,45 +188,41 @@ class CacheController(object):
         freshness_lifetime = 0
 
         # Check the max-age pragma in the cache control header
-        if 'max-age' in resp_cc:
-            freshness_lifetime = resp_cc['max-age']
-            logger.debug('Freshness lifetime from max-age: %i',
-                         freshness_lifetime)
+        if "max-age" in resp_cc:
+            freshness_lifetime = resp_cc["max-age"]
+            logger.debug("Freshness lifetime from max-age: %i", freshness_lifetime)
 
         # If there isn't a max-age, check for an expires header
-        elif 'expires' in headers:
-            expires = parsedate_tz(headers['expires'])
+        elif "expires" in headers:
+            expires = parsedate_tz(headers["expires"])
             if expires is not None:
                 expire_time = calendar.timegm(expires) - date
                 freshness_lifetime = max(0, expire_time)
-                logger.debug("Freshness lifetime from expires: %i",
-                             freshness_lifetime)
+                logger.debug("Freshness lifetime from expires: %i", freshness_lifetime)
 
         # Determine if we are setting freshness limit in the
         # request. Note, this overrides what was in the response.
-        if 'max-age' in cc:
-            freshness_lifetime = cc['max-age']
-            logger.debug('Freshness lifetime from request max-age: %i',
-                         freshness_lifetime)
+        if "max-age" in cc:
+            freshness_lifetime = cc["max-age"]
+            logger.debug(
+                "Freshness lifetime from request max-age: %i", freshness_lifetime
+            )
 
-        if 'min-fresh' in cc:
-            min_fresh = cc['min-fresh']
+        if "min-fresh" in cc:
+            min_fresh = cc["min-fresh"]
             # adjust our current age by our min fresh
             current_age += min_fresh
-            logger.debug('Adjusted current age from min-fresh: %i',
-                         current_age)
+            logger.debug("Adjusted current age from min-fresh: %i", current_age)
 
         # Return entry if it is fresh enough
         if freshness_lifetime > current_age:
             logger.debug('The response is "fresh", returning cached response')
-            logger.debug('%i > %i', freshness_lifetime, current_age)
+            logger.debug("%i > %i", freshness_lifetime, current_age)
             return resp
 
         # we're not fresh. If we don't have an Etag, clear it out
-        if 'etag' not in headers:
-            logger.debug(
-                'The cached response is "stale" with no etag, purging'
-            )
+        if "etag" not in headers:
+            logger.debug('The cached response is "stale" with no etag, purging')
             self.cache.delete(cache_url)
 
         # return the original handler
@@ -232,16 +236,15 @@ class CacheController(object):
         if resp:
             headers = CaseInsensitiveDict(resp.headers)
 
-            if 'etag' in headers:
-                new_headers['If-None-Match'] = headers['ETag']
+            if "etag" in headers:
+                new_headers["If-None-Match"] = headers["ETag"]
 
-            if 'last-modified' in headers:
-                new_headers['If-Modified-Since'] = headers['Last-Modified']
+            if "last-modified" in headers:
+                new_headers["If-Modified-Since"] = headers["Last-Modified"]
 
         return new_headers
 
-    def cache_response(self, request, response, body=None,
-                       status_codes=None):
+    def cache_response(self, request, response, body=None, status_codes=None):
         """
         Algorithm for caching requests.
 
@@ -252,9 +255,7 @@ class CacheController(object):
         cacheable_status_codes = status_codes or self.cacheable_status_codes
         if response.status not in cacheable_status_codes:
             logger.debug(
-                'Status code %s not in %s',
-                response.status,
-                cacheable_status_codes
+                "Status code %s not in %s", response.status, cacheable_status_codes
             )
             return
 
@@ -264,10 +265,12 @@ class CacheController(object):
         # Content-Length is valid then we can check to see if the body we've
         # been given matches the expected size, and if it doesn't we'll just
         # skip trying to cache it.
-        if (body is not None and
-                "content-length" in response_headers and
-                response_headers["content-length"].isdigit() and
-                int(response_headers["content-length"]) != len(body)):
+        if (
+            body is not None
+            and "content-length" in response_headers
+            and response_headers["content-length"].isdigit()
+            and int(response_headers["content-length"]) != len(body)
+        ):
             return
 
         cc_req = self.parse_cache_control(request.headers)
@@ -278,53 +281,49 @@ class CacheController(object):
 
         # Delete it from the cache if we happen to have it stored there
         no_store = False
-        if 'no-store' in cc:
+        if "no-store" in cc:
             no_store = True
             logger.debug('Response header has "no-store"')
-        if 'no-store' in cc_req:
+        if "no-store" in cc_req:
             no_store = True
             logger.debug('Request header has "no-store"')
         if no_store and self.cache.get(cache_url):
             logger.debug('Purging existing cache entry to honor "no-store"')
             self.cache.delete(cache_url)
+        if no_store:
+            return
 
         # If we've been given an etag, then keep the response
-        if self.cache_etags and 'etag' in response_headers:
-            logger.debug('Caching due to etag')
+        if self.cache_etags and "etag" in response_headers:
+            logger.debug("Caching due to etag")
             self.cache.set(
-                cache_url,
-                self.serializer.dumps(request, response, body=body),
+                cache_url, self.serializer.dumps(request, response, body=body)
             )
 
         # Add to the cache any 301s. We do this before looking that
         # the Date headers.
         elif response.status == 301:
-            logger.debug('Caching permanant redirect')
-            self.cache.set(
-                cache_url,
-                self.serializer.dumps(request, response)
-            )
+            logger.debug("Caching permanant redirect")
+            self.cache.set(cache_url, self.serializer.dumps(request, response))
 
         # Add to the cache if the response headers demand it. If there
         # is no date header then we can't do anything about expiring
         # the cache.
-        elif 'date' in response_headers:
+        elif "date" in response_headers:
             # cache when there is a max-age > 0
-            if 'max-age' in cc and cc['max-age'] > 0:
-                logger.debug('Caching b/c date exists and max-age > 0')
+            if "max-age" in cc and cc["max-age"] > 0:
+                logger.debug("Caching b/c date exists and max-age > 0")
                 self.cache.set(
-                    cache_url,
-                    self.serializer.dumps(request, response, body=body),
+                    cache_url, self.serializer.dumps(request, response, body=body)
                 )
 
             # If the request can expire, it means we should cache it
             # in the meantime.
-            elif 'expires' in response_headers:
-                if response_headers['expires']:
-                    logger.debug('Caching b/c of expires header')
+            elif "expires" in response_headers:
+                if response_headers["expires"]:
+                    logger.debug("Caching b/c of expires header")
                     self.cache.set(
-                        cache_url,
-                        self.serializer.dumps(request, response, body=body),
+                        cache_url, self.serializer.dumps(request, response, body=body)
                     )
 
     def update_cached_response(self, request, response):
@@ -336,10 +335,7 @@ class CacheController(object):
         """
         cache_url = self.cache_url(request.url)
 
-        cached_response = self.serializer.loads(
-            request,
-            self.cache.get(cache_url)
-        )
+        cached_response = self.serializer.loads(request, self.cache.get(cache_url))
 
         if not cached_response:
             # we didn't have a cached response
@@ -352,22 +348,20 @@ class CacheController(object):
         # the cached body invalid. But... just in case, we'll be sure
         # to strip out ones we know that might be problmatic due to
         # typical assumptions.
-        excluded_headers = [
-            "content-length",
-        ]
+        excluded_headers = ["content-length"]
 
         cached_response.headers.update(
-            dict((k, v) for k, v in response.headers.items()
-                 if k.lower() not in excluded_headers)
+            dict(
+                (k, v)
+                for k, v in response.headers.items()
+                if k.lower() not in excluded_headers
+            )
         )
 
         # we want a 200 b/c we have content via the cache
         cached_response.status = 200
 
         # update our cache
-        self.cache.set(
-            cache_url,
-            self.serializer.dumps(request, cached_response),
-        )
+        self.cache.set(cache_url, self.serializer.dumps(request, cached_response))
 
         return cached_response
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/filewrapper.py b/pipenv/patched/notpip/_vendor/cachecontrol/filewrapper.py
index f1e1ce05..30ed4c5a 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/filewrapper.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/filewrapper.py
@@ -27,17 +27,19 @@ class CallbackFileWrapper(object):
         # self.__fp hasn't been set.
         #
         # [0] https://docs.python.org/2/reference/expressions.html#atom-identifiers
-        fp = self.__getattribute__('_CallbackFileWrapper__fp')
+        fp = self.__getattribute__("_CallbackFileWrapper__fp")
         return getattr(fp, name)
 
     def __is_fp_closed(self):
         try:
             return self.__fp.fp is None
+
         except AttributeError:
             pass
 
         try:
             return self.__fp.closed
+
         except AttributeError:
             pass
 
@@ -66,7 +68,7 @@ class CallbackFileWrapper(object):
 
     def _safe_read(self, amt):
         data = self.__fp._safe_read(amt)
-        if amt == 2 and data == b'\r\n':
+        if amt == 2 and data == b"\r\n":
             # urllib executes this read to toss the CRLF at the end
             # of the chunk.
             return data
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/heuristics.py b/pipenv/patched/notpip/_vendor/cachecontrol/heuristics.py
index f182ff02..6c0e9790 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/heuristics.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/heuristics.py
@@ -46,7 +46,7 @@ class BaseHeuristic(object):
             response.headers.update(updated_headers)
             warning_header_value = self.warning(response)
             if warning_header_value is not None:
-                response.headers.update({'Warning': warning_header_value})
+                response.headers.update({"Warning": warning_header_value})
 
         return response
 
@@ -56,15 +56,15 @@ class OneDayCache(BaseHeuristic):
     Cache the response by providing an expires 1 day in the
     future.
     """
+
     def update_headers(self, response):
         headers = {}
 
-        if 'expires' not in response.headers:
-            date = parsedate(response.headers['date'])
-            expires = expire_after(timedelta(days=1),
-                                   date=datetime(*date[:6]))
-            headers['expires'] = datetime_to_header(expires)
-            headers['cache-control'] = 'public'
+        if "expires" not in response.headers:
+            date = parsedate(response.headers["date"])
+            expires = expire_after(timedelta(days=1), date=datetime(*date[:6]))
+            headers["expires"] = datetime_to_header(expires)
+            headers["cache-control"] = "public"
         return headers
 
 
@@ -78,13 +78,10 @@ class ExpiresAfter(BaseHeuristic):
 
     def update_headers(self, response):
         expires = expire_after(self.delta)
-        return {
-            'expires': datetime_to_header(expires),
-            'cache-control': 'public',
-        }
+        return {"expires": datetime_to_header(expires), "cache-control": "public"}
 
     def warning(self, response):
-        tmpl = '110 - Automatically cached for %s. Response might be stale'
+        tmpl = "110 - Automatically cached for %s. Response might be stale"
         return tmpl % self.delta
 
 
@@ -100,27 +97,27 @@ class LastModified(BaseHeuristic):
     http://lxr.mozilla.org/mozilla-release/source/netwerk/protocol/http/nsHttpResponseHead.cpp#397
     Unlike mozilla we limit this to 24-hr.
     """
-    cacheable_by_default_statuses = set([
+    cacheable_by_default_statuses = {
         200, 203, 204, 206, 300, 301, 404, 405, 410, 414, 501
-    ])
+    }
 
     def update_headers(self, resp):
         headers = resp.headers
 
-        if 'expires' in headers:
+        if "expires" in headers:
             return {}
 
-        if 'cache-control' in headers and headers['cache-control'] != 'public':
+        if "cache-control" in headers and headers["cache-control"] != "public":
             return {}
 
         if resp.status not in self.cacheable_by_default_statuses:
             return {}
 
-        if 'date' not in headers or 'last-modified' not in headers:
+        if "date" not in headers or "last-modified" not in headers:
             return {}
 
-        date = calendar.timegm(parsedate_tz(headers['date']))
-        last_modified = parsedate(headers['last-modified'])
+        date = calendar.timegm(parsedate_tz(headers["date"]))
+        last_modified = parsedate(headers["last-modified"])
         if date is None or last_modified is None:
             return {}
 
@@ -132,7 +129,7 @@ class LastModified(BaseHeuristic):
             return {}
 
         expires = date + freshness_lifetime
-        return {'expires': time.strftime(TIME_FMT, time.gmtime(expires))}
+        return {"expires": time.strftime(TIME_FMT, time.gmtime(expires))}
 
     def warning(self, resp):
         return None
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/serialize.py b/pipenv/patched/notpip/_vendor/cachecontrol/serialize.py
index 0f552ba8..e7106c02 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/serialize.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/serialize.py
@@ -48,23 +48,22 @@ class Serializer(object):
             u"response": {
                 u"body": body,
                 u"headers": dict(
-                    (text_type(k), text_type(v))
-                    for k, v in response.headers.items()
+                    (text_type(k), text_type(v)) for k, v in response.headers.items()
                 ),
                 u"status": response.status,
                 u"version": response.version,
                 u"reason": text_type(response.reason),
                 u"strict": response.strict,
                 u"decode_content": response.decode_content,
-            },
+            }
         }
 
         # Construct our vary headers
         data[u"vary"] = {}
         if u"vary" in response_headers:
-            varied_headers = response_headers[u'vary'].split(',')
+            varied_headers = response_headers[u"vary"].split(",")
             for header in varied_headers:
-                header = header.strip()
+                header = text_type(header).strip()
                 header_value = request.headers.get(header, None)
                 if header_value is not None:
                     header_value = text_type(header_value)
@@ -95,7 +94,8 @@ class Serializer(object):
 
         # Dispatch to the actual load method for the given version
         try:
-            return getattr(self, "_loads_v{0}".format(ver))(request, data)
+            return getattr(self, "_loads_v{}".format(ver))(request, data)
+
         except AttributeError:
             # This is a version we don't have a loads function for, so we'll
             # just treat it as a miss and return None
@@ -118,11 +118,11 @@ class Serializer(object):
 
         body_raw = cached["response"].pop("body")
 
-        headers = CaseInsensitiveDict(data=cached['response']['headers'])
-        if headers.get('transfer-encoding', '') == 'chunked':
-            headers.pop('transfer-encoding')
+        headers = CaseInsensitiveDict(data=cached["response"]["headers"])
+        if headers.get("transfer-encoding", "") == "chunked":
+            headers.pop("transfer-encoding")
 
-        cached['response']['headers'] = headers
+        cached["response"]["headers"] = headers
 
         try:
             body = io.BytesIO(body_raw)
@@ -133,13 +133,9 @@ class Serializer(object):
             # fail with:
             #
             #     TypeError: 'str' does not support the buffer interface
-            body = io.BytesIO(body_raw.encode('utf8'))
+            body = io.BytesIO(body_raw.encode("utf8"))
 
-        return HTTPResponse(
-            body=body,
-            preload_content=False,
-            **cached["response"]
-        )
+        return HTTPResponse(body=body, preload_content=False, **cached["response"])
 
     def _loads_v0(self, request, data):
         # The original legacy cache data. This doesn't contain enough
@@ -162,16 +158,12 @@ class Serializer(object):
             return
 
         # We need to decode the items that we've base64 encoded
-        cached["response"]["body"] = _b64_decode_bytes(
-            cached["response"]["body"]
-        )
+        cached["response"]["body"] = _b64_decode_bytes(cached["response"]["body"])
         cached["response"]["headers"] = dict(
             (_b64_decode_str(k), _b64_decode_str(v))
             for k, v in cached["response"]["headers"].items()
         )
-        cached["response"]["reason"] = _b64_decode_str(
-            cached["response"]["reason"],
-        )
+        cached["response"]["reason"] = _b64_decode_str(cached["response"]["reason"])
         cached["vary"] = dict(
             (_b64_decode_str(k), _b64_decode_str(v) if v is not None else v)
             for k, v in cached["vary"].items()
@@ -187,7 +179,7 @@ class Serializer(object):
 
     def _loads_v4(self, request, data):
         try:
-            cached = msgpack.loads(data, encoding='utf-8')
+            cached = msgpack.loads(data, encoding="utf-8")
         except ValueError:
             return
 
diff --git a/pipenv/patched/notpip/_vendor/cachecontrol/wrapper.py b/pipenv/patched/notpip/_vendor/cachecontrol/wrapper.py
index b50a6e27..265bfc8b 100644
--- a/pipenv/patched/notpip/_vendor/cachecontrol/wrapper.py
+++ b/pipenv/patched/notpip/_vendor/cachecontrol/wrapper.py
@@ -2,14 +2,16 @@ from .adapter import CacheControlAdapter
 from .cache import DictCache
 
 
-def CacheControl(sess,
-                 cache=None,
-                 cache_etags=True,
-                 serializer=None,
-                 heuristic=None,
-                 controller_class=None,
-                 adapter_class=None,
-                 cacheable_methods=None):
+def CacheControl(
+    sess,
+    cache=None,
+    cache_etags=True,
+    serializer=None,
+    heuristic=None,
+    controller_class=None,
+    adapter_class=None,
+    cacheable_methods=None,
+):
 
     cache = cache or DictCache()
     adapter_class = adapter_class or CacheControlAdapter
@@ -19,9 +21,9 @@ def CacheControl(sess,
         serializer=serializer,
         heuristic=heuristic,
         controller_class=controller_class,
-        cacheable_methods=cacheable_methods
+        cacheable_methods=cacheable_methods,
     )
-    sess.mount('http://', adapter)
-    sess.mount('https://', adapter)
+    sess.mount("http://", adapter)
+    sess.mount("https://", adapter)
 
     return sess
diff --git a/pipenv/patched/notpip/_vendor/certifi/__init__.py b/pipenv/patched/notpip/_vendor/certifi/__init__.py
index 556193ce..0c4963ef 100644
--- a/pipenv/patched/notpip/_vendor/certifi/__init__.py
+++ b/pipenv/patched/notpip/_vendor/certifi/__init__.py
@@ -1,3 +1,3 @@
 from .core import where, old_where
 
-__version__ = "2018.01.18"
+__version__ = "2018.04.16"
diff --git a/pipenv/patched/notpip/_vendor/certifi/cacert.pem b/pipenv/patched/notpip/_vendor/certifi/cacert.pem
index 101ac98f..2713f541 100644
--- a/pipenv/patched/notpip/_vendor/certifi/cacert.pem
+++ b/pipenv/patched/notpip/_vendor/certifi/cacert.pem
@@ -3483,39 +3483,6 @@ AAoACxGV2lZFA4gKn2fQ1XmxqI1AbQ3CekD6819kR5LLU7m7Wc5P/dAVUwHY3+vZ
 5nbv0CO7O6l5s9UCKc2Jo5YPSjXnTkLAdc0Hz+Ys63su
 -----END CERTIFICATE-----
 
-# Issuer: CN=T\xdcRKTRUST Elektronik Sertifika Hizmet Sa\u011flay\u0131c\u0131s\u0131 H5 O=T\xdcRKTRUST Bilgi \u0130leti\u015fim ve Bili\u015fim G\xfcvenli\u011fi Hizmetleri A.\u015e.
-# Subject: CN=T\xdcRKTRUST Elektronik Sertifika Hizmet Sa\u011flay\u0131c\u0131s\u0131 H5 O=T\xdcRKTRUST Bilgi \u0130leti\u015fim ve Bili\u015fim G\xfcvenli\u011fi Hizmetleri A.\u015e.
-# Label: "T\xdcRKTRUST Elektronik Sertifika Hizmet Sa\u011flay\u0131c\u0131s\u0131 H5"
-# Serial: 156233699172481
-# MD5 Fingerprint: da:70:8e:f0:22:df:93:26:f6:5f:9f:d3:15:06:52:4e
-# SHA1 Fingerprint: c4:18:f6:4d:46:d1:df:00:3d:27:30:13:72:43:a9:12:11:c6:75:fb
-# SHA256 Fingerprint: 49:35:1b:90:34:44:c1:85:cc:dc:5c:69:3d:24:d8:55:5c:b2:08:d6:a8:14:13:07:69:9f:4a:f0:63:19:9d:78
------BEGIN CERTIFICATE-----
-MIIEJzCCAw+gAwIBAgIHAI4X/iQggTANBgkqhkiG9w0BAQsFADCBsTELMAkGA1UE
-BhMCVFIxDzANBgNVBAcMBkFua2FyYTFNMEsGA1UECgxEVMOcUktUUlVTVCBCaWxn
-aSDEsGxldGnFn2ltIHZlIEJpbGnFn2ltIEfDvHZlbmxpxJ9pIEhpem1ldGxlcmkg
-QS7Fni4xQjBABgNVBAMMOVTDnFJLVFJVU1QgRWxla3Ryb25payBTZXJ0aWZpa2Eg
-SGl6bWV0IFNhxJ9sYXnEsWPEsXPEsSBINTAeFw0xMzA0MzAwODA3MDFaFw0yMzA0
-MjgwODA3MDFaMIGxMQswCQYDVQQGEwJUUjEPMA0GA1UEBwwGQW5rYXJhMU0wSwYD
-VQQKDERUw5xSS1RSVVNUIEJpbGdpIMSwbGV0acWfaW0gdmUgQmlsacWfaW0gR8O8
-dmVubGnEn2kgSGl6bWV0bGVyaSBBLsWeLjFCMEAGA1UEAww5VMOcUktUUlVTVCBF
-bGVrdHJvbmlrIFNlcnRpZmlrYSBIaXptZXQgU2HEn2xhecSxY8Sxc8SxIEg1MIIB
-IjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEApCUZ4WWe60ghUEoI5RHwWrom
-/4NZzkQqL/7hzmAD/I0Dpe3/a6i6zDQGn1k19uwsu537jVJp45wnEFPzpALFp/kR
-Gml1bsMdi9GYjZOHp3GXDSHHmflS0yxjXVW86B8BSLlg/kJK9siArs1mep5Fimh3
-4khon6La8eHBEJ/rPCmBp+EyCNSgBbGM+42WAA4+Jd9ThiI7/PS98wl+d+yG6w8z
-5UNP9FR1bSmZLmZaQ9/LXMrI5Tjxfjs1nQ/0xVqhzPMggCTTV+wVunUlm+hkS7M0
-hO8EuPbJbKoCPrZV4jI3X/xml1/N1p7HIL9Nxqw/dV8c7TKcfGkAaZHjIxhT6QID
-AQABo0IwQDAdBgNVHQ4EFgQUVpkHHtOsDGlktAxQR95DLL4gwPswDgYDVR0PAQH/
-BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAJ5FdnsX
-SDLyOIspve6WSk6BGLFRRyDN0GSxDsnZAdkJzsiZ3GglE9Rc8qPoBP5yCccLqh0l
-VX6Wmle3usURehnmp349hQ71+S4pL+f5bFgWV1Al9j4uPqrtd3GqqpmWRgqujuwq
-URawXs3qZwQcWDD1YIq9pr1N5Za0/EKJAWv2cMhQOQwt1WbZyNKzMrcbGW3LM/nf
-peYVhDfwwvJllpKQd/Ct9JDpEXjXk4nAPQu6KfTomZ1yju2dL+6SfaHx/126M2CF
-Yv4HAqGEVka+lgqaE9chTLd8B59OTj+RdPsnnRHM3eaxynFNExc5JsUpISuTKWqW
-+qtB4Uu2NQvAmxU=
------END CERTIFICATE-----
-
 # Issuer: CN=Certinomis - Root CA O=Certinomis OU=0002 433998903
 # Subject: CN=Certinomis - Root CA O=Certinomis OU=0002 433998903
 # Label: "Certinomis - Root CA"
diff --git a/pipenv/patched/notpip/_vendor/distlib/t32.exe b/pipenv/patched/notpip/_vendor/distlib/t32.exe
old mode 100755
new mode 100644
diff --git a/pipenv/patched/notpip/_vendor/distlib/t64.exe b/pipenv/patched/notpip/_vendor/distlib/t64.exe
old mode 100755
new mode 100644
diff --git a/pipenv/patched/notpip/_vendor/distlib/w32.exe b/pipenv/patched/notpip/_vendor/distlib/w32.exe
old mode 100755
new mode 100644
diff --git a/pipenv/patched/notpip/_vendor/distlib/w64.exe b/pipenv/patched/notpip/_vendor/distlib/w64.exe
old mode 100755
new mode 100644
diff --git a/pipenv/patched/notpip/_vendor/distro.py b/pipenv/patched/notpip/_vendor/distro.py
index 39bfce79..aa4defc3 100644
--- a/pipenv/patched/notpip/_vendor/distro.py
+++ b/pipenv/patched/notpip/_vendor/distro.py
@@ -1,4 +1,4 @@
-# Copyright 2015,2016 Nir Cohen
+# Copyright 2015,2016,2017 Nir Cohen
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -23,7 +23,7 @@ functionality. An alternative implementation became necessary because Python
 3.5 deprecated this function, and Python 3.7 is expected to remove it
 altogether. Its predecessor function :py:func:`platform.dist` was already
 deprecated since Python 2.6 and is also expected to be removed in Python 3.7.
-Still, there are many cases in which access to Linux distribution information
+Still, there are many cases in which access to OS distribution information
 is needed. See `Python issue 1322 <https://bugs.python.org/issue1322>`_ for
 more information.
 """
@@ -94,7 +94,7 @@ _DISTRO_RELEASE_IGNORE_BASENAMES = (
 
 def linux_distribution(full_distribution_name=True):
     """
-    Return information about the current Linux distribution as a tuple
+    Return information about the current OS distribution as a tuple
     ``(id_name, version, codename)`` with items as follows:
 
     * ``id_name``:  If *full_distribution_name* is false, the result of
@@ -110,22 +110,22 @@ def linux_distribution(full_distribution_name=True):
 
     The data it returns may not exactly be the same, because it uses more data
     sources than the original function, and that may lead to different data if
-    the Linux distribution is not consistent across multiple data sources it
+    the OS distribution is not consistent across multiple data sources it
     provides (there are indeed such distributions ...).
 
     Another reason for differences is the fact that the :func:`distro.id`
     method normalizes the distro ID string to a reliable machine-readable value
-    for a number of popular Linux distributions.
+    for a number of popular OS distributions.
     """
     return _distro.linux_distribution(full_distribution_name)
 
 
 def id():
     """
-    Return the distro ID of the current Linux distribution, as a
+    Return the distro ID of the current distribution, as a
     machine-readable string.
 
-    For a number of Linux distributions, the returned distro ID value is
+    For a number of OS distributions, the returned distro ID value is
     *reliable*, in the sense that it is documented and that it does not change
     across releases of the distribution.
 
@@ -158,6 +158,9 @@ def id():
     "scientific"    Scientific Linux
     "slackware"     Slackware
     "xenserver"     XenServer
+    "openbsd"       OpenBSD
+    "netbsd"        NetBSD
+    "freebsd"       FreeBSD
     ==============  =========================================
 
     If you have a need to get distros for reliable IDs added into this set,
@@ -187,7 +190,7 @@ def id():
     * a normalization of the ID is performed, based upon
       `normalization tables`_. The purpose of this normalization is to ensure
       that the ID is as reliable as possible, even across incompatible changes
-      in the Linux distributions. A common reason for an incompatible change is
+      in the OS distributions. A common reason for an incompatible change is
       the addition of an os-release file, or the addition of the lsb_release
       command, with ID values that differ from what was previously determined
       from the distro release file name.
@@ -197,7 +200,7 @@ def id():
 
 def name(pretty=False):
     """
-    Return the name of the current Linux distribution, as a human-readable
+    Return the name of the current OS distribution, as a human-readable
     string.
 
     If *pretty* is false, the name is returned without version or codename.
@@ -236,7 +239,7 @@ def name(pretty=False):
 
 def version(pretty=False, best=False):
     """
-    Return the version of the current Linux distribution, as a human-readable
+    Return the version of the current OS distribution, as a human-readable
     string.
 
     If *pretty* is false, the version is returned without codename (e.g.
@@ -280,7 +283,7 @@ def version(pretty=False, best=False):
 
 def version_parts(best=False):
     """
-    Return the version of the current Linux distribution as a tuple
+    Return the version of the current OS distribution as a tuple
     ``(major, minor, build_number)`` with items as follows:
 
     * ``major``:  The result of :func:`distro.major_version`.
@@ -297,7 +300,7 @@ def version_parts(best=False):
 
 def major_version(best=False):
     """
-    Return the major version of the current Linux distribution, as a string,
+    Return the major version of the current OS distribution, as a string,
     if provided.
     Otherwise, the empty string is returned. The major version is the first
     part of the dot-separated version string.
@@ -310,7 +313,7 @@ def major_version(best=False):
 
 def minor_version(best=False):
     """
-    Return the minor version of the current Linux distribution, as a string,
+    Return the minor version of the current OS distribution, as a string,
     if provided.
     Otherwise, the empty string is returned. The minor version is the second
     part of the dot-separated version string.
@@ -323,7 +326,7 @@ def minor_version(best=False):
 
 def build_number(best=False):
     """
-    Return the build number of the current Linux distribution, as a string,
+    Return the build number of the current OS distribution, as a string,
     if provided.
     Otherwise, the empty string is returned. The build number is the third part
     of the dot-separated version string.
@@ -337,7 +340,7 @@ def build_number(best=False):
 def like():
     """
     Return a space-separated list of distro IDs of distributions that are
-    closely related to the current Linux distribution in regards to packaging
+    closely related to the current OS distribution in regards to packaging
     and programming interfaces, for example distributions the current
     distribution is a derivative from.
 
@@ -353,7 +356,7 @@ def like():
 
 def codename():
     """
-    Return the codename for the release of the current Linux distribution,
+    Return the codename for the release of the current OS distribution,
     as a string.
 
     If the distribution does not have a codename, an empty string is returned.
@@ -377,7 +380,7 @@ def codename():
 
 def info(pretty=False, best=False):
     """
-    Return certain machine-readable information items about the current Linux
+    Return certain machine-readable information items about the current OS
     distribution in a dictionary, as shown in the following example:
 
     .. sourcecode:: python
@@ -422,7 +425,7 @@ def info(pretty=False, best=False):
 def os_release_info():
     """
     Return a dictionary containing key-value pairs for the information items
-    from the os-release file data source of the current Linux distribution.
+    from the os-release file data source of the current OS distribution.
 
     See `os-release file`_ for details about these information items.
     """
@@ -432,7 +435,7 @@ def os_release_info():
 def lsb_release_info():
     """
     Return a dictionary containing key-value pairs for the information items
-    from the lsb_release command data source of the current Linux distribution.
+    from the lsb_release command data source of the current OS distribution.
 
     See `lsb_release command output`_ for details about these information
     items.
@@ -443,17 +446,25 @@ def lsb_release_info():
 def distro_release_info():
     """
     Return a dictionary containing key-value pairs for the information items
-    from the distro release file data source of the current Linux distribution.
+    from the distro release file data source of the current OS distribution.
 
     See `distro release file`_ for details about these information items.
     """
     return _distro.distro_release_info()
 
 
+def uname_info():
+    """
+    Return a dictionary containing key-value pairs for the information items
+    from the distro release file data source of the current OS distribution.
+    """
+    return _distro.uname_info()
+
+
 def os_release_attr(attribute):
     """
     Return a single named information item from the os-release file data source
-    of the current Linux distribution.
+    of the current OS distribution.
 
     Parameters:
 
@@ -472,7 +483,7 @@ def os_release_attr(attribute):
 def lsb_release_attr(attribute):
     """
     Return a single named information item from the lsb_release command output
-    data source of the current Linux distribution.
+    data source of the current OS distribution.
 
     Parameters:
 
@@ -492,7 +503,7 @@ def lsb_release_attr(attribute):
 def distro_release_attr(attribute):
     """
     Return a single named information item from the distro release file
-    data source of the current Linux distribution.
+    data source of the current OS distribution.
 
     Parameters:
 
@@ -508,6 +519,23 @@ def distro_release_attr(attribute):
     return _distro.distro_release_attr(attribute)
 
 
+def uname_attr(attribute):
+    """
+    Return a single named information item from the distro release file
+    data source of the current OS distribution.
+
+    Parameters:
+
+    * ``attribute`` (string): Key of the information item.
+
+    Returns:
+
+    * (string): Value of the information item, if the item exists.
+                The empty string, if the item does not exist.
+    """
+    return _distro.uname_attr(attribute)
+
+
 class cached_property(object):
     """A version of @property which caches the value.  On access, it calls the
     underlying function and sets the value in `__dict__` so future accesses
@@ -525,13 +553,13 @@ class cached_property(object):
 
 class LinuxDistribution(object):
     """
-    Provides information about a Linux distribution.
+    Provides information about a OS distribution.
 
     This package creates a private module-global instance of this class with
     default initialization arguments, that is used by the
     `consolidated accessor functions`_ and `single source accessor functions`_.
     By using default initialization arguments, that module-global instance
-    returns data about the current Linux distribution (i.e. the distro this
+    returns data about the current OS distribution (i.e. the distro this
     package runs on).
 
     Normally, it is not necessary to create additional instances of this class.
@@ -544,7 +572,8 @@ class LinuxDistribution(object):
     def __init__(self,
                  include_lsb=True,
                  os_release_file='',
-                 distro_release_file=''):
+                 distro_release_file='',
+                 include_uname=True):
         """
         The initialization method of this class gathers information from the
         available data sources, and stores that in private instance attributes.
@@ -578,6 +607,11 @@ class LinuxDistribution(object):
           distro release file can be found, the data source for the distro
           release file will be empty.
 
+        * ``include_name`` (bool): Controls whether uname command output is
+          included as a data source. If the uname command is not available in
+          the program execution path the data source for the uname command will
+          be empty.
+
         Public instance attributes:
 
         * ``os_release_file`` (string): The path name of the
@@ -591,6 +625,10 @@ class LinuxDistribution(object):
         * ``include_lsb`` (bool): The result of the ``include_lsb`` parameter.
           This controls whether the lsb information will be loaded.
 
+        * ``include_uname`` (bool): The result of the ``include_uname``
+          parameter. This controls whether the uname information will
+          be loaded.
+
         Raises:
 
         * :py:exc:`IOError`: Some I/O issue with an os-release file or distro
@@ -607,6 +645,7 @@ class LinuxDistribution(object):
             os.path.join(_UNIXCONFDIR, _OS_RELEASE_BASENAME)
         self.distro_release_file = distro_release_file or ''  # updated later
         self.include_lsb = include_lsb
+        self.include_uname = include_uname
 
     def __repr__(self):
         """Return repr of all info
@@ -616,14 +655,16 @@ class LinuxDistribution(object):
             "os_release_file={self.os_release_file!r}, " \
             "distro_release_file={self.distro_release_file!r}, " \
             "include_lsb={self.include_lsb!r}, " \
+            "include_uname={self.include_uname!r}, " \
             "_os_release_info={self._os_release_info!r}, " \
             "_lsb_release_info={self._lsb_release_info!r}, " \
-            "_distro_release_info={self._distro_release_info!r})".format(
+            "_distro_release_info={self._distro_release_info!r}, " \
+            "_uname_info={self._uname_info!r})".format(
                 self=self)
 
     def linux_distribution(self, full_distribution_name=True):
         """
-        Return information about the Linux distribution that is compatible
+        Return information about the OS distribution that is compatible
         with Python's :func:`platform.linux_distribution`, supporting a subset
         of its parameters.
 
@@ -636,7 +677,7 @@ class LinuxDistribution(object):
         )
 
     def id(self):
-        """Return the distro ID of the Linux distribution, as a string.
+        """Return the distro ID of the OS distribution, as a string.
 
         For details, see :func:`distro.id`.
         """
@@ -656,22 +697,28 @@ class LinuxDistribution(object):
         if distro_id:
             return normalize(distro_id, NORMALIZED_DISTRO_ID)
 
+        distro_id = self.uname_attr('id')
+        if distro_id:
+            return normalize(distro_id, NORMALIZED_DISTRO_ID)
+
         return ''
 
     def name(self, pretty=False):
         """
-        Return the name of the Linux distribution, as a string.
+        Return the name of the OS distribution, as a string.
 
         For details, see :func:`distro.name`.
         """
         name = self.os_release_attr('name') \
             or self.lsb_release_attr('distributor_id') \
-            or self.distro_release_attr('name')
+            or self.distro_release_attr('name') \
+            or self.uname_attr('name')
         if pretty:
             name = self.os_release_attr('pretty_name') \
                 or self.lsb_release_attr('description')
             if not name:
-                name = self.distro_release_attr('name')
+                name = self.distro_release_attr('name') \
+                       or self.uname_attr('name')
                 version = self.version(pretty=True)
                 if version:
                     name = name + ' ' + version
@@ -679,7 +726,7 @@ class LinuxDistribution(object):
 
     def version(self, pretty=False, best=False):
         """
-        Return the version of the Linux distribution, as a string.
+        Return the version of the OS distribution, as a string.
 
         For details, see :func:`distro.version`.
         """
@@ -690,7 +737,8 @@ class LinuxDistribution(object):
             self._parse_distro_release_content(
                 self.os_release_attr('pretty_name')).get('version_id', ''),
             self._parse_distro_release_content(
-                self.lsb_release_attr('description')).get('version_id', '')
+                self.lsb_release_attr('description')).get('version_id', ''),
+            self.uname_attr('release')
         ]
         version = ''
         if best:
@@ -712,7 +760,7 @@ class LinuxDistribution(object):
 
     def version_parts(self, best=False):
         """
-        Return the version of the Linux distribution, as a tuple of version
+        Return the version of the OS distribution, as a tuple of version
         numbers.
 
         For details, see :func:`distro.version_parts`.
@@ -736,7 +784,7 @@ class LinuxDistribution(object):
 
     def minor_version(self, best=False):
         """
-        Return the minor version number of the Linux distribution.
+        Return the minor version number of the current distribution.
 
         For details, see :func:`distro.minor_version`.
         """
@@ -744,7 +792,7 @@ class LinuxDistribution(object):
 
     def build_number(self, best=False):
         """
-        Return the build number of the Linux distribution.
+        Return the build number of the current distribution.
 
         For details, see :func:`distro.build_number`.
         """
@@ -752,7 +800,7 @@ class LinuxDistribution(object):
 
     def like(self):
         """
-        Return the IDs of distributions that are like the Linux distribution.
+        Return the IDs of distributions that are like the OS distribution.
 
         For details, see :func:`distro.like`.
         """
@@ -760,7 +808,7 @@ class LinuxDistribution(object):
 
     def codename(self):
         """
-        Return the codename of the Linux distribution.
+        Return the codename of the OS distribution.
 
         For details, see :func:`distro.codename`.
         """
@@ -771,7 +819,7 @@ class LinuxDistribution(object):
 
     def info(self, pretty=False, best=False):
         """
-        Return certain machine-readable information about the Linux
+        Return certain machine-readable information about the OS
         distribution.
 
         For details, see :func:`distro.info`.
@@ -791,7 +839,7 @@ class LinuxDistribution(object):
     def os_release_info(self):
         """
         Return a dictionary containing key-value pairs for the information
-        items from the os-release file data source of the Linux distribution.
+        items from the os-release file data source of the OS distribution.
 
         For details, see :func:`distro.os_release_info`.
         """
@@ -800,7 +848,7 @@ class LinuxDistribution(object):
     def lsb_release_info(self):
         """
         Return a dictionary containing key-value pairs for the information
-        items from the lsb_release command data source of the Linux
+        items from the lsb_release command data source of the OS
         distribution.
 
         For details, see :func:`distro.lsb_release_info`.
@@ -810,17 +858,25 @@ class LinuxDistribution(object):
     def distro_release_info(self):
         """
         Return a dictionary containing key-value pairs for the information
-        items from the distro release file data source of the Linux
+        items from the distro release file data source of the OS
         distribution.
 
         For details, see :func:`distro.distro_release_info`.
         """
         return self._distro_release_info
 
+    def uname_info(self):
+        """
+        Return a dictionary containing key-value pairs for the information
+        items from the uname command data source of the OS distribution.
+
+        For details, see :func:`distro.uname_info`.
+        """
+
     def os_release_attr(self, attribute):
         """
         Return a single named information item from the os-release file data
-        source of the Linux distribution.
+        source of the OS distribution.
 
         For details, see :func:`distro.os_release_attr`.
         """
@@ -829,7 +885,7 @@ class LinuxDistribution(object):
     def lsb_release_attr(self, attribute):
         """
         Return a single named information item from the lsb_release command
-        output data source of the Linux distribution.
+        output data source of the OS distribution.
 
         For details, see :func:`distro.lsb_release_attr`.
         """
@@ -838,12 +894,21 @@ class LinuxDistribution(object):
     def distro_release_attr(self, attribute):
         """
         Return a single named information item from the distro release file
-        data source of the Linux distribution.
+        data source of the OS distribution.
 
         For details, see :func:`distro.distro_release_attr`.
         """
         return self._distro_release_info.get(attribute, '')
 
+    def uname_attr(self, attribute):
+        """
+        Return a single named information item from the uname command
+        output data source of the OS distribution.
+
+        For details, see :func:`distro.uname_release_attr`.
+        """
+        return self._uname_info.get(attribute, '')
+
     @cached_property
     def _os_release_info(self):
         """
@@ -960,6 +1025,34 @@ class LinuxDistribution(object):
             props.update({k.replace(' ', '_').lower(): v.strip()})
         return props
 
+    @cached_property
+    def _uname_info(self):
+        with open(os.devnull, 'w') as devnull:
+            try:
+                cmd = ('uname', '-rs')
+                stdout = subprocess.check_output(cmd, stderr=devnull)
+            except OSError:
+                return {}
+        content = stdout.decode(sys.getfilesystemencoding()).splitlines()
+        return self._parse_uname_content(content)
+
+    @staticmethod
+    def _parse_uname_content(lines):
+        props = {}
+        match = re.search(r'^([^\s]+)\s+([\d\.]+)', lines[0].strip())
+        if match:
+            name, version = match.groups()
+
+            # This is to prevent the Linux kernel version from
+            # appearing as the 'best' version on otherwise
+            # identifiable distributions.
+            if name == 'Linux':
+                return {}
+            props['id'] = name.lower()
+            props['name'] = name
+            props['release'] = version
+        return props
+
     @cached_property
     def _distro_release_info(self):
         """
@@ -1082,7 +1175,7 @@ def main():
     logger.setLevel(logging.DEBUG)
     logger.addHandler(logging.StreamHandler(sys.stdout))
 
-    parser = argparse.ArgumentParser(description="Linux distro info tool")
+    parser = argparse.ArgumentParser(description="OS distro info tool")
     parser.add_argument(
         '--json',
         '-j',
diff --git a/pipenv/patched/notpip/_vendor/idna/LICENSE.rst b/pipenv/patched/notpip/_vendor/idna/LICENSE.rst
index 9d38815e..3ee64fba 100644
--- a/pipenv/patched/notpip/_vendor/idna/LICENSE.rst
+++ b/pipenv/patched/notpip/_vendor/idna/LICENSE.rst
@@ -1,7 +1,7 @@
 License
 -------
 
-Copyright (c) 2013-2017, Kim Davies. All rights reserved.
+Copyright (c) 2013-2018, Kim Davies. All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
diff --git a/pipenv/patched/notpip/_vendor/idna/core.py b/pipenv/patched/notpip/_vendor/idna/core.py
index b55b6645..090c2c18 100644
--- a/pipenv/patched/notpip/_vendor/idna/core.py
+++ b/pipenv/patched/notpip/_vendor/idna/core.py
@@ -34,7 +34,11 @@ class InvalidCodepointContext(IDNAError):
 
 
 def _combining_class(cp):
-    return unicodedata.combining(unichr(cp))
+    v = unicodedata.combining(unichr(cp))
+    if v == 0:
+        if not unicodedata.name(unichr(cp)):
+            raise ValueError("Unknown character in unicodedata")
+    return v
 
 def _is_script(cp, script):
     return intranges_contain(ord(cp), idnadata.scripts[script])
@@ -71,7 +75,6 @@ def check_bidi(label, check_ltr=False):
             raise IDNABidiError('Unknown directionality in label {0} at position {1}'.format(repr(label), idx))
         if direction in ['R', 'AL', 'AN']:
             bidi_label = True
-            break
     if not bidi_label and not check_ltr:
         return True
 
@@ -244,8 +247,13 @@ def check_label(label):
         if intranges_contain(cp_value, idnadata.codepoint_classes['PVALID']):
             continue
         elif intranges_contain(cp_value, idnadata.codepoint_classes['CONTEXTJ']):
-            if not valid_contextj(label, pos):
-                raise InvalidCodepointContext('Joiner {0} not allowed at position {1} in {2}'.format(_unot(cp_value), pos+1, repr(label)))
+            try:
+                if not valid_contextj(label, pos):
+                    raise InvalidCodepointContext('Joiner {0} not allowed at position {1} in {2}'.format(
+                        _unot(cp_value), pos+1, repr(label)))
+            except ValueError:
+                raise IDNAError('Unknown codepoint adjacent to joiner {0} at position {1} in {2}'.format(
+                    _unot(cp_value), pos+1, repr(label)))
         elif intranges_contain(cp_value, idnadata.codepoint_classes['CONTEXTO']):
             if not valid_contexto(label, pos):
                 raise InvalidCodepointContext('Codepoint {0} not allowed at position {1} in {2}'.format(_unot(cp_value), pos+1, repr(label)))
@@ -317,10 +325,10 @@ def uts46_remap(domain, std3_rules=True, transitional=False):
             replacement = uts46row[2] if len(uts46row) == 3 else None
             if (status == "V" or
                     (status == "D" and not transitional) or
-                    (status == "3" and std3_rules and replacement is None)):
+                    (status == "3" and not std3_rules and replacement is None)):
                 output += char
             elif replacement is not None and (status == "M" or
-                    (status == "3" and std3_rules) or
+                    (status == "3" and not std3_rules) or
                     (status == "D" and transitional)):
                 output += replacement
             elif status != "I":
@@ -344,15 +352,17 @@ def encode(s, strict=False, uts46=False, std3_rules=False, transitional=False):
         labels = s.split('.')
     else:
         labels = _unicode_dots_re.split(s)
-    while labels and not labels[0]:
-        del labels[0]
-    if not labels:
+    if not labels or labels == ['']:
         raise IDNAError('Empty domain')
     if labels[-1] == '':
         del labels[-1]
         trailing_dot = True
     for label in labels:
-        result.append(alabel(label))
+        s = alabel(label)
+        if s:
+            result.append(s)
+        else:
+            raise IDNAError('Empty label')
     if trailing_dot:
         result.append(b'')
     s = b'.'.join(result)
@@ -373,15 +383,17 @@ def decode(s, strict=False, uts46=False, std3_rules=False):
         labels = _unicode_dots_re.split(s)
     else:
         labels = s.split(u'.')
-    while labels and not labels[0]:
-        del labels[0]
-    if not labels:
+    if not labels or labels == ['']:
         raise IDNAError('Empty domain')
     if not labels[-1]:
         del labels[-1]
         trailing_dot = True
     for label in labels:
-        result.append(ulabel(label))
+        s = ulabel(label)
+        if s:
+            result.append(s)
+        else:
+            raise IDNAError('Empty label')
     if trailing_dot:
         result.append(u'')
     return u'.'.join(result)
diff --git a/pipenv/patched/notpip/_vendor/idna/idnadata.py b/pipenv/patched/notpip/_vendor/idna/idnadata.py
index c48f1b50..17974e23 100644
--- a/pipenv/patched/notpip/_vendor/idna/idnadata.py
+++ b/pipenv/patched/notpip/_vendor/idna/idnadata.py
@@ -1,11 +1,12 @@
 # This file is automatically generated by tools/idna-data
 
-__version__ = "6.3.0"
+__version__ = "10.0.0"
 scripts = {
     'Greek': (
         0x37000000374,
         0x37500000378,
         0x37a0000037e,
+        0x37f00000380,
         0x38400000385,
         0x38600000387,
         0x3880000038b,
@@ -34,7 +35,9 @@ scripts = {
         0x1ff200001ff5,
         0x1ff600001fff,
         0x212600002127,
-        0x101400001018b,
+        0xab650000ab66,
+        0x101400001018f,
+        0x101a0000101a1,
         0x1d2000001d246,
     ),
     'Han': (
@@ -46,12 +49,14 @@ scripts = {
         0x30210000302a,
         0x30380000303c,
         0x340000004db6,
-        0x4e0000009fcd,
+        0x4e0000009feb,
         0xf9000000fa6e,
         0xfa700000fada,
         0x200000002a6d7,
         0x2a7000002b735,
         0x2b7400002b81e,
+        0x2b8200002cea2,
+        0x2ceb00002ebe1,
         0x2f8000002fa1e,
     ),
     'Hebrew': (
@@ -68,7 +73,7 @@ scripts = {
     'Hiragana': (
         0x304100003097,
         0x309d000030a0,
-        0x1b0010001b002,
+        0x1b0010001b11f,
         0x1f2000001f201,
     ),
     'Katakana': (
@@ -88,6 +93,7 @@ joining_types = {
     0x602: 85,
     0x603: 85,
     0x604: 85,
+    0x605: 85,
     0x608: 85,
     0x60b: 85,
     0x620: 68,
@@ -365,7 +371,7 @@ joining_types = {
     0x844: 68,
     0x845: 68,
     0x846: 82,
-    0x847: 68,
+    0x847: 82,
     0x848: 68,
     0x849: 82,
     0x84a: 68,
@@ -373,7 +379,7 @@ joining_types = {
     0x84c: 68,
     0x84d: 68,
     0x84e: 68,
-    0x84f: 82,
+    0x84f: 68,
     0x850: 68,
     0x851: 68,
     0x852: 68,
@@ -383,7 +389,19 @@ joining_types = {
     0x856: 85,
     0x857: 85,
     0x858: 85,
+    0x860: 68,
+    0x861: 85,
+    0x862: 68,
+    0x863: 68,
+    0x864: 68,
+    0x865: 68,
+    0x866: 85,
+    0x867: 82,
+    0x868: 68,
+    0x869: 82,
+    0x86a: 82,
     0x8a0: 68,
+    0x8a1: 68,
     0x8a2: 68,
     0x8a3: 68,
     0x8a4: 68,
@@ -395,6 +413,23 @@ joining_types = {
     0x8aa: 82,
     0x8ab: 82,
     0x8ac: 82,
+    0x8ad: 85,
+    0x8ae: 82,
+    0x8af: 68,
+    0x8b0: 68,
+    0x8b1: 82,
+    0x8b2: 82,
+    0x8b3: 68,
+    0x8b4: 68,
+    0x8b6: 68,
+    0x8b7: 68,
+    0x8b8: 68,
+    0x8b9: 82,
+    0x8ba: 68,
+    0x8bb: 68,
+    0x8bc: 68,
+    0x8bd: 68,
+    0x8e2: 85,
     0x1806: 85,
     0x1807: 68,
     0x180a: 67,
@@ -492,8 +527,8 @@ joining_types = {
     0x1882: 85,
     0x1883: 85,
     0x1884: 85,
-    0x1885: 85,
-    0x1886: 85,
+    0x1885: 84,
+    0x1886: 84,
     0x1887: 68,
     0x1888: 68,
     0x1889: 68,
@@ -531,6 +566,7 @@ joining_types = {
     0x18aa: 68,
     0x200c: 85,
     0x200d: 67,
+    0x202f: 85,
     0x2066: 85,
     0x2067: 85,
     0x2068: 85,
@@ -587,6 +623,141 @@ joining_types = {
     0xa871: 68,
     0xa872: 76,
     0xa873: 85,
+    0x10ac0: 68,
+    0x10ac1: 68,
+    0x10ac2: 68,
+    0x10ac3: 68,
+    0x10ac4: 68,
+    0x10ac5: 82,
+    0x10ac6: 85,
+    0x10ac7: 82,
+    0x10ac8: 85,
+    0x10ac9: 82,
+    0x10aca: 82,
+    0x10acb: 85,
+    0x10acc: 85,
+    0x10acd: 76,
+    0x10ace: 82,
+    0x10acf: 82,
+    0x10ad0: 82,
+    0x10ad1: 82,
+    0x10ad2: 82,
+    0x10ad3: 68,
+    0x10ad4: 68,
+    0x10ad5: 68,
+    0x10ad6: 68,
+    0x10ad7: 76,
+    0x10ad8: 68,
+    0x10ad9: 68,
+    0x10ada: 68,
+    0x10adb: 68,
+    0x10adc: 68,
+    0x10add: 82,
+    0x10ade: 68,
+    0x10adf: 68,
+    0x10ae0: 68,
+    0x10ae1: 82,
+    0x10ae2: 85,
+    0x10ae3: 85,
+    0x10ae4: 82,
+    0x10aeb: 68,
+    0x10aec: 68,
+    0x10aed: 68,
+    0x10aee: 68,
+    0x10aef: 82,
+    0x10b80: 68,
+    0x10b81: 82,
+    0x10b82: 68,
+    0x10b83: 82,
+    0x10b84: 82,
+    0x10b85: 82,
+    0x10b86: 68,
+    0x10b87: 68,
+    0x10b88: 68,
+    0x10b89: 82,
+    0x10b8a: 68,
+    0x10b8b: 68,
+    0x10b8c: 82,
+    0x10b8d: 68,
+    0x10b8e: 82,
+    0x10b8f: 82,
+    0x10b90: 68,
+    0x10b91: 82,
+    0x10ba9: 82,
+    0x10baa: 82,
+    0x10bab: 82,
+    0x10bac: 82,
+    0x10bad: 68,
+    0x10bae: 68,
+    0x10baf: 85,
+    0x1e900: 68,
+    0x1e901: 68,
+    0x1e902: 68,
+    0x1e903: 68,
+    0x1e904: 68,
+    0x1e905: 68,
+    0x1e906: 68,
+    0x1e907: 68,
+    0x1e908: 68,
+    0x1e909: 68,
+    0x1e90a: 68,
+    0x1e90b: 68,
+    0x1e90c: 68,
+    0x1e90d: 68,
+    0x1e90e: 68,
+    0x1e90f: 68,
+    0x1e910: 68,
+    0x1e911: 68,
+    0x1e912: 68,
+    0x1e913: 68,
+    0x1e914: 68,
+    0x1e915: 68,
+    0x1e916: 68,
+    0x1e917: 68,
+    0x1e918: 68,
+    0x1e919: 68,
+    0x1e91a: 68,
+    0x1e91b: 68,
+    0x1e91c: 68,
+    0x1e91d: 68,
+    0x1e91e: 68,
+    0x1e91f: 68,
+    0x1e920: 68,
+    0x1e921: 68,
+    0x1e922: 68,
+    0x1e923: 68,
+    0x1e924: 68,
+    0x1e925: 68,
+    0x1e926: 68,
+    0x1e927: 68,
+    0x1e928: 68,
+    0x1e929: 68,
+    0x1e92a: 68,
+    0x1e92b: 68,
+    0x1e92c: 68,
+    0x1e92d: 68,
+    0x1e92e: 68,
+    0x1e92f: 68,
+    0x1e930: 68,
+    0x1e931: 68,
+    0x1e932: 68,
+    0x1e933: 68,
+    0x1e934: 68,
+    0x1e935: 68,
+    0x1e936: 68,
+    0x1e937: 68,
+    0x1e938: 68,
+    0x1e939: 68,
+    0x1e93a: 68,
+    0x1e93b: 68,
+    0x1e93c: 68,
+    0x1e93d: 68,
+    0x1e93e: 68,
+    0x1e93f: 68,
+    0x1e940: 68,
+    0x1e941: 68,
+    0x1e942: 68,
+    0x1e943: 68,
 }
 codepoint_classes = {
     'PVALID': (
@@ -858,6 +1029,10 @@ codepoint_classes = {
         0x52300000524,
         0x52500000526,
         0x52700000528,
+        0x5290000052a,
+        0x52b0000052c,
+        0x52d0000052e,
+        0x52f00000530,
         0x5590000055a,
         0x56100000587,
         0x591000005be,
@@ -881,15 +1056,14 @@ codepoint_classes = {
         0x7c0000007f6,
         0x8000000082e,
         0x8400000085c,
-        0x8a0000008a1,
-        0x8a2000008ad,
-        0x8e4000008ff,
-        0x90000000958,
+        0x8600000086b,
+        0x8a0000008b5,
+        0x8b6000008be,
+        0x8d4000008e2,
+        0x8e300000958,
         0x96000000964,
         0x96600000970,
-        0x97100000978,
-        0x97900000980,
-        0x98100000984,
+        0x97100000984,
         0x9850000098d,
         0x98f00000991,
         0x993000009a9,
@@ -902,6 +1076,7 @@ codepoint_classes = {
         0x9d7000009d8,
         0x9e0000009e4,
         0x9e6000009f2,
+        0x9fc000009fd,
         0xa0100000a04,
         0xa0500000a0b,
         0xa0f00000a11,
@@ -930,6 +1105,7 @@ codepoint_classes = {
         0xad000000ad1,
         0xae000000ae4,
         0xae600000af0,
+        0xaf900000b00,
         0xb0100000b04,
         0xb0500000b0d,
         0xb0f00000b11,
@@ -960,20 +1136,19 @@ codepoint_classes = {
         0xbd000000bd1,
         0xbd700000bd8,
         0xbe600000bf0,
-        0xc0100000c04,
+        0xc0000000c04,
         0xc0500000c0d,
         0xc0e00000c11,
         0xc1200000c29,
-        0xc2a00000c34,
-        0xc3500000c3a,
+        0xc2a00000c3a,
         0xc3d00000c45,
         0xc4600000c49,
         0xc4a00000c4e,
         0xc5500000c57,
-        0xc5800000c5a,
+        0xc5800000c5b,
         0xc6000000c64,
         0xc6600000c70,
-        0xc8200000c84,
+        0xc8000000c84,
         0xc8500000c8d,
         0xc8e00000c91,
         0xc9200000ca9,
@@ -987,15 +1162,14 @@ codepoint_classes = {
         0xce000000ce4,
         0xce600000cf0,
         0xcf100000cf3,
-        0xd0200000d04,
+        0xd0000000d04,
         0xd0500000d0d,
         0xd0e00000d11,
-        0xd1200000d3b,
-        0xd3d00000d45,
+        0xd1200000d45,
         0xd4600000d49,
         0xd4a00000d4f,
-        0xd5700000d58,
-        0xd6000000d64,
+        0xd5400000d58,
+        0xd5f00000d64,
         0xd6600000d70,
         0xd7a00000d80,
         0xd8200000d84,
@@ -1008,6 +1182,7 @@ codepoint_classes = {
         0xdcf00000dd5,
         0xdd600000dd7,
         0xdd800000de0,
+        0xde600000df0,
         0xdf200000df4,
         0xe0100000e33,
         0xe3400000e3b,
@@ -1082,11 +1257,12 @@ codepoint_classes = {
         0x13180000135b,
         0x135d00001360,
         0x138000001390,
-        0x13a0000013f5,
+        0x13a0000013f6,
         0x14010000166d,
         0x166f00001680,
         0x16810000169b,
         0x16a0000016eb,
+        0x16f1000016f9,
         0x17000000170d,
         0x170e00001715,
         0x172000001735,
@@ -1103,7 +1279,7 @@ codepoint_classes = {
         0x182000001878,
         0x1880000018ab,
         0x18b0000018f6,
-        0x19000000191d,
+        0x19000000191f,
         0x19200000192c,
         0x19300000193c,
         0x19460000196e,
@@ -1117,6 +1293,7 @@ codepoint_classes = {
         0x1a7f00001a8a,
         0x1a9000001a9a,
         0x1aa700001aa8,
+        0x1ab000001abe,
         0x1b0000001b4c,
         0x1b5000001b5a,
         0x1b6b00001b74,
@@ -1125,15 +1302,15 @@ codepoint_classes = {
         0x1c4000001c4a,
         0x1c4d00001c7e,
         0x1cd000001cd3,
-        0x1cd400001cf7,
+        0x1cd400001cfa,
         0x1d0000001d2c,
         0x1d2f00001d30,
         0x1d3b00001d3c,
         0x1d4e00001d4f,
         0x1d6b00001d78,
         0x1d7900001d9b,
-        0x1dc000001de7,
-        0x1dfc00001e00,
+        0x1dc000001dfa,
+        0x1dfb00001e00,
         0x1e0100001e02,
         0x1e0300001e04,
         0x1e0500001e06,
@@ -1367,11 +1544,11 @@ codepoint_classes = {
         0x309d0000309f,
         0x30a1000030fb,
         0x30fc000030ff,
-        0x31050000312e,
+        0x31050000312f,
         0x31a0000031bb,
         0x31f000003200,
         0x340000004db6,
-        0x4e0000009fcd,
+        0x4e0000009feb,
         0xa0000000a48d,
         0xa4d00000a4fe,
         0xa5000000a60d,
@@ -1413,7 +1590,9 @@ codepoint_classes = {
         0xa6930000a694,
         0xa6950000a696,
         0xa6970000a698,
-        0xa69f0000a6e6,
+        0xa6990000a69a,
+        0xa69b0000a69c,
+        0xa69e0000a6e6,
         0xa6f00000a6f2,
         0xa7170000a720,
         0xa7230000a724,
@@ -1463,30 +1642,39 @@ codepoint_classes = {
         0xa7850000a786,
         0xa7870000a789,
         0xa78c0000a78d,
-        0xa78e0000a78f,
+        0xa78e0000a790,
         0xa7910000a792,
-        0xa7930000a794,
+        0xa7930000a796,
+        0xa7970000a798,
+        0xa7990000a79a,
+        0xa79b0000a79c,
+        0xa79d0000a79e,
+        0xa79f0000a7a0,
         0xa7a10000a7a2,
         0xa7a30000a7a4,
         0xa7a50000a7a6,
         0xa7a70000a7a8,
         0xa7a90000a7aa,
+        0xa7b50000a7b6,
+        0xa7b70000a7b8,
+        0xa7f70000a7f8,
         0xa7fa0000a828,
         0xa8400000a874,
-        0xa8800000a8c5,
+        0xa8800000a8c6,
         0xa8d00000a8da,
         0xa8e00000a8f8,
         0xa8fb0000a8fc,
+        0xa8fd0000a8fe,
         0xa9000000a92e,
         0xa9300000a954,
         0xa9800000a9c1,
         0xa9cf0000a9da,
+        0xa9e00000a9ff,
         0xaa000000aa37,
         0xaa400000aa4e,
         0xaa500000aa5a,
         0xaa600000aa77,
-        0xaa7a0000aa7c,
-        0xaa800000aac3,
+        0xaa7a0000aac3,
         0xaadb0000aade,
         0xaae00000aaf0,
         0xaaf20000aaf7,
@@ -1495,6 +1683,8 @@ codepoint_classes = {
         0xab110000ab17,
         0xab200000ab27,
         0xab280000ab2f,
+        0xab300000ab5b,
+        0xab600000ab66,
         0xabc00000abeb,
         0xabec0000abee,
         0xabf00000abfa,
@@ -1507,7 +1697,7 @@ codepoint_classes = {
         0xfa230000fa25,
         0xfa270000fa2a,
         0xfb1e0000fb1f,
-        0xfe200000fe27,
+        0xfe200000fe30,
         0xfe730000fe74,
         0x100000001000c,
         0x1000d00010027,
@@ -1519,20 +1709,32 @@ codepoint_classes = {
         0x101fd000101fe,
         0x102800001029d,
         0x102a0000102d1,
-        0x103000001031f,
-        0x1033000010341,
+        0x102e0000102e1,
+        0x1030000010320,
+        0x1032d00010341,
         0x103420001034a,
+        0x103500001037b,
         0x103800001039e,
         0x103a0000103c4,
         0x103c8000103d0,
         0x104280001049e,
         0x104a0000104aa,
+        0x104d8000104fc,
+        0x1050000010528,
+        0x1053000010564,
+        0x1060000010737,
+        0x1074000010756,
+        0x1076000010768,
         0x1080000010806,
         0x1080800010809,
         0x1080a00010836,
         0x1083700010839,
         0x1083c0001083d,
         0x1083f00010856,
+        0x1086000010877,
+        0x108800001089f,
+        0x108e0000108f3,
+        0x108f4000108f6,
         0x1090000010916,
         0x109200001093a,
         0x10980000109b8,
@@ -1545,31 +1747,137 @@ codepoint_classes = {
         0x10a3800010a3b,
         0x10a3f00010a40,
         0x10a6000010a7d,
+        0x10a8000010a9d,
+        0x10ac000010ac8,
+        0x10ac900010ae7,
         0x10b0000010b36,
         0x10b4000010b56,
         0x10b6000010b73,
+        0x10b8000010b92,
         0x10c0000010c49,
+        0x10cc000010cf3,
         0x1100000011047,
         0x1106600011070,
-        0x11080000110bb,
+        0x1107f000110bb,
         0x110d0000110e9,
         0x110f0000110fa,
         0x1110000011135,
         0x1113600011140,
+        0x1115000011174,
+        0x1117600011177,
         0x11180000111c5,
-        0x111d0000111da,
+        0x111ca000111cd,
+        0x111d0000111db,
+        0x111dc000111dd,
+        0x1120000011212,
+        0x1121300011238,
+        0x1123e0001123f,
+        0x1128000011287,
+        0x1128800011289,
+        0x1128a0001128e,
+        0x1128f0001129e,
+        0x1129f000112a9,
+        0x112b0000112eb,
+        0x112f0000112fa,
+        0x1130000011304,
+        0x113050001130d,
+        0x1130f00011311,
+        0x1131300011329,
+        0x1132a00011331,
+        0x1133200011334,
+        0x113350001133a,
+        0x1133c00011345,
+        0x1134700011349,
+        0x1134b0001134e,
+        0x1135000011351,
+        0x1135700011358,
+        0x1135d00011364,
+        0x113660001136d,
+        0x1137000011375,
+        0x114000001144b,
+        0x114500001145a,
+        0x11480000114c6,
+        0x114c7000114c8,
+        0x114d0000114da,
+        0x11580000115b6,
+        0x115b8000115c1,
+        0x115d8000115de,
+        0x1160000011641,
+        0x1164400011645,
+        0x116500001165a,
         0x11680000116b8,
         0x116c0000116ca,
-        0x120000001236f,
+        0x117000001171a,
+        0x1171d0001172c,
+        0x117300001173a,
+        0x118c0000118ea,
+        0x118ff00011900,
+        0x11a0000011a3f,
+        0x11a4700011a48,
+        0x11a5000011a84,
+        0x11a8600011a9a,
+        0x11ac000011af9,
+        0x11c0000011c09,
+        0x11c0a00011c37,
+        0x11c3800011c41,
+        0x11c5000011c5a,
+        0x11c7200011c90,
+        0x11c9200011ca8,
+        0x11ca900011cb7,
+        0x11d0000011d07,
+        0x11d0800011d0a,
+        0x11d0b00011d37,
+        0x11d3a00011d3b,
+        0x11d3c00011d3e,
+        0x11d3f00011d48,
+        0x11d5000011d5a,
+        0x120000001239a,
+        0x1248000012544,
         0x130000001342f,
+        0x1440000014647,
         0x1680000016a39,
+        0x16a4000016a5f,
+        0x16a6000016a6a,
+        0x16ad000016aee,
+        0x16af000016af5,
+        0x16b0000016b37,
+        0x16b4000016b44,
+        0x16b5000016b5a,
+        0x16b6300016b78,
+        0x16b7d00016b90,
         0x16f0000016f45,
         0x16f5000016f7f,
         0x16f8f00016fa0,
-        0x1b0000001b002,
+        0x16fe000016fe2,
+        0x17000000187ed,
+        0x1880000018af3,
+        0x1b0000001b11f,
+        0x1b1700001b2fc,
+        0x1bc000001bc6b,
+        0x1bc700001bc7d,
+        0x1bc800001bc89,
+        0x1bc900001bc9a,
+        0x1bc9d0001bc9f,
+        0x1da000001da37,
+        0x1da3b0001da6d,
+        0x1da750001da76,
+        0x1da840001da85,
+        0x1da9b0001daa0,
+        0x1daa10001dab0,
+        0x1e0000001e007,
+        0x1e0080001e019,
+        0x1e01b0001e022,
+        0x1e0230001e025,
+        0x1e0260001e02b,
+        0x1e8000001e8c5,
+        0x1e8d00001e8d7,
+        0x1e9220001e94b,
+        0x1e9500001e95a,
         0x200000002a6d7,
         0x2a7000002b735,
         0x2b7400002b81e,
+        0x2b8200002cea2,
+        0x2ceb00002ebe1,
     ),
     'CONTEXTJ': (
         0x200c0000200e,
diff --git a/pipenv/patched/notpip/_vendor/idna/package_data.py b/pipenv/patched/notpip/_vendor/idna/package_data.py
index fc331392..39c192ba 100644
--- a/pipenv/patched/notpip/_vendor/idna/package_data.py
+++ b/pipenv/patched/notpip/_vendor/idna/package_data.py
@@ -1,2 +1,2 @@
-__version__ = '2.6'
+__version__ = '2.7'
 
diff --git a/pipenv/patched/notpip/_vendor/idna/uts46data.py b/pipenv/patched/notpip/_vendor/idna/uts46data.py
index f9b3236f..79731cb9 100644
--- a/pipenv/patched/notpip/_vendor/idna/uts46data.py
+++ b/pipenv/patched/notpip/_vendor/idna/uts46data.py
@@ -4,7 +4,7 @@
 """IDNA Mapping Table from UTS46."""
 
 
-__version__ = "6.3.0"
+__version__ = "10.0.0"
 def _seg_0():
     return [
     (0x0, '3'),
@@ -635,7 +635,8 @@ def _seg_6():
     (0x37A, '3', u' '),
     (0x37B, 'V'),
     (0x37E, '3', u';'),
-    (0x37F, 'X'),
+    (0x37F, 'M', u''),
+    (0x380, 'X'),
     (0x384, '3', u' '),
     (0x385, '3', u' '),
     (0x386, 'M', u''),
@@ -730,11 +731,11 @@ def _seg_6():
     (0x400, 'M', u''),
     (0x401, 'M', u''),
     (0x402, 'M', u''),
-    (0x403, 'M', u''),
     ]
 
 def _seg_7():
     return [
+    (0x403, 'M', u''),
     (0x404, 'M', u''),
     (0x405, 'M', u''),
     (0x406, 'M', u''),
@@ -834,11 +835,11 @@ def _seg_7():
     (0x49B, 'V'),
     (0x49C, 'M', u''),
     (0x49D, 'V'),
-    (0x49E, 'M', u''),
     ]
 
 def _seg_8():
     return [
+    (0x49E, 'M', u''),
     (0x49F, 'V'),
     (0x4A0, 'M', u''),
     (0x4A1, 'V'),
@@ -938,11 +939,11 @@ def _seg_8():
     (0x500, 'M', u''),
     (0x501, 'V'),
     (0x502, 'M', u''),
-    (0x503, 'V'),
     ]
 
 def _seg_9():
     return [
+    (0x503, 'V'),
     (0x504, 'M', u''),
     (0x505, 'V'),
     (0x506, 'M', u''),
@@ -979,7 +980,15 @@ def _seg_9():
     (0x525, 'V'),
     (0x526, 'M', u''),
     (0x527, 'V'),
-    (0x528, 'X'),
+    (0x528, 'M', u''),
+    (0x529, 'V'),
+    (0x52A, 'M', u''),
+    (0x52B, 'V'),
+    (0x52C, 'M', u''),
+    (0x52D, 'V'),
+    (0x52E, 'M', u''),
+    (0x52F, 'V'),
+    (0x530, 'X'),
     (0x531, 'M', u''),
     (0x532, 'M', u''),
     (0x533, 'M', u''),
@@ -1026,7 +1035,7 @@ def _seg_9():
     (0x588, 'X'),
     (0x589, 'V'),
     (0x58B, 'X'),
-    (0x58F, 'V'),
+    (0x58D, 'V'),
     (0x590, 'X'),
     (0x591, 'V'),
     (0x5C8, 'X'),
@@ -1034,6 +1043,10 @@ def _seg_9():
     (0x5EB, 'X'),
     (0x5F0, 'V'),
     (0x5F5, 'X'),
+    ]
+
+def _seg_10():
+    return [
     (0x606, 'V'),
     (0x61C, 'X'),
     (0x61E, 'V'),
@@ -1043,10 +1056,6 @@ def _seg_9():
     (0x678, 'M', u''),
     (0x679, 'V'),
     (0x6DD, 'X'),
-    ]
-
-def _seg_10():
-    return [
     (0x6DE, 'V'),
     (0x70E, 'X'),
     (0x710, 'V'),
@@ -1063,13 +1072,15 @@ def _seg_10():
     (0x85C, 'X'),
     (0x85E, 'V'),
     (0x85F, 'X'),
+    (0x860, 'V'),
+    (0x86B, 'X'),
     (0x8A0, 'V'),
-    (0x8A1, 'X'),
-    (0x8A2, 'V'),
-    (0x8AD, 'X'),
-    (0x8E4, 'V'),
-    (0x8FF, 'X'),
-    (0x900, 'V'),
+    (0x8B5, 'X'),
+    (0x8B6, 'V'),
+    (0x8BE, 'X'),
+    (0x8D4, 'V'),
+    (0x8E2, 'X'),
+    (0x8E3, 'V'),
     (0x958, 'M', u''),
     (0x959, 'M', u''),
     (0x95A, 'M', u''),
@@ -1079,10 +1090,6 @@ def _seg_10():
     (0x95E, 'M', u''),
     (0x95F, 'M', u''),
     (0x960, 'V'),
-    (0x978, 'X'),
-    (0x979, 'V'),
-    (0x980, 'X'),
-    (0x981, 'V'),
     (0x984, 'X'),
     (0x985, 'V'),
     (0x98D, 'X'),
@@ -1111,7 +1118,7 @@ def _seg_10():
     (0x9E0, 'V'),
     (0x9E4, 'X'),
     (0x9E6, 'V'),
-    (0x9FC, 'X'),
+    (0x9FE, 'X'),
     (0xA01, 'V'),
     (0xA04, 'X'),
     (0xA05, 'V'),
@@ -1140,6 +1147,10 @@ def _seg_10():
     (0xA4E, 'X'),
     (0xA51, 'V'),
     (0xA52, 'X'),
+    ]
+
+def _seg_11():
+    return [
     (0xA59, 'M', u''),
     (0xA5A, 'M', u''),
     (0xA5B, 'M', u''),
@@ -1147,10 +1158,6 @@ def _seg_10():
     (0xA5D, 'X'),
     (0xA5E, 'M', u''),
     (0xA5F, 'X'),
-    ]
-
-def _seg_11():
-    return [
     (0xA66, 'V'),
     (0xA76, 'X'),
     (0xA81, 'V'),
@@ -1179,6 +1186,8 @@ def _seg_11():
     (0xAE4, 'X'),
     (0xAE6, 'V'),
     (0xAF2, 'X'),
+    (0xAF9, 'V'),
+    (0xB00, 'X'),
     (0xB01, 'V'),
     (0xB04, 'X'),
     (0xB05, 'V'),
@@ -1240,8 +1249,12 @@ def _seg_11():
     (0xBD8, 'X'),
     (0xBE6, 'V'),
     (0xBFB, 'X'),
-    (0xC01, 'V'),
+    (0xC00, 'V'),
     (0xC04, 'X'),
+    ]
+
+def _seg_12():
+    return [
     (0xC05, 'V'),
     (0xC0D, 'X'),
     (0xC0E, 'V'),
@@ -1249,12 +1262,6 @@ def _seg_11():
     (0xC12, 'V'),
     (0xC29, 'X'),
     (0xC2A, 'V'),
-    (0xC34, 'X'),
-    (0xC35, 'V'),
-    ]
-
-def _seg_12():
-    return [
     (0xC3A, 'X'),
     (0xC3D, 'V'),
     (0xC45, 'X'),
@@ -1265,14 +1272,12 @@ def _seg_12():
     (0xC55, 'V'),
     (0xC57, 'X'),
     (0xC58, 'V'),
-    (0xC5A, 'X'),
+    (0xC5B, 'X'),
     (0xC60, 'V'),
     (0xC64, 'X'),
     (0xC66, 'V'),
     (0xC70, 'X'),
     (0xC78, 'V'),
-    (0xC80, 'X'),
-    (0xC82, 'V'),
     (0xC84, 'X'),
     (0xC85, 'V'),
     (0xC8D, 'X'),
@@ -1300,27 +1305,21 @@ def _seg_12():
     (0xCF0, 'X'),
     (0xCF1, 'V'),
     (0xCF3, 'X'),
-    (0xD02, 'V'),
+    (0xD00, 'V'),
     (0xD04, 'X'),
     (0xD05, 'V'),
     (0xD0D, 'X'),
     (0xD0E, 'V'),
     (0xD11, 'X'),
     (0xD12, 'V'),
-    (0xD3B, 'X'),
-    (0xD3D, 'V'),
     (0xD45, 'X'),
     (0xD46, 'V'),
     (0xD49, 'X'),
     (0xD4A, 'V'),
-    (0xD4F, 'X'),
-    (0xD57, 'V'),
-    (0xD58, 'X'),
-    (0xD60, 'V'),
+    (0xD50, 'X'),
+    (0xD54, 'V'),
     (0xD64, 'X'),
     (0xD66, 'V'),
-    (0xD76, 'X'),
-    (0xD79, 'V'),
     (0xD80, 'X'),
     (0xD82, 'V'),
     (0xD84, 'X'),
@@ -1342,6 +1341,8 @@ def _seg_12():
     (0xDD7, 'X'),
     (0xDD8, 'V'),
     (0xDE0, 'X'),
+    (0xDE6, 'V'),
+    (0xDF0, 'X'),
     (0xDF2, 'V'),
     (0xDF5, 'X'),
     (0xE01, 'V'),
@@ -1354,11 +1355,11 @@ def _seg_12():
     (0xE83, 'X'),
     (0xE84, 'V'),
     (0xE85, 'X'),
-    (0xE87, 'V'),
     ]
 
 def _seg_13():
     return [
+    (0xE87, 'V'),
     (0xE89, 'X'),
     (0xE8A, 'V'),
     (0xE8B, 'X'),
@@ -1458,11 +1459,11 @@ def _seg_13():
     (0x124E, 'X'),
     (0x1250, 'V'),
     (0x1257, 'X'),
-    (0x1258, 'V'),
     ]
 
 def _seg_14():
     return [
+    (0x1258, 'V'),
     (0x1259, 'X'),
     (0x125A, 'V'),
     (0x125E, 'X'),
@@ -1493,13 +1494,20 @@ def _seg_14():
     (0x1380, 'V'),
     (0x139A, 'X'),
     (0x13A0, 'V'),
-    (0x13F5, 'X'),
+    (0x13F6, 'X'),
+    (0x13F8, 'M', u''),
+    (0x13F9, 'M', u''),
+    (0x13FA, 'M', u''),
+    (0x13FB, 'M', u''),
+    (0x13FC, 'M', u''),
+    (0x13FD, 'M', u''),
+    (0x13FE, 'X'),
     (0x1400, 'V'),
     (0x1680, 'X'),
     (0x1681, 'V'),
     (0x169D, 'X'),
     (0x16A0, 'V'),
-    (0x16F1, 'X'),
+    (0x16F9, 'X'),
     (0x1700, 'V'),
     (0x170D, 'X'),
     (0x170E, 'V'),
@@ -1536,7 +1544,7 @@ def _seg_14():
     (0x18B0, 'V'),
     (0x18F6, 'X'),
     (0x1900, 'V'),
-    (0x191D, 'X'),
+    (0x191F, 'X'),
     (0x1920, 'V'),
     (0x192C, 'X'),
     (0x1930, 'V'),
@@ -1555,6 +1563,10 @@ def _seg_14():
     (0x19DB, 'X'),
     (0x19DE, 'V'),
     (0x1A1C, 'X'),
+    ]
+
+def _seg_15():
+    return [
     (0x1A1E, 'V'),
     (0x1A5F, 'X'),
     (0x1A60, 'V'),
@@ -1563,12 +1575,10 @@ def _seg_14():
     (0x1A8A, 'X'),
     (0x1A90, 'V'),
     (0x1A9A, 'X'),
-    ]
-
-def _seg_15():
-    return [
     (0x1AA0, 'V'),
     (0x1AAE, 'X'),
+    (0x1AB0, 'V'),
+    (0x1ABF, 'X'),
     (0x1B00, 'V'),
     (0x1B4C, 'X'),
     (0x1B50, 'V'),
@@ -1580,11 +1590,19 @@ def _seg_15():
     (0x1C3B, 'V'),
     (0x1C4A, 'X'),
     (0x1C4D, 'V'),
-    (0x1C80, 'X'),
+    (0x1C80, 'M', u''),
+    (0x1C81, 'M', u''),
+    (0x1C82, 'M', u''),
+    (0x1C83, 'M', u''),
+    (0x1C84, 'M', u''),
+    (0x1C86, 'M', u''),
+    (0x1C87, 'M', u''),
+    (0x1C88, 'M', u''),
+    (0x1C89, 'X'),
     (0x1CC0, 'V'),
     (0x1CC8, 'X'),
     (0x1CD0, 'V'),
-    (0x1CF7, 'X'),
+    (0x1CFA, 'X'),
     (0x1D00, 'V'),
     (0x1D2C, 'M', u'a'),
     (0x1D2D, 'M', u''),
@@ -1649,6 +1667,10 @@ def _seg_15():
     (0x1D68, 'M', u''),
     (0x1D69, 'M', u''),
     (0x1D6A, 'M', u''),
+    ]
+
+def _seg_16():
+    return [
     (0x1D6B, 'V'),
     (0x1D78, 'M', u''),
     (0x1D79, 'V'),
@@ -1667,10 +1689,6 @@ def _seg_15():
     (0x1DA7, 'M', u''),
     (0x1DA8, 'M', u''),
     (0x1DA9, 'M', u''),
-    ]
-
-def _seg_16():
-    return [
     (0x1DAA, 'M', u''),
     (0x1DAB, 'M', u''),
     (0x1DAC, 'M', u''),
@@ -1694,8 +1712,8 @@ def _seg_16():
     (0x1DBE, 'M', u''),
     (0x1DBF, 'M', u''),
     (0x1DC0, 'V'),
-    (0x1DE7, 'X'),
-    (0x1DFC, 'V'),
+    (0x1DFA, 'X'),
+    (0x1DFB, 'V'),
     (0x1E00, 'M', u''),
     (0x1E01, 'V'),
     (0x1E02, 'M', u''),
@@ -1753,6 +1771,10 @@ def _seg_16():
     (0x1E36, 'M', u''),
     (0x1E37, 'V'),
     (0x1E38, 'M', u''),
+    ]
+
+def _seg_17():
+    return [
     (0x1E39, 'V'),
     (0x1E3A, 'M', u''),
     (0x1E3B, 'V'),
@@ -1771,10 +1793,6 @@ def _seg_16():
     (0x1E48, 'M', u''),
     (0x1E49, 'V'),
     (0x1E4A, 'M', u''),
-    ]
-
-def _seg_17():
-    return [
     (0x1E4B, 'V'),
     (0x1E4C, 'M', u''),
     (0x1E4D, 'V'),
@@ -1857,6 +1875,10 @@ def _seg_17():
     (0x1E9F, 'V'),
     (0x1EA0, 'M', u''),
     (0x1EA1, 'V'),
+    ]
+
+def _seg_18():
+    return [
     (0x1EA2, 'M', u''),
     (0x1EA3, 'V'),
     (0x1EA4, 'M', u''),
@@ -1875,10 +1897,6 @@ def _seg_17():
     (0x1EB1, 'V'),
     (0x1EB2, 'M', u''),
     (0x1EB3, 'V'),
-    ]
-
-def _seg_18():
-    return [
     (0x1EB4, 'M', u''),
     (0x1EB5, 'V'),
     (0x1EB6, 'M', u''),
@@ -1961,6 +1979,10 @@ def _seg_18():
     (0x1F0B, 'M', u''),
     (0x1F0C, 'M', u''),
     (0x1F0D, 'M', u''),
+    ]
+
+def _seg_19():
+    return [
     (0x1F0E, 'M', u''),
     (0x1F0F, 'M', u''),
     (0x1F10, 'V'),
@@ -1979,10 +2001,6 @@ def _seg_18():
     (0x1F2B, 'M', u''),
     (0x1F2C, 'M', u''),
     (0x1F2D, 'M', u''),
-    ]
-
-def _seg_19():
-    return [
     (0x1F2E, 'M', u''),
     (0x1F2F, 'M', u''),
     (0x1F30, 'V'),
@@ -2065,6 +2083,10 @@ def _seg_19():
     (0x1F9A, 'M', u''),
     (0x1F9B, 'M', u''),
     (0x1F9C, 'M', u''),
+    ]
+
+def _seg_20():
+    return [
     (0x1F9D, 'M', u''),
     (0x1F9E, 'M', u''),
     (0x1F9F, 'M', u''),
@@ -2083,10 +2105,6 @@ def _seg_19():
     (0x1FAC, 'M', u''),
     (0x1FAD, 'M', u''),
     (0x1FAE, 'M', u''),
-    ]
-
-def _seg_20():
-    return [
     (0x1FAF, 'M', u''),
     (0x1FB0, 'V'),
     (0x1FB2, 'M', u''),
@@ -2169,6 +2187,10 @@ def _seg_20():
     (0x2024, 'X'),
     (0x2027, 'V'),
     (0x2028, 'X'),
+    ]
+
+def _seg_21():
+    return [
     (0x202F, '3', u' '),
     (0x2030, 'V'),
     (0x2033, 'M', u''),
@@ -2187,10 +2209,6 @@ def _seg_20():
     (0x204A, 'V'),
     (0x2057, 'M', u''),
     (0x2058, 'V'),
-    ]
-
-def _seg_21():
-    return [
     (0x205F, '3', u' '),
     (0x2060, 'I'),
     (0x2061, 'X'),
@@ -2244,7 +2262,7 @@ def _seg_21():
     (0x20A0, 'V'),
     (0x20A8, 'M', u'rs'),
     (0x20A9, 'V'),
-    (0x20BB, 'X'),
+    (0x20C0, 'X'),
     (0x20D0, 'V'),
     (0x20F1, 'X'),
     (0x2100, '3', u'a/c'),
@@ -2273,6 +2291,10 @@ def _seg_21():
     (0x2120, 'M', u'sm'),
     (0x2121, 'M', u'tel'),
     (0x2122, 'M', u'tm'),
+    ]
+
+def _seg_22():
+    return [
     (0x2123, 'V'),
     (0x2124, 'M', u'z'),
     (0x2125, 'V'),
@@ -2291,10 +2313,6 @@ def _seg_21():
     (0x2133, 'M', u'm'),
     (0x2134, 'M', u'o'),
     (0x2135, 'M', u''),
-    ]
-
-def _seg_22():
-    return [
     (0x2136, 'M', u''),
     (0x2137, 'M', u''),
     (0x2138, 'M', u''),
@@ -2363,7 +2381,8 @@ def _seg_22():
     (0x2183, 'X'),
     (0x2184, 'V'),
     (0x2189, 'M', u'03'),
-    (0x218A, 'X'),
+    (0x218A, 'V'),
+    (0x218C, 'X'),
     (0x2190, 'V'),
     (0x222C, 'M', u''),
     (0x222D, 'M', u''),
@@ -2376,10 +2395,12 @@ def _seg_22():
     (0x226E, '3'),
     (0x2270, 'V'),
     (0x2329, 'M', u''),
+    ]
+
+def _seg_23():
+    return [
     (0x232A, 'M', u''),
     (0x232B, 'V'),
-    (0x23F4, 'X'),
-    (0x2400, 'V'),
     (0x2427, 'X'),
     (0x2440, 'V'),
     (0x244B, 'X'),
@@ -2395,10 +2416,6 @@ def _seg_22():
     (0x2469, 'M', u'10'),
     (0x246A, 'M', u'11'),
     (0x246B, 'M', u'12'),
-    ]
-
-def _seg_23():
-    return [
     (0x246C, 'M', u'13'),
     (0x246D, 'M', u'14'),
     (0x246E, 'M', u'15'),
@@ -2482,6 +2499,10 @@ def _seg_23():
     (0x24CF, 'M', u'z'),
     (0x24D0, 'M', u'a'),
     (0x24D1, 'M', u'b'),
+    ]
+
+def _seg_24():
+    return [
     (0x24D2, 'M', u'c'),
     (0x24D3, 'M', u'd'),
     (0x24D4, 'M', u'e'),
@@ -2499,10 +2520,6 @@ def _seg_23():
     (0x24E0, 'M', u'q'),
     (0x24E1, 'M', u'r'),
     (0x24E2, 'M', u's'),
-    ]
-
-def _seg_24():
-    return [
     (0x24E3, 'M', u't'),
     (0x24E4, 'M', u'u'),
     (0x24E5, 'M', u'v'),
@@ -2512,8 +2529,6 @@ def _seg_24():
     (0x24E9, 'M', u'z'),
     (0x24EA, 'M', u'0'),
     (0x24EB, 'V'),
-    (0x2700, 'X'),
-    (0x2701, 'V'),
     (0x2A0C, 'M', u''),
     (0x2A0D, 'V'),
     (0x2A74, '3', u'::='),
@@ -2522,9 +2537,17 @@ def _seg_24():
     (0x2A77, 'V'),
     (0x2ADC, 'M', u''),
     (0x2ADD, 'V'),
-    (0x2B4D, 'X'),
-    (0x2B50, 'V'),
-    (0x2B5A, 'X'),
+    (0x2B74, 'X'),
+    (0x2B76, 'V'),
+    (0x2B96, 'X'),
+    (0x2B98, 'V'),
+    (0x2BBA, 'X'),
+    (0x2BBD, 'V'),
+    (0x2BC9, 'X'),
+    (0x2BCA, 'V'),
+    (0x2BD3, 'X'),
+    (0x2BEC, 'V'),
+    (0x2BF0, 'X'),
     (0x2C00, 'M', u''),
     (0x2C01, 'M', u''),
     (0x2C02, 'M', u''),
@@ -2580,6 +2603,10 @@ def _seg_24():
     (0x2C62, 'M', u''),
     (0x2C63, 'M', u''),
     (0x2C64, 'M', u''),
+    ]
+
+def _seg_25():
+    return [
     (0x2C65, 'V'),
     (0x2C67, 'M', u''),
     (0x2C68, 'V'),
@@ -2603,10 +2630,6 @@ def _seg_24():
     (0x2C80, 'M', u''),
     (0x2C81, 'V'),
     (0x2C82, 'M', u''),
-    ]
-
-def _seg_25():
-    return [
     (0x2C83, 'V'),
     (0x2C84, 'M', u''),
     (0x2C85, 'V'),
@@ -2684,6 +2707,10 @@ def _seg_25():
     (0x2CCD, 'V'),
     (0x2CCE, 'M', u''),
     (0x2CCF, 'V'),
+    ]
+
+def _seg_26():
+    return [
     (0x2CD0, 'M', u''),
     (0x2CD1, 'V'),
     (0x2CD2, 'M', u''),
@@ -2707,10 +2734,6 @@ def _seg_25():
     (0x2CEB, 'M', u''),
     (0x2CEC, 'V'),
     (0x2CED, 'M', u''),
-    ]
-
-def _seg_26():
-    return [
     (0x2CEE, 'V'),
     (0x2CF2, 'M', u''),
     (0x2CF3, 'V'),
@@ -2745,7 +2768,7 @@ def _seg_26():
     (0x2DD8, 'V'),
     (0x2DDF, 'X'),
     (0x2DE0, 'V'),
-    (0x2E3C, 'X'),
+    (0x2E4A, 'X'),
     (0x2E80, 'V'),
     (0x2E9A, 'X'),
     (0x2E9B, 'V'),
@@ -2788,6 +2811,10 @@ def _seg_26():
     (0x2F20, 'M', u''),
     (0x2F21, 'M', u''),
     (0x2F22, 'M', u''),
+    ]
+
+def _seg_27():
+    return [
     (0x2F23, 'M', u''),
     (0x2F24, 'M', u''),
     (0x2F25, 'M', u''),
@@ -2811,10 +2838,6 @@ def _seg_26():
     (0x2F37, 'M', u''),
     (0x2F38, 'M', u''),
     (0x2F39, 'M', u''),
-    ]
-
-def _seg_27():
-    return [
     (0x2F3A, 'M', u''),
     (0x2F3B, 'M', u''),
     (0x2F3C, 'M', u''),
@@ -2892,6 +2915,10 @@ def _seg_27():
     (0x2F84, 'M', u''),
     (0x2F85, 'M', u''),
     (0x2F86, 'M', u''),
+    ]
+
+def _seg_28():
+    return [
     (0x2F87, 'M', u''),
     (0x2F88, 'M', u''),
     (0x2F89, 'M', u''),
@@ -2915,10 +2942,6 @@ def _seg_27():
     (0x2F9B, 'M', u''),
     (0x2F9C, 'M', u''),
     (0x2F9D, 'M', u''),
-    ]
-
-def _seg_28():
-    return [
     (0x2F9E, 'M', u''),
     (0x2F9F, 'M', u''),
     (0x2FA0, 'M', u''),
@@ -2996,9 +3019,13 @@ def _seg_28():
     (0x309F, 'M', u''),
     (0x30A0, 'V'),
     (0x30FF, 'M', u''),
+    ]
+
+def _seg_29():
+    return [
     (0x3100, 'X'),
     (0x3105, 'V'),
-    (0x312E, 'X'),
+    (0x312F, 'X'),
     (0x3131, 'M', u''),
     (0x3132, 'M', u''),
     (0x3133, 'M', u''),
@@ -3019,10 +3046,6 @@ def _seg_28():
     (0x3142, 'M', u''),
     (0x3143, 'M', u''),
     (0x3144, 'M', u''),
-    ]
-
-def _seg_29():
-    return [
     (0x3145, 'M', u''),
     (0x3146, 'M', u''),
     (0x3147, 'M', u''),
@@ -3100,6 +3123,10 @@ def _seg_29():
     (0x318F, 'X'),
     (0x3190, 'V'),
     (0x3192, 'M', u''),
+    ]
+
+def _seg_30():
+    return [
     (0x3193, 'M', u''),
     (0x3194, 'M', u''),
     (0x3195, 'M', u''),
@@ -3123,10 +3150,6 @@ def _seg_29():
     (0x3202, '3', u'()'),
     (0x3203, '3', u'()'),
     (0x3204, '3', u'()'),
-    ]
-
-def _seg_30():
-    return [
     (0x3205, '3', u'()'),
     (0x3206, '3', u'()'),
     (0x3207, '3', u'()'),
@@ -3204,6 +3227,10 @@ def _seg_30():
     (0x3256, 'M', u'26'),
     (0x3257, 'M', u'27'),
     (0x3258, 'M', u'28'),
+    ]
+
+def _seg_31():
+    return [
     (0x3259, 'M', u'29'),
     (0x325A, 'M', u'30'),
     (0x325B, 'M', u'31'),
@@ -3227,10 +3254,6 @@ def _seg_30():
     (0x326D, 'M', u''),
     (0x326E, 'M', u''),
     (0x326F, 'M', u''),
-    ]
-
-def _seg_31():
-    return [
     (0x3270, 'M', u''),
     (0x3271, 'M', u''),
     (0x3272, 'M', u''),
@@ -3308,6 +3331,10 @@ def _seg_31():
     (0x32BA, 'M', u'45'),
     (0x32BB, 'M', u'46'),
     (0x32BC, 'M', u'47'),
+    ]
+
+def _seg_32():
+    return [
     (0x32BD, 'M', u'48'),
     (0x32BE, 'M', u'49'),
     (0x32BF, 'M', u'50'),
@@ -3331,10 +3358,6 @@ def _seg_31():
     (0x32D1, 'M', u''),
     (0x32D2, 'M', u''),
     (0x32D3, 'M', u''),
-    ]
-
-def _seg_32():
-    return [
     (0x32D4, 'M', u''),
     (0x32D5, 'M', u''),
     (0x32D6, 'M', u''),
@@ -3412,6 +3435,10 @@ def _seg_32():
     (0x331E, 'M', u''),
     (0x331F, 'M', u''),
     (0x3320, 'M', u''),
+    ]
+
+def _seg_33():
+    return [
     (0x3321, 'M', u''),
     (0x3322, 'M', u''),
     (0x3323, 'M', u''),
@@ -3435,10 +3462,6 @@ def _seg_32():
     (0x3335, 'M', u''),
     (0x3336, 'M', u''),
     (0x3337, 'M', u''),
-    ]
-
-def _seg_33():
-    return [
     (0x3338, 'M', u''),
     (0x3339, 'M', u''),
     (0x333A, 'M', u''),
@@ -3516,6 +3539,10 @@ def _seg_33():
     (0x3382, 'M', u'a'),
     (0x3383, 'M', u'ma'),
     (0x3384, 'M', u'ka'),
+    ]
+
+def _seg_34():
+    return [
     (0x3385, 'M', u'kb'),
     (0x3386, 'M', u'mb'),
     (0x3387, 'M', u'gb'),
@@ -3539,10 +3566,6 @@ def _seg_33():
     (0x3399, 'M', u'fm'),
     (0x339A, 'M', u'nm'),
     (0x339B, 'M', u'm'),
-    ]
-
-def _seg_34():
-    return [
     (0x339C, 'M', u'mm'),
     (0x339D, 'M', u'cm'),
     (0x339E, 'M', u'km'),
@@ -3620,6 +3643,10 @@ def _seg_34():
     (0x33E6, 'M', u'7'),
     (0x33E7, 'M', u'8'),
     (0x33E8, 'M', u'9'),
+    ]
+
+def _seg_35():
+    return [
     (0x33E9, 'M', u'10'),
     (0x33EA, 'M', u'11'),
     (0x33EB, 'M', u'12'),
@@ -3643,14 +3670,10 @@ def _seg_34():
     (0x33FD, 'M', u'30'),
     (0x33FE, 'M', u'31'),
     (0x33FF, 'M', u'gal'),
-    ]
-
-def _seg_35():
-    return [
     (0x3400, 'V'),
     (0x4DB6, 'X'),
     (0x4DC0, 'V'),
-    (0x9FCD, 'X'),
+    (0x9FEB, 'X'),
     (0xA000, 'V'),
     (0xA48D, 'X'),
     (0xA490, 'V'),
@@ -3724,11 +3747,20 @@ def _seg_35():
     (0xA692, 'M', u''),
     (0xA693, 'V'),
     (0xA694, 'M', u''),
+    ]
+
+def _seg_36():
+    return [
     (0xA695, 'V'),
     (0xA696, 'M', u''),
     (0xA697, 'V'),
-    (0xA698, 'X'),
-    (0xA69F, 'V'),
+    (0xA698, 'M', u''),
+    (0xA699, 'V'),
+    (0xA69A, 'M', u''),
+    (0xA69B, 'V'),
+    (0xA69C, 'M', u''),
+    (0xA69D, 'M', u''),
+    (0xA69E, 'V'),
     (0xA6F8, 'X'),
     (0xA700, 'V'),
     (0xA722, 'M', u''),
@@ -3747,10 +3779,6 @@ def _seg_35():
     (0xA72F, 'V'),
     (0xA732, 'M', u''),
     (0xA733, 'V'),
-    ]
-
-def _seg_36():
-    return [
     (0xA734, 'M', u''),
     (0xA735, 'V'),
     (0xA736, 'M', u''),
@@ -3823,6 +3851,10 @@ def _seg_36():
     (0xA780, 'M', u''),
     (0xA781, 'V'),
     (0xA782, 'M', u''),
+    ]
+
+def _seg_37():
+    return [
     (0xA783, 'V'),
     (0xA784, 'M', u''),
     (0xA785, 'V'),
@@ -3832,12 +3864,20 @@ def _seg_36():
     (0xA78C, 'V'),
     (0xA78D, 'M', u''),
     (0xA78E, 'V'),
-    (0xA78F, 'X'),
     (0xA790, 'M', u''),
     (0xA791, 'V'),
     (0xA792, 'M', u''),
     (0xA793, 'V'),
-    (0xA794, 'X'),
+    (0xA796, 'M', u''),
+    (0xA797, 'V'),
+    (0xA798, 'M', u''),
+    (0xA799, 'V'),
+    (0xA79A, 'M', u''),
+    (0xA79B, 'V'),
+    (0xA79C, 'M', u''),
+    (0xA79D, 'V'),
+    (0xA79E, 'M', u''),
+    (0xA79F, 'V'),
     (0xA7A0, 'M', u''),
     (0xA7A1, 'V'),
     (0xA7A2, 'M', u''),
@@ -3849,12 +3889,22 @@ def _seg_36():
     (0xA7A8, 'M', u''),
     (0xA7A9, 'V'),
     (0xA7AA, 'M', u''),
-    (0xA7AB, 'X'),
+    (0xA7AB, 'M', u''),
+    (0xA7AC, 'M', u''),
+    (0xA7AD, 'M', u''),
+    (0xA7AE, 'M', u''),
+    (0xA7AF, 'X'),
+    (0xA7B0, 'M', u''),
+    (0xA7B1, 'M', u''),
+    (0xA7B2, 'M', u''),
+    (0xA7B3, 'M', u''),
+    (0xA7B4, 'M', u''),
+    (0xA7B5, 'V'),
+    (0xA7B6, 'M', u''),
+    (0xA7B7, 'V'),
+    (0xA7B8, 'X'),
+    (0xA7F7, 'V'),
     (0xA7F8, 'M', u''),
-    ]
-
-def _seg_37():
-    return [
     (0xA7F9, 'M', u''),
     (0xA7FA, 'V'),
     (0xA82C, 'X'),
@@ -3863,11 +3913,11 @@ def _seg_37():
     (0xA840, 'V'),
     (0xA878, 'X'),
     (0xA880, 'V'),
-    (0xA8C5, 'X'),
+    (0xA8C6, 'X'),
     (0xA8CE, 'V'),
     (0xA8DA, 'X'),
     (0xA8E0, 'V'),
-    (0xA8FC, 'X'),
+    (0xA8FE, 'X'),
     (0xA900, 'V'),
     (0xA954, 'X'),
     (0xA95F, 'V'),
@@ -3877,7 +3927,7 @@ def _seg_37():
     (0xA9CF, 'V'),
     (0xA9DA, 'X'),
     (0xA9DE, 'V'),
-    (0xA9E0, 'X'),
+    (0xA9FF, 'X'),
     (0xAA00, 'V'),
     (0xAA37, 'X'),
     (0xAA40, 'V'),
@@ -3885,8 +3935,6 @@ def _seg_37():
     (0xAA50, 'V'),
     (0xAA5A, 'X'),
     (0xAA5C, 'V'),
-    (0xAA7C, 'X'),
-    (0xAA80, 'V'),
     (0xAAC3, 'X'),
     (0xAADB, 'V'),
     (0xAAF7, 'X'),
@@ -3900,6 +3948,97 @@ def _seg_37():
     (0xAB27, 'X'),
     (0xAB28, 'V'),
     (0xAB2F, 'X'),
+    (0xAB30, 'V'),
+    (0xAB5C, 'M', u''),
+    (0xAB5D, 'M', u''),
+    (0xAB5E, 'M', u''),
+    (0xAB5F, 'M', u''),
+    (0xAB60, 'V'),
+    (0xAB66, 'X'),
+    ]
+
+def _seg_38():
+    return [
+    (0xAB70, 'M', u''),
+    (0xAB71, 'M', u''),
+    (0xAB72, 'M', u''),
+    (0xAB73, 'M', u''),
+    (0xAB74, 'M', u''),
+    (0xAB75, 'M', u''),
+    (0xAB76, 'M', u''),
+    (0xAB77, 'M', u''),
+    (0xAB78, 'M', u''),
+    (0xAB79, 'M', u''),
+    (0xAB7A, 'M', u''),
+    (0xAB7B, 'M', u''),
+    (0xAB7C, 'M', u''),
+    (0xAB7D, 'M', u''),
+    (0xAB7E, 'M', u''),
+    (0xAB7F, 'M', u''),
+    (0xAB80, 'M', u''),
+    (0xAB81, 'M', u''),
+    (0xAB82, 'M', u''),
+    (0xAB83, 'M', u''),
+    (0xAB84, 'M', u''),
+    (0xAB85, 'M', u''),
+    (0xAB86, 'M', u''),
+    (0xAB87, 'M', u''),
+    (0xAB88, 'M', u''),
+    (0xAB89, 'M', u''),
+    (0xAB8A, 'M', u''),
+    (0xAB8B, 'M', u''),
+    (0xAB8C, 'M', u''),
+    (0xAB8D, 'M', u''),
+    (0xAB8E, 'M', u''),
+    (0xAB8F, 'M', u''),
+    (0xAB90, 'M', u''),
+    (0xAB91, 'M', u''),
+    (0xAB92, 'M', u''),
+    (0xAB93, 'M', u''),
+    (0xAB94, 'M', u''),
+    (0xAB95, 'M', u''),
+    (0xAB96, 'M', u''),
+    (0xAB97, 'M', u''),
+    (0xAB98, 'M', u''),
+    (0xAB99, 'M', u''),
+    (0xAB9A, 'M', u''),
+    (0xAB9B, 'M', u''),
+    (0xAB9C, 'M', u''),
+    (0xAB9D, 'M', u''),
+    (0xAB9E, 'M', u''),
+    (0xAB9F, 'M', u''),
+    (0xABA0, 'M', u''),
+    (0xABA1, 'M', u''),
+    (0xABA2, 'M', u''),
+    (0xABA3, 'M', u''),
+    (0xABA4, 'M', u''),
+    (0xABA5, 'M', u''),
+    (0xABA6, 'M', u''),
+    (0xABA7, 'M', u''),
+    (0xABA8, 'M', u''),
+    (0xABA9, 'M', u''),
+    (0xABAA, 'M', u''),
+    (0xABAB, 'M', u''),
+    (0xABAC, 'M', u''),
+    (0xABAD, 'M', u''),
+    (0xABAE, 'M', u''),
+    (0xABAF, 'M', u''),
+    (0xABB0, 'M', u''),
+    (0xABB1, 'M', u''),
+    (0xABB2, 'M', u''),
+    (0xABB3, 'M', u''),
+    (0xABB4, 'M', u''),
+    (0xABB5, 'M', u''),
+    (0xABB6, 'M', u''),
+    (0xABB7, 'M', u''),
+    (0xABB8, 'M', u''),
+    (0xABB9, 'M', u''),
+    (0xABBA, 'M', u''),
+    (0xABBB, 'M', u''),
+    (0xABBC, 'M', u''),
+    (0xABBD, 'M', u''),
+    (0xABBE, 'M', u''),
+    (0xABBF, 'M', u''),
     (0xABC0, 'V'),
     (0xABEE, 'X'),
     (0xABF0, 'V'),
@@ -3920,6 +4059,10 @@ def _seg_37():
     (0xF907, 'M', u''),
     (0xF909, 'M', u''),
     (0xF90A, 'M', u''),
+    ]
+
+def _seg_39():
+    return [
     (0xF90B, 'M', u''),
     (0xF90C, 'M', u''),
     (0xF90D, 'M', u''),
@@ -3955,10 +4098,6 @@ def _seg_37():
     (0xF92B, 'M', u''),
     (0xF92C, 'M', u''),
     (0xF92D, 'M', u''),
-    ]
-
-def _seg_38():
-    return [
     (0xF92E, 'M', u''),
     (0xF92F, 'M', u''),
     (0xF930, 'M', u''),
@@ -4024,6 +4163,10 @@ def _seg_38():
     (0xF96C, 'M', u''),
     (0xF96D, 'M', u''),
     (0xF96E, 'M', u''),
+    ]
+
+def _seg_40():
+    return [
     (0xF96F, 'M', u''),
     (0xF970, 'M', u''),
     (0xF971, 'M', u''),
@@ -4059,10 +4202,6 @@ def _seg_38():
     (0xF98F, 'M', u''),
     (0xF990, 'M', u''),
     (0xF991, 'M', u''),
-    ]
-
-def _seg_39():
-    return [
     (0xF992, 'M', u''),
     (0xF993, 'M', u''),
     (0xF994, 'M', u''),
@@ -4128,6 +4267,10 @@ def _seg_39():
     (0xF9D0, 'M', u''),
     (0xF9D1, 'M', u''),
     (0xF9D2, 'M', u''),
+    ]
+
+def _seg_41():
+    return [
     (0xF9D3, 'M', u''),
     (0xF9D4, 'M', u''),
     (0xF9D5, 'M', u''),
@@ -4163,10 +4306,6 @@ def _seg_39():
     (0xF9F3, 'M', u''),
     (0xF9F4, 'M', u''),
     (0xF9F5, 'M', u''),
-    ]
-
-def _seg_40():
-    return [
     (0xF9F6, 'M', u''),
     (0xF9F7, 'M', u''),
     (0xF9F8, 'M', u''),
@@ -4232,6 +4371,10 @@ def _seg_40():
     (0xFA39, 'M', u''),
     (0xFA3A, 'M', u''),
     (0xFA3B, 'M', u''),
+    ]
+
+def _seg_42():
+    return [
     (0xFA3C, 'M', u''),
     (0xFA3D, 'M', u''),
     (0xFA3E, 'M', u''),
@@ -4267,10 +4410,6 @@ def _seg_40():
     (0xFA5C, 'M', u''),
     (0xFA5D, 'M', u''),
     (0xFA5F, 'M', u''),
-    ]
-
-def _seg_41():
-    return [
     (0xFA60, 'M', u''),
     (0xFA61, 'M', u''),
     (0xFA62, 'M', u''),
@@ -4336,6 +4475,10 @@ def _seg_41():
     (0xFA9F, 'M', u''),
     (0xFAA0, 'M', u''),
     (0xFAA1, 'M', u''),
+    ]
+
+def _seg_43():
+    return [
     (0xFAA2, 'M', u''),
     (0xFAA3, 'M', u''),
     (0xFAA4, 'M', u''),
@@ -4371,10 +4514,6 @@ def _seg_41():
     (0xFAC2, 'M', u''),
     (0xFAC3, 'M', u''),
     (0xFAC4, 'M', u''),
-    ]
-
-def _seg_42():
-    return [
     (0xFAC5, 'M', u''),
     (0xFAC6, 'M', u''),
     (0xFAC7, 'M', u''),
@@ -4440,6 +4579,10 @@ def _seg_42():
     (0xFB38, 'M', u''),
     (0xFB39, 'M', u''),
     (0xFB3A, 'M', u''),
+    ]
+
+def _seg_44():
+    return [
     (0xFB3B, 'M', u''),
     (0xFB3C, 'M', u''),
     (0xFB3D, 'X'),
@@ -4475,10 +4618,6 @@ def _seg_42():
     (0xFB7A, 'M', u''),
     (0xFB7E, 'M', u''),
     (0xFB82, 'M', u''),
-    ]
-
-def _seg_43():
-    return [
     (0xFB84, 'M', u''),
     (0xFB86, 'M', u''),
     (0xFB88, 'M', u''),
@@ -4544,6 +4683,10 @@ def _seg_43():
     (0xFC19, 'M', u''),
     (0xFC1A, 'M', u''),
     (0xFC1B, 'M', u''),
+    ]
+
+def _seg_45():
+    return [
     (0xFC1C, 'M', u''),
     (0xFC1D, 'M', u''),
     (0xFC1E, 'M', u''),
@@ -4579,10 +4722,6 @@ def _seg_43():
     (0xFC3C, 'M', u''),
     (0xFC3D, 'M', u''),
     (0xFC3E, 'M', u''),
-    ]
-
-def _seg_44():
-    return [
     (0xFC3F, 'M', u''),
     (0xFC40, 'M', u''),
     (0xFC41, 'M', u''),
@@ -4648,6 +4787,10 @@ def _seg_44():
     (0xFC7D, 'M', u''),
     (0xFC7E, 'M', u''),
     (0xFC7F, 'M', u''),
+    ]
+
+def _seg_46():
+    return [
     (0xFC80, 'M', u''),
     (0xFC81, 'M', u''),
     (0xFC82, 'M', u''),
@@ -4683,10 +4826,6 @@ def _seg_44():
     (0xFCA0, 'M', u''),
     (0xFCA1, 'M', u''),
     (0xFCA2, 'M', u''),
-    ]
-
-def _seg_45():
-    return [
     (0xFCA3, 'M', u''),
     (0xFCA4, 'M', u''),
     (0xFCA5, 'M', u''),
@@ -4752,6 +4891,10 @@ def _seg_45():
     (0xFCE1, 'M', u''),
     (0xFCE2, 'M', u''),
     (0xFCE3, 'M', u''),
+    ]
+
+def _seg_47():
+    return [
     (0xFCE4, 'M', u''),
     (0xFCE5, 'M', u''),
     (0xFCE6, 'M', u''),
@@ -4787,10 +4930,6 @@ def _seg_45():
     (0xFD04, 'M', u''),
     (0xFD05, 'M', u''),
     (0xFD06, 'M', u''),
-    ]
-
-def _seg_46():
-    return [
     (0xFD07, 'M', u''),
     (0xFD08, 'M', u''),
     (0xFD09, 'M', u''),
@@ -4856,6 +4995,10 @@ def _seg_46():
     (0xFD57, 'M', u''),
     (0xFD58, 'M', u''),
     (0xFD5A, 'M', u''),
+    ]
+
+def _seg_48():
+    return [
     (0xFD5B, 'M', u''),
     (0xFD5C, 'M', u''),
     (0xFD5D, 'M', u''),
@@ -4891,10 +5034,6 @@ def _seg_46():
     (0xFD87, 'M', u''),
     (0xFD89, 'M', u''),
     (0xFD8A, 'M', u''),
-    ]
-
-def _seg_47():
-    return [
     (0xFD8B, 'M', u''),
     (0xFD8C, 'M', u''),
     (0xFD8D, 'M', u''),
@@ -4960,6 +5099,10 @@ def _seg_47():
     (0xFDF3, 'M', u''),
     (0xFDF4, 'M', u''),
     (0xFDF5, 'M', u''),
+    ]
+
+def _seg_49():
+    return [
     (0xFDF6, 'M', u''),
     (0xFDF7, 'M', u''),
     (0xFDF8, 'M', u''),
@@ -4981,7 +5124,7 @@ def _seg_47():
     (0xFE18, 'M', u''),
     (0xFE19, 'X'),
     (0xFE20, 'V'),
-    (0xFE27, 'X'),
+    (0xFE30, 'X'),
     (0xFE31, 'M', u''),
     (0xFE32, 'M', u''),
     (0xFE33, '3', u'_'),
@@ -4995,10 +5138,6 @@ def _seg_47():
     (0xFE3C, 'M', u''),
     (0xFE3D, 'M', u''),
     (0xFE3E, 'M', u''),
-    ]
-
-def _seg_48():
-    return [
     (0xFE3F, 'M', u''),
     (0xFE40, 'M', u''),
     (0xFE41, 'M', u''),
@@ -5064,6 +5203,10 @@ def _seg_48():
     (0xFE8F, 'M', u''),
     (0xFE93, 'M', u''),
     (0xFE95, 'M', u''),
+    ]
+
+def _seg_50():
+    return [
     (0xFE99, 'M', u''),
     (0xFE9D, 'M', u''),
     (0xFEA1, 'M', u''),
@@ -5099,10 +5242,6 @@ def _seg_48():
     (0xFF00, 'X'),
     (0xFF01, '3', u'!'),
     (0xFF02, '3', u'"'),
-    ]
-
-def _seg_49():
-    return [
     (0xFF03, '3', u'#'),
     (0xFF04, '3', u'$'),
     (0xFF05, '3', u'%'),
@@ -5168,6 +5307,10 @@ def _seg_49():
     (0xFF41, 'M', u'a'),
     (0xFF42, 'M', u'b'),
     (0xFF43, 'M', u'c'),
+    ]
+
+def _seg_51():
+    return [
     (0xFF44, 'M', u'd'),
     (0xFF45, 'M', u'e'),
     (0xFF46, 'M', u'f'),
@@ -5203,10 +5346,6 @@ def _seg_49():
     (0xFF64, 'M', u''),
     (0xFF65, 'M', u''),
     (0xFF66, 'M', u''),
-    ]
-
-def _seg_50():
-    return [
     (0xFF67, 'M', u''),
     (0xFF68, 'M', u''),
     (0xFF69, 'M', u''),
@@ -5272,6 +5411,10 @@ def _seg_50():
     (0xFFA5, 'M', u''),
     (0xFFA6, 'M', u''),
     (0xFFA7, 'M', u''),
+    ]
+
+def _seg_52():
+    return [
     (0xFFA8, 'M', u''),
     (0xFFA9, 'M', u''),
     (0xFFAA, 'M', u''),
@@ -5307,10 +5450,6 @@ def _seg_50():
     (0xFFCB, 'M', u''),
     (0xFFCC, 'M', u''),
     (0xFFCD, 'M', u''),
-    ]
-
-def _seg_51():
-    return [
     (0xFFCE, 'M', u''),
     (0xFFCF, 'M', u''),
     (0xFFD0, 'X'),
@@ -5360,21 +5499,29 @@ def _seg_51():
     (0x10107, 'V'),
     (0x10134, 'X'),
     (0x10137, 'V'),
-    (0x1018B, 'X'),
+    (0x1018F, 'X'),
     (0x10190, 'V'),
     (0x1019C, 'X'),
+    (0x101A0, 'V'),
+    (0x101A1, 'X'),
     (0x101D0, 'V'),
     (0x101FE, 'X'),
     (0x10280, 'V'),
     (0x1029D, 'X'),
     (0x102A0, 'V'),
     (0x102D1, 'X'),
+    (0x102E0, 'V'),
+    (0x102FC, 'X'),
     (0x10300, 'V'),
-    (0x1031F, 'X'),
-    (0x10320, 'V'),
     (0x10324, 'X'),
-    (0x10330, 'V'),
+    (0x1032D, 'V'),
+    ]
+
+def _seg_53():
+    return [
     (0x1034B, 'X'),
+    (0x10350, 'V'),
+    (0x1037B, 'X'),
     (0x10380, 'V'),
     (0x1039E, 'X'),
     (0x1039F, 'V'),
@@ -5411,10 +5558,6 @@ def _seg_51():
     (0x1041B, 'M', u''),
     (0x1041C, 'M', u''),
     (0x1041D, 'M', u''),
-    ]
-
-def _seg_52():
-    return [
     (0x1041E, 'M', u''),
     (0x1041F, 'M', u''),
     (0x10420, 'M', u''),
@@ -5429,6 +5572,61 @@ def _seg_52():
     (0x1049E, 'X'),
     (0x104A0, 'V'),
     (0x104AA, 'X'),
+    (0x104B0, 'M', u''),
+    (0x104B1, 'M', u''),
+    (0x104B2, 'M', u''),
+    (0x104B3, 'M', u''),
+    (0x104B4, 'M', u''),
+    (0x104B5, 'M', u''),
+    (0x104B6, 'M', u''),
+    (0x104B7, 'M', u''),
+    (0x104B8, 'M', u''),
+    (0x104B9, 'M', u''),
+    (0x104BA, 'M', u''),
+    (0x104BB, 'M', u''),
+    (0x104BC, 'M', u''),
+    (0x104BD, 'M', u''),
+    (0x104BE, 'M', u''),
+    (0x104BF, 'M', u''),
+    (0x104C0, 'M', u''),
+    (0x104C1, 'M', u''),
+    (0x104C2, 'M', u''),
+    (0x104C3, 'M', u''),
+    (0x104C4, 'M', u''),
+    (0x104C5, 'M', u''),
+    (0x104C6, 'M', u''),
+    (0x104C7, 'M', u''),
+    (0x104C8, 'M', u''),
+    (0x104C9, 'M', u''),
+    (0x104CA, 'M', u''),
+    (0x104CB, 'M', u''),
+    (0x104CC, 'M', u''),
+    (0x104CD, 'M', u''),
+    (0x104CE, 'M', u''),
+    (0x104CF, 'M', u''),
+    (0x104D0, 'M', u''),
+    (0x104D1, 'M', u''),
+    (0x104D2, 'M', u''),
+    (0x104D3, 'M', u''),
+    (0x104D4, 'X'),
+    (0x104D8, 'V'),
+    (0x104FC, 'X'),
+    (0x10500, 'V'),
+    (0x10528, 'X'),
+    (0x10530, 'V'),
+    (0x10564, 'X'),
+    (0x1056F, 'V'),
+    (0x10570, 'X'),
+    (0x10600, 'V'),
+    (0x10737, 'X'),
+    ]
+
+def _seg_54():
+    return [
+    (0x10740, 'V'),
+    (0x10756, 'X'),
+    (0x10760, 'V'),
+    (0x10768, 'X'),
     (0x10800, 'V'),
     (0x10806, 'X'),
     (0x10808, 'V'),
@@ -5442,8 +5640,14 @@ def _seg_52():
     (0x1083F, 'V'),
     (0x10856, 'X'),
     (0x10857, 'V'),
-    (0x10860, 'X'),
-    (0x10900, 'V'),
+    (0x1089F, 'X'),
+    (0x108A7, 'V'),
+    (0x108B0, 'X'),
+    (0x108E0, 'V'),
+    (0x108F3, 'X'),
+    (0x108F4, 'V'),
+    (0x108F6, 'X'),
+    (0x108FB, 'V'),
     (0x1091C, 'X'),
     (0x1091F, 'V'),
     (0x1093A, 'X'),
@@ -5451,9 +5655,9 @@ def _seg_52():
     (0x10940, 'X'),
     (0x10980, 'V'),
     (0x109B8, 'X'),
-    (0x109BE, 'V'),
-    (0x109C0, 'X'),
-    (0x10A00, 'V'),
+    (0x109BC, 'V'),
+    (0x109D0, 'X'),
+    (0x109D2, 'V'),
     (0x10A04, 'X'),
     (0x10A05, 'V'),
     (0x10A07, 'X'),
@@ -5470,7 +5674,11 @@ def _seg_52():
     (0x10A50, 'V'),
     (0x10A59, 'X'),
     (0x10A60, 'V'),
-    (0x10A80, 'X'),
+    (0x10AA0, 'X'),
+    (0x10AC0, 'V'),
+    (0x10AE7, 'X'),
+    (0x10AEB, 'V'),
+    (0x10AF7, 'X'),
     (0x10B00, 'V'),
     (0x10B36, 'X'),
     (0x10B39, 'V'),
@@ -5478,16 +5686,80 @@ def _seg_52():
     (0x10B58, 'V'),
     (0x10B73, 'X'),
     (0x10B78, 'V'),
-    (0x10B80, 'X'),
+    (0x10B92, 'X'),
+    (0x10B99, 'V'),
+    (0x10B9D, 'X'),
+    (0x10BA9, 'V'),
+    (0x10BB0, 'X'),
     (0x10C00, 'V'),
     (0x10C49, 'X'),
+    (0x10C80, 'M', u''),
+    (0x10C81, 'M', u''),
+    (0x10C82, 'M', u''),
+    (0x10C83, 'M', u''),
+    (0x10C84, 'M', u''),
+    (0x10C85, 'M', u''),
+    (0x10C86, 'M', u''),
+    (0x10C87, 'M', u''),
+    (0x10C88, 'M', u''),
+    (0x10C89, 'M', u''),
+    (0x10C8A, 'M', u''),
+    (0x10C8B, 'M', u''),
+    (0x10C8C, 'M', u''),
+    (0x10C8D, 'M', u''),
+    (0x10C8E, 'M', u''),
+    (0x10C8F, 'M', u''),
+    (0x10C90, 'M', u''),
+    (0x10C91, 'M', u''),
+    (0x10C92, 'M', u''),
+    (0x10C93, 'M', u''),
+    (0x10C94, 'M', u''),
+    (0x10C95, 'M', u''),
+    (0x10C96, 'M', u''),
+    (0x10C97, 'M', u''),
+    (0x10C98, 'M', u''),
+    (0x10C99, 'M', u''),
+    (0x10C9A, 'M', u''),
+    (0x10C9B, 'M', u''),
+    (0x10C9C, 'M', u''),
+    (0x10C9D, 'M', u''),
+    ]
+
+def _seg_55():
+    return [
+    (0x10C9E, 'M', u''),
+    (0x10C9F, 'M', u''),
+    (0x10CA0, 'M', u''),
+    (0x10CA1, 'M', u''),
+    (0x10CA2, 'M', u''),
+    (0x10CA3, 'M', u''),
+    (0x10CA4, 'M', u''),
+    (0x10CA5, 'M', u''),
+    (0x10CA6, 'M', u''),
+    (0x10CA7, 'M', u''),
+    (0x10CA8, 'M', u''),
+    (0x10CA9, 'M', u''),
+    (0x10CAA, 'M', u''),
+    (0x10CAB, 'M', u''),
+    (0x10CAC, 'M', u''),
+    (0x10CAD, 'M', u''),
+    (0x10CAE, 'M', u''),
+    (0x10CAF, 'M', u''),
+    (0x10CB0, 'M', u''),
+    (0x10CB1, 'M', u''),
+    (0x10CB2, 'M', u''),
+    (0x10CB3, 'X'),
+    (0x10CC0, 'V'),
+    (0x10CF3, 'X'),
+    (0x10CFA, 'V'),
+    (0x10D00, 'X'),
     (0x10E60, 'V'),
     (0x10E7F, 'X'),
     (0x11000, 'V'),
     (0x1104E, 'X'),
     (0x11052, 'V'),
     (0x11070, 'X'),
-    (0x11080, 'V'),
+    (0x1107F, 'V'),
     (0x110BD, 'X'),
     (0x110BE, 'V'),
     (0x110C2, 'X'),
@@ -5499,36 +5771,235 @@ def _seg_52():
     (0x11135, 'X'),
     (0x11136, 'V'),
     (0x11144, 'X'),
+    (0x11150, 'V'),
+    (0x11177, 'X'),
     (0x11180, 'V'),
-    (0x111C9, 'X'),
+    (0x111CE, 'X'),
     (0x111D0, 'V'),
-    (0x111DA, 'X'),
+    (0x111E0, 'X'),
+    (0x111E1, 'V'),
+    (0x111F5, 'X'),
+    (0x11200, 'V'),
+    (0x11212, 'X'),
+    (0x11213, 'V'),
+    (0x1123F, 'X'),
+    (0x11280, 'V'),
+    (0x11287, 'X'),
+    (0x11288, 'V'),
+    (0x11289, 'X'),
+    (0x1128A, 'V'),
+    (0x1128E, 'X'),
+    (0x1128F, 'V'),
+    (0x1129E, 'X'),
+    (0x1129F, 'V'),
+    (0x112AA, 'X'),
+    (0x112B0, 'V'),
+    (0x112EB, 'X'),
+    (0x112F0, 'V'),
+    (0x112FA, 'X'),
+    (0x11300, 'V'),
+    (0x11304, 'X'),
+    (0x11305, 'V'),
+    (0x1130D, 'X'),
+    (0x1130F, 'V'),
+    (0x11311, 'X'),
+    (0x11313, 'V'),
+    (0x11329, 'X'),
+    (0x1132A, 'V'),
+    (0x11331, 'X'),
+    (0x11332, 'V'),
+    (0x11334, 'X'),
+    (0x11335, 'V'),
+    (0x1133A, 'X'),
+    (0x1133C, 'V'),
+    (0x11345, 'X'),
+    (0x11347, 'V'),
+    (0x11349, 'X'),
+    (0x1134B, 'V'),
+    (0x1134E, 'X'),
+    (0x11350, 'V'),
+    (0x11351, 'X'),
+    (0x11357, 'V'),
+    (0x11358, 'X'),
+    (0x1135D, 'V'),
+    (0x11364, 'X'),
+    (0x11366, 'V'),
+    (0x1136D, 'X'),
+    (0x11370, 'V'),
+    (0x11375, 'X'),
+    ]
+
+def _seg_56():
+    return [
+    (0x11400, 'V'),
+    (0x1145A, 'X'),
+    (0x1145B, 'V'),
+    (0x1145C, 'X'),
+    (0x1145D, 'V'),
+    (0x1145E, 'X'),
+    (0x11480, 'V'),
+    (0x114C8, 'X'),
+    (0x114D0, 'V'),
+    (0x114DA, 'X'),
+    (0x11580, 'V'),
+    (0x115B6, 'X'),
+    (0x115B8, 'V'),
+    (0x115DE, 'X'),
+    (0x11600, 'V'),
+    (0x11645, 'X'),
+    (0x11650, 'V'),
+    (0x1165A, 'X'),
+    (0x11660, 'V'),
+    (0x1166D, 'X'),
     (0x11680, 'V'),
     (0x116B8, 'X'),
     (0x116C0, 'V'),
     (0x116CA, 'X'),
+    (0x11700, 'V'),
+    (0x1171A, 'X'),
+    (0x1171D, 'V'),
+    (0x1172C, 'X'),
+    (0x11730, 'V'),
+    (0x11740, 'X'),
+    (0x118A0, 'M', u''),
+    (0x118A1, 'M', u''),
+    (0x118A2, 'M', u''),
+    (0x118A3, 'M', u''),
+    (0x118A4, 'M', u''),
+    (0x118A5, 'M', u''),
+    (0x118A6, 'M', u''),
+    (0x118A7, 'M', u''),
+    (0x118A8, 'M', u''),
+    (0x118A9, 'M', u''),
+    (0x118AA, 'M', u''),
+    (0x118AB, 'M', u''),
+    (0x118AC, 'M', u''),
+    (0x118AD, 'M', u''),
+    (0x118AE, 'M', u''),
+    (0x118AF, 'M', u''),
+    (0x118B0, 'M', u''),
+    (0x118B1, 'M', u''),
+    (0x118B2, 'M', u''),
+    (0x118B3, 'M', u''),
+    (0x118B4, 'M', u''),
+    (0x118B5, 'M', u''),
+    (0x118B6, 'M', u''),
+    (0x118B7, 'M', u''),
+    (0x118B8, 'M', u''),
+    (0x118B9, 'M', u''),
+    (0x118BA, 'M', u''),
+    (0x118BB, 'M', u''),
+    (0x118BC, 'M', u''),
+    (0x118BD, 'M', u''),
+    (0x118BE, 'M', u''),
+    (0x118BF, 'M', u''),
+    (0x118C0, 'V'),
+    (0x118F3, 'X'),
+    (0x118FF, 'V'),
+    (0x11900, 'X'),
+    (0x11A00, 'V'),
+    (0x11A48, 'X'),
+    (0x11A50, 'V'),
+    (0x11A84, 'X'),
+    (0x11A86, 'V'),
+    (0x11A9D, 'X'),
+    (0x11A9E, 'V'),
+    (0x11AA3, 'X'),
+    (0x11AC0, 'V'),
+    (0x11AF9, 'X'),
+    (0x11C00, 'V'),
+    (0x11C09, 'X'),
+    (0x11C0A, 'V'),
+    (0x11C37, 'X'),
+    (0x11C38, 'V'),
+    (0x11C46, 'X'),
+    (0x11C50, 'V'),
+    (0x11C6D, 'X'),
+    (0x11C70, 'V'),
+    (0x11C90, 'X'),
+    (0x11C92, 'V'),
+    (0x11CA8, 'X'),
+    (0x11CA9, 'V'),
+    (0x11CB7, 'X'),
+    (0x11D00, 'V'),
+    (0x11D07, 'X'),
+    (0x11D08, 'V'),
+    (0x11D0A, 'X'),
+    (0x11D0B, 'V'),
+    (0x11D37, 'X'),
+    (0x11D3A, 'V'),
+    (0x11D3B, 'X'),
+    (0x11D3C, 'V'),
+    (0x11D3E, 'X'),
+    ]
+
+def _seg_57():
+    return [
+    (0x11D3F, 'V'),
+    (0x11D48, 'X'),
+    (0x11D50, 'V'),
+    (0x11D5A, 'X'),
     (0x12000, 'V'),
-    (0x1236F, 'X'),
+    (0x1239A, 'X'),
     (0x12400, 'V'),
-    (0x12463, 'X'),
+    (0x1246F, 'X'),
     (0x12470, 'V'),
-    (0x12474, 'X'),
+    (0x12475, 'X'),
+    (0x12480, 'V'),
+    (0x12544, 'X'),
     (0x13000, 'V'),
     (0x1342F, 'X'),
-    ]
-
-def _seg_53():
-    return [
+    (0x14400, 'V'),
+    (0x14647, 'X'),
     (0x16800, 'V'),
     (0x16A39, 'X'),
+    (0x16A40, 'V'),
+    (0x16A5F, 'X'),
+    (0x16A60, 'V'),
+    (0x16A6A, 'X'),
+    (0x16A6E, 'V'),
+    (0x16A70, 'X'),
+    (0x16AD0, 'V'),
+    (0x16AEE, 'X'),
+    (0x16AF0, 'V'),
+    (0x16AF6, 'X'),
+    (0x16B00, 'V'),
+    (0x16B46, 'X'),
+    (0x16B50, 'V'),
+    (0x16B5A, 'X'),
+    (0x16B5B, 'V'),
+    (0x16B62, 'X'),
+    (0x16B63, 'V'),
+    (0x16B78, 'X'),
+    (0x16B7D, 'V'),
+    (0x16B90, 'X'),
     (0x16F00, 'V'),
     (0x16F45, 'X'),
     (0x16F50, 'V'),
     (0x16F7F, 'X'),
     (0x16F8F, 'V'),
     (0x16FA0, 'X'),
+    (0x16FE0, 'V'),
+    (0x16FE2, 'X'),
+    (0x17000, 'V'),
+    (0x187ED, 'X'),
+    (0x18800, 'V'),
+    (0x18AF3, 'X'),
     (0x1B000, 'V'),
-    (0x1B002, 'X'),
+    (0x1B11F, 'X'),
+    (0x1B170, 'V'),
+    (0x1B2FC, 'X'),
+    (0x1BC00, 'V'),
+    (0x1BC6B, 'X'),
+    (0x1BC70, 'V'),
+    (0x1BC7D, 'X'),
+    (0x1BC80, 'V'),
+    (0x1BC89, 'X'),
+    (0x1BC90, 'V'),
+    (0x1BC9A, 'X'),
+    (0x1BC9C, 'V'),
+    (0x1BCA0, 'I'),
+    (0x1BCA4, 'X'),
     (0x1D000, 'V'),
     (0x1D0F6, 'X'),
     (0x1D100, 'V'),
@@ -5551,7 +6022,7 @@ def _seg_53():
     (0x1D1BF, 'M', u''),
     (0x1D1C0, 'M', u''),
     (0x1D1C1, 'V'),
-    (0x1D1DE, 'X'),
+    (0x1D1E9, 'X'),
     (0x1D200, 'V'),
     (0x1D246, 'X'),
     (0x1D300, 'V'),
@@ -5564,6 +6035,10 @@ def _seg_53():
     (0x1D403, 'M', u'd'),
     (0x1D404, 'M', u'e'),
     (0x1D405, 'M', u'f'),
+    ]
+
+def _seg_58():
+    return [
     (0x1D406, 'M', u'g'),
     (0x1D407, 'M', u'h'),
     (0x1D408, 'M', u'i'),
@@ -5619,10 +6094,6 @@ def _seg_53():
     (0x1D43A, 'M', u'g'),
     (0x1D43B, 'M', u'h'),
     (0x1D43C, 'M', u'i'),
-    ]
-
-def _seg_54():
-    return [
     (0x1D43D, 'M', u'j'),
     (0x1D43E, 'M', u'k'),
     (0x1D43F, 'M', u'l'),
@@ -5668,6 +6139,10 @@ def _seg_54():
     (0x1D467, 'M', u'z'),
     (0x1D468, 'M', u'a'),
     (0x1D469, 'M', u'b'),
+    ]
+
+def _seg_59():
+    return [
     (0x1D46A, 'M', u'c'),
     (0x1D46B, 'M', u'd'),
     (0x1D46C, 'M', u'e'),
@@ -5723,10 +6198,6 @@ def _seg_54():
     (0x1D49E, 'M', u'c'),
     (0x1D49F, 'M', u'd'),
     (0x1D4A0, 'X'),
-    ]
-
-def _seg_55():
-    return [
     (0x1D4A2, 'M', u'g'),
     (0x1D4A3, 'X'),
     (0x1D4A5, 'M', u'j'),
@@ -5772,6 +6243,10 @@ def _seg_55():
     (0x1D4CE, 'M', u'y'),
     (0x1D4CF, 'M', u'z'),
     (0x1D4D0, 'M', u'a'),
+    ]
+
+def _seg_60():
+    return [
     (0x1D4D1, 'M', u'b'),
     (0x1D4D2, 'M', u'c'),
     (0x1D4D3, 'M', u'd'),
@@ -5827,10 +6302,6 @@ def _seg_55():
     (0x1D505, 'M', u'b'),
     (0x1D506, 'X'),
     (0x1D507, 'M', u'd'),
-    ]
-
-def _seg_56():
-    return [
     (0x1D508, 'M', u'e'),
     (0x1D509, 'M', u'f'),
     (0x1D50A, 'M', u'g'),
@@ -5876,6 +6347,10 @@ def _seg_56():
     (0x1D533, 'M', u'v'),
     (0x1D534, 'M', u'w'),
     (0x1D535, 'M', u'x'),
+    ]
+
+def _seg_61():
+    return [
     (0x1D536, 'M', u'y'),
     (0x1D537, 'M', u'z'),
     (0x1D538, 'M', u'a'),
@@ -5931,10 +6406,6 @@ def _seg_56():
     (0x1D56C, 'M', u'a'),
     (0x1D56D, 'M', u'b'),
     (0x1D56E, 'M', u'c'),
-    ]
-
-def _seg_57():
-    return [
     (0x1D56F, 'M', u'd'),
     (0x1D570, 'M', u'e'),
     (0x1D571, 'M', u'f'),
@@ -5980,6 +6451,10 @@ def _seg_57():
     (0x1D599, 'M', u't'),
     (0x1D59A, 'M', u'u'),
     (0x1D59B, 'M', u'v'),
+    ]
+
+def _seg_62():
+    return [
     (0x1D59C, 'M', u'w'),
     (0x1D59D, 'M', u'x'),
     (0x1D59E, 'M', u'y'),
@@ -6035,10 +6510,6 @@ def _seg_57():
     (0x1D5D0, 'M', u'w'),
     (0x1D5D1, 'M', u'x'),
     (0x1D5D2, 'M', u'y'),
-    ]
-
-def _seg_58():
-    return [
     (0x1D5D3, 'M', u'z'),
     (0x1D5D4, 'M', u'a'),
     (0x1D5D5, 'M', u'b'),
@@ -6084,6 +6555,10 @@ def _seg_58():
     (0x1D5FD, 'M', u'p'),
     (0x1D5FE, 'M', u'q'),
     (0x1D5FF, 'M', u'r'),
+    ]
+
+def _seg_63():
+    return [
     (0x1D600, 'M', u's'),
     (0x1D601, 'M', u't'),
     (0x1D602, 'M', u'u'),
@@ -6139,10 +6614,6 @@ def _seg_58():
     (0x1D634, 'M', u's'),
     (0x1D635, 'M', u't'),
     (0x1D636, 'M', u'u'),
-    ]
-
-def _seg_59():
-    return [
     (0x1D637, 'M', u'v'),
     (0x1D638, 'M', u'w'),
     (0x1D639, 'M', u'x'),
@@ -6188,6 +6659,10 @@ def _seg_59():
     (0x1D661, 'M', u'l'),
     (0x1D662, 'M', u'm'),
     (0x1D663, 'M', u'n'),
+    ]
+
+def _seg_64():
+    return [
     (0x1D664, 'M', u'o'),
     (0x1D665, 'M', u'p'),
     (0x1D666, 'M', u'q'),
@@ -6243,10 +6718,6 @@ def _seg_59():
     (0x1D698, 'M', u'o'),
     (0x1D699, 'M', u'p'),
     (0x1D69A, 'M', u'q'),
-    ]
-
-def _seg_60():
-    return [
     (0x1D69B, 'M', u'r'),
     (0x1D69C, 'M', u's'),
     (0x1D69D, 'M', u't'),
@@ -6292,6 +6763,10 @@ def _seg_60():
     (0x1D6C6, 'M', u''),
     (0x1D6C7, 'M', u''),
     (0x1D6C8, 'M', u''),
+    ]
+
+def _seg_65():
+    return [
     (0x1D6C9, 'M', u''),
     (0x1D6CA, 'M', u''),
     (0x1D6CB, 'M', u''),
@@ -6347,10 +6822,6 @@ def _seg_60():
     (0x1D6FE, 'M', u''),
     (0x1D6FF, 'M', u''),
     (0x1D700, 'M', u''),
-    ]
-
-def _seg_61():
-    return [
     (0x1D701, 'M', u''),
     (0x1D702, 'M', u''),
     (0x1D703, 'M', u''),
@@ -6396,6 +6867,10 @@ def _seg_61():
     (0x1D72C, 'M', u''),
     (0x1D72D, 'M', u''),
     (0x1D72E, 'M', u''),
+    ]
+
+def _seg_66():
+    return [
     (0x1D72F, 'M', u''),
     (0x1D730, 'M', u''),
     (0x1D731, 'M', u''),
@@ -6451,10 +6926,6 @@ def _seg_61():
     (0x1D764, 'M', u''),
     (0x1D765, 'M', u''),
     (0x1D766, 'M', u''),
-    ]
-
-def _seg_62():
-    return [
     (0x1D767, 'M', u''),
     (0x1D768, 'M', u''),
     (0x1D769, 'M', u''),
@@ -6500,6 +6971,10 @@ def _seg_62():
     (0x1D792, 'M', u''),
     (0x1D793, 'M', u''),
     (0x1D794, 'M', u''),
+    ]
+
+def _seg_67():
+    return [
     (0x1D795, 'M', u''),
     (0x1D796, 'M', u''),
     (0x1D797, 'M', u''),
@@ -6555,10 +7030,6 @@ def _seg_62():
     (0x1D7CA, 'M', u''),
     (0x1D7CC, 'X'),
     (0x1D7CE, 'M', u'0'),
-    ]
-
-def _seg_63():
-    return [
     (0x1D7CF, 'M', u'1'),
     (0x1D7D0, 'M', u'2'),
     (0x1D7D1, 'M', u'3'),
@@ -6604,11 +7075,74 @@ def _seg_63():
     (0x1D7F9, 'M', u'3'),
     (0x1D7FA, 'M', u'4'),
     (0x1D7FB, 'M', u'5'),
+    ]
+
+def _seg_68():
+    return [
     (0x1D7FC, 'M', u'6'),
     (0x1D7FD, 'M', u'7'),
     (0x1D7FE, 'M', u'8'),
     (0x1D7FF, 'M', u'9'),
-    (0x1D800, 'X'),
+    (0x1D800, 'V'),
+    (0x1DA8C, 'X'),
+    (0x1DA9B, 'V'),
+    (0x1DAA0, 'X'),
+    (0x1DAA1, 'V'),
+    (0x1DAB0, 'X'),
+    (0x1E000, 'V'),
+    (0x1E007, 'X'),
+    (0x1E008, 'V'),
+    (0x1E019, 'X'),
+    (0x1E01B, 'V'),
+    (0x1E022, 'X'),
+    (0x1E023, 'V'),
+    (0x1E025, 'X'),
+    (0x1E026, 'V'),
+    (0x1E02B, 'X'),
+    (0x1E800, 'V'),
+    (0x1E8C5, 'X'),
+    (0x1E8C7, 'V'),
+    (0x1E8D7, 'X'),
+    (0x1E900, 'M', u''),
+    (0x1E901, 'M', u''),
+    (0x1E902, 'M', u''),
+    (0x1E903, 'M', u''),
+    (0x1E904, 'M', u''),
+    (0x1E905, 'M', u''),
+    (0x1E906, 'M', u''),
+    (0x1E907, 'M', u''),
+    (0x1E908, 'M', u''),
+    (0x1E909, 'M', u''),
+    (0x1E90A, 'M', u''),
+    (0x1E90B, 'M', u''),
+    (0x1E90C, 'M', u''),
+    (0x1E90D, 'M', u''),
+    (0x1E90E, 'M', u''),
+    (0x1E90F, 'M', u''),
+    (0x1E910, 'M', u''),
+    (0x1E911, 'M', u''),
+    (0x1E912, 'M', u''),
+    (0x1E913, 'M', u''),
+    (0x1E914, 'M', u''),
+    (0x1E915, 'M', u''),
+    (0x1E916, 'M', u''),
+    (0x1E917, 'M', u''),
+    (0x1E918, 'M', u''),
+    (0x1E919, 'M', u''),
+    (0x1E91A, 'M', u''),
+    (0x1E91B, 'M', u''),
+    (0x1E91C, 'M', u''),
+    (0x1E91D, 'M', u''),
+    (0x1E91E, 'M', u''),
+    (0x1E91F, 'M', u''),
+    (0x1E920, 'M', u''),
+    (0x1E921, 'M', u''),
+    (0x1E922, 'V'),
+    (0x1E94B, 'X'),
+    (0x1E950, 'V'),
+    (0x1E95A, 'X'),
+    (0x1E95E, 'V'),
+    (0x1E960, 'X'),
     (0x1EE00, 'M', u''),
     (0x1EE01, 'M', u''),
     (0x1EE02, 'M', u''),
@@ -6645,6 +7179,10 @@ def _seg_63():
     (0x1EE21, 'M', u''),
     (0x1EE22, 'M', u''),
     (0x1EE23, 'X'),
+    ]
+
+def _seg_69():
+    return [
     (0x1EE24, 'M', u''),
     (0x1EE25, 'X'),
     (0x1EE27, 'M', u''),
@@ -6659,10 +7197,6 @@ def _seg_63():
     (0x1EE30, 'M', u''),
     (0x1EE31, 'M', u''),
     (0x1EE32, 'M', u''),
-    ]
-
-def _seg_64():
-    return [
     (0x1EE33, 'X'),
     (0x1EE34, 'M', u''),
     (0x1EE35, 'M', u''),
@@ -6749,6 +7283,10 @@ def _seg_64():
     (0x1EE90, 'M', u''),
     (0x1EE91, 'M', u''),
     (0x1EE92, 'M', u''),
+    ]
+
+def _seg_70():
+    return [
     (0x1EE93, 'M', u''),
     (0x1EE94, 'M', u''),
     (0x1EE95, 'M', u''),
@@ -6763,10 +7301,6 @@ def _seg_64():
     (0x1EEA2, 'M', u''),
     (0x1EEA3, 'M', u''),
     (0x1EEA4, 'X'),
-    ]
-
-def _seg_65():
-    return [
     (0x1EEA5, 'M', u''),
     (0x1EEA6, 'M', u''),
     (0x1EEA7, 'M', u''),
@@ -6800,11 +7334,11 @@ def _seg_65():
     (0x1F0A0, 'V'),
     (0x1F0AF, 'X'),
     (0x1F0B1, 'V'),
-    (0x1F0BF, 'X'),
+    (0x1F0C0, 'X'),
     (0x1F0C1, 'V'),
     (0x1F0D0, 'X'),
     (0x1F0D1, 'V'),
-    (0x1F0E0, 'X'),
+    (0x1F0F6, 'X'),
     (0x1F101, '3', u'0,'),
     (0x1F102, '3', u'1,'),
     (0x1F103, '3', u'2,'),
@@ -6815,7 +7349,8 @@ def _seg_65():
     (0x1F108, '3', u'7,'),
     (0x1F109, '3', u'8,'),
     (0x1F10A, '3', u'9,'),
-    (0x1F10B, 'X'),
+    (0x1F10B, 'V'),
+    (0x1F10D, 'X'),
     (0x1F110, '3', u'(a)'),
     (0x1F111, '3', u'(b)'),
     (0x1F112, '3', u'(c)'),
@@ -6852,6 +7387,10 @@ def _seg_65():
     (0x1F131, 'M', u'b'),
     (0x1F132, 'M', u'c'),
     (0x1F133, 'M', u'd'),
+    ]
+
+def _seg_71():
+    return [
     (0x1F134, 'M', u'e'),
     (0x1F135, 'M', u'f'),
     (0x1F136, 'M', u'g'),
@@ -6867,10 +7406,6 @@ def _seg_65():
     (0x1F140, 'M', u'q'),
     (0x1F141, 'M', u'r'),
     (0x1F142, 'M', u's'),
-    ]
-
-def _seg_66():
-    return [
     (0x1F143, 'M', u't'),
     (0x1F144, 'M', u'u'),
     (0x1F145, 'M', u'v'),
@@ -6891,7 +7426,7 @@ def _seg_66():
     (0x1F170, 'V'),
     (0x1F190, 'M', u'dj'),
     (0x1F191, 'V'),
-    (0x1F19B, 'X'),
+    (0x1F1AD, 'X'),
     (0x1F1E6, 'V'),
     (0x1F200, 'M', u''),
     (0x1F201, 'M', u''),
@@ -6940,7 +7475,8 @@ def _seg_66():
     (0x1F238, 'M', u''),
     (0x1F239, 'M', u''),
     (0x1F23A, 'M', u''),
-    (0x1F23B, 'X'),
+    (0x1F23B, 'M', u''),
+    (0x1F23C, 'X'),
     (0x1F240, 'M', u''),
     (0x1F241, 'M', u''),
     (0x1F242, 'M', u''),
@@ -6954,52 +7490,56 @@ def _seg_66():
     (0x1F250, 'M', u''),
     (0x1F251, 'M', u''),
     (0x1F252, 'X'),
-    (0x1F300, 'V'),
-    (0x1F321, 'X'),
-    (0x1F330, 'V'),
-    (0x1F336, 'X'),
-    (0x1F337, 'V'),
-    (0x1F37D, 'X'),
-    (0x1F380, 'V'),
-    (0x1F394, 'X'),
-    (0x1F3A0, 'V'),
-    (0x1F3C5, 'X'),
-    (0x1F3C6, 'V'),
-    (0x1F3CB, 'X'),
-    (0x1F3E0, 'V'),
-    (0x1F3F1, 'X'),
-    (0x1F400, 'V'),
-    (0x1F43F, 'X'),
-    (0x1F440, 'V'),
+    (0x1F260, 'V'),
     ]
 
-def _seg_67():
+def _seg_72():
     return [
-    (0x1F441, 'X'),
-    (0x1F442, 'V'),
-    (0x1F4F8, 'X'),
-    (0x1F4F9, 'V'),
-    (0x1F4FD, 'X'),
-    (0x1F500, 'V'),
-    (0x1F53E, 'X'),
-    (0x1F540, 'V'),
-    (0x1F544, 'X'),
-    (0x1F550, 'V'),
-    (0x1F568, 'X'),
-    (0x1F5FB, 'V'),
-    (0x1F641, 'X'),
-    (0x1F645, 'V'),
-    (0x1F650, 'X'),
-    (0x1F680, 'V'),
-    (0x1F6C6, 'X'),
+    (0x1F266, 'X'),
+    (0x1F300, 'V'),
+    (0x1F6D5, 'X'),
+    (0x1F6E0, 'V'),
+    (0x1F6ED, 'X'),
+    (0x1F6F0, 'V'),
+    (0x1F6F9, 'X'),
     (0x1F700, 'V'),
     (0x1F774, 'X'),
+    (0x1F780, 'V'),
+    (0x1F7D5, 'X'),
+    (0x1F800, 'V'),
+    (0x1F80C, 'X'),
+    (0x1F810, 'V'),
+    (0x1F848, 'X'),
+    (0x1F850, 'V'),
+    (0x1F85A, 'X'),
+    (0x1F860, 'V'),
+    (0x1F888, 'X'),
+    (0x1F890, 'V'),
+    (0x1F8AE, 'X'),
+    (0x1F900, 'V'),
+    (0x1F90C, 'X'),
+    (0x1F910, 'V'),
+    (0x1F93F, 'X'),
+    (0x1F940, 'V'),
+    (0x1F94D, 'X'),
+    (0x1F950, 'V'),
+    (0x1F96C, 'X'),
+    (0x1F980, 'V'),
+    (0x1F998, 'X'),
+    (0x1F9C0, 'V'),
+    (0x1F9C1, 'X'),
+    (0x1F9D0, 'V'),
+    (0x1F9E7, 'X'),
     (0x20000, 'V'),
     (0x2A6D7, 'X'),
     (0x2A700, 'V'),
     (0x2B735, 'X'),
     (0x2B740, 'V'),
     (0x2B81E, 'X'),
+    (0x2B820, 'V'),
+    (0x2CEA2, 'X'),
+    (0x2CEB0, 'V'),
+    (0x2EBE1, 'X'),
     (0x2F800, 'M', u''),
     (0x2F801, 'M', u''),
     (0x2F802, 'M', u''),
@@ -7055,6 +7595,10 @@ def _seg_67():
     (0x2F836, 'M', u''),
     (0x2F837, 'M', u''),
     (0x2F838, 'M', u''),
+    ]
+
+def _seg_73():
+    return [
     (0x2F839, 'M', u''),
     (0x2F83A, 'M', u''),
     (0x2F83B, 'M', u''),
@@ -7075,10 +7619,6 @@ def _seg_67():
     (0x2F84B, 'M', u''),
     (0x2F84C, 'M', u''),
     (0x2F84D, 'M', u''),
-    ]
-
-def _seg_68():
-    return [
     (0x2F84E, 'M', u''),
     (0x2F84F, 'M', u''),
     (0x2F850, 'M', u''),
@@ -7159,6 +7699,10 @@ def _seg_68():
     (0x2F89E, 'M', u''),
     (0x2F89F, 'M', u''),
     (0x2F8A0, 'M', u''),
+    ]
+
+def _seg_74():
+    return [
     (0x2F8A1, 'M', u''),
     (0x2F8A2, 'M', u''),
     (0x2F8A3, 'M', u''),
@@ -7179,10 +7723,6 @@ def _seg_68():
     (0x2F8B2, 'M', u''),
     (0x2F8B3, 'M', u''),
     (0x2F8B4, 'M', u''),
-    ]
-
-def _seg_69():
-    return [
     (0x2F8B5, 'M', u''),
     (0x2F8B6, 'M', u''),
     (0x2F8B7, 'M', u''),
@@ -7263,6 +7803,10 @@ def _seg_69():
     (0x2F902, 'M', u''),
     (0x2F903, 'M', u''),
     (0x2F904, 'M', u''),
+    ]
+
+def _seg_75():
+    return [
     (0x2F905, 'M', u''),
     (0x2F906, 'M', u''),
     (0x2F907, 'M', u''),
@@ -7283,10 +7827,6 @@ def _seg_69():
     (0x2F916, 'M', u''),
     (0x2F917, 'M', u''),
     (0x2F918, 'M', u''),
-    ]
-
-def _seg_70():
-    return [
     (0x2F919, 'M', u''),
     (0x2F91A, 'M', u''),
     (0x2F91B, 'M', u''),
@@ -7367,6 +7907,10 @@ def _seg_70():
     (0x2F969, 'M', u''),
     (0x2F96A, 'M', u''),
     (0x2F96B, 'M', u''),
+    ]
+
+def _seg_76():
+    return [
     (0x2F96C, 'M', u''),
     (0x2F96D, 'M', u''),
     (0x2F96E, 'M', u''),
@@ -7387,10 +7931,6 @@ def _seg_70():
     (0x2F97D, 'M', u''),
     (0x2F97E, 'M', u''),
     (0x2F97F, 'M', u''),
-    ]
-
-def _seg_71():
-    return [
     (0x2F980, 'M', u''),
     (0x2F981, 'M', u''),
     (0x2F982, 'M', u''),
@@ -7471,6 +8011,10 @@ def _seg_71():
     (0x2F9CD, 'M', u''),
     (0x2F9CE, 'M', u''),
     (0x2F9CF, 'M', u''),
+    ]
+
+def _seg_77():
+    return [
     (0x2F9D0, 'M', u''),
     (0x2F9D1, 'M', u''),
     (0x2F9D2, 'M', u''),
@@ -7491,10 +8035,6 @@ def _seg_71():
     (0x2F9E1, 'M', u''),
     (0x2F9E2, 'M', u''),
     (0x2F9E3, 'M', u''),
-    ]
-
-def _seg_72():
-    return [
     (0x2F9E4, 'M', u''),
     (0x2F9E5, 'M', u''),
     (0x2F9E6, 'M', u''),
@@ -7631,4 +8171,9 @@ uts46data = tuple(
     + _seg_70()
     + _seg_71()
     + _seg_72()
+    + _seg_73()
+    + _seg_74()
+    + _seg_75()
+    + _seg_76()
+    + _seg_77()
 )
diff --git a/pipenv/patched/notpip/_vendor/ipaddress.py b/pipenv/patched/notpip/_vendor/ipaddress.py
index 8cfdd58a..f2d07668 100644
--- a/pipenv/patched/notpip/_vendor/ipaddress.py
+++ b/pipenv/patched/notpip/_vendor/ipaddress.py
@@ -14,7 +14,7 @@ from __future__ import unicode_literals
 import itertools
 import struct
 
-__version__ = '1.0.19'
+__version__ = '1.0.22'
 
 # Compatibility functions
 _compat_int_types = (int,)
diff --git a/pipenv/patched/notpip/_vendor/pkg_resources/__init__.py b/pipenv/patched/notpip/_vendor/pkg_resources/__init__.py
index 3f3a0a4e..ed57821d 100644
--- a/pipenv/patched/notpip/_vendor/pkg_resources/__init__.py
+++ b/pipenv/patched/notpip/_vendor/pkg_resources/__init__.py
@@ -377,11 +377,7 @@ def get_build_platform():
     XXX Currently this is the same as ``distutils.util.get_platform()``, but it
     needs some hacks for Linux and Mac OS X.
     """
-    try:
-        # Python 2.7 or >=3.2
-        from sysconfig import get_platform
-    except ImportError:
-        from distutils.util import get_platform
+    from sysconfig import get_platform
 
     plat = get_platform()
     if sys.platform == "darwin" and not plat.startswith('macosx-'):
@@ -1518,12 +1514,10 @@ class DefaultProvider(EggProvider):
 
     @classmethod
     def _register(cls):
-        loader_cls = getattr(
-            importlib_machinery,
-            'SourceFileLoader',
-            type(None),
-        )
-        register_loader_type(loader_cls, cls)
+        loader_names = 'SourceFileLoader', 'SourcelessFileLoader',
+        for name in loader_names:
+            loader_cls = getattr(importlib_machinery, name, type(None))
+            register_loader_type(loader_cls, cls)
 
 
 DefaultProvider._register()
@@ -2669,6 +2663,19 @@ class Distribution(object):
             raise AttributeError(attr)
         return getattr(self._provider, attr)
 
+    def __dir__(self):
+        return list(
+            set(super(Distribution, self).__dir__())
+            | set(
+                attr for attr in self._provider.__dir__()
+                if not attr.startswith('_')
+            )
+        )
+
+    if not hasattr(object, '__dir__'):
+        # python 2.7 not supported
+        del __dir__
+
     @classmethod
     def from_filename(cls, filename, metadata=None, **kw):
         return cls.from_location(
diff --git a/pipenv/patched/notpip/_vendor/progress/__init__.py b/pipenv/patched/notpip/_vendor/progress/__init__.py
index 09dfc1eb..a41f65dc 100644
--- a/pipenv/patched/notpip/_vendor/progress/__init__.py
+++ b/pipenv/patched/notpip/_vendor/progress/__init__.py
@@ -21,7 +21,7 @@ from sys import stderr
 from time import time
 
 
-__version__ = '1.3'
+__version__ = '1.4'
 
 
 class Infinite(object):
diff --git a/pipenv/patched/notpip/_vendor/progress/bar.py b/pipenv/patched/notpip/_vendor/progress/bar.py
index 5ee968f0..025e61c4 100644
--- a/pipenv/patched/notpip/_vendor/progress/bar.py
+++ b/pipenv/patched/notpip/_vendor/progress/bar.py
@@ -15,6 +15,9 @@
 # OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 
 from __future__ import unicode_literals
+
+import sys
+
 from . import Progress
 from .helpers import WritelnMixin
 
@@ -61,7 +64,10 @@ class FillingCirclesBar(ChargingBar):
 
 
 class IncrementalBar(Bar):
-    phases = (' ', '', '', '', '', '', '', '', '')
+    if sys.platform.startswith('win'):
+        phases = (u' ', u'', u'')
+    else:
+        phases = (' ', '', '', '', '', '', '', '', '')
 
     def update(self):
         nphases = len(self.phases)
diff --git a/pipenv/patched/notpip/_vendor/progress/helpers.py b/pipenv/patched/notpip/_vendor/progress/helpers.py
index 9ed90b2b..0cde44ec 100644
--- a/pipenv/patched/notpip/_vendor/progress/helpers.py
+++ b/pipenv/patched/notpip/_vendor/progress/helpers.py
@@ -28,14 +28,14 @@ class WriteMixin(object):
         if message:
             self.message = message
 
-        if self.file.isatty():
+        if self.file and self.file.isatty():
             if self.hide_cursor:
                 print(HIDE_CURSOR, end='', file=self.file)
             print(self.message, end='', file=self.file)
             self.file.flush()
 
     def write(self, s):
-        if self.file.isatty():
+        if self.file and self.file.isatty():
             b = '\b' * self._width
             c = s.ljust(self._width)
             print(b + c, end='', file=self.file)
@@ -43,7 +43,7 @@ class WriteMixin(object):
             self.file.flush()
 
     def finish(self):
-        if self.file.isatty() and self.hide_cursor:
+        if self.file and self.file.isatty() and self.hide_cursor:
             print(SHOW_CURSOR, end='', file=self.file)
 
 
@@ -55,21 +55,21 @@ class WritelnMixin(object):
         if message:
             self.message = message
 
-        if self.file.isatty() and self.hide_cursor:
+        if self.file and self.file.isatty() and self.hide_cursor:
             print(HIDE_CURSOR, end='', file=self.file)
 
     def clearln(self):
-        if self.file.isatty():
+        if self.file and self.file.isatty():
             print('\r\x1b[K', end='', file=self.file)
 
     def writeln(self, line):
-        if self.file.isatty():
+        if self.file and self.file.isatty():
             self.clearln()
             print(line, end='', file=self.file)
             self.file.flush()
 
     def finish(self):
-        if self.file.isatty():
+        if self.file and self.file.isatty():
             print(file=self.file)
             if self.hide_cursor:
                 print(SHOW_CURSOR, end='', file=self.file)
diff --git a/pipenv/patched/notpip/_vendor/pytoml/parser.py b/pipenv/patched/notpip/_vendor/pytoml/parser.py
index 7fc3d34d..e03a03fb 100644
--- a/pipenv/patched/notpip/_vendor/pytoml/parser.py
+++ b/pipenv/patched/notpip/_vendor/pytoml/parser.py
@@ -6,35 +6,35 @@ if sys.version_info[0] == 2:
 else:
     _chr = chr
 
-def load(fin, translate=lambda t, x, v: v):
-    return loads(fin.read(), translate=translate, filename=getattr(fin, 'name', repr(fin)))
+def load(fin, translate=lambda t, x, v: v, object_pairs_hook=dict):
+    return loads(fin.read(), translate=translate, object_pairs_hook=object_pairs_hook, filename=getattr(fin, 'name', repr(fin)))
 
-def loads(s, filename='<string>', translate=lambda t, x, v: v):
+def loads(s, filename='<string>', translate=lambda t, x, v: v, object_pairs_hook=dict):
     if isinstance(s, bytes):
         s = s.decode('utf-8')
 
     s = s.replace('\r\n', '\n')
 
-    root = {}
-    tables = {}
+    root = object_pairs_hook()
+    tables = object_pairs_hook()
     scope = root
 
     src = _Source(s, filename=filename)
-    ast = _p_toml(src)
+    ast = _p_toml(src, object_pairs_hook=object_pairs_hook)
 
     def error(msg):
         raise TomlError(msg, pos[0], pos[1], filename)
 
-    def process_value(v):
+    def process_value(v, object_pairs_hook):
         kind, text, value, pos = v
         if kind == 'str' and value.startswith('\n'):
             value = value[1:]
         if kind == 'array':
             if value and any(k != value[0][0] for k, t, v, p in value[1:]):
                 error('array-type-mismatch')
-            value = [process_value(item) for item in value]
+            value = [process_value(item, object_pairs_hook=object_pairs_hook) for item in value]
         elif kind == 'table':
-            value = dict([(k, process_value(value[k])) for k in value])
+            value = object_pairs_hook([(k, process_value(value[k], object_pairs_hook=object_pairs_hook)) for k in value])
         return translate(kind, text, value)
 
     for kind, value, pos in ast:
@@ -42,7 +42,7 @@ def loads(s, filename='<string>', translate=lambda t, x, v: v):
             k, v = value
             if k in scope:
                 error('duplicate_keys. Key "{0}" was used more than once.'.format(k))
-            scope[k] = process_value(v)
+            scope[k] = process_value(v, object_pairs_hook=object_pairs_hook)
         else:
             is_table_array = (kind == 'table_array')
             cur = tables
@@ -50,19 +50,19 @@ def loads(s, filename='<string>', translate=lambda t, x, v: v):
                 if isinstance(cur.get(name), list):
                     d, cur = cur[name][-1]
                 else:
-                    d, cur = cur.setdefault(name, (None, {}))
+                    d, cur = cur.setdefault(name, (None, object_pairs_hook()))
 
-            scope = {}
+            scope = object_pairs_hook()
             name = value[-1]
             if name not in cur:
                 if is_table_array:
-                    cur[name] = [(scope, {})]
+                    cur[name] = [(scope, object_pairs_hook())]
                 else:
-                    cur[name] = (scope, {})
+                    cur[name] = (scope, object_pairs_hook())
             elif isinstance(cur[name], list):
                 if not is_table_array:
                     error('table_type_mismatch')
-                cur[name].append((scope, {}))
+                cur[name].append((scope, object_pairs_hook()))
             else:
                 if is_table_array:
                     error('table_type_mismatch')
@@ -73,7 +73,7 @@ def loads(s, filename='<string>', translate=lambda t, x, v: v):
 
     def merge_tables(scope, tables):
         if scope is None:
-            scope = {}
+            scope = object_pairs_hook()
         for k in tables:
             if k in scope:
                 error('key_table_conflict')
@@ -225,7 +225,7 @@ _datetime_re = re.compile(r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2})(\.\d
 _basicstr_ml_re = re.compile(r'(?:(?:|"|"")[^"\\\000-\011\013-\037])*')
 _litstr_re = re.compile(r"[^'\000-\037]*")
 _litstr_ml_re = re.compile(r"(?:(?:|'|'')(?:[^'\000-\011\013-\037]))*")
-def _p_value(s):
+def _p_value(s, object_pairs_hook):
     pos = s.pos()
 
     if s.consume('true'):
@@ -283,7 +283,7 @@ def _p_value(s):
         with s:
             while True:
                 _p_ews(s)
-                items.append(_p_value(s))
+                items.append(_p_value(s, object_pairs_hook=object_pairs_hook))
                 s.commit()
                 _p_ews(s)
                 s.expect(',')
@@ -294,13 +294,13 @@ def _p_value(s):
 
     if s.consume('{'):
         _p_ws(s)
-        items = {}
+        items = object_pairs_hook()
         if not s.consume('}'):
             k = _p_key(s)
             _p_ws(s)
             s.expect('=')
             _p_ws(s)
-            items[k] = _p_value(s)
+            items[k] = _p_value(s, object_pairs_hook=object_pairs_hook)
             _p_ws(s)
             while s.consume(','):
                 _p_ws(s)
@@ -308,14 +308,14 @@ def _p_value(s):
                 _p_ws(s)
                 s.expect('=')
                 _p_ws(s)
-                items[k] = _p_value(s)
+                items[k] = _p_value(s, object_pairs_hook=object_pairs_hook)
                 _p_ws(s)
             s.expect('}')
         return 'table', None, items, pos
 
     s.fail()
 
-def _p_stmt(s):
+def _p_stmt(s, object_pairs_hook):
     pos = s.pos()
     if s.consume(   '['):
         is_array = s.consume('[')
@@ -335,19 +335,19 @@ def _p_stmt(s):
     _p_ws(s)
     s.expect('=')
     _p_ws(s)
-    value = _p_value(s)
+    value = _p_value(s, object_pairs_hook=object_pairs_hook)
     return 'kv', (key, value), pos
 
 _stmtsep_re = re.compile(r'(?:[ \t]*(?:#[^\n]*)?\n)+[ \t]*')
-def _p_toml(s):
+def _p_toml(s, object_pairs_hook):
     stmts = []
     _p_ews(s)
     with s:
-        stmts.append(_p_stmt(s))
+        stmts.append(_p_stmt(s, object_pairs_hook=object_pairs_hook))
         while True:
             s.commit()
             s.expect_re(_stmtsep_re)
-            stmts.append(_p_stmt(s))
+            stmts.append(_p_stmt(s, object_pairs_hook=object_pairs_hook))
     _p_ews(s)
     s.expect_eof()
     return stmts
diff --git a/pipenv/patched/notpip/_vendor/requests/LICENSE b/pipenv/patched/notpip/_vendor/requests/LICENSE
index db78ea69..2e68b82e 100644
--- a/pipenv/patched/notpip/_vendor/requests/LICENSE
+++ b/pipenv/patched/notpip/_vendor/requests/LICENSE
@@ -1,4 +1,4 @@
-Copyright 2017 Kenneth Reitz
+Copyright 2018 Kenneth Reitz
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
diff --git a/pipenv/patched/notpip/_vendor/requests/__init__.py b/pipenv/patched/notpip/_vendor/requests/__init__.py
index 9270af26..8e7576c1 100644
--- a/pipenv/patched/notpip/_vendor/requests/__init__.py
+++ b/pipenv/patched/notpip/_vendor/requests/__init__.py
@@ -57,10 +57,10 @@ def check_compatibility(urllib3_version, chardet_version):
     # Check urllib3 for compatibility.
     major, minor, patch = urllib3_version  # noqa: F811
     major, minor, patch = int(major), int(minor), int(patch)
-    # urllib3 >= 1.21.1, <= 1.22
+    # urllib3 >= 1.21.1, <= 1.23
     assert major == 1
     assert minor >= 21
-    assert minor <= 22
+    assert minor <= 23
 
     # Check chardet for compatibility.
     major, minor, patch = chardet_version.split('.')[:3]
@@ -71,6 +71,17 @@ def check_compatibility(urllib3_version, chardet_version):
     assert patch >= 2
 
 
+def _check_cryptography(cryptography_version):
+    # cryptography < 1.3.4
+    try:
+        cryptography_version = list(map(int, cryptography_version.split('.')))
+    except ValueError:
+        return
+
+    if cryptography_version < [1, 3, 4]:
+        warning = 'Old version of cryptography ({0}) may cause slowdown.'.format(cryptography_version)
+        warnings.warn(warning, RequestsDependencyWarning)
+
 # Check imported dependencies for compatibility.
 try:
     check_compatibility(urllib3.__version__, chardet.__version__)
@@ -85,6 +96,10 @@ if not WINDOWS:
     try:
         from pipenv.patched.notpip._vendor.urllib3.contrib import pyopenssl
         pyopenssl.inject_into_urllib3()
+
+        # Check cryptography version
+        from cryptography import __version__ as cryptography_version
+        _check_cryptography(cryptography_version)
     except ImportError:
         pass
 
diff --git a/pipenv/patched/notpip/_vendor/requests/__version__.py b/pipenv/patched/notpip/_vendor/requests/__version__.py
index dc33eef6..ef61ec0f 100644
--- a/pipenv/patched/notpip/_vendor/requests/__version__.py
+++ b/pipenv/patched/notpip/_vendor/requests/__version__.py
@@ -5,10 +5,10 @@
 __title__ = 'requests'
 __description__ = 'Python HTTP for Humans.'
 __url__ = 'http://python-requests.org'
-__version__ = '2.18.4'
-__build__ = 0x021804
+__version__ = '2.19.1'
+__build__ = 0x021901
 __author__ = 'Kenneth Reitz'
 __author_email__ = 'me@kennethreitz.org'
 __license__ = 'Apache 2.0'
-__copyright__ = 'Copyright 2017 Kenneth Reitz'
+__copyright__ = 'Copyright 2018 Kenneth Reitz'
 __cake__ = u'\u2728 \U0001f370 \u2728'
diff --git a/pipenv/patched/notpip/_vendor/requests/adapters.py b/pipenv/patched/notpip/_vendor/requests/adapters.py
index 58cea3a6..014c2675 100644
--- a/pipenv/patched/notpip/_vendor/requests/adapters.py
+++ b/pipenv/patched/notpip/_vendor/requests/adapters.py
@@ -13,6 +13,7 @@ import socket
 
 from pipenv.patched.notpip._vendor.urllib3.poolmanager import PoolManager, proxy_from_url
 from pipenv.patched.notpip._vendor.urllib3.response import HTTPResponse
+from pipenv.patched.notpip._vendor.urllib3.util import parse_url
 from pipenv.patched.notpip._vendor.urllib3.util import Timeout as TimeoutSauce
 from pipenv.patched.notpip._vendor.urllib3.util.retry import Retry
 from pipenv.patched.notpip._vendor.urllib3.exceptions import ClosedPoolError
@@ -28,13 +29,13 @@ from pipenv.patched.notpip._vendor.urllib3.exceptions import ResponseError
 
 from .models import Response
 from .compat import urlparse, basestring
-from .utils import (DEFAULT_CA_BUNDLE_PATH, get_encoding_from_headers,
-                    prepend_scheme_if_needed, get_auth_from_url, urldefragauth,
-                    select_proxy)
+from .utils import (DEFAULT_CA_BUNDLE_PATH, extract_zipped_paths,
+                    get_encoding_from_headers, prepend_scheme_if_needed,
+                    get_auth_from_url, urldefragauth, select_proxy)
 from .structures import CaseInsensitiveDict
 from .cookies import extract_cookies_to_jar
 from .exceptions import (ConnectionError, ConnectTimeout, ReadTimeout, SSLError,
-                         ProxyError, RetryError, InvalidSchema)
+                         ProxyError, RetryError, InvalidSchema, InvalidProxyURL)
 from .auth import _basic_auth_str
 
 try:
@@ -219,7 +220,7 @@ class HTTPAdapter(BaseAdapter):
                 cert_loc = verify
 
             if not cert_loc:
-                cert_loc = DEFAULT_CA_BUNDLE_PATH
+                cert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)
 
             if not cert_loc or not os.path.exists(cert_loc):
                 raise IOError("Could not find a suitable TLS CA certificate bundle, "
@@ -300,6 +301,10 @@ class HTTPAdapter(BaseAdapter):
 
         if proxy:
             proxy = prepend_scheme_if_needed(proxy, 'http')
+            proxy_url = parse_url(proxy)
+            if not proxy_url.host:
+                raise InvalidProxyURL("Please check proxy URL. It is malformed"
+                                      " and could be missing the host.")
             proxy_manager = self.proxy_manager_for(proxy)
             conn = proxy_manager.connection_from_url(url)
         else:
@@ -406,7 +411,7 @@ class HTTPAdapter(BaseAdapter):
 
         self.cert_verify(conn, request.url, verify, cert)
         url = self.request_url(request, proxies)
-        self.add_headers(request)
+        self.add_headers(request, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies)
 
         chunked = not (request.body is None or 'Content-Length' in request.headers)
 
diff --git a/pipenv/patched/notpip/_vendor/requests/api.py b/pipenv/patched/notpip/_vendor/requests/api.py
index bc2115c1..a2cc84d7 100644
--- a/pipenv/patched/notpip/_vendor/requests/api.py
+++ b/pipenv/patched/notpip/_vendor/requests/api.py
@@ -20,7 +20,7 @@ def request(method, url, **kwargs):
     :param url: URL for the new :class:`Request` object.
     :param params: (optional) Dictionary or bytes to be sent in the query string for the :class:`Request`.
     :param data: (optional) Dictionary or list of tuples ``[(key, value)]`` (will be form-encoded), bytes, or file-like object to send in the body of the :class:`Request`.
-    :param json: (optional) json data to send in the body of the :class:`Request`.
+    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
     :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
     :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
     :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
diff --git a/pipenv/patched/notpip/_vendor/requests/auth.py b/pipenv/patched/notpip/_vendor/requests/auth.py
index 1a182dff..4ae45947 100644
--- a/pipenv/patched/notpip/_vendor/requests/auth.py
+++ b/pipenv/patched/notpip/_vendor/requests/auth.py
@@ -153,6 +153,18 @@ class HTTPDigestAuth(AuthBase):
                     x = x.encode('utf-8')
                 return hashlib.sha1(x).hexdigest()
             hash_utf8 = sha_utf8
+        elif _algorithm == 'SHA-256':
+            def sha256_utf8(x):
+                if isinstance(x, str):
+                    x = x.encode('utf-8')
+                return hashlib.sha256(x).hexdigest()
+            hash_utf8 = sha256_utf8
+        elif _algorithm == 'SHA-512':
+            def sha512_utf8(x):
+                if isinstance(x, str):
+                    x = x.encode('utf-8')
+                return hashlib.sha512(x).hexdigest()
+            hash_utf8 = sha512_utf8
 
         KD = lambda s, d: hash_utf8("%s:%s" % (s, d))
 
diff --git a/pipenv/patched/notpip/_vendor/requests/compat.py b/pipenv/patched/notpip/_vendor/requests/compat.py
index 011972c6..4fbd6231 100644
--- a/pipenv/patched/notpip/_vendor/requests/compat.py
+++ b/pipenv/patched/notpip/_vendor/requests/compat.py
@@ -47,6 +47,7 @@ if is_py2:
     import cookielib
     from Cookie import Morsel
     from StringIO import StringIO
+    from collections import Callable, Mapping, MutableMapping
 
     from pipenv.patched.notpip._vendor.urllib3.packages.ordered_dict import OrderedDict
 
@@ -64,6 +65,7 @@ elif is_py3:
     from http.cookies import Morsel
     from io import StringIO
     from collections import OrderedDict
+    from collections.abc import Callable, Mapping, MutableMapping
 
     builtin_str = str
     str = str
diff --git a/pipenv/patched/notpip/_vendor/requests/cookies.py b/pipenv/patched/notpip/_vendor/requests/cookies.py
index ab3c88b9..50883a84 100644
--- a/pipenv/patched/notpip/_vendor/requests/cookies.py
+++ b/pipenv/patched/notpip/_vendor/requests/cookies.py
@@ -12,10 +12,9 @@ requests.utils imports from here, so be careful with imports.
 import copy
 import time
 import calendar
-import collections
 
 from ._internal_utils import to_native_string
-from .compat import cookielib, urlparse, urlunparse, Morsel
+from .compat import cookielib, urlparse, urlunparse, Morsel, MutableMapping
 
 try:
     import threading
@@ -169,7 +168,7 @@ class CookieConflictError(RuntimeError):
     """
 
 
-class RequestsCookieJar(cookielib.CookieJar, collections.MutableMapping):
+class RequestsCookieJar(cookielib.CookieJar, MutableMapping):
     """Compatibility class; is a cookielib.CookieJar, but exposes a dict
     interface.
 
@@ -415,9 +414,14 @@ class RequestsCookieJar(cookielib.CookieJar, collections.MutableMapping):
     def copy(self):
         """Return a copy of this RequestsCookieJar."""
         new_cj = RequestsCookieJar()
+        new_cj.set_policy(self.get_policy())
         new_cj.update(self)
         return new_cj
 
+    def get_policy(self):
+        """Return the CookiePolicy instance used."""
+        return self._policy
+
 
 def _copy_cookie_jar(jar):
     if jar is None:
diff --git a/pipenv/patched/notpip/_vendor/requests/exceptions.py b/pipenv/patched/notpip/_vendor/requests/exceptions.py
index ea644a8f..78b573c7 100644
--- a/pipenv/patched/notpip/_vendor/requests/exceptions.py
+++ b/pipenv/patched/notpip/_vendor/requests/exceptions.py
@@ -85,6 +85,10 @@ class InvalidHeader(RequestException, ValueError):
     """The header value provided was somehow invalid."""
 
 
+class InvalidProxyURL(InvalidURL):
+    """The proxy URL provided is invalid."""
+
+
 class ChunkedEncodingError(RequestException):
     """The server declared chunked encoding but sent an invalid chunk."""
 
diff --git a/pipenv/patched/notpip/_vendor/requests/help.py b/pipenv/patched/notpip/_vendor/requests/help.py
index abc097a7..eba69edb 100644
--- a/pipenv/patched/notpip/_vendor/requests/help.py
+++ b/pipenv/patched/notpip/_vendor/requests/help.py
@@ -13,7 +13,7 @@ from pipenv.patched.notpip._vendor import chardet
 from . import __version__ as requests_version
 
 try:
-    from .packages.urllib3.contrib import pyopenssl
+    from pipenv.patched.notpip._vendor.urllib3.contrib import pyopenssl
 except ImportError:
     pyopenssl = None
     OpenSSL = None
diff --git a/pipenv/patched/notpip/_vendor/requests/models.py b/pipenv/patched/notpip/_vendor/requests/models.py
index 157a2782..6708f09b 100644
--- a/pipenv/patched/notpip/_vendor/requests/models.py
+++ b/pipenv/patched/notpip/_vendor/requests/models.py
@@ -7,7 +7,6 @@ requests.models
 This module contains the primary objects that power Requests.
 """
 
-import collections
 import datetime
 import sys
 
@@ -37,6 +36,7 @@ from .utils import (
     stream_decode_response_unicode, to_key_val_list, parse_header_links,
     iter_slices, guess_json_utf, super_len, check_header_validity)
 from .compat import (
+    Callable, Mapping,
     cookielib, urlunparse, urlsplit, urlencode, str, bytes,
     is_py2, chardet, builtin_str, basestring)
 from .compat import json as complexjson
@@ -155,8 +155,12 @@ class RequestEncodingMixin(object):
 
             if isinstance(fp, (str, bytes, bytearray)):
                 fdata = fp
-            else:
+            elif hasattr(fp, 'read'):
                 fdata = fp.read()
+            elif fp is None:
+                continue
+            else:
+                fdata = fp
 
             rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)
             rf.make_multipart(content_type=ft)
@@ -174,10 +178,10 @@ class RequestHooksMixin(object):
         if event not in self.hooks:
             raise ValueError('Unsupported event specified, with event name "%s"' % (event))
 
-        if isinstance(hook, collections.Callable):
+        if isinstance(hook, Callable):
             self.hooks[event].append(hook)
         elif hasattr(hook, '__iter__'):
-            self.hooks[event].extend(h for h in hook if isinstance(h, collections.Callable))
+            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))
 
     def deregister_hook(self, event, hook):
         """Deregister a previously registered hook.
@@ -461,7 +465,7 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):
 
         is_stream = all([
             hasattr(data, '__iter__'),
-            not isinstance(data, (basestring, list, tuple, collections.Mapping))
+            not isinstance(data, (basestring, list, tuple, Mapping))
         ])
 
         try:
@@ -686,11 +690,11 @@ class Response(object):
 
     @property
     def ok(self):
-        """Returns True if :attr:`status_code` is less than 400.
+        """Returns True if :attr:`status_code` is less than 400, False if not.
 
         This attribute checks if the status code of the response is between
         400 and 600 to see if there was a client error or a server error. If
-        the status code, is between 200 and 400, this will return True. This
+        the status code is between 200 and 400, this will return True. This
         is **not** a check to see if the response code is ``200 OK``.
         """
         try:
@@ -820,7 +824,7 @@ class Response(object):
             if self.status_code == 0 or self.raw is None:
                 self._content = None
             else:
-                self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
+                self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''
 
         self._content_consumed = True
         # don't need to release the connection; that's been handled by urllib3
diff --git a/pipenv/patched/notpip/_vendor/requests/sessions.py b/pipenv/patched/notpip/_vendor/requests/sessions.py
index 6570e733..ba135268 100644
--- a/pipenv/patched/notpip/_vendor/requests/sessions.py
+++ b/pipenv/patched/notpip/_vendor/requests/sessions.py
@@ -8,13 +8,12 @@ This module provides a Session object to manage and persist settings across
 requests (cookies, auth, proxies).
 """
 import os
-import platform
+import sys
 import time
-from collections import Mapping
 from datetime import timedelta
 
 from .auth import _basic_auth_str
-from .compat import cookielib, is_py3, OrderedDict, urljoin, urlparse
+from .compat import cookielib, is_py3, OrderedDict, urljoin, urlparse, Mapping
 from .cookies import (
     cookiejar_from_dict, extract_cookies_to_jar, RequestsCookieJar, merge_cookies)
 from .models import Request, PreparedRequest, DEFAULT_REDIRECT_LIMIT
@@ -38,8 +37,8 @@ from .status_codes import codes
 from .models import REDIRECT_STATI
 
 # Preferred clock, based on which one is more accurate on a given system.
-if platform.system() == 'Windows':
-    try:  # Python 3.3+
+if sys.platform == 'win32':
+    try:  # Python 3.4+
         preferred_clock = time.perf_counter
     except AttributeError:  # Earlier than Python 3.
         preferred_clock = time.clock
@@ -123,6 +122,7 @@ class SessionRedirectMixin(object):
         hist = []  # keep track of history
 
         url = self.get_redirect_target(resp)
+        previous_fragment = urlparse(req.url).fragment
         while url:
             prepared_request = req.copy()
 
@@ -147,8 +147,12 @@ class SessionRedirectMixin(object):
                 parsed_rurl = urlparse(resp.url)
                 url = '%s:%s' % (to_native_string(parsed_rurl.scheme), url)
 
-            # The scheme should be lower case...
+            # Normalize url case and attach previous fragment if needed (RFC 7231 7.1.2)
             parsed = urlparse(url)
+            if parsed.fragment == '' and previous_fragment:
+                parsed = parsed._replace(fragment=previous_fragment)
+            elif parsed.fragment:
+                previous_fragment = parsed.fragment
             url = parsed.geturl()
 
             # Facilitate relative 'location' headers, as allowed by RFC 7231.
@@ -696,7 +700,7 @@ class Session(SessionRedirectMixin):
         """
         for (prefix, adapter) in self.adapters.items():
 
-            if url.lower().startswith(prefix):
+            if url.lower().startswith(prefix.lower()):
                 return adapter
 
         # Nothing matches :-/
diff --git a/pipenv/patched/notpip/_vendor/requests/status_codes.py b/pipenv/patched/notpip/_vendor/requests/status_codes.py
index dee89190..ff462c6c 100644
--- a/pipenv/patched/notpip/_vendor/requests/status_codes.py
+++ b/pipenv/patched/notpip/_vendor/requests/status_codes.py
@@ -1,5 +1,22 @@
 # -*- coding: utf-8 -*-
 
+"""
+The ``codes`` object defines a mapping from common names for HTTP statuses
+to their numerical codes, accessible either as attributes or as dictionary
+items.
+
+>>> requests.codes['temporary_redirect']
+307
+>>> requests.codes.teapot
+418
+>>> requests.codes['\o/']
+200
+
+Some codes have multiple names, and both upper- and lower-case versions of
+the names are allowed. For example, ``codes.ok``, ``codes.OK``, and
+``codes.okay`` all correspond to the HTTP status code 200.
+"""
+
 from .structures import LookupDict
 
 _codes = {
@@ -84,8 +101,20 @@ _codes = {
 
 codes = LookupDict(name='status_codes')
 
-for code, titles in _codes.items():
-    for title in titles:
-        setattr(codes, title, code)
-        if not title.startswith(('\\', '/')):
-            setattr(codes, title.upper(), code)
+def _init():
+    for code, titles in _codes.items():
+        for title in titles:
+            setattr(codes, title, code)
+            if not title.startswith(('\\', '/')):
+                setattr(codes, title.upper(), code)
+
+    def doc(code):
+        names = ', '.join('``%s``' % n for n in _codes[code])
+        return '* %d: %s' % (code, names)
+
+    global __doc__
+    __doc__ = (__doc__ + '\n' +
+               '\n'.join(doc(code) for code in sorted(_codes))
+               if __doc__ is not None else None)
+
+_init()
diff --git a/pipenv/patched/notpip/_vendor/requests/structures.py b/pipenv/patched/notpip/_vendor/requests/structures.py
index 05d2b3f5..da930e28 100644
--- a/pipenv/patched/notpip/_vendor/requests/structures.py
+++ b/pipenv/patched/notpip/_vendor/requests/structures.py
@@ -7,16 +7,14 @@ requests.structures
 Data structures that power Requests.
 """
 
-import collections
+from .compat import OrderedDict, Mapping, MutableMapping
 
-from .compat import OrderedDict
 
-
-class CaseInsensitiveDict(collections.MutableMapping):
+class CaseInsensitiveDict(MutableMapping):
     """A case-insensitive ``dict``-like object.
 
     Implements all methods and operations of
-    ``collections.MutableMapping`` as well as dict's ``copy``. Also
+    ``MutableMapping`` as well as dict's ``copy``. Also
     provides ``lower_items``.
 
     All keys are expected to be strings. The structure remembers the
@@ -71,7 +69,7 @@ class CaseInsensitiveDict(collections.MutableMapping):
         )
 
     def __eq__(self, other):
-        if isinstance(other, collections.Mapping):
+        if isinstance(other, Mapping):
             other = CaseInsensitiveDict(other)
         else:
             return NotImplemented
diff --git a/pipenv/patched/notpip/_vendor/requests/utils.py b/pipenv/patched/notpip/_vendor/requests/utils.py
index 5c47de98..431f6be0 100644
--- a/pipenv/patched/notpip/_vendor/requests/utils.py
+++ b/pipenv/patched/notpip/_vendor/requests/utils.py
@@ -8,17 +8,17 @@ This module provides utility functions that are used within Requests
 that are also useful for external consumption.
 """
 
-import cgi
 import codecs
-import collections
 import contextlib
 import io
 import os
-import platform
 import re
 import socket
 import struct
+import sys
+import tempfile
 import warnings
+import zipfile
 
 from .__version__ import __version__
 from . import certs
@@ -28,7 +28,7 @@ from .compat import parse_http_list as _parse_list_header
 from .compat import (
     quote, urlparse, bytes, str, OrderedDict, unquote, getproxies,
     proxy_bypass, urlunparse, basestring, integer_types, is_py3,
-    proxy_bypass_environment, getproxies_environment)
+    proxy_bypass_environment, getproxies_environment, Mapping)
 from .cookies import cookiejar_from_dict
 from .structures import CaseInsensitiveDict
 from .exceptions import (
@@ -39,19 +39,25 @@ NETRC_FILES = ('.netrc', '_netrc')
 DEFAULT_CA_BUNDLE_PATH = certs.where()
 
 
-if platform.system() == 'Windows':
+if sys.platform == 'win32':
     # provide a proxy_bypass version on Windows without DNS lookups
 
     def proxy_bypass_registry(host):
-        if is_py3:
-            import winreg
-        else:
-            import _winreg as winreg
+        try:
+            if is_py3:
+                import winreg
+            else:
+                import _winreg as winreg
+        except ImportError:
+            return False
+
         try:
             internetSettings = winreg.OpenKey(winreg.HKEY_CURRENT_USER,
                 r'Software\Microsoft\Windows\CurrentVersion\Internet Settings')
-            proxyEnable = winreg.QueryValueEx(internetSettings,
-                                              'ProxyEnable')[0]
+            # ProxyEnable could be REG_SZ or REG_DWORD, normalizing it
+            proxyEnable = int(winreg.QueryValueEx(internetSettings,
+                                              'ProxyEnable')[0])
+            # ProxyOverride is almost always a string
             proxyOverride = winreg.QueryValueEx(internetSettings,
                                                 'ProxyOverride')[0]
         except OSError:
@@ -216,6 +222,38 @@ def guess_filename(obj):
         return os.path.basename(name)
 
 
+def extract_zipped_paths(path):
+    """Replace nonexistent paths that look like they refer to a member of a zip
+    archive with the location of an extracted copy of the target, or else
+    just return the provided path unchanged.
+    """
+    if os.path.exists(path):
+        # this is already a valid path, no need to do anything further
+        return path
+
+    # find the first valid part of the provided path and treat that as a zip archive
+    # assume the rest of the path is the name of a member in the archive
+    archive, member = os.path.split(path)
+    while archive and not os.path.exists(archive):
+        archive, prefix = os.path.split(archive)
+        member = '/'.join([prefix, member])
+
+    if not zipfile.is_zipfile(archive):
+        return path
+
+    zip_file = zipfile.ZipFile(archive)
+    if member not in zip_file.namelist():
+        return path
+
+    # we have a valid zip archive and a valid member of that archive
+    tmp = tempfile.gettempdir()
+    extracted_path = os.path.join(tmp, *member.split('/'))
+    if not os.path.exists(extracted_path):
+        extracted_path = zip_file.extract(member, path=tmp)
+
+    return extracted_path
+
+
 def from_key_val_list(value):
     """Take an object and test to see if it can be represented as a
     dictionary. Unless it can not be represented as such, return an
@@ -262,7 +300,7 @@ def to_key_val_list(value):
     if isinstance(value, (str, bytes, bool, int)):
         raise ValueError('cannot encode objects that are not 2-tuples')
 
-    if isinstance(value, collections.Mapping):
+    if isinstance(value, Mapping):
         value = value.items()
 
     return list(value)
@@ -407,6 +445,31 @@ def get_encodings_from_content(content):
             xml_re.findall(content))
 
 
+def _parse_content_type_header(header):
+    """Returns content type and parameters from given header
+
+    :param header: string
+    :return: tuple containing content type and dictionary of
+         parameters
+    """
+
+    tokens = header.split(';')
+    content_type, params = tokens[0].strip(), tokens[1:]
+    params_dict = {}
+    items_to_strip = "\"' "
+
+    for param in params:
+        param = param.strip()
+        if param:
+            key, value = param, True
+            index_of_equals = param.find("=")
+            if index_of_equals != -1:
+                key = param[:index_of_equals].strip(items_to_strip)
+                value = param[index_of_equals + 1:].strip(items_to_strip)
+            params_dict[key] = value
+    return content_type, params_dict
+
+
 def get_encoding_from_headers(headers):
     """Returns encodings from given HTTP Header Dict.
 
@@ -419,7 +482,7 @@ def get_encoding_from_headers(headers):
     if not content_type:
         return None
 
-    content_type, params = cgi.parse_header(content_type)
+    content_type, params = _parse_content_type_header(content_type)
 
     if 'charset' in params:
         return params['charset'].strip("'\"")
@@ -632,6 +695,8 @@ def should_bypass_proxies(url, no_proxy):
 
     :rtype: bool
     """
+    # Prioritize lowercase environment variables over uppercase
+    # to keep a consistent behaviour with other http projects (curl, wget).
     get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())
 
     # First check whether no_proxy is defined. If it is, check that the URL
@@ -639,28 +704,31 @@ def should_bypass_proxies(url, no_proxy):
     no_proxy_arg = no_proxy
     if no_proxy is None:
         no_proxy = get_proxy('no_proxy')
-    netloc = urlparse(url).netloc
+    parsed = urlparse(url)
 
     if no_proxy:
         # We need to check whether we match here. We need to see if we match
-        # the end of the netloc, both with and without the port.
+        # the end of the hostname, both with and without the port.
         no_proxy = (
             host for host in no_proxy.replace(' ', '').split(',') if host
         )
 
-        ip = netloc.split(':')[0]
-        if is_ipv4_address(ip):
+        if is_ipv4_address(parsed.hostname):
             for proxy_ip in no_proxy:
                 if is_valid_cidr(proxy_ip):
-                    if address_in_network(ip, proxy_ip):
+                    if address_in_network(parsed.hostname, proxy_ip):
                         return True
-                elif ip == proxy_ip:
+                elif parsed.hostname == proxy_ip:
                     # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                     # matches the IP of the index
                     return True
         else:
+            host_with_port = parsed.hostname
+            if parsed.port:
+                host_with_port += ':{0}'.format(parsed.port)
+
             for host in no_proxy:
-                if netloc.endswith(host) or netloc.split(':')[0].endswith(host):
+                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                     # The URL does match something in no_proxy, so we don't want
                     # to apply the proxies on this URL.
                     return True
@@ -673,7 +741,7 @@ def should_bypass_proxies(url, no_proxy):
     # legitimate problems.
     with set_environ('no_proxy', no_proxy_arg):
         try:
-            bypass = proxy_bypass(netloc)
+            bypass = proxy_bypass(parsed.hostname)
         except (TypeError, socket.gaierror):
             bypass = False
 
@@ -743,7 +811,7 @@ def default_headers():
 
 
 def parse_header_links(value):
-    """Return a dict of parsed link headers proxies.
+    """Return a list of parsed link headers proxies.
 
     i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"
 
@@ -754,6 +822,10 @@ def parse_header_links(value):
 
     replace_chars = ' \'"'
 
+    value = value.strip(replace_chars)
+    if not value:
+        return links
+
     for val in re.split(', *<', value):
         try:
             url, params = val.split(';', 1)
diff --git a/pipenv/patched/notpip/_vendor/urllib3/__init__.py b/pipenv/patched/notpip/_vendor/urllib3/__init__.py
index aaa6b1c6..4bd533b5 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/__init__.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/__init__.py
@@ -32,7 +32,7 @@ except ImportError:
 
 __author__ = 'Andrey Petrov (andrey.petrov@shazow.net)'
 __license__ = 'MIT'
-__version__ = '1.22'
+__version__ = '1.23'
 
 __all__ = (
     'HTTPConnectionPool',
diff --git a/pipenv/patched/notpip/_vendor/urllib3/_collections.py b/pipenv/patched/notpip/_vendor/urllib3/_collections.py
index 5df2372c..6e36b84e 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/_collections.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/_collections.py
@@ -1,5 +1,8 @@
 from __future__ import absolute_import
-from collections import Mapping, MutableMapping
+try:
+    from collections.abc import Mapping, MutableMapping
+except ImportError:
+    from collections import Mapping, MutableMapping
 try:
     from threading import RLock
 except ImportError:  # Platform-specific: No threads available
@@ -15,6 +18,7 @@ try:  # Python 2.7+
     from collections import OrderedDict
 except ImportError:
     from .packages.ordered_dict import OrderedDict
+from .exceptions import InvalidHeader
 from .packages.six import iterkeys, itervalues, PY3
 
 
@@ -305,13 +309,22 @@ class HTTPHeaderDict(MutableMapping):
         # python2.7 does not expose a proper API for exporting multiheaders
         # efficiently. This function re-reads raw lines from the message
         # object and extracts the multiheaders properly.
+        obs_fold_continued_leaders = (' ', '\t')
         headers = []
 
         for line in message.headers:
-            if line.startswith((' ', '\t')):
-                key, value = headers[-1]
-                headers[-1] = (key, value + '\r\n' + line.rstrip())
-                continue
+            if line.startswith(obs_fold_continued_leaders):
+                if not headers:
+                    # We received a header line that starts with OWS as described
+                    # in RFC-7230 S3.2.4. This indicates a multiline header, but
+                    # there exists no previous header to which we can attach it.
+                    raise InvalidHeader(
+                        'Header continuation with no previous header: %s' % line
+                    )
+                else:
+                    key, value = headers[-1]
+                    headers[-1] = (key, value + ' ' + line.strip())
+                    continue
 
             key, value = line.split(':', 1)
             headers.append((key, value.strip()))
diff --git a/pipenv/patched/notpip/_vendor/urllib3/connection.py b/pipenv/patched/notpip/_vendor/urllib3/connection.py
index c0d83299..a03b573f 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/connection.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/connection.py
@@ -56,10 +56,11 @@ port_by_scheme = {
     'https': 443,
 }
 
-# When updating RECENT_DATE, move it to
-# within two years of the current date, and no
-# earlier than 6 months ago.
-RECENT_DATE = datetime.date(2016, 1, 1)
+# When updating RECENT_DATE, move it to within two years of the current date,
+# and not less than 6 months ago.
+# Example: if Today is 2018-01-01, then RECENT_DATE should be any date on or
+# after 2016-01-01 (today - 2 years) AND before 2017-07-01 (today - 6 months)
+RECENT_DATE = datetime.date(2017, 6, 30)
 
 
 class DummyConnection(object):
@@ -124,6 +125,35 @@ class HTTPConnection(_HTTPConnection, object):
         # Superclass also sets self.source_address in Python 2.7+.
         _HTTPConnection.__init__(self, *args, **kw)
 
+    @property
+    def host(self):
+        """
+        Getter method to remove any trailing dots that indicate the hostname is an FQDN.
+
+        In general, SSL certificates don't include the trailing dot indicating a
+        fully-qualified domain name, and thus, they don't validate properly when
+        checked against a domain name that includes the dot. In addition, some
+        servers may not expect to receive the trailing dot when provided.
+
+        However, the hostname with trailing dot is critical to DNS resolution; doing a
+        lookup with the trailing dot will properly only resolve the appropriate FQDN,
+        whereas a lookup without a trailing dot will search the system's search domain
+        list. Thus, it's important to keep the original host around for use only in
+        those cases where it's appropriate (i.e., when doing DNS lookup to establish the
+        actual TCP connection across which we're going to send HTTP requests).
+        """
+        return self._dns_host.rstrip('.')
+
+    @host.setter
+    def host(self, value):
+        """
+        Setter for the `host` property.
+
+        We assume that only urllib3 uses the _dns_host attribute; httplib itself
+        only uses `host`, and it seems reasonable that other libraries follow suit.
+        """
+        self._dns_host = value
+
     def _new_conn(self):
         """ Establish a socket connection and set nodelay settings on it.
 
@@ -138,7 +168,7 @@ class HTTPConnection(_HTTPConnection, object):
 
         try:
             conn = connection.create_connection(
-                (self.host, self.port), self.timeout, **extra_kw)
+                (self._dns_host, self.port), self.timeout, **extra_kw)
 
         except SocketTimeout as e:
             raise ConnectTimeoutError(
diff --git a/pipenv/patched/notpip/_vendor/urllib3/connectionpool.py b/pipenv/patched/notpip/_vendor/urllib3/connectionpool.py
index ec9600f8..8fcb0bce 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/connectionpool.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/connectionpool.py
@@ -40,13 +40,10 @@ from .util.request import set_file_position
 from .util.response import assert_header_parsing
 from .util.retry import Retry
 from .util.timeout import Timeout
-from .util.url import get_host, Url
+from .util.url import get_host, Url, NORMALIZABLE_SCHEMES
+from .util.queue import LifoQueue
 
 
-if six.PY2:
-    # Queue is imported for side effects on MS Windows
-    import Queue as _unused_module_Queue  # noqa: F401
-
 xrange = six.moves.xrange
 
 log = logging.getLogger(__name__)
@@ -62,13 +59,13 @@ class ConnectionPool(object):
     """
 
     scheme = None
-    QueueCls = queue.LifoQueue
+    QueueCls = LifoQueue
 
     def __init__(self, host, port=None):
         if not host:
             raise LocationValueError("No host specified.")
 
-        self.host = _ipv6_host(host).lower()
+        self.host = _ipv6_host(host, self.scheme)
         self._proxy_host = host.lower()
         self.port = port
 
@@ -204,8 +201,8 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
         Return a fresh :class:`HTTPConnection`.
         """
         self.num_connections += 1
-        log.debug("Starting new HTTP connection (%d): %s",
-                  self.num_connections, self.host)
+        log.debug("Starting new HTTP connection (%d): %s:%s",
+                  self.num_connections, self.host, self.port or "80")
 
         conn = self.ConnectionCls(host=self.host, port=self.port,
                                   timeout=self.timeout.connect_timeout,
@@ -411,6 +408,8 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
         """
         Close all pooled connections and disable the pool.
         """
+        if self.pool is None:
+            return
         # Disable access to the pool
         old_pool, self.pool = self.pool, None
 
@@ -434,7 +433,7 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
         # TODO: Add optional support for socket.gethostbyname checking.
         scheme, host, port = get_host(url)
 
-        host = _ipv6_host(host).lower()
+        host = _ipv6_host(host, self.scheme)
 
         # Use explicit default port for comparison when none is given
         if self.port and not port:
@@ -820,8 +819,8 @@ class HTTPSConnectionPool(HTTPConnectionPool):
         Return a fresh :class:`httplib.HTTPSConnection`.
         """
         self.num_connections += 1
-        log.debug("Starting new HTTPS connection (%d): %s",
-                  self.num_connections, self.host)
+        log.debug("Starting new HTTPS connection (%d): %s:%s",
+                  self.num_connections, self.host, self.port or "443")
 
         if not self.ConnectionCls or self.ConnectionCls is DummyConnection:
             raise SSLError("Can't connect to HTTPS URL because the SSL "
@@ -886,7 +885,7 @@ def connection_from_url(url, **kw):
         return HTTPConnectionPool(host, port=port, **kw)
 
 
-def _ipv6_host(host):
+def _ipv6_host(host, scheme):
     """
     Process IPv6 address literals
     """
@@ -902,4 +901,6 @@ def _ipv6_host(host):
     # percent sign might be URIencoded, convert it back into ASCII
     if host.startswith('[') and host.endswith(']'):
         host = host.replace('%25', '%').strip('[]')
+    if scheme in NORMALIZABLE_SCHEMES:
+        host = host.lower()
     return host
diff --git a/pipenv/patched/notpip/_vendor/urllib3/contrib/_securetransport/low_level.py b/pipenv/patched/notpip/_vendor/urllib3/contrib/_securetransport/low_level.py
index 5e3494bc..b13cd9e7 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/contrib/_securetransport/low_level.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/contrib/_securetransport/low_level.py
@@ -111,6 +111,9 @@ def _cert_array_from_pem(pem_bundle):
     Given a bundle of certs in PEM format, turns them into a CFArray of certs
     that can be used to validate a cert chain.
     """
+    # Normalize the PEM bundle's line endings.
+    pem_bundle = pem_bundle.replace(b"\r\n", b"\n")
+
     der_certs = [
         base64.b64decode(match.group(1))
         for match in _PEM_CERTS_RE.finditer(pem_bundle)
@@ -183,8 +186,8 @@ def _temporary_keychain():
     # some random bytes to password-protect the keychain we're creating, so we
     # ask for 40 random bytes.
     random_bytes = os.urandom(40)
-    filename = base64.b64encode(random_bytes[:8]).decode('utf-8')
-    password = base64.b64encode(random_bytes[8:])  # Must be valid UTF-8
+    filename = base64.b16encode(random_bytes[:8]).decode('utf-8')
+    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8
     tempdirectory = tempfile.mkdtemp()
 
     keychain_path = os.path.join(tempdirectory, filename).encode('utf-8')
diff --git a/pipenv/patched/notpip/_vendor/urllib3/contrib/appengine.py b/pipenv/patched/notpip/_vendor/urllib3/contrib/appengine.py
index 504d5bf0..06586352 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/contrib/appengine.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/contrib/appengine.py
@@ -236,12 +236,21 @@ class AppEngineManager(RequestMethods):
             encodings.remove('chunked')
             urlfetch_resp.headers['transfer-encoding'] = ','.join(encodings)
 
-        return HTTPResponse(
+        original_response = HTTPResponse(
             # In order for decoding to work, we must present the content as
             # a file-like object.
+            body=BytesIO(urlfetch_resp.content),
+            msg=urlfetch_resp.header_msg,
+            headers=urlfetch_resp.headers,
+            status=urlfetch_resp.status_code,
+            **response_kw
+        )
+
+        return HTTPResponse(
             body=BytesIO(urlfetch_resp.content),
             headers=urlfetch_resp.headers,
             status=urlfetch_resp.status_code,
+            original_response=original_response,
             **response_kw
         )
 
diff --git a/pipenv/patched/notpip/_vendor/urllib3/contrib/pyopenssl.py b/pipenv/patched/notpip/_vendor/urllib3/contrib/pyopenssl.py
index 62bd3e13..7787d4e4 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/contrib/pyopenssl.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/contrib/pyopenssl.py
@@ -47,6 +47,12 @@ import OpenSSL.SSL
 from cryptography import x509
 from cryptography.hazmat.backends.openssl import backend as openssl_backend
 from cryptography.hazmat.backends.openssl.x509 import _Certificate
+try:
+    from cryptography.x509 import UnsupportedExtension
+except ImportError:
+    # UnsupportedExtension is gone in cryptography >= 2.1.0
+    class UnsupportedExtension(Exception):
+        pass
 
 from socket import timeout, error as SocketError
 from io import BytesIO
@@ -199,7 +205,7 @@ def get_subj_alt_name(peer_cert):
     except x509.ExtensionNotFound:
         # No such extension, return the empty list.
         return []
-    except (x509.DuplicateExtension, x509.UnsupportedExtension,
+    except (x509.DuplicateExtension, UnsupportedExtension,
             x509.UnsupportedGeneralNameType, UnicodeError) as e:
         # A problem has been found with the quality of the certificate. Assume
         # no SAN field is present.
@@ -267,8 +273,7 @@ class WrappedSocket(object):
             else:
                 raise
         except OpenSSL.SSL.WantReadError:
-            rd = util.wait_for_read(self.socket, self.socket.gettimeout())
-            if not rd:
+            if not util.wait_for_read(self.socket, self.socket.gettimeout()):
                 raise timeout('The read operation timed out')
             else:
                 return self.recv(*args, **kwargs)
@@ -289,8 +294,7 @@ class WrappedSocket(object):
             else:
                 raise
         except OpenSSL.SSL.WantReadError:
-            rd = util.wait_for_read(self.socket, self.socket.gettimeout())
-            if not rd:
+            if not util.wait_for_read(self.socket, self.socket.gettimeout()):
                 raise timeout('The read operation timed out')
             else:
                 return self.recv_into(*args, **kwargs)
@@ -303,8 +307,7 @@ class WrappedSocket(object):
             try:
                 return self.connection.send(data)
             except OpenSSL.SSL.WantWriteError:
-                wr = util.wait_for_write(self.socket, self.socket.gettimeout())
-                if not wr:
+                if not util.wait_for_write(self.socket, self.socket.gettimeout()):
                     raise timeout()
                 continue
             except OpenSSL.SSL.SysCallError as e:
@@ -418,7 +421,7 @@ class PyOpenSSLContext(object):
             self._ctx.load_verify_locations(BytesIO(cadata))
 
     def load_cert_chain(self, certfile, keyfile=None, password=None):
-        self._ctx.use_certificate_file(certfile)
+        self._ctx.use_certificate_chain_file(certfile)
         if password is not None:
             self._ctx.set_passwd_cb(lambda max_length, prompt_twice, userdata: password)
         self._ctx.use_privatekey_file(keyfile or certfile)
@@ -440,8 +443,7 @@ class PyOpenSSLContext(object):
             try:
                 cnx.do_handshake()
             except OpenSSL.SSL.WantReadError:
-                rd = util.wait_for_read(sock, sock.gettimeout())
-                if not rd:
+                if not util.wait_for_read(sock, sock.gettimeout()):
                     raise timeout('select timed out')
                 continue
             except OpenSSL.SSL.Error as e:
diff --git a/pipenv/patched/notpip/_vendor/urllib3/contrib/securetransport.py b/pipenv/patched/notpip/_vendor/urllib3/contrib/securetransport.py
index 2cac70f7..77cb59ed 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/contrib/securetransport.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/contrib/securetransport.py
@@ -51,11 +51,6 @@ except ImportError:  # Platform-specific: Python 3
     _fileobject = None
     from ..packages.backports.makefile import backport_makefile
 
-try:
-    memoryview(b'')
-except NameError:
-    raise ImportError("SecureTransport only works on Pythons with memoryview")
-
 __all__ = ['inject_into_urllib3', 'extract_from_urllib3']
 
 # SNI always works
@@ -88,7 +83,7 @@ _connection_ref_lock = threading.Lock()
 SSL_WRITE_BLOCKSIZE = 16384
 
 # This is our equivalent of util.ssl_.DEFAULT_CIPHERS, but expanded out to
-# individual cipher suites. We need to do this becuase this is how
+# individual cipher suites. We need to do this because this is how
 # SecureTransport wants them.
 CIPHER_SUITES = [
     SecurityConst.TLS_AES_256_GCM_SHA384,
@@ -195,21 +190,18 @@ def _read_callback(connection_id, data_buffer, data_length_pointer):
         timeout = wrapped_socket.gettimeout()
         error = None
         read_count = 0
-        buffer = (ctypes.c_char * requested_length).from_address(data_buffer)
-        buffer_view = memoryview(buffer)
 
         try:
             while read_count < requested_length:
                 if timeout is None or timeout >= 0:
-                    readables = util.wait_for_read([base_socket], timeout)
-                    if not readables:
+                    if not util.wait_for_read(base_socket, timeout):
                         raise socket.error(errno.EAGAIN, 'timed out')
 
-                # We need to tell ctypes that we have a buffer that can be
-                # written to. Upsettingly, we do that like this:
-                chunk_size = base_socket.recv_into(
-                    buffer_view[read_count:requested_length]
+                remaining = requested_length - read_count
+                buffer = (ctypes.c_char * remaining).from_address(
+                    data_buffer + read_count
                 )
+                chunk_size = base_socket.recv_into(buffer, remaining)
                 read_count += chunk_size
                 if not chunk_size:
                     if not read_count:
@@ -219,7 +211,8 @@ def _read_callback(connection_id, data_buffer, data_length_pointer):
             error = e.errno
 
             if error is not None and error != errno.EAGAIN:
-                if error == errno.ECONNRESET:
+                data_length_pointer[0] = read_count
+                if error == errno.ECONNRESET or error == errno.EPIPE:
                     return SecurityConst.errSSLClosedAbort
                 raise
 
@@ -257,8 +250,7 @@ def _write_callback(connection_id, data_buffer, data_length_pointer):
         try:
             while sent < bytes_to_write:
                 if timeout is None or timeout >= 0:
-                    writables = util.wait_for_write([base_socket], timeout)
-                    if not writables:
+                    if not util.wait_for_write(base_socket, timeout):
                         raise socket.error(errno.EAGAIN, 'timed out')
                 chunk_sent = base_socket.send(data)
                 sent += chunk_sent
@@ -270,11 +262,13 @@ def _write_callback(connection_id, data_buffer, data_length_pointer):
             error = e.errno
 
             if error is not None and error != errno.EAGAIN:
-                if error == errno.ECONNRESET:
+                data_length_pointer[0] = sent
+                if error == errno.ECONNRESET or error == errno.EPIPE:
                     return SecurityConst.errSSLClosedAbort
                 raise
 
         data_length_pointer[0] = sent
+
         if sent != bytes_to_write:
             return SecurityConst.errSSLWouldBlock
 
@@ -399,7 +393,7 @@ class WrappedSocket(object):
             if trust:
                 CoreFoundation.CFRelease(trust)
 
-            if cert_array is None:
+            if cert_array is not None:
                 CoreFoundation.CFRelease(cert_array)
 
         # Ok, now we can look at what the result was.
diff --git a/pipenv/patched/notpip/_vendor/urllib3/contrib/socks.py b/pipenv/patched/notpip/_vendor/urllib3/contrib/socks.py
index 39e92fde..811e312e 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/contrib/socks.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/contrib/socks.py
@@ -152,6 +152,10 @@ class SOCKSProxyManager(PoolManager):
                  num_pools=10, headers=None, **connection_pool_kw):
         parsed = parse_url(proxy_url)
 
+        if username is None and password is None and parsed.auth is not None:
+            split = parsed.auth.split(':')
+            if len(split) == 2:
+                username, password = split
         if parsed.scheme == 'socks5':
             socks_version = socks.PROXY_TYPE_SOCKS5
             rdns = False
diff --git a/pipenv/patched/notpip/_vendor/urllib3/exceptions.py b/pipenv/patched/notpip/_vendor/urllib3/exceptions.py
index 6c4be581..7bbaa987 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/exceptions.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/exceptions.py
@@ -154,7 +154,7 @@ class ResponseError(HTTPError):
 
 
 class SecurityWarning(HTTPWarning):
-    "Warned when perfoming security reducing actions"
+    "Warned when performing security reducing actions"
     pass
 
 
diff --git a/pipenv/patched/notpip/_vendor/urllib3/fields.py b/pipenv/patched/notpip/_vendor/urllib3/fields.py
index 19b0ae0c..37fe64a3 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/fields.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/fields.py
@@ -121,7 +121,7 @@ class RequestField(object):
         'Content-Disposition' fields.
 
         :param header_parts:
-            A sequence of (k, v) typles or a :class:`dict` of (k, v) to format
+            A sequence of (k, v) tuples or a :class:`dict` of (k, v) to format
             as `k1="v1"; k2="v2"; ...`.
         """
         parts = []
diff --git a/pipenv/patched/notpip/_vendor/urllib3/filepost.py b/pipenv/patched/notpip/_vendor/urllib3/filepost.py
index cd11cee4..78f1e19b 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/filepost.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/filepost.py
@@ -1,7 +1,8 @@
 from __future__ import absolute_import
+import binascii
 import codecs
+import os
 
-from uuid import uuid4
 from io import BytesIO
 
 from .packages import six
@@ -15,7 +16,10 @@ def choose_boundary():
     """
     Our embarrassingly-simple replacement for mimetools.choose_boundary.
     """
-    return uuid4().hex
+    boundary = binascii.hexlify(os.urandom(16))
+    if six.PY3:
+        boundary = boundary.decode('ascii')
+    return boundary
 
 
 def iter_field_objects(fields):
@@ -65,7 +69,7 @@ def encode_multipart_formdata(fields, boundary=None):
 
     :param boundary:
         If not specified, then a random boundary will be generated using
-        :func:`mimetools.choose_boundary`.
+        :func:`urllib3.filepost.choose_boundary`.
     """
     body = BytesIO()
     if boundary is None:
diff --git a/pipenv/patched/notpip/_vendor/urllib3/poolmanager.py b/pipenv/patched/notpip/_vendor/urllib3/poolmanager.py
index 4ae91744..506a3c9b 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/poolmanager.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/poolmanager.py
@@ -312,8 +312,9 @@ class PoolManager(RequestMethods):
 
         kw['assert_same_host'] = False
         kw['redirect'] = False
+
         if 'headers' not in kw:
-            kw['headers'] = self.headers
+            kw['headers'] = self.headers.copy()
 
         if self.proxy is not None and u.scheme == "http":
             response = conn.urlopen(method, url, **kw)
@@ -335,6 +336,14 @@ class PoolManager(RequestMethods):
         if not isinstance(retries, Retry):
             retries = Retry.from_int(retries, redirect=redirect)
 
+        # Strip headers marked as unsafe to forward to the redirected location.
+        # Check remove_headers_on_redirect to avoid a potential network call within
+        # conn.is_same_host() which may use socket.gethostbyname() in the future.
+        if (retries.remove_headers_on_redirect
+                and not conn.is_same_host(redirect_location)):
+            for header in retries.remove_headers_on_redirect:
+                kw['headers'].pop(header, None)
+
         try:
             retries = retries.increment(method, url, response=response, _pool=conn)
         except MaxRetryError:
@@ -358,7 +367,7 @@ class ProxyManager(PoolManager):
         The URL of the proxy to be used.
 
     :param proxy_headers:
-        A dictionary contaning headers that will be sent to the proxy. In case
+        A dictionary containing headers that will be sent to the proxy. In case
         of HTTP they are being sent with each request, while in the
         HTTPS/CONNECT case they are sent only once. Could be used for proxy
         authentication.
diff --git a/pipenv/patched/notpip/_vendor/urllib3/request.py b/pipenv/patched/notpip/_vendor/urllib3/request.py
index c0fddff0..1be33341 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/request.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/request.py
@@ -44,8 +44,8 @@ class RequestMethods(object):
     def urlopen(self, method, url, body=None, headers=None,
                 encode_multipart=True, multipart_boundary=None,
                 **kw):  # Abstract
-        raise NotImplemented("Classes extending RequestMethods must implement "
-                             "their own ``urlopen`` method.")
+        raise NotImplementedError("Classes extending RequestMethods must implement "
+                                  "their own ``urlopen`` method.")
 
     def request(self, method, url, fields=None, headers=None, **urlopen_kw):
         """
@@ -60,6 +60,8 @@ class RequestMethods(object):
         """
         method = method.upper()
 
+        urlopen_kw['request_url'] = url
+
         if method in self._encode_url_methods:
             return self.request_encode_url(method, url, fields=fields,
                                            headers=headers,
@@ -117,7 +119,7 @@ class RequestMethods(object):
             }
 
         When uploading a file, providing a filename (the first parameter of the
-        tuple) is optional but recommended to best mimick behavior of browsers.
+        tuple) is optional but recommended to best mimic behavior of browsers.
 
         Note that if ``headers`` are supplied, the 'Content-Type' header will
         be overwritten because it depends on the dynamic random boundary string
diff --git a/pipenv/patched/notpip/_vendor/urllib3/response.py b/pipenv/patched/notpip/_vendor/urllib3/response.py
index d3e5a1e6..9873cb94 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/response.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/response.py
@@ -52,18 +52,42 @@ class DeflateDecoder(object):
                 self._data = None
 
 
+class GzipDecoderState(object):
+
+    FIRST_MEMBER = 0
+    OTHER_MEMBERS = 1
+    SWALLOW_DATA = 2
+
+
 class GzipDecoder(object):
 
     def __init__(self):
         self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
+        self._state = GzipDecoderState.FIRST_MEMBER
 
     def __getattr__(self, name):
         return getattr(self._obj, name)
 
     def decompress(self, data):
-        if not data:
-            return data
-        return self._obj.decompress(data)
+        ret = binary_type()
+        if self._state == GzipDecoderState.SWALLOW_DATA or not data:
+            return ret
+        while True:
+            try:
+                ret += self._obj.decompress(data)
+            except zlib.error:
+                previous_state = self._state
+                # Ignore data after the first error
+                self._state = GzipDecoderState.SWALLOW_DATA
+                if previous_state == GzipDecoderState.OTHER_MEMBERS:
+                    # Allow trailing garbage acceptable in other gzip clients
+                    return ret
+                raise
+            data = self._obj.unused_data
+            if not data:
+                return ret
+            self._state = GzipDecoderState.OTHER_MEMBERS
+            self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
 
 
 def _get_decoder(mode):
@@ -89,9 +113,8 @@ class HTTPResponse(io.IOBase):
         If True, the response's body will be preloaded during construction.
 
     :param decode_content:
-        If True, attempts to decode specific content-encoding's based on headers
-        (like 'gzip' and 'deflate') will be skipped and raw data will be used
-        instead.
+        If True, will attempt to decode the body based on the
+        'content-encoding' header.
 
     :param original_response:
         When this HTTPResponse wrapper is generated from an httplib.HTTPResponse
@@ -112,8 +135,9 @@ class HTTPResponse(io.IOBase):
 
     def __init__(self, body='', headers=None, status=0, version=0, reason=None,
                  strict=0, preload_content=True, decode_content=True,
-                 original_response=None, pool=None, connection=None,
-                 retries=None, enforce_content_length=False, request_method=None):
+                 original_response=None, pool=None, connection=None, msg=None,
+                 retries=None, enforce_content_length=False,
+                 request_method=None, request_url=None):
 
         if isinstance(headers, HTTPHeaderDict):
             self.headers = headers
@@ -132,6 +156,8 @@ class HTTPResponse(io.IOBase):
         self._fp = None
         self._original_response = original_response
         self._fp_bytes_read = 0
+        self.msg = msg
+        self._request_url = request_url
 
         if body and isinstance(body, (basestring, binary_type)):
             self._body = body
@@ -191,6 +217,9 @@ class HTTPResponse(io.IOBase):
     def connection(self):
         return self._connection
 
+    def isclosed(self):
+        return is_fp_closed(self._fp)
+
     def tell(self):
         """
         Obtain the number of bytes pulled over the wire so far. May differ from
@@ -205,18 +234,18 @@ class HTTPResponse(io.IOBase):
         """
         length = self.headers.get('content-length')
 
-        if length is not None and self.chunked:
-            # This Response will fail with an IncompleteRead if it can't be
-            # received as chunked. This method falls back to attempt reading
-            # the response before raising an exception.
-            log.warning("Received response with both Content-Length and "
-                        "Transfer-Encoding set. This is expressly forbidden "
-                        "by RFC 7230 sec 3.3.2. Ignoring Content-Length and "
-                        "attempting to process response as Transfer-Encoding: "
-                        "chunked.")
-            return None
-
-        elif length is not None:
+        if length is not None:
+            if self.chunked:
+                # This Response will fail with an IncompleteRead if it can't be
+                # received as chunked. This method falls back to attempt reading
+                # the response before raising an exception.
+                log.warning("Received response with both Content-Length and "
+                            "Transfer-Encoding set. This is expressly forbidden "
+                            "by RFC 7230 sec 3.3.2. Ignoring Content-Length and "
+                            "attempting to process response as Transfer-Encoding: "
+                            "chunked.")
+                return None
+
             try:
                 # RFC 7230 section 3.3.2 specifies multiple content lengths can
                 # be sent in a single Content-Length header
@@ -573,6 +602,11 @@ class HTTPResponse(io.IOBase):
         Similar to :meth:`HTTPResponse.read`, but with an additional
         parameter: ``decode_content``.
 
+        :param amt:
+            How much of the content to read. If specified, caching is skipped
+            because it doesn't make sense to cache partial content as the full
+            response.
+
         :param decode_content:
             If True, will attempt to decode the body based on the
             'content-encoding' header.
@@ -588,12 +622,17 @@ class HTTPResponse(io.IOBase):
                 "Body should be httplib.HTTPResponse like. "
                 "It should have have an fp attribute which returns raw chunks.")
 
-        # Don't bother reading the body of a HEAD request.
-        if self._original_response and is_response_to_head(self._original_response):
-            self._original_response.close()
-            return
-
         with self._error_catcher():
+            # Don't bother reading the body of a HEAD request.
+            if self._original_response and is_response_to_head(self._original_response):
+                self._original_response.close()
+                return
+
+            # If a response is already read and closed
+            # then return immediately.
+            if self._fp.fp is None:
+                return
+
             while True:
                 self._update_chunk_length()
                 if self.chunk_left == 0:
@@ -624,3 +663,14 @@ class HTTPResponse(io.IOBase):
             # We read everything; close the "file".
             if self._original_response:
                 self._original_response.close()
+
+    def geturl(self):
+        """
+        Returns the URL that was the source of this response.
+        If the request that generated this response redirected, this method
+        will return the final redirect location.
+        """
+        if self.retries is not None and len(self.retries.history):
+            return self.retries.history[-1].redirect_location
+        else:
+            return self._request_url
diff --git a/pipenv/patched/notpip/_vendor/urllib3/util/connection.py b/pipenv/patched/notpip/_vendor/urllib3/util/connection.py
index bf699cfd..5cf488f4 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/util/connection.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/util/connection.py
@@ -1,7 +1,6 @@
 from __future__ import absolute_import
 import socket
-from .wait import wait_for_read
-from .selectors import HAS_SELECT, SelectorError
+from .wait import NoWayToWaitForSocketError, wait_for_read
 
 
 def is_connection_dropped(conn):  # Platform-specific
@@ -19,14 +18,11 @@ def is_connection_dropped(conn):  # Platform-specific
         return False
     if sock is None:  # Connection already closed (such as by httplib).
         return True
-
-    if not HAS_SELECT:
-        return False
-
     try:
-        return bool(wait_for_read(sock, timeout=0.0))
-    except SelectorError:
-        return True
+        # Returns True if readable, which here means it's been dropped
+        return wait_for_read(sock, timeout=0.0)
+    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine
+        return False
 
 
 # This function is copied from socket.py in the Python 2.7 standard
diff --git a/pipenv/patched/notpip/_vendor/urllib3/util/queue.py b/pipenv/patched/notpip/_vendor/urllib3/util/queue.py
new file mode 100644
index 00000000..d3d379a1
--- /dev/null
+++ b/pipenv/patched/notpip/_vendor/urllib3/util/queue.py
@@ -0,0 +1,21 @@
+import collections
+from ..packages import six
+from ..packages.six.moves import queue
+
+if six.PY2:
+    # Queue is imported for side effects on MS Windows. See issue #229.
+    import Queue as _unused_module_Queue  # noqa: F401
+
+
+class LifoQueue(queue.Queue):
+    def _init(self, _):
+        self.queue = collections.deque()
+
+    def _qsize(self, len=len):
+        return len(self.queue)
+
+    def _put(self, item):
+        self.queue.append(item)
+
+    def _get(self):
+        return self.queue.pop()
diff --git a/pipenv/patched/notpip/_vendor/urllib3/util/retry.py b/pipenv/patched/notpip/_vendor/urllib3/util/retry.py
index c603cb49..7ad3dc66 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/util/retry.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/util/retry.py
@@ -19,6 +19,7 @@ from ..packages import six
 
 log = logging.getLogger(__name__)
 
+
 # Data structure for representing the metadata of requests that result in a retry.
 RequestHistory = namedtuple('RequestHistory', ["method", "url", "error",
                                                "status", "redirect_location"])
@@ -139,6 +140,10 @@ class Retry(object):
         Whether to respect Retry-After header on status codes defined as
         :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.
 
+    :param iterable remove_headers_on_redirect:
+        Sequence of headers to remove from the request when a response
+        indicating a redirect is returned before firing off the redirected
+        request.
     """
 
     DEFAULT_METHOD_WHITELIST = frozenset([
@@ -146,13 +151,16 @@ class Retry(object):
 
     RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])
 
+    DEFAULT_REDIRECT_HEADERS_BLACKLIST = frozenset(['Authorization'])
+
     #: Maximum backoff time.
     BACKOFF_MAX = 120
 
     def __init__(self, total=10, connect=None, read=None, redirect=None, status=None,
                  method_whitelist=DEFAULT_METHOD_WHITELIST, status_forcelist=None,
                  backoff_factor=0, raise_on_redirect=True, raise_on_status=True,
-                 history=None, respect_retry_after_header=True):
+                 history=None, respect_retry_after_header=True,
+                 remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST):
 
         self.total = total
         self.connect = connect
@@ -171,6 +179,7 @@ class Retry(object):
         self.raise_on_status = raise_on_status
         self.history = history or tuple()
         self.respect_retry_after_header = respect_retry_after_header
+        self.remove_headers_on_redirect = remove_headers_on_redirect
 
     def new(self, **kw):
         params = dict(
@@ -182,6 +191,7 @@ class Retry(object):
             raise_on_redirect=self.raise_on_redirect,
             raise_on_status=self.raise_on_status,
             history=self.history,
+            remove_headers_on_redirect=self.remove_headers_on_redirect
         )
         params.update(kw)
         return type(self)(**params)
diff --git a/pipenv/patched/notpip/_vendor/urllib3/util/selectors.py b/pipenv/patched/notpip/_vendor/urllib3/util/selectors.py
deleted file mode 100644
index d75cb266..00000000
--- a/pipenv/patched/notpip/_vendor/urllib3/util/selectors.py
+++ /dev/null
@@ -1,581 +0,0 @@
-# Backport of selectors.py from Python 3.5+ to support Python < 3.4
-# Also has the behavior specified in PEP 475 which is to retry syscalls
-# in the case of an EINTR error. This module is required because selectors34
-# does not follow this behavior and instead returns that no dile descriptor
-# events have occurred rather than retry the syscall. The decision to drop
-# support for select.devpoll is made to maintain 100% test coverage.
-
-import errno
-import math
-import select
-import socket
-import sys
-import time
-from collections import namedtuple, Mapping
-
-try:
-    monotonic = time.monotonic
-except (AttributeError, ImportError):  # Python 3.3<
-    monotonic = time.time
-
-EVENT_READ = (1 << 0)
-EVENT_WRITE = (1 << 1)
-
-HAS_SELECT = True  # Variable that shows whether the platform has a selector.
-_SYSCALL_SENTINEL = object()  # Sentinel in case a system call returns None.
-_DEFAULT_SELECTOR = None
-
-
-class SelectorError(Exception):
-    def __init__(self, errcode):
-        super(SelectorError, self).__init__()
-        self.errno = errcode
-
-    def __repr__(self):
-        return "<SelectorError errno={0}>".format(self.errno)
-
-    def __str__(self):
-        return self.__repr__()
-
-
-def _fileobj_to_fd(fileobj):
-    """ Return a file descriptor from a file object. If
-    given an integer will simply return that integer back. """
-    if isinstance(fileobj, int):
-        fd = fileobj
-    else:
-        try:
-            fd = int(fileobj.fileno())
-        except (AttributeError, TypeError, ValueError):
-            raise ValueError("Invalid file object: {0!r}".format(fileobj))
-    if fd < 0:
-        raise ValueError("Invalid file descriptor: {0}".format(fd))
-    return fd
-
-
-# Determine which function to use to wrap system calls because Python 3.5+
-# already handles the case when system calls are interrupted.
-if sys.version_info >= (3, 5):
-    def _syscall_wrapper(func, _, *args, **kwargs):
-        """ This is the short-circuit version of the below logic
-        because in Python 3.5+ all system calls automatically restart
-        and recalculate their timeouts. """
-        try:
-            return func(*args, **kwargs)
-        except (OSError, IOError, select.error) as e:
-            errcode = None
-            if hasattr(e, "errno"):
-                errcode = e.errno
-            raise SelectorError(errcode)
-else:
-    def _syscall_wrapper(func, recalc_timeout, *args, **kwargs):
-        """ Wrapper function for syscalls that could fail due to EINTR.
-        All functions should be retried if there is time left in the timeout
-        in accordance with PEP 475. """
-        timeout = kwargs.get("timeout", None)
-        if timeout is None:
-            expires = None
-            recalc_timeout = False
-        else:
-            timeout = float(timeout)
-            if timeout < 0.0:  # Timeout less than 0 treated as no timeout.
-                expires = None
-            else:
-                expires = monotonic() + timeout
-
-        args = list(args)
-        if recalc_timeout and "timeout" not in kwargs:
-            raise ValueError(
-                "Timeout must be in args or kwargs to be recalculated")
-
-        result = _SYSCALL_SENTINEL
-        while result is _SYSCALL_SENTINEL:
-            try:
-                result = func(*args, **kwargs)
-            # OSError is thrown by select.select
-            # IOError is thrown by select.epoll.poll
-            # select.error is thrown by select.poll.poll
-            # Aren't we thankful for Python 3.x rework for exceptions?
-            except (OSError, IOError, select.error) as e:
-                # select.error wasn't a subclass of OSError in the past.
-                errcode = None
-                if hasattr(e, "errno"):
-                    errcode = e.errno
-                elif hasattr(e, "args"):
-                    errcode = e.args[0]
-
-                # Also test for the Windows equivalent of EINTR.
-                is_interrupt = (errcode == errno.EINTR or (hasattr(errno, "WSAEINTR") and
-                                                           errcode == errno.WSAEINTR))
-
-                if is_interrupt:
-                    if expires is not None:
-                        current_time = monotonic()
-                        if current_time > expires:
-                            raise OSError(errno=errno.ETIMEDOUT)
-                        if recalc_timeout:
-                            if "timeout" in kwargs:
-                                kwargs["timeout"] = expires - current_time
-                    continue
-                if errcode:
-                    raise SelectorError(errcode)
-                else:
-                    raise
-        return result
-
-
-SelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])
-
-
-class _SelectorMapping(Mapping):
-    """ Mapping of file objects to selector keys """
-
-    def __init__(self, selector):
-        self._selector = selector
-
-    def __len__(self):
-        return len(self._selector._fd_to_key)
-
-    def __getitem__(self, fileobj):
-        try:
-            fd = self._selector._fileobj_lookup(fileobj)
-            return self._selector._fd_to_key[fd]
-        except KeyError:
-            raise KeyError("{0!r} is not registered.".format(fileobj))
-
-    def __iter__(self):
-        return iter(self._selector._fd_to_key)
-
-
-class BaseSelector(object):
-    """ Abstract Selector class
-
-    A selector supports registering file objects to be monitored
-    for specific I/O events.
-
-    A file object is a file descriptor or any object with a
-    `fileno()` method. An arbitrary object can be attached to the
-    file object which can be used for example to store context info,
-    a callback, etc.
-
-    A selector can use various implementations (select(), poll(), epoll(),
-    and kqueue()) depending on the platform. The 'DefaultSelector' class uses
-    the most efficient implementation for the current platform.
-    """
-    def __init__(self):
-        # Maps file descriptors to keys.
-        self._fd_to_key = {}
-
-        # Read-only mapping returned by get_map()
-        self._map = _SelectorMapping(self)
-
-    def _fileobj_lookup(self, fileobj):
-        """ Return a file descriptor from a file object.
-        This wraps _fileobj_to_fd() to do an exhaustive
-        search in case the object is invalid but we still
-        have it in our map. Used by unregister() so we can
-        unregister an object that was previously registered
-        even if it is closed. It is also used by _SelectorMapping
-        """
-        try:
-            return _fileobj_to_fd(fileobj)
-        except ValueError:
-
-            # Search through all our mapped keys.
-            for key in self._fd_to_key.values():
-                if key.fileobj is fileobj:
-                    return key.fd
-
-            # Raise ValueError after all.
-            raise
-
-    def register(self, fileobj, events, data=None):
-        """ Register a file object for a set of events to monitor. """
-        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):
-            raise ValueError("Invalid events: {0!r}".format(events))
-
-        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)
-
-        if key.fd in self._fd_to_key:
-            raise KeyError("{0!r} (FD {1}) is already registered"
-                           .format(fileobj, key.fd))
-
-        self._fd_to_key[key.fd] = key
-        return key
-
-    def unregister(self, fileobj):
-        """ Unregister a file object from being monitored. """
-        try:
-            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))
-        except KeyError:
-            raise KeyError("{0!r} is not registered".format(fileobj))
-
-        # Getting the fileno of a closed socket on Windows errors with EBADF.
-        except socket.error as e:  # Platform-specific: Windows.
-            if e.errno != errno.EBADF:
-                raise
-            else:
-                for key in self._fd_to_key.values():
-                    if key.fileobj is fileobj:
-                        self._fd_to_key.pop(key.fd)
-                        break
-                else:
-                    raise KeyError("{0!r} is not registered".format(fileobj))
-        return key
-
-    def modify(self, fileobj, events, data=None):
-        """ Change a registered file object monitored events and data. """
-        # NOTE: Some subclasses optimize this operation even further.
-        try:
-            key = self._fd_to_key[self._fileobj_lookup(fileobj)]
-        except KeyError:
-            raise KeyError("{0!r} is not registered".format(fileobj))
-
-        if events != key.events:
-            self.unregister(fileobj)
-            key = self.register(fileobj, events, data)
-
-        elif data != key.data:
-            # Use a shortcut to update the data.
-            key = key._replace(data=data)
-            self._fd_to_key[key.fd] = key
-
-        return key
-
-    def select(self, timeout=None):
-        """ Perform the actual selection until some monitored file objects
-        are ready or the timeout expires. """
-        raise NotImplementedError()
-
-    def close(self):
-        """ Close the selector. This must be called to ensure that all
-        underlying resources are freed. """
-        self._fd_to_key.clear()
-        self._map = None
-
-    def get_key(self, fileobj):
-        """ Return the key associated with a registered file object. """
-        mapping = self.get_map()
-        if mapping is None:
-            raise RuntimeError("Selector is closed")
-        try:
-            return mapping[fileobj]
-        except KeyError:
-            raise KeyError("{0!r} is not registered".format(fileobj))
-
-    def get_map(self):
-        """ Return a mapping of file objects to selector keys """
-        return self._map
-
-    def _key_from_fd(self, fd):
-        """ Return the key associated to a given file descriptor
-         Return None if it is not found. """
-        try:
-            return self._fd_to_key[fd]
-        except KeyError:
-            return None
-
-    def __enter__(self):
-        return self
-
-    def __exit__(self, *args):
-        self.close()
-
-
-# Almost all platforms have select.select()
-if hasattr(select, "select"):
-    class SelectSelector(BaseSelector):
-        """ Select-based selector. """
-        def __init__(self):
-            super(SelectSelector, self).__init__()
-            self._readers = set()
-            self._writers = set()
-
-        def register(self, fileobj, events, data=None):
-            key = super(SelectSelector, self).register(fileobj, events, data)
-            if events & EVENT_READ:
-                self._readers.add(key.fd)
-            if events & EVENT_WRITE:
-                self._writers.add(key.fd)
-            return key
-
-        def unregister(self, fileobj):
-            key = super(SelectSelector, self).unregister(fileobj)
-            self._readers.discard(key.fd)
-            self._writers.discard(key.fd)
-            return key
-
-        def _select(self, r, w, timeout=None):
-            """ Wrapper for select.select because timeout is a positional arg """
-            return select.select(r, w, [], timeout)
-
-        def select(self, timeout=None):
-            # Selecting on empty lists on Windows errors out.
-            if not len(self._readers) and not len(self._writers):
-                return []
-
-            timeout = None if timeout is None else max(timeout, 0.0)
-            ready = []
-            r, w, _ = _syscall_wrapper(self._select, True, self._readers,
-                                       self._writers, timeout)
-            r = set(r)
-            w = set(w)
-            for fd in r | w:
-                events = 0
-                if fd in r:
-                    events |= EVENT_READ
-                if fd in w:
-                    events |= EVENT_WRITE
-
-                key = self._key_from_fd(fd)
-                if key:
-                    ready.append((key, events & key.events))
-            return ready
-
-
-if hasattr(select, "poll"):
-    class PollSelector(BaseSelector):
-        """ Poll-based selector """
-        def __init__(self):
-            super(PollSelector, self).__init__()
-            self._poll = select.poll()
-
-        def register(self, fileobj, events, data=None):
-            key = super(PollSelector, self).register(fileobj, events, data)
-            event_mask = 0
-            if events & EVENT_READ:
-                event_mask |= select.POLLIN
-            if events & EVENT_WRITE:
-                event_mask |= select.POLLOUT
-            self._poll.register(key.fd, event_mask)
-            return key
-
-        def unregister(self, fileobj):
-            key = super(PollSelector, self).unregister(fileobj)
-            self._poll.unregister(key.fd)
-            return key
-
-        def _wrap_poll(self, timeout=None):
-            """ Wrapper function for select.poll.poll() so that
-            _syscall_wrapper can work with only seconds. """
-            if timeout is not None:
-                if timeout <= 0:
-                    timeout = 0
-                else:
-                    # select.poll.poll() has a resolution of 1 millisecond,
-                    # round away from zero to wait *at least* timeout seconds.
-                    timeout = math.ceil(timeout * 1e3)
-
-            result = self._poll.poll(timeout)
-            return result
-
-        def select(self, timeout=None):
-            ready = []
-            fd_events = _syscall_wrapper(self._wrap_poll, True, timeout=timeout)
-            for fd, event_mask in fd_events:
-                events = 0
-                if event_mask & ~select.POLLIN:
-                    events |= EVENT_WRITE
-                if event_mask & ~select.POLLOUT:
-                    events |= EVENT_READ
-
-                key = self._key_from_fd(fd)
-                if key:
-                    ready.append((key, events & key.events))
-
-            return ready
-
-
-if hasattr(select, "epoll"):
-    class EpollSelector(BaseSelector):
-        """ Epoll-based selector """
-        def __init__(self):
-            super(EpollSelector, self).__init__()
-            self._epoll = select.epoll()
-
-        def fileno(self):
-            return self._epoll.fileno()
-
-        def register(self, fileobj, events, data=None):
-            key = super(EpollSelector, self).register(fileobj, events, data)
-            events_mask = 0
-            if events & EVENT_READ:
-                events_mask |= select.EPOLLIN
-            if events & EVENT_WRITE:
-                events_mask |= select.EPOLLOUT
-            _syscall_wrapper(self._epoll.register, False, key.fd, events_mask)
-            return key
-
-        def unregister(self, fileobj):
-            key = super(EpollSelector, self).unregister(fileobj)
-            try:
-                _syscall_wrapper(self._epoll.unregister, False, key.fd)
-            except SelectorError:
-                # This can occur when the fd was closed since registry.
-                pass
-            return key
-
-        def select(self, timeout=None):
-            if timeout is not None:
-                if timeout <= 0:
-                    timeout = 0.0
-                else:
-                    # select.epoll.poll() has a resolution of 1 millisecond
-                    # but luckily takes seconds so we don't need a wrapper
-                    # like PollSelector. Just for better rounding.
-                    timeout = math.ceil(timeout * 1e3) * 1e-3
-                timeout = float(timeout)
-            else:
-                timeout = -1.0  # epoll.poll() must have a float.
-
-            # We always want at least 1 to ensure that select can be called
-            # with no file descriptors registered. Otherwise will fail.
-            max_events = max(len(self._fd_to_key), 1)
-
-            ready = []
-            fd_events = _syscall_wrapper(self._epoll.poll, True,
-                                         timeout=timeout,
-                                         maxevents=max_events)
-            for fd, event_mask in fd_events:
-                events = 0
-                if event_mask & ~select.EPOLLIN:
-                    events |= EVENT_WRITE
-                if event_mask & ~select.EPOLLOUT:
-                    events |= EVENT_READ
-
-                key = self._key_from_fd(fd)
-                if key:
-                    ready.append((key, events & key.events))
-            return ready
-
-        def close(self):
-            self._epoll.close()
-            super(EpollSelector, self).close()
-
-
-if hasattr(select, "kqueue"):
-    class KqueueSelector(BaseSelector):
-        """ Kqueue / Kevent-based selector """
-        def __init__(self):
-            super(KqueueSelector, self).__init__()
-            self._kqueue = select.kqueue()
-
-        def fileno(self):
-            return self._kqueue.fileno()
-
-        def register(self, fileobj, events, data=None):
-            key = super(KqueueSelector, self).register(fileobj, events, data)
-            if events & EVENT_READ:
-                kevent = select.kevent(key.fd,
-                                       select.KQ_FILTER_READ,
-                                       select.KQ_EV_ADD)
-
-                _syscall_wrapper(self._kqueue.control, False, [kevent], 0, 0)
-
-            if events & EVENT_WRITE:
-                kevent = select.kevent(key.fd,
-                                       select.KQ_FILTER_WRITE,
-                                       select.KQ_EV_ADD)
-
-                _syscall_wrapper(self._kqueue.control, False, [kevent], 0, 0)
-
-            return key
-
-        def unregister(self, fileobj):
-            key = super(KqueueSelector, self).unregister(fileobj)
-            if key.events & EVENT_READ:
-                kevent = select.kevent(key.fd,
-                                       select.KQ_FILTER_READ,
-                                       select.KQ_EV_DELETE)
-                try:
-                    _syscall_wrapper(self._kqueue.control, False, [kevent], 0, 0)
-                except SelectorError:
-                    pass
-            if key.events & EVENT_WRITE:
-                kevent = select.kevent(key.fd,
-                                       select.KQ_FILTER_WRITE,
-                                       select.KQ_EV_DELETE)
-                try:
-                    _syscall_wrapper(self._kqueue.control, False, [kevent], 0, 0)
-                except SelectorError:
-                    pass
-
-            return key
-
-        def select(self, timeout=None):
-            if timeout is not None:
-                timeout = max(timeout, 0)
-
-            max_events = len(self._fd_to_key) * 2
-            ready_fds = {}
-
-            kevent_list = _syscall_wrapper(self._kqueue.control, True,
-                                           None, max_events, timeout)
-
-            for kevent in kevent_list:
-                fd = kevent.ident
-                event_mask = kevent.filter
-                events = 0
-                if event_mask == select.KQ_FILTER_READ:
-                    events |= EVENT_READ
-                if event_mask == select.KQ_FILTER_WRITE:
-                    events |= EVENT_WRITE
-
-                key = self._key_from_fd(fd)
-                if key:
-                    if key.fd not in ready_fds:
-                        ready_fds[key.fd] = (key, events & key.events)
-                    else:
-                        old_events = ready_fds[key.fd][1]
-                        ready_fds[key.fd] = (key, (events | old_events) & key.events)
-
-            return list(ready_fds.values())
-
-        def close(self):
-            self._kqueue.close()
-            super(KqueueSelector, self).close()
-
-
-if not hasattr(select, 'select'):  # Platform-specific: AppEngine
-    HAS_SELECT = False
-
-
-def _can_allocate(struct):
-    """ Checks that select structs can be allocated by the underlying
-    operating system, not just advertised by the select module. We don't
-    check select() because we'll be hopeful that most platforms that
-    don't have it available will not advertise it. (ie: GAE) """
-    try:
-        # select.poll() objects won't fail until used.
-        if struct == 'poll':
-            p = select.poll()
-            p.poll(0)
-
-        # All others will fail on allocation.
-        else:
-            getattr(select, struct)().close()
-        return True
-    except (OSError, AttributeError) as e:
-        return False
-
-
-# Choose the best implementation, roughly:
-# kqueue == epoll > poll > select. Devpoll not supported. (See above)
-# select() also can't accept a FD > FD_SETSIZE (usually around 1024)
-def DefaultSelector():
-    """ This function serves as a first call for DefaultSelector to
-    detect if the select module is being monkey-patched incorrectly
-    by eventlet, greenlet, and preserve proper behavior. """
-    global _DEFAULT_SELECTOR
-    if _DEFAULT_SELECTOR is None:
-        if _can_allocate('kqueue'):
-            _DEFAULT_SELECTOR = KqueueSelector
-        elif _can_allocate('epoll'):
-            _DEFAULT_SELECTOR = EpollSelector
-        elif _can_allocate('poll'):
-            _DEFAULT_SELECTOR = PollSelector
-        elif hasattr(select, 'select'):
-            _DEFAULT_SELECTOR = SelectSelector
-        else:  # Platform-specific: AppEngine
-            raise ValueError('Platform does not have a selector')
-    return _DEFAULT_SELECTOR()
diff --git a/pipenv/patched/notpip/_vendor/urllib3/util/ssl_.py b/pipenv/patched/notpip/_vendor/urllib3/util/ssl_.py
index 0d0799b9..a0868c22 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/util/ssl_.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/util/ssl_.py
@@ -2,11 +2,13 @@ from __future__ import absolute_import
 import errno
 import warnings
 import hmac
+import socket
 
 from binascii import hexlify, unhexlify
 from hashlib import md5, sha1, sha256
 
 from ..exceptions import SSLError, InsecurePlatformWarning, SNIMissingWarning
+from ..packages import six
 
 
 SSLContext = None
@@ -53,6 +55,27 @@ except ImportError:
     OP_NO_SSLv2, OP_NO_SSLv3 = 0x1000000, 0x2000000
     OP_NO_COMPRESSION = 0x20000
 
+
+# Python 2.7 and earlier didn't have inet_pton on non-Linux
+# so we fallback on inet_aton in those cases. This means that
+# we can only detect IPv4 addresses in this case.
+if hasattr(socket, 'inet_pton'):
+    inet_pton = socket.inet_pton
+else:
+    # Maybe we can use ipaddress if the user has urllib3[secure]?
+    try:
+        from pipenv.patched.notpip._vendor import ipaddress
+
+        def inet_pton(_, host):
+            if isinstance(host, six.binary_type):
+                host = host.decode('ascii')
+            return ipaddress.ip_address(host)
+
+    except ImportError:  # Platform-specific: Non-Linux
+        def inet_pton(_, host):
+            return socket.inet_aton(host)
+
+
 # A secure default.
 # Sources for more information on TLS ciphers:
 #
@@ -183,7 +206,7 @@ def resolve_cert_reqs(candidate):
     the wrap_socket function/method from the ssl module.
     Defaults to :data:`ssl.CERT_NONE`.
     If given a string it is assumed to be the name of the constant in the
-    :mod:`ssl` module or its abbrevation.
+    :mod:`ssl` module or its abbreviation.
     (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.
     If it's neither `None` nor a string we assume it is already the numeric
     constant which can directly be passed to wrap_socket.
@@ -325,17 +348,49 @@ def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,
 
     if certfile:
         context.load_cert_chain(certfile, keyfile)
-    if HAS_SNI:  # Platform-specific: OpenSSL with enabled SNI
-        return context.wrap_socket(sock, server_hostname=server_hostname)
-
-    warnings.warn(
-        'An HTTPS request has been made, but the SNI (Subject Name '
-        'Indication) extension to TLS is not available on this platform. '
-        'This may cause the server to present an incorrect TLS '
-        'certificate, which can cause validation failures. You can upgrade to '
-        'a newer version of Python to solve this. For more information, see '
-        'https://urllib3.readthedocs.io/en/latest/advanced-usage.html'
-        '#ssl-warnings',
-        SNIMissingWarning
-    )
+
+    # If we detect server_hostname is an IP address then the SNI
+    # extension should not be used according to RFC3546 Section 3.1
+    # We shouldn't warn the user if SNI isn't available but we would
+    # not be using SNI anyways due to IP address for server_hostname.
+    if ((server_hostname is not None and not is_ipaddress(server_hostname))
+            or IS_SECURETRANSPORT):
+        if HAS_SNI and server_hostname is not None:
+            return context.wrap_socket(sock, server_hostname=server_hostname)
+
+        warnings.warn(
+            'An HTTPS request has been made, but the SNI (Server Name '
+            'Indication) extension to TLS is not available on this platform. '
+            'This may cause the server to present an incorrect TLS '
+            'certificate, which can cause validation failures. You can upgrade to '
+            'a newer version of Python to solve this. For more information, see '
+            'https://urllib3.readthedocs.io/en/latest/advanced-usage.html'
+            '#ssl-warnings',
+            SNIMissingWarning
+        )
+
     return context.wrap_socket(sock)
+
+
+def is_ipaddress(hostname):
+    """Detects whether the hostname given is an IP address.
+
+    :param str hostname: Hostname to examine.
+    :return: True if the hostname is an IP address, False otherwise.
+    """
+    if six.PY3 and isinstance(hostname, six.binary_type):
+        # IDN A-label bytes are ASCII compatible.
+        hostname = hostname.decode('ascii')
+
+    families = [socket.AF_INET]
+    if hasattr(socket, 'AF_INET6'):
+        families.append(socket.AF_INET6)
+
+    for af in families:
+        try:
+            inet_pton(af, hostname)
+        except (socket.error, ValueError, OSError):
+            pass
+        else:
+            return True
+    return False
diff --git a/pipenv/patched/notpip/_vendor/urllib3/util/wait.py b/pipenv/patched/notpip/_vendor/urllib3/util/wait.py
index cb396e50..fa686eff 100644
--- a/pipenv/patched/notpip/_vendor/urllib3/util/wait.py
+++ b/pipenv/patched/notpip/_vendor/urllib3/util/wait.py
@@ -1,40 +1,153 @@
-from .selectors import (
-    HAS_SELECT,
-    DefaultSelector,
-    EVENT_READ,
-    EVENT_WRITE
-)
-
-
-def _wait_for_io_events(socks, events, timeout=None):
-    """ Waits for IO events to be available from a list of sockets
-    or optionally a single socket if passed in. Returns a list of
-    sockets that can be interacted with immediately. """
-    if not HAS_SELECT:
-        raise ValueError('Platform does not have a selector')
-    if not isinstance(socks, list):
-        # Probably just a single socket.
-        if hasattr(socks, "fileno"):
-            socks = [socks]
-        # Otherwise it might be a non-list iterable.
+import errno
+from functools import partial
+import select
+import sys
+try:
+    from time import monotonic
+except ImportError:
+    from time import time as monotonic
+
+__all__ = ["NoWayToWaitForSocketError", "wait_for_read", "wait_for_write"]
+
+
+class NoWayToWaitForSocketError(Exception):
+    pass
+
+
+# How should we wait on sockets?
+#
+# There are two types of APIs you can use for waiting on sockets: the fancy
+# modern stateful APIs like epoll/kqueue, and the older stateless APIs like
+# select/poll. The stateful APIs are more efficient when you have a lots of
+# sockets to keep track of, because you can set them up once and then use them
+# lots of times. But we only ever want to wait on a single socket at a time
+# and don't want to keep track of state, so the stateless APIs are actually
+# more efficient. So we want to use select() or poll().
+#
+# Now, how do we choose between select() and poll()? On traditional Unixes,
+# select() has a strange calling convention that makes it slow, or fail
+# altogether, for high-numbered file descriptors. The point of poll() is to fix
+# that, so on Unixes, we prefer poll().
+#
+# On Windows, there is no poll() (or at least Python doesn't provide a wrapper
+# for it), but that's OK, because on Windows, select() doesn't have this
+# strange calling convention; plain select() works fine.
+#
+# So: on Windows we use select(), and everywhere else we use poll(). We also
+# fall back to select() in case poll() is somehow broken or missing.
+
+if sys.version_info >= (3, 5):
+    # Modern Python, that retries syscalls by default
+    def _retry_on_intr(fn, timeout):
+        return fn(timeout)
+else:
+    # Old and broken Pythons.
+    def _retry_on_intr(fn, timeout):
+        if timeout is not None and timeout <= 0:
+            return fn(timeout)
+
+        if timeout is None:
+            deadline = float("inf")
         else:
-            socks = list(socks)
-    with DefaultSelector() as selector:
-        for sock in socks:
-            selector.register(sock, events)
-        return [key[0].fileobj for key in
-                selector.select(timeout) if key[1] & events]
-
-
-def wait_for_read(socks, timeout=None):
-    """ Waits for reading to be available from a list of sockets
-    or optionally a single socket if passed in. Returns a list of
-    sockets that can be read from immediately. """
-    return _wait_for_io_events(socks, EVENT_READ, timeout)
-
-
-def wait_for_write(socks, timeout=None):
-    """ Waits for writing to be available from a list of sockets
-    or optionally a single socket if passed in. Returns a list of
-    sockets that can be written to immediately. """
-    return _wait_for_io_events(socks, EVENT_WRITE, timeout)
+            deadline = monotonic() + timeout
+
+        while True:
+            try:
+                return fn(timeout)
+            # OSError for 3 <= pyver < 3.5, select.error for pyver <= 2.7
+            except (OSError, select.error) as e:
+                # 'e.args[0]' incantation works for both OSError and select.error
+                if e.args[0] != errno.EINTR:
+                    raise
+                else:
+                    timeout = deadline - monotonic()
+                    if timeout < 0:
+                        timeout = 0
+                    if timeout == float("inf"):
+                        timeout = None
+                    continue
+
+
+def select_wait_for_socket(sock, read=False, write=False, timeout=None):
+    if not read and not write:
+        raise RuntimeError("must specify at least one of read=True, write=True")
+    rcheck = []
+    wcheck = []
+    if read:
+        rcheck.append(sock)
+    if write:
+        wcheck.append(sock)
+    # When doing a non-blocking connect, most systems signal success by
+    # marking the socket writable. Windows, though, signals success by marked
+    # it as "exceptional". We paper over the difference by checking the write
+    # sockets for both conditions. (The stdlib selectors module does the same
+    # thing.)
+    fn = partial(select.select, rcheck, wcheck, wcheck)
+    rready, wready, xready = _retry_on_intr(fn, timeout)
+    return bool(rready or wready or xready)
+
+
+def poll_wait_for_socket(sock, read=False, write=False, timeout=None):
+    if not read and not write:
+        raise RuntimeError("must specify at least one of read=True, write=True")
+    mask = 0
+    if read:
+        mask |= select.POLLIN
+    if write:
+        mask |= select.POLLOUT
+    poll_obj = select.poll()
+    poll_obj.register(sock, mask)
+
+    # For some reason, poll() takes timeout in milliseconds
+    def do_poll(t):
+        if t is not None:
+            t *= 1000
+        return poll_obj.poll(t)
+
+    return bool(_retry_on_intr(do_poll, timeout))
+
+
+def null_wait_for_socket(*args, **kwargs):
+    raise NoWayToWaitForSocketError("no select-equivalent available")
+
+
+def _have_working_poll():
+    # Apparently some systems have a select.poll that fails as soon as you try
+    # to use it, either due to strange configuration or broken monkeypatching
+    # from libraries like eventlet/greenlet.
+    try:
+        poll_obj = select.poll()
+        poll_obj.poll(0)
+    except (AttributeError, OSError):
+        return False
+    else:
+        return True
+
+
+def wait_for_socket(*args, **kwargs):
+    # We delay choosing which implementation to use until the first time we're
+    # called. We could do it at import time, but then we might make the wrong
+    # decision if someone goes wild with monkeypatching select.poll after
+    # we're imported.
+    global wait_for_socket
+    if _have_working_poll():
+        wait_for_socket = poll_wait_for_socket
+    elif hasattr(select, "select"):
+        wait_for_socket = select_wait_for_socket
+    else:  # Platform-specific: Appengine.
+        wait_for_socket = null_wait_for_socket
+    return wait_for_socket(*args, **kwargs)
+
+
+def wait_for_read(sock, timeout=None):
+    """ Waits for reading to be available on a given socket.
+    Returns True if the socket is readable, or False if the timeout expired.
+    """
+    return wait_for_socket(sock, read=True, timeout=timeout)
+
+
+def wait_for_write(sock, timeout=None):
+    """ Waits for writing to be available on a given socket.
+    Returns True if the socket is readable, or False if the timeout expired.
+    """
+    return wait_for_socket(sock, write=True, timeout=timeout)
diff --git a/pipenv/patched/notpip/_vendor/vendor.txt b/pipenv/patched/notpip/_vendor/vendor.txt
index 3994e709..b9854e9a 100644
--- a/pipenv/patched/notpip/_vendor/vendor.txt
+++ b/pipenv/patched/notpip/_vendor/vendor.txt
@@ -1,22 +1,22 @@
 appdirs==1.4.3
 distlib==0.2.7
-distro==1.2.0
+distro==1.3.0
 html5lib==1.0.1
 six==1.11.0
 colorama==0.3.9
-CacheControl==0.12.4
+CacheControl==0.12.5
 msgpack-python==0.5.6
 lockfile==0.12.2
-progress==1.3
-ipaddress==1.0.19  # Only needed on 2.6 and 2.7
+progress==1.4
+ipaddress==1.0.22  # Only needed on 2.6 and 2.7
 packaging==17.1
 pyparsing==2.2.0
-pytoml==0.1.14
+pytoml==0.1.16
 retrying==1.3.3
-requests==2.18.4
+requests==2.19.1
     chardet==3.0.4
-    idna==2.6
-    urllib3==1.22
-    certifi==2018.1.18
-setuptools==39.1.0
+    idna==2.7
+    urllib3==1.23
+    certifi==2018.4.16
+setuptools==39.2.0
 webencodings==0.5.1
